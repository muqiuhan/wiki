<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[韩暮秋的个人维基]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://muqiuhan.github.io/wiki/</link><image><url>https://muqiuhan.github.io/wiki/lib/media/favicon.png</url><title>韩暮秋的个人维基</title><link>https://muqiuhan.github.io/wiki/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Wed, 19 Feb 2025 01:21:56 GMT</lastBuildDate><atom:link href="https://muqiuhan.github.io/wiki/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Wed, 19 Feb 2025 01:21:55 GMT</pubDate><copyright><![CDATA[韩暮秋]]></copyright><ttl>60</ttl><dc:creator>韩暮秋</dc:creator><item><title><![CDATA[Repository pattern in Typescript]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typescript" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typescript</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typescript" class="tag" target="_blank" rel="noopener nofollow">#typescript</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a> <br>
The Repository design pattern is a <a data-tooltip-position="top" aria-label="https://www.geeksforgeeks.org/structural-design-patterns/" rel="noopener nofollow" class="external-link" href="https://www.geeksforgeeks.org/structural-design-patterns/" target="_blank">structural pattern</a> that abstracts data access, providing a centralized way to manage data operations. By separating the data layer from business logic, it enhances code maintainability, testability, and flexibility, making it easier to work with various data sources in an application.
<br>抽象化数据访问层的主要作用是将应用的业务逻辑与对数据库的数据访问等实现细节进行解耦。DB 框架的更改不应该影响到应用程序的核心服务，并且它们应该对业务逻辑代码透明。<br>业务服务应该只依赖于抽象，而不是实现，数据服务同理。<br><br><br>例如，此时有一个书籍存储的微服务，并且有一个“添加新书籍”的操作用例，在此用例中可以通过使用 Book 这个 Repository 来添加新书籍，而 Book 具体使用什么数据库，怎么存并不是业务需要关心的事情。<br><img alt="Pasted image 20250219090610.png" src="https://muqiuhan.github.io/wiki/computer-science/software-engineering/repository-pattern-in-typescript/attachments/pasted-image-20250219090610.png"><br><br>假设有以下实体类型定义：<br>export class Author {
  firstName: string;
  lastName: string;
}

export class Genre {
  name: string;
}

export class Book {
  title: string;
  author: Author;
  genre: Genre;
  publishDate: Date;
}
<br><br>第一步是抽象化出一个通用的 Repository ，其中包含通用的操作：<br>export abstract class IGenericRepository&lt;T&gt; {
  abstract getAll(): Promise&lt;T[]&gt;;

  abstract get(id: string): Promise&lt;T&gt;;

  abstract create(item: T): Promise&lt;T&gt;;

  abstract update(id: string, item: T);
}
<br>
<br>这里具体有那些函数可以根据需要（业务）自己定义。
<br>类型参数 T 表示每个实体。
<br>接着，定义一个数据服务，其中包含使用 IGenericRepositoiry 定义的具体实例对应的 Repository：<br>import { Author, Book, Genre } from '../entities';
import { IGenericRepository } from './generic-repository.abstract';

export abstract class IDataServices {
  abstract authors: IGenericRepository&lt;Author&gt;;

  abstract books: IGenericRepository&lt;Book&gt;;

  abstract genres: IGenericRepository&lt;Genre&gt;;
}
<br>
<br>这里有三个 Repository 分别对应不同的实体。
<br>IGenericRepository 中定义的函数是每个 Repository 公开的通用存储函数。
<br>以上这些就是对业务中的实体的 Repository 抽象，这样就隔离开了业务和存储逻辑，例如使用 MongoDB，可以实现一个 MongoGenericRepository：<br>import { Model } from 'mongoose';
import { IGenericRepository } from '../../../core';

export class MongoGenericRepository&lt;T&gt; implements IGenericRepository&lt;T&gt; {
  private _repository: Model&lt;T&gt;;
  private _populateOnFind: string[];

  constructor(repository: Model&lt;T&gt;, populateOnFind: string[] = []) {
    this._repository = repository;
    this._populateOnFind = populateOnFind;
  }

  getAll(): Promise&lt;T[]&gt; {
    return this._repository.find().populate(this._populateOnFind).exec();
  }

  get(id: any): Promise&lt;T&gt; {
    return this._repository.findById(id).populate(this._populateOnFind).exec();
  }

  create(item: T): Promise&lt;T&gt; {
    return this._repository.create(item);
  }

  update(id: string, item: T) {
    return this._repository.findByIdAndUpdate(id, item);
  }
}
<br>然后实现一个 MongoDataServices:<br>import { Injectable, OnApplicationBootstrap } from '@nestjs/common';
import { InjectModel } from '@nestjs/mongoose';
import { Model } from 'mongoose';
import { IDataServices } from '../../../core';
import { MongoGenericRepository } from './mongo-generic-repository';
import {
  Author,
  AuthorDocument,
  Book,
  BookDocument,
  Genre,
  GenreDocument,
} from './model';

@Injectable()
export class MongoDataServices
  implements IDataServices, OnApplicationBootstrap
{
  authors: MongoGenericRepository&lt;Author&gt;;
  books: MongoGenericRepository&lt;Book&gt;;
  genres: MongoGenericRepository&lt;Genre&gt;;

  constructor(
    @InjectModel(Author.name)
    private AuthorRepository: Model&lt;AuthorDocument&gt;,
    @InjectModel(Book.name)
    private BookRepository: Model&lt;BookDocument&gt;,
    @InjectModel(Genre.name)
    private GenreRepository: Model&lt;GenreDocument&gt;,
  ) {}

  onApplicationBootstrap() {
    this.authors = new MongoGenericRepository&lt;Author&gt;(this.AuthorRepository);
    this.books = new MongoGenericRepository&lt;Book&gt;(this.BookRepository, [
      'author',
      'genre',
    ]);
    this.genres = new MongoGenericRepository&lt;Genre&gt;(this.GenreRepository);
  }
}
<br>以上。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/software-engineering/repository-pattern-in-typescript/repository-pattern-in-typescript.html</link><guid isPermaLink="false">Computer Science/Software Engineering/Repository pattern in Typescript/Repository pattern in Typescript.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 19 Feb 2025 01:21:13 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/software-engineering/repository-pattern-in-typescript/attachments/pasted-image-20250219090610.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/software-engineering/repository-pattern-in-typescript/attachments/pasted-image-20250219090610.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Intro to Databases (for people who don’t know a whole lot about them)]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:database" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#database</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:database" class="tag" target="_blank" rel="noopener nofollow">#database</a><br>I was a CS major from an Ivy League university who’s now a software developer at <a data-tooltip-position="top" aria-label="https://www.brandverity.com/" rel="noopener nofollow" class="external-link" href="https://www.brandverity.com/" target="_blank">an awesome company</a> — and I don’t know much about databases.<br>I’m guessing I’m not the only one. It didn’t seem to be a beginner-friendly subject. The specific class wasn’t required. Any time storage systems were mentioned, they were on a higher level, very theoretical. There seems to be a common issue around CS graduates knowing very little about real-world software development (source control? deployment? huh?), and it’s up to us to figure most of this stuff out after the fact.<br>But I’m embracing my lack of knowledge on the subject, and blogging is a cool thing. Here’s hoping for some clarity of thought via writing.<br><br>Besides the fact that it wasn’t super accessible or approachable in college, it’s probably also taken me so long to do any self-learning here because it just seemed so intimidating and mysterious (but this might just be me). I switched from design to CS two years into college with no prior experience, and I was grasping at things that were readily available and that piqued my interest. For whatever reason, databases seemed like a thing that more experienced or “smarter” people were into. “Backend work” was something I veered away from, and the word “database” conjured images of highly technical, complicated systems with jargon I just wouldn’t understand. It was much easier to pretend it was ~magic~ and leave it for others to figure out (for now — it was always my intention to pick it all up eventually).<br><img src="https://miro.medium.com/v2/resize:fit:998/1*66lYXVFX3rJxRu-8Ze3meg.gif" referrerpolicy="no-referrer"><br>Basically how I viewed data storage and retrieval.<br><br>Google defines database as “a structured set of data held in a computer, especially one that is accessible in various ways.” At its most basic, a database is just a way of storing and organizing information. Ideally it is organized in such a way that it can be easily accessed, managed, and updated.<br>I like metaphors, so this simple definition of a database for me is like a toolbox. You’ve got lots of screws, nails, bits, a couple different hammers… A toolbox is a storage system that allows you to easily organize and access all of these things. Whenever you need a tool, you go to the toolbox. Maybe you have labels on the drawers — those will help you find, say, a cordless power drill. But now you need the right battery for the drill. You look in your “battery” drawer, but how do you find the one that fits this particular drill? You can run through all of your batteries using trial and error, but that seems inefficient. You think, ‘Maybe I should store my batteries with their respective drills, link them in some way.’ That might be a viable solution. But if you need all of your batteries (because you’re setting up a nice new charging station maybe?), will you have to access each of your drills to get them? Maybe one battery fits multiple drills? Also, toolboxes are great for storing disjointed tools and pieces, but you wouldn’t want to have to take your car apart and store every piece separately whenever you park it in the garage. In that case, you would want to store your car as a single entry in the database (ahem garage), and access its pieces through it.<br><img src="https://miro.medium.com/v2/resize:fit:1180/1*Xfxl8HoQqqg_KtpEsEcskw.jpeg" referrerpolicy="no-referrer"><br>At least I can easily find the alternator this way, right?<br>This example is contrived, but reveals some issues you’ll have to consider when choosing a database or how to store your data within it.<br><br>If you start directionlessly googling “databases” (like I did), you’ll soon realize there are several different types and lots of terminology surrounding them. So let’s try and clear up any potential language barriers.<br>While I’m sure someone has written books on each of these (some of which I should probably read), I’ll try to keep my definitions relatively simple. These were all terms that I came across while doing this research that I thought could use some quick explanation.<br>
<br>Query  

<br>A query is a single action taken on a database, a request presented in a predefined format. This is typically one of SELECT, INSERT, UPDATE, or DELETE.  
<br>We also use ‘query’ to describe a request from a user for information from a database. “Hey toolbox, could you get me the names of all the tools in the ‘wrenches’ drawer?” might look something like SELECT ToolName FROM Wrenches.


<br>Transaction<br>
A transaction is a sequence of operations (queries) that make up a single unit of work performed against a database. For example, Rob paying George $20 is a transaction that consists of two UPDATE operations; reducing Rob’s balance by $20 and increasing George’s.
<br>ACID: Atomicity, Consistency, Isolation, Durability<br>
In most popular databases, a transaction is only qualified as a transaction if it exhibits the four “ACID” properties:<br>
- Atomicity: Each transaction is a unique, atomic unit of work. If one operation fails, data remains unchanged. It’s all or nothing. Rob will never lose $20 without George being paid.  

<br>Consistency: All data written to the database is subject to any rules defined. When completed, a transaction must leave all data in a consistent state.  
<br>Isolation: Changes made in a transaction are not visible to other transactions until they are complete.  
<br>Durability: Changes completed by a transaction are stored and available in the database, even in the event of a system failure.


<br>Schema<br>
- A database schema is the skeleton or structure of a database; a logical blueprint of how the database is constructed and how things relate to each other (with tables/relations, indices, etc).  

<br>Some schemas are static (defined before a program is written), and some are dynamic (defined by the program or data itself).


<br>DBMS: database management system<br>
<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Database" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Database" target="_blank">Wikipedia</a> has a great summary: “A database management system is a software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases.” MySQL, PostgreSQL, Oracle — these are database management systems.
<br>Middleware<br>
Database-oriented middleware is “all the software that connects some application to some database.” Some definitions include the DBMS under this category. Middleware might also facilitate access to a DBMS via a web server for example, without having to worry about database-specific characteristics.
<br>Distributed vs Centralized Databases  

<br>As their names imply, a centralized database has only one database file, kept at a single location on a given network; a distributed database is composed of multiple database files stored in multiple physical locations, all controlled by a central DBMS.  
<br>Distributed databases are more complex, and require additional work to keep the data stored up-to-date and to avoid redundancy. However, they provide parallelization (which balances the load between several servers), preventing bottlenecking when a large number of requests come through.  
<br>Centralized databases make data integrity easier to maintain; once data is stored, outdated or inaccurate data (stale data) is no longer available in other places. However, it may be more difficult to retrieve lost or overwritten data in a centralized database, since it lacks easily accessible copies by nature.


<br>Scalability<br>
Scalability is the capability of a database to handle a growing amount of data. There are two types of scalability:  

<br>Vertical scalability is simply adding more capacity to a single machine. Virtually every database is vertically scalable.  
<br>Horizontal scalability refers to adding capacity by adding more machines. The DBMS needs to be able to partition, manage, and maintain data across all machines.


<br><img src="https://miro.medium.com/v2/resize:fit:1400/1*pWp5uSIjn0TgU9pnJe9MzA.png" referrerpolicy="no-referrer"><br>Vertical vs Horizontal Scaling. Little buckets don’t need as much brawn to carry, but they do require better coordination.<br><br>
<br>“relational”  

<br>I highly recommend <a data-tooltip-position="top" aria-label="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" rel="noopener nofollow" class="external-link" href="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" target="_blank">this article</a>, which explains, “The word ‘relational’ in a ‘relational database’ has nothing to do with relationships. It’s about relations from relational algebra.”  
<br>In a relational database, each relation is a set of tuples. Each tuple is a list of attributes, which represents a single item in the database. Each tuple (“row”) in a relation (“table”) shares the same attributes (“columns”). Each attribute has a well-defined data type (int, string, etc), defined ahead of time — schema in a relational database is static.  
<br>Examples include: Oracle, MySQL, SQLite, PostgreSQL


<br><img src="https://miro.medium.com/v2/resize:fit:700/1*ko1siDrIKwAdrEA1P5o--g.png" referrerpolicy="no-referrer"><br>Thanks, <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Relational_database#Terminology" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Relational_database#Terminology" target="_blank">Wikipedia</a>.<br>
<br>SQL: Structured Query Language<br>
SQL is a programming language based on relational algebra used to manipulate and retrieve data in a relational database. note: In the bullet above, I’m intentionally separating the relational database terminology (relation, tuple, attribute) from the SQL terminology (table, row, column) in order to provide some clarity and accuracy. Again, see <a data-tooltip-position="top" aria-label="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" rel="noopener nofollow" class="external-link" href="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" target="_blank">this post</a> for more details on this.
<br>Illustrative Example:<br>
We could store all the data behind a blog in a relational database. One relation will represent our blog posts, each of which will have ‘post_title’ and ‘post_content’ attributes as well as a unique ‘post_id’ (a primary key). Another relation might store all of the comments on a blog. Each item here will also have attributes like ‘comment_author’, ‘comment_content’, and ‘comment_id’ (again, a primary key), as well as its own ‘post_id.’ This attribute is a foreign key, and tells us which blog post each comment “relates” to. When we want to open a webpage for post #2 for example, we might say to the database: “select everything from the ‘posts’ table where the ID of the post is 2,” and then “select everything from the comments table where the ‘post_id’ is 2.”
<br>JOIN operations<br>
- A JOIN operation combines rows from multiple tables in one query. There are a few different types of joins and reasons for using them, but t<a data-tooltip-position="top" aria-label="http://www.sql-join.com/" rel="noopener nofollow" class="external-link" href="http://www.sql-join.com/" target="_blank">his page</a> provides good explanations and examples.<br>
- These operations are typically only used with relational databases and so are mentioned often when characterizing “relational” functionality.
<br>**Normalization and Denormalization  

<br>Normalization** is the process of organizing the relations and attributes of a relational database in a way that reduces redundancy and improves data integrity (accurate, consistent, up-to-date data). Data might be arranged based on dependencies between attributes, for example — we might prevent repeating information by using JOIN operations.  
<br>Denormalization then, is the process of adding redundant data in order to speed up complex queries. We might include the data from one table in another to eliminate the second table and reduce the number of JOIN operations.


<br>ORM: Object-Relational Mapping<br>
ORM is a technique for translating the logical representation of objects (as in object-oriented programming) into a more atomized form that is capable of being stored in a relational database (and back again when they are retrieved). I won’t go into more detail here, but it’s good to know it exists.
<br><br>
<br>“non-relational”<br>
At it’s simplest, a non-relational database is one that doesn’t use the relational model; no relations (tables) with tuples (rows) and attributes (columns). This title covers a pretty wide range of models, typically grouped into four categories: key-value stores, graph stores, column stores, and document stores.
<br><img src="https://miro.medium.com/v2/resize:fit:1280/1*Pq8DSf6o1z2N-Qc30yAoPA.jpeg" referrerpolicy="no-referrer"><br>No tables.<br>
<br>Illustrative Example:  

<br>When we set up our blog posts and comments in a relational database, it worked in the same way as the drawers of our toolbox. But, much like our drill and battery example, does it make sense to always store our blog posts in one place, and comments in another? They’re clearly related, and it’s probably rare that we’d want to look at a post’s comments without also wanting the post itself. If we used a non-relational database, opening a webpage for post #2 might look something like this: “select post #2 and everything related to it.” In this case, that would mean a ‘title’ and ‘content’, as well as a list of comments. And since we’re no longer constrained by rows always sharing the same columns, we can associate any arbitrary data with any blog posts as well — maybe some have tags, others images, or as your blog grows, you’d like some of your new posts to link to live Twitter streams. With the non-relational model, we don’t need to know ahead of time that all of our blog posts have the same attributes, and as we add attributes to newer items, we are not required to also add that “column” to all previous items as well.  
<br>This model also works well for the car example from earlier in this post. If you have three cars in your garage, it doesn’t make sense to store all of their tires together, seats together, radiators together… Instead, you store an entire car and everything related to it in its own “document.”  
<br>However, there may be a downside to this. If you wanted to know how many seats (or comments, or batteries) you have total, you may have to go through every car and count each seat individually. There are ways around this of course, but it’s less trivial than just opening up the “seats” drawer and checking your total, especially on much larger scales.


<br>NoSQL<br>
“NoSQL” originally referred to “non-SQL” or “non-relational” when describing a database. Sometimes “NoSQL” is also meant to mean “Not only SQL”, to emphasize that they don’t prohibit SQL or SQL-like query languages; they just avoid functionality like relation/table schemas and JOIN operations.
<br>Key-Value Store  

<br>Key-value stores don’t use the pre-defined structure of relational databases, but instead treat all of their data as a single collection of items. For example, a screwdriver in our toolbox might have attributes like “drive_type”, “length”, and “size”, but a hammer may only have one attribute: “size”. Instead of storing (often empty) “drive_type” and “length” fields for every item in your toolbox, a “hammer_01” key will return only the information relevant to it.  
<br>Success with this model lies in its simplicity. Like a map or a dictionary, each key-value pair defines a link between some unique “key” (like a name, ID, or URL) and its “value” (an image, a file, a string, int, list, etc). There are no fields, so the entire value must be updated if changes are made. Key-value stores are generally fast, scalable, and flexible.  
<br>Examples include: Dynamo, MemcacheDB, Redis


<br>Graph Store  

<br>Graph stores are a little more complicated.Using graph structures, this type of database is made for dealing with interconnected data — think social media connections, a family tree, or a food chain. Items in the database are represented by “nodes”, and “edges” directly represent the relationships between them. Both nodes and edges can store additional “properties”: id, name, type, etc.


<br><img src="https://miro.medium.com/v2/resize:fit:1232/1*pK3tfUnVRfdCeUtS76ixAQ.png" referrerpolicy="no-referrer"><br>Something <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Graph_database#Description" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Graph_database#Description" target="_blank">like this</a>.<br>
<br>
The strength of a graph database is in traversing the connections between items, but their scalability is limited.  

<br>
Examples include: Allegro, OrientDB, Virtuoso

<br>
Column Store  

<br>Row-oriented databases describe single items as rows, and store all the data in a particular table’s rows together: ‘hammer_01’, ‘medium’, ‘blue’; ‘hammer_02’, ‘large’, ‘yellow’. A column store, on the other hand, generally stores all the values of a particular column together: ‘hammer_01’, ‘hammer_02’; ‘medium’, ‘large’; ‘blue’, ‘yellow’.  
<br>This can definitely get confusing, but the two map data very differently. In a row-oriented system, the primary key is the row ID, mapped to its data. In the column-oriented system, the primary key is the data, mapping back to row IDs. This allows for some very quick aggregations like totals and averages.  
<br>Examples include: Accumulo, Cassandra, HBase


<br>
Document Store  

<br>Document stores treat all information for a given item in the database as a single instance in the database (each of which can have its own structure and attributes, like other non-relational databases). These “documents” can generally be thought of as sets of key-value pairs: {ToolName: “hammer_01”, Size: “medium”, Color: “blue”}  
<br>Documents can be independent units, which makes performance and horizontal scalability better, and unstructured data can be stored easily.  
<br>Examples include: Apache CouchDB, MongoDB, Azure DocumentDB.


<br>
Object or Object-Oriented Database<br>
Not as common as other non-relational databases, an object or object-oriented database is ones in which data is represented in the form of “objects” (with attributes and methods) as used in object-oriented programming. This type might be used in place of a relational database and ORM, and may make sense when the data is complex or there are complex many-to-many relationships involved. Beware its language dependence and difficulty with ad-hoc queries though.

<br><br>Now that we know some stuff about databases, how can we apply that knowledge? How do you compare/test/benchmark different databases? What does it look like when they’re actually implemented, or when you have many working together?<br>All of this and more coming soon in blog post dos.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/database-system/intro-to-databases-(for-people-who-don’t-know-a-whole-lot-about-them).html</link><guid isPermaLink="false">Computer Science/Database System/Intro to Databases (for people who don’t know a whole lot about them).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:44:55 GMT</pubDate><enclosure url="https://miro.medium.com/v2/resize:fit:998/1*66lYXVFX3rJxRu-8Ze3meg.gif" length="0" type="image/gif"/><content:encoded>&lt;figure&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:998/1*66lYXVFX3rJxRu-8Ze3meg.gif"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Prisma 关系型数据库的 Self-relations]]></title><description><![CDATA[ 
 <br>
A relation field can also reference its own model, in this case the relation is called a self-relation. Self-relations can be of any cardinality, 1-1, 1-n and m-n.
<br><br>model User {
  id          Int     @id @default(autoincrement())
  name        String?
  successorId Int?    @unique
  successor   User?   @relation("BlogOwnerHistory", fields: [successorId], references: [id])
  predecessor User?   @relation("BlogOwnerHistory")
}
<br>User 展现了这样一个模型：<br>
<br>User 可以有一个或零个前驱（predecessor）
<br>User 可以有一个或零个后继（successor）
<br>注意：不能要求前驱和后继都必须存在，这两个必须有一个是可选的，否则没办法创建第一个 User。<br>要创建一对一的 self-relation：<br>
<br>关系的两端都必须定义一个共享相同名称的 @relation 属性（BlogOwnerHistory）
<br>关系字段必须是<a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations#relation-fields" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations#relation-fields" target="_blank">完全注释</a>的。例如 successor 字段需要定义 field 和 references 参数。
<br>关系字段必须由外键支持。successor 字段由 successorId 外键提供支持，该外键引用 id 字段中的值。successorId 还需要 @unique 属性来保证一对一的关系。****
<br>
一对一的 self-relation 需要两个端点，即使这两个端点是同一条数据。
<br>而在关系型数据库中，一对一的 self-relation 可以用如下 SQL 描述：<br>CREATE TABLE "User" (
    id SERIAL PRIMARY KEY,
    "name" TEXT,
    "successorId" INTEGER
);

ALTER TABLE "User" ADD CONSTRAINT fk_successor_user FOREIGN KEY ("successorId") REFERENCES "User" (id);

ALTER TABLE "User" ADD CONSTRAINT successor_unique UNIQUE ("successorId");
<br><br>model User {
  id        Int     @id @default(autoincrement())
  name      String?
  teacherId Int?
  teacher   User?   @relation("TeacherStudents", fields: [teacherId], references: [id])
  students  User[]  @relation("TeacherStudents")
}
<br>User 展现了这样一个模型：<br>
<br>一个 User 只能有零个或一个 teacher
<br>一个 User 可以有零个或多个 students
<br>
可以通过将 teacher 字段设<a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/models#optional-and-mandatory-fields" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/models#optional-and-mandatory-fields" target="_blank">为 required</a> 来要求每个 User 都有一名 teacher。
<br>用 SQL 描述 User model：<br>CREATE TABLE "User" (
    id SERIAL PRIMARY KEY,
    "name" TEXT,
    "teacherId" INTEGER
);

ALTER TABLE "User" ADD CONSTRAINT fk_teacherid_user FOREIGN KEY ("teacherId") REFERENCES "User" (id);
<br>teacherId 没有使用 UNIQUE 约束，这代表着多个 students 可以有同一个 teacher<br><br>model User {
  id         Int     @id @default(autoincrement())
  name       String?
  followedBy User[]  @relation("UserFollows")
  following  User[]  @relation("UserFollows")
}
<br>
<br>一个 User 可以被零个或多个 Users 关注
<br>一个 User 可以关注零个或多个 Users
<br>
对于关系型数据库，多对多的关系是隐式的，这意味着 Prisma ORM 会在底层数据库中维护一个 <a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#relation-tables" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#relation-tables" target="_blank">relation table</a>:<br>
A relation table (also sometimes called a JOIN, link or pivot table) connects two or more other tables and therefore creates a relation between them. Creating relation tables is a common data modelling practice in SQL to represent relationships between different entities. In essence it means that "one m-n relation is modeled as two 1-n relations in the database".
We recommend using <a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#implicit-many-to-many-relations" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#implicit-many-to-many-relations" target="_blank">implicit</a> m-n-relations, where Prisma ORM automatically generates the relation table in the underlying database. <a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#explicit-many-to-many-relations" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#explicit-many-to-many-relations" target="_blank">Explicit</a> m-n-relations should be used when you need to store additional data in the relations, such as the date the relation was created.
<br>如果需要需要通过多对多的关系来保存其他字段，也可以创建<a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#explicit-many-to-many-relations" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#explicit-many-to-many-relations" target="_blank">显式</a>的多对多 self 关系：<br>model User {
  id         Int       @id @default(autoincrement())
  name       String?
  followedBy Follows[] @relation("followedBy")
  following  Follows[] @relation("following")
}

model Follows {
  followedBy   User @relation("followedBy", fields: [followedById], references: [id])
  followedById Int
  following    User @relation("following", fields: [followingId], references: [id])
  followingId  Int

  @@id([followingId, followedById])
}
<br>在关系型数据库中，可以用如下 SQL 描述:<br>CREATE TABLE "User" (
    id integer DEFAULT nextval('"User_id_seq"'::regclass) PRIMARY KEY,
    name text
);
CREATE TABLE "_UserFollows" (
    "A" integer NOT NULL REFERENCES "User"(id) ON DELETE CASCADE ON UPDATE CASCADE,
    "B" integer NOT NULL REFERENCES "User"(id) ON DELETE CASCADE ON UPDATE CASCADE
);
<br><br>model User {
  id         Int     @id @default(autoincrement())
  name       String?
  teacherId  Int?
  teacher    User?   @relation("TeacherStudents", fields: [teacherId], references: [id])
  students   User[]  @relation("TeacherStudents")
  followedBy User[]  @relation("UserFollows")
  following  User[]  @relation("UserFollows")
}
<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations#relation-fields" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations#relation-fields" target="_blank">fully annotated</a>
<br><a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/models#optional-and-mandatory-fields" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/models#optional-and-mandatory-fields" target="_blank">required</a>
<br><a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#implicit-many-to-many-relations" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#implicit-many-to-many-relations" target="_blank">implicit</a>
<br><a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#relation-tables" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#relation-tables" target="_blank">relation table</a>
<br><a data-tooltip-position="top" aria-label="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#explicit-many-to-many-relations" rel="noopener nofollow" class="external-link" href="https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/many-to-many-relations#explicit-many-to-many-relations" target="_blank">explicit</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/database-system/prisma-关系型数据库的-self-relations.html</link><guid isPermaLink="false">Computer Science/Database System/Prisma 关系型数据库的 Self-relations.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Tue, 18 Feb 2025 01:48:13 GMT</pubDate></item><item><title><![CDATA[Flink 在风控场景实时特征落地实战]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:flink" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#flink</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:flink" class="tag" target="_blank" rel="noopener nofollow">#flink</a> <a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <a href="https://muqiuhan.github.io/wiki?query=tag:java" class="tag" target="_blank" rel="noopener nofollow">#java</a> <br><br><br>二十一世纪，信息化时代到来，互联网行业的发展速度远快于其他行业。一旦商业模式跑通，有利可图，资本立刻蜂拥而至，助推更多企业不断的入场进行快速的复制迭代，企图成为下一个“行业领头羊”。<br>
​<br>带着资本入场的玩家因为不会有资金的压力，只会更多的关注业务发展，却忽略了业务上的风险点。强大如拼多多也被“薅羊毛”大军光顾损失千万。<br>
​<br>风控，即风险管理（risk management），是一个管理过程，包括对风险的定义、测量、评估和应对风险的策略。目的是将可避免的风险、成本及损失极小化[1]。<br><br>互联网企业每时每刻都面临着黑灰产的各种攻击。业务安全团队需要事先评估业务流程中有风险的地方，再设置卡点，用来采集相关业务信息，识别当前请求是否有风险。专家经验（防控策略）就是在长期以往的对抗中产生的。<br>
​<br>策略的部署需要一个个特征来支持，那什么是特征？<br>
特征分为基础型特征、衍生型特征、统计型特征等，举例如下：<br>
<br>基础型特征：可以直接从业务获取的，如订单的金额、买家的手机号码、买家地址、卖家地址等
<br>衍生特征：需要二次计算，如买家到买家的距离、手机号前3位等
<br>统计型特征：需要实时统计的，如5分钟内某手机号下购买订单数、10分钟内购买金额大于2w元订单数等
<br>​<br>随着业务的迅猛发展，单纯的专家经验已不能满足风险识别需求，算法团队的加入使得拦截效果变得更加精准。算法部门人员通过统一算法工程框架，解决了模型和特征迭代的系统性问题，极大地提升了迭代效率。<br>
​<br>根据功能不同，算法平台可划分为三部分：模型服务、模型训练和特征平台。其中，模型服务用于提供在线模型预估，模型训练用于提供模型的训练产出，特征平台则提供特征和样本的数据支撑。本文将重点阐述特征平台在建设过程中实时计算遇到的挑战以及优化思路。<br>
​<br><br><br>业务发展的初期，我们可以通过硬编码的方式满足策略人员提出的特征需求，协同也比较好。但随着业务发展越来越快，业务线越来越多，营销玩法越来越复杂，用户数和请求量成几何倍上升。适用于早期的硬编码方式出现了策略分散无法管理、逻辑同业务强耦合、策略更新迭代率受限于开发、对接成本高等多种问题。此时，我们急需一套线上可配置、可热更新、可快速试错的特征管理平台。<br><br><br>如果你熟悉 Flink DataStream API，那你肯定会发现 Flink 的设计天然满足风控实时特征计算场景，我们只需要简单的几步即可统计指标，如下图所示：<br>
<img alt="Pasted image 20241224191713.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191713.png"><br>
Flink DataStream 流图<br>
​<br>实时特征统计样例代码如下：<br>// 数据流，如topic
DataStream&lt;ObjectNode&gt; dataStream = ...

SingleOutputStreamOperator&lt;AllDecisionAnalyze&gt; windowOperator = dataStream
                // 过滤
                .filter(this::filterStrategy)
                // 数据转换
                .flatMap(this::convertData)
                // 配置watermark
                .assignTimestampsAndWatermarks(timestampAndWatermarkAssigner(config))
                // 分组
                .keyBy(this::keyByStrategy)
                // 5分钟滚动窗口
                .window(TumblingEventTimeWindows.of(Time.seconds(300)))
                // 自定义聚合函数，内部逻辑自定义
                .aggregate(AllDecisionAnalyzeCountAgg.create(), AllDecisionAnalyzeWindowFunction.create());
<br>1.0框架不足：<br>
<br>特征强依赖开发人员编码，简单的统计特征可以抽象，稍微复杂点就需要定制
<br>迭代效率低，策略提需求、产品排期、研发介入、测试保障、一套流程走完交付最少也是两周
<br>特征强耦合，任务拆分难，一个 JOB 包含太多逻辑，可能新上的特征逻辑会影响之前稳定的指标
<br>总的来说，1.0在业务初期很适合，但随着业务发展，研发速度逐渐成为瓶颈，不符合可持续、可管理的实时特征清洗架构。<br>
​<br><br>1.0架构的弊端在于需求到研发采用不同的语言体系，如何高效的转化需求，甚至是直接让策略人员配置特征清洗逻辑直接上线？如果按照两周一迭代的速度，可能线上早被黑灰产薅的“面目全非”了。<br>
​<br>此时我们研发团队注意到 Flink SQL，SQL 是最通用的数据分析语言，数分、策略、运营基本必备技能，可以说 SQL 是转换需求代价最小的实现方式之一。<br>
​<br>看一个 Flink SQL 实现示例：<br>-- error 日志监控
-- kafka source
CREATE TABLE rcp_server_log (
    thread varchar,
    level varchar,
    loggerName varchar,
    message varchar,
    endOfBatch varchar,
    loggerFqcn varchar,
    instant varchar,
    threadId varchar,
    threadPriority varchar,
    appName varchar,
    triggerTime as LOCALTIMESTAMP,
    proctime as PROCTIME(),

    WATERMARK FOR triggerTime AS triggerTime - INTERVAL '5' SECOND
) WITH (
    'connector.type' = 'kafka',
    'connector.version' = '0.11',
    'connector.topic' = '${sinkTopic}',
    'connector.startup-mode' = 'latest-offset',
    'connector.properties.group.id' = 'streaming-metric',
    'connector.properties.bootstrap.servers' = '${sinkBootstrapServers}',
    'connector.properties.zookeeper.connect' = '${sinkZookeeperConnect}}',
    'update-mode' = 'append',
    'format.type' = 'json'
);

-- 此处省略 sink_feature_indicator 创建，参考 source table
-- 按天 按城市 各业务线决策分布
INSERT INTO sink_feature_indicator
SELECT
    level,
    loggerName,
    COUNT(*)
FROM rcp_server_log
WHERE
    (level &lt;&gt; 'INFO' AND `appName` &lt;&gt; 'AppTestService')
    OR loggerName &lt;&gt; 'com.test'
GROUP BY
    TUMBLE(triggerTime, INTERVAL '5' SECOND),
    level,
    loggerName;
<br>我们在开发 Flink SQL 支持平台过程中，遇到如下问题：<br>
<br>一个 SQL 如果清洗一个指标，那么数据源将极大浪费
<br>SQL merge，即一个检测如果同源 SQL 则进行合并，此时将极大增加作业复杂度，且无法定义边界
<br>SQL 上线需要停机重启，此时如果任务中包含大量稳定指标，会不会是临界点
<br><br><br><img alt="Pasted image 20241224191805.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191805.png"><br><br>策略/算法人员每天需要观测实时和离线数据分析线上是否存在风险，针对有风险的场景，会设计防控策略，透传到研发侧其实就是一个个实时特征的开发。所以实时特征的上线速度、质量交付、易用性完全决定了线上风险场景能否及时堵漏的关键。<br>
​<br>在统一实时特征计算平台构建之前，实时特征的产出上主要有以下问题：<br>
<br>交付速度慢，迭代开发：策略提出到产品，再到研发，提测，在上线观测是否稳定，速度奇慢
<br>强耦合，牵一发动全身：怪兽任务，包含很多业务特征，各业务混在一起，没有优先级保证
<br>重复性开发：由于没有统一的实时特征管理平台，很多特征其实已经存在，只是名字不一样，造成极大浪费
<br>平台话建设，最重要的是“整个流程的抽象”，平台话的目标应该是能用、易用、好用。基于如上思想，我们尝提取实时特征研发痛点：模板化 + 配置化，即平台提供一个实时特征的创建模板，用户基于该模板，可以通过简单的配置即可生成自己需要的实时特征。<br><img alt="Pasted image 20241224191816.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191816.png"><br>Flink 实时计算架构图<br><br>数据源清洗：不同数据源抽象 Flink Connector，标准输出供下游使用<br>
数据拆分：1拆N，一条实时消息可能包含多种消息，此时需要数据裂变<br>
动态配置：允许在不停机 JOB 情况下，动态更新或新增清洗逻辑，涉及特征的清洗逻辑下发<br>
脚本加载：Groovy 支持，热更新<br>
RTC: 即 Real-Time Calculate，实时特征计算，高度抽象的封装模块<br>
任务感知：基于特征业务域、优先级、稳定性，隔离任务，业务解耦<br><br>统一查询SDK: 实时特征统一查询SDK，屏蔽底层实现逻辑<br>
基于统一的 Flink 实时计算架构，我们重新设计了实时特征清洗架构<br><img alt="Pasted image 20241224191831.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191831.png"><br>Flink 实时计算数据流图<br><br>特征底层的存储应该是“原子性”的，即最小不可分割单位。为何如此设计？实时统计特征是和窗口大小挂钩的，不同策略人员防控对特征窗口大小有不同的要求，举例如下：<br>
<br>可信设备判定场景：其中当前手机号登录时长窗口应适中，不宜过短，防扰动
<br>提现欺诈判定场景：其中当前手机号登录时长窗口应尽量短，短途快速提现的，结合其它维度，快速定位风险
<br>​<br>基于上述，急需一套通用的实时特征读取模块，满足策略人员任意窗口需求，同时满足研发人员快速的配置清洗需求。我们重构后特征配置模块如下：<br>
<img alt="Pasted image 20241224191842.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191842.png"><br>实时特征模块：<br>
<br>特征唯一标识
<br>特征名称
<br>是否支持窗口：滑动、滚动、固定大小窗口
<br>事件切片单位：分钟、小时、天、周
<br>主属性：即分组列，可以多个
<br>从属性：聚合函数使用，如去重所需输入基础特征
<br>​<br>业务留给风控的时间不多，大多数场景在 100 ms 以内，实时特征获取就更短了，从以往的研发经验看，RT 需要控制在 10 ms 以内，以确保策略执行不会超时。所以我们的存储使用 Redis，确保性能不是瓶颈。<br>
<img alt="Pasted image 20241224191851.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191851.png"><br>
如上述，实时特征计算模块强依赖于上游消息内传递的“主属性” 和 “从属性”，此阶段也是研发需要介入的地方，如果消息内主属性字段不存在，则需要研发补全，此时不得不加入代码的发版，那又会回到原始阶段面临的问题：Flink Job 需要不停的重启，这显然是不能接受的。<br>
此时我们想到了 Groovy，能否让 Flink + Groovy，直接热部署代码？答案是肯定的！<br>
​<br>由于我们抽象了整个 Flink Job 的计算流图，算子本身是不需要变更的，即 DAG 是固定不变的，变得是算子内部关联事件的清洗逻辑。所以，只要关联清洗逻辑和清洗代码本身变更，即不需要重启 Flink Job 完成热部署。<br><img alt="Pasted image 20241224191904.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191904.png"><br>
研发或策略人员在管理后台（Operating System）添加清洗脚本，并存入数据库。Flink Job 脚本缓存模块此时会感知脚本的新增或修改（如何感知看下文整体流程详解）<br>
<br>warm up：脚本首次运行较耗时，首次启动或者缓存更新时提前预热执行，保证真实流量进入脚本快速执行
<br>cache：缓存已经在好的 Groovy 脚本
<br>Push/Poll：缓存更新采用推拉两种模式，确保信息不回丢失
<br>router：脚本路由，确保消息能寻找到对应脚本并执行<br>
脚本加载核心代码：
<br>    // 缓存，否则无限加载下去会 metaspace outOfMemory
    private final static Map&lt;String, GroovyObject&gt; groovyObjectCache = new ConcurrentHashMap&lt;&gt;();

    /**
     * 加载脚本
     * @param script
     * @return
     */
    public static GroovyObject buildScript(String script) {
        if (StringUtils.isEmpty(script)) {
            throw new RuntimeException("script is empty");
        }

        String cacheKey = DigestUtils.md5DigestAsHex(script.getBytes());
        if (groovyObjectCache.containsKey(cacheKey)) {
            log.debug("groovyObjectCache hit");
            return groovyObjectCache.get(cacheKey);
        }

        GroovyClassLoader classLoader = new GroovyClassLoader();
        try {
            Class&lt;?&gt; groovyClass = classLoader.parseClass(script);
            GroovyObject groovyObject = (GroovyObject) groovyClass.newInstance();
            classLoader.clearCache();

            groovyObjectCache.put(cacheKey, groovyObject);
            log.info("groovy buildScript success: {}", groovyObject);
            return groovyObject;
        } catch (Exception e) {
            throw new RuntimeException("buildScript error", e);
        } finally {
            try {
                classLoader.close();
            } catch (IOException e) {
                log.error("close GroovyClassLoader error", e);
            }
        }
    }
<br><br>策略需要统计的消息维度很杂，涉及多个业务，研发本身也有监控用到的实时特征需求。所以实时特征对应的数据源是多种多样的。所幸 Flink 是支持多种数据源接入的，对于一些特定的数据源，我们只需要继承实现 Flink Connector 即可满足需求，我将拿 Kafka举例，整体流程是如何清洗实时统计特征的。<br>
​<br>首先介绍风控整体数据流，多个业务场景对接风控中台，风控内部核心链路是：决策引擎、规则引擎、特征服务。<br>
一次业务请求决策，我们会异步记录下来，并发送Kafka消息，用于实时特征计算 &amp; 离线埋点。<br><img alt="Pasted image 20241224191930.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191930.png"><br><br>Flink 实时计算 Job 在接收到 MQ 消息后，首先是消息模板标准化解析，不同的 Topic 对应消息格式不一致，JSON、CSV、异构（如错误日志类消息，空格隔断，对象内包含 JSON 对象）等。<br>
​<br>为方便下游算子统一处理，标准化后消息结构如下 JSON 结构：<br>public class RcpPreProcessData {

    /**
     * 渠道，可以直接写topic即可
     */
    private String channel;

    /**
     * 消息分类 channel + eventCode 应唯一确定一类消息
     */
    private String eventCode;

    /**
     * 所有主从属性
     */
    private Map&lt;String, Object&gt; featureParamMap;

    /**
     * 原始消息
     */
    private ObjectNode node;
    
}

<br><br>一条“富消息”可能包含大量的业务信息，某些实时特征可能需要分别统计。举例，一条业务请求风控的上下文消息，包含本次消息是否拒绝，即命中了多少策略规则，命中的规则是数组，可能包含多条命中规则。此时如果想基于一条命中的规则去关联其它属性统计，就需要用到消息的裂变，由1变N。<br>
​<br>消息裂变的逻辑由运营后台通过 Groovy 脚本编写，定位清洗脚本逻辑则是 channel（父） + eventCode（子），此处寻找逻辑分“父子”，“父”逻辑对当前 channel 下所有逻辑适用，避免单独配置 N 个 eventCode 的繁琐，“子”逻辑则对特定的eventCode适用。<br>
​<br><br>消息的清洗就是我们需要知道特征需要哪些主从属性，带着目的清洗更清晰，定位清洗的脚本同上，依然依据 channel + eventCode 实现。清洗出的主从属性存在 featureParamMap 中，供下游实时计算使用。<br>
​<br>此处需要注意的是，我们一直是带着原始消息向下传递的，但如果已经确认了清洗的主从属性，那么原始消息就没有存在的必要了，此时我们需要“剪枝”，节省 RPC 调用过程 I/O 流量的消耗。<br>
​<br>至此，一条原始消息已经加工成只包含 channel（渠道）、eventCode（事件类型）、featureParamMap（所有主从属性），下游算子只需要且仅需要这些信息即可计算。<br>
​<br><br>依然同上面两个算子，实时计算算子依赖 channel + eventCode 查找到对应实时特征元数据，一个事件可能存在多个实时特征配置，运营平台填写好实时特征配置后，依据缓存更新机制，快速分发到任务中，依据 Key构造器 生成对应的 Key，传递下游直接 Sink 到 Redis中。<br>
​<br><br>任务的排查是基于完善的监控上实现的，Flink 提供了很多有用的 Metric 供我们排查问题，如下是我罗列的常见的任务异常，希望对你有所帮助。<br><br>出现上面这个异常的可能原因是因为：<br>
<br>大窗口：90% TM 内存爆表，都是大窗口导致的
<br>内存泄漏：如果是自定义节点，且涉及到缓存等很容易导致内存膨胀
<br>解决办法：<br>
<br>合理制定窗口导线，合理分配 TM 内存（1.10默认是1G），聚合数据应交由 Back State 管理，不建议自己写对象存储
<br>可 attach heap 快照排查异常，分析工具如 MAT，需要一定的调优经验，也能快速定位问题
<br>​<br><br>出现上面这个异常的可能原因是因为：<br>
<br>数据倾斜：90%的反压，一定是数据倾斜导致的
<br>并行度并未设置好，错误估计数据流量或单个算子计算性能
<br>​<br>解决办法：<br>
<br>数据清洗参考下文
<br>对于并行度，可以在消息传递过程中埋点，看各个节点cost
<br>​<br><br>核心思路：<br>
<br>key 加随机数，然后执行 keyby 时会根据新 key 进行分区，此时会打散 key 的分布，不会造成数据倾斜问题
<br>二次 keyby 进行结果统计
<br>​<br>打散逻辑核心代码：<br>public class KeyByRouter {

    private final static String SPLIT_CHAR = "#";

    /**
     * 不能太散，否则二次聚合还是会有数据倾斜
     *
     * @param sourceKey
     * @return
     */
    public static String randomKey(String sourceKey) {
        int endExclusive = (int) Math.pow(2, 7);
        return sourceKey + SPLIT_CHAR + (RandomUtils.nextInt(0, endExclusive) + 1);
    }

    public static String restoreKey(String randomKey) {
        if (StringUtils.isEmpty(randomKey)) {
            return null;
        }

        return randomKey.split(SPLIT_CHAR)[0];
    }
}

<br><br>出现上面这个异常的可能原因是因为：<br>
<br>作业本身处于反压的情况，做 Checkpoint 可能失败了，所以暂停保留状态的时候做 Savepoint 肯定也会失败
<br>作业的状态很大，做 Savepoint 超时了
<br>作业设置的 Checkpoint 超时时间较短，导致 SavePoint 还没有做完，作业就丢弃了这次 Savepoint 的状态
<br>解决办法：<br>
<br>代码设置 Checkpoint 的超时时间尽量的长一些，比如 10min，对于状态很大的作业，可以设置更大
<br>如果作业不需要保留状态，那么直接暂停作业，然后重启就行
<br><br>这篇文章分别从实时特征清洗框架演进，特征可配置，特征清洗逻辑热部署等方面介绍了目前较稳定的实时计算可行架构。经过近两年的迭代，目前这套架构在稳定性、资源利用率、性能开销上有最优的表现，给业务策略人员及业务算法人员提供了有力的支撑。<br>
​<br>未来，我们期望特征的配置还是回归 SQL 化，虽然目前配置已经足够简单，但是毕竟属于我们自己打造的“领域设计语言”，对新来的的策略人员 &amp; 产品人员有一定的学习成本，我们期望的是能够通过像 SQL 这种全域通过用语言来配置化，类似 Hive 离线查询一样，屏蔽了底层复杂的计算逻辑，助力业务更好的发展。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/flink-在风控场景实时特征落地实战.html</link><guid isPermaLink="false">Computer Science/Distributed System/Flink 在风控场景实时特征落地实战/Flink 在风控场景实时特征落地实战.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:45:38 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191713.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/flink-在风控场景实时特征落地实战/attachments/pasted-image-20241224191713.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[In Search of an Understandable Consensus Algorithm(Extended Version)]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:raft" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#raft</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:raft" class="tag" target="_blank" rel="noopener nofollow">#raft</a><br>寻找一种可理解的一致性算法(拓展版)<br><br><br>Raft is a consensus algorithm for managing a replicated log.<br>
It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos;<br>
this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems.<br>
In order to enhance understandability, Raft separates the key elements of consensus,<br>
such as leader election, log replication, and safety,<br>
and it enforces a stronger degree of coherency to reduce the number of states that must be considered.<br>
Results from a user study demonstrate that Raft is easier for students to learn than Paxos.<br>
Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.<br>Raft是一种用于管理复制日志的一致性算法。<br>
其和(multi-)Paxos算法作用相同，并且和Paxos一样高效，但其结构与Paxos不同；这使得Raft比起Paxos更容易理解同时也为构建实际可行的系统提供了一个更好的基础。<br>
为了让Raft更容易理解，Raft拆分了有关一致性的关键元素，例如leader选举，日志复制以及安全性等，并通过增强一致性的程度以减少必须被考虑的状态数量。<br>
用户的研究成果表示Raft比起Paxos要更容易让学生进行学习。<br>
Raft还包含了一个改变集群成员的新机制，其使用重叠的大多数(overlapping majorities)来保证安全。<br><br>Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.<br>
Because of this, they play a key role in building reliable large-scale software systems.<br>
Paxos has dominated the discussion of consensus algorithms over the last decade:<br>
most implementations of consensus are based on Paxos or influenced by it,<br>
and Paxos has become the primary vehicle used to teach students about consensus.<br>一致性算法允许一个机器的集群作为一个具有一致性的组来进行工作，使得在一些成员出现故障时集群依然能正常工作。<br>
正因为如此，在构建可靠的大规模软件系统时其起到了关键的作用。<br>
Paxos主导了过去十年中关于一致性算法的讨论：<br>
大多数的一致性的实现都给予Paxos或者受其影响，并且Paxos成为了教导学生一致性相关知识的主要工具。<br>Unfortunately, Paxos is quite difficult to understand, in spite of numerous attempts to make it more approachable.<br>
Furthermore, its architecture requires complex changes to support practical systems.<br>
As a result, both system builders and students struggle with Paxos.<br>不幸的是，Paxos相当难理解，尽管很多人试图让其变得更易理解。<br>
此外，为了支持实际的系统其架构需要进行复杂的改变。<br>
因此，所有的系统构建者和学生都在于Paxos进行斗争。<br>After struggling with Paxos ourselves,<br>
we set out to find a new consensus algorithm that could provide a better foundation for system building and education.<br>
Our approach was unusual in that our primary goal was understandability:<br>
could we define a consensus algorithm for practical systems and describe it in a way that is significantly easier to learn than Paxos?<br>
Furthermore, we wanted the algorithm to facilitate the development of intuitions that are essential for system builders.<br>
It was important not just for the algorithm to work, but for it to be obvious why it works.<br>在与Paxos斗争后，我们开始着手去寻找一种新的一致性算法，其能够为构建系统和教育提供更好的支持。<br>
我们的方法是不同寻常的，因为我们的主要目标是(增进)可理解性：我们可以为实际的系统定义一个一致性算法并以比Paxos更容易学习的方式去描述它吗？<br>
此外，我们希望该算法能够促进直觉的发展，这对系统构建者来说是必要的。<br>
重要的不仅仅是算法是如何工作的，理解算法为什么能工作也很重要。<br>The result of this work is a consensus algorithm called Raft.<br>
In designing Raft we applied specific techniques to improve understandability,<br>
including decomposition (Raft separates leader election, log replication, and safety)<br>
and state space reduction (relative to Paxos, Raft reduces the degree of nondeterminism and the ways servers can be inconsistent with each other).<br>
A user study with 43 students at two universities shows that Raft is significantly easier to understand than Paxos:<br>
after learning both algorithms, 33 of these students were able to answer questions about Raft better than questions about Paxos.<br>这项工作的成果是一个名为Raft的一致性算法。<br>
在设计Raft时，我们应用了特别的技术来改善可理解性，包括分解(Raft将leader选举，日志复制和安全性进行了分解)<br>
以及状态空间的缩减(相对于Paxos，Raft缩减了不确定性的程度以及服务器之间彼此不一致的方式)。<br>
一项对两所大学中的43名学生的调查显示Raft比Paxos容易理解的多：在学习了两种算法后，相比回答Paxos相关问题，其中33名学生能更好的回答关于Raft的问题。<br>Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov’s Viewstamped Replication),<br>
but it has several novel features:<br>
<br>Strong leader: Raft uses a stronger form of leadership than other consensus algorithms.<br>
For example, log entries only flow from the leader to other servers.<br>
This simplifies the management of the replicated log and makes Raft easier to understand.
<br>Leader election: Raft uses randomized timers to elect leaders.<br>
This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm,<br>
while resolving conflicts simply and rapidly.
<br>Membership changes: Raft’s mechanism for changing the set of servers in the cluster uses a new joint consensus approach<br>
where the majorities of two different configurations overlap during transitions.<br>
This allows the cluster to continue operating normally during configuration changes.
<br>Raft与已有的一致性算法在很多方面都很相似(尤其是Oki和Liskov的Viewstamped Replication算法)，但Raft有几个新颖的功能：<br>
<br>Strong leader: Raft使用比其它一致性算法更强力的leader。<br>
举个例子，日志条目仅从leader流向其它服务器。这简化了被复制日志的管理并且使得Raft更加容易被理解。
<br>Leader election: Raft使用随机计时器来选举leader。<br>
这只在任何一致性算法都需要的心跳检测中增加了少量机制，同时简单且快速的解决冲突。
<br>Membership changes: Raft用于改变集群中服务器集合的机制使用了一种新的联合的一致性方法，其中两个不同配置的多数在过渡期间是重叠的。<br>
这允许集群在配置改变时继续正常工作。
<br>We believe that Raft is superior to Paxos and other consensus algorithms, both for educational purposes and as a foundation for implementation.<br>
It is simpler and more understandable than other algorithms;<br>
it is described completely enough to meet the needs of a practical system;<br>
it has several open-source implementations and is used by several companies;<br>
its safety properties have been formally specified and proven; and its efficiency is comparable to other algorithms.<br>我们认为，无论是处于教育的目的还是作为实际(系统)的实现，Raft都是胜过Paxos和其它一致性算法的。<br>
它比其它算法更加简单和容易理解；<br>
它被详细的描述使得其足以满足实际系统的需要；<br>
它有着几个开源的实现并且被几家公司所使用；<br>
它的安全性已经被正式的认定和证明；并且它的效率与其它算法相当。<br>The remainder of the paper introduces the replicated state machine problem (Section 2),<br>
discusses the strengths and weaknesses of Paxos (Section 3),<br>
describes our general approach to understandability (Section 4),<br>
presents the Raft consensus algorithm (Sections 5–8),<br>
evaluates Raft (Section 9), and discusses related work (Section 10).<br>本文的剩余部分介绍了复制状态机问题(第2节)，<br>
讨论了Paxos的优缺点(第3节)，<br>
描述了我们使算法易于理解的一般性方法(第4节)，<br>
提出了Raft一致性算法(第5-8节)，<br>
评估了Raft(第9节)，并且讨论了相关的工作(第10节)。<br><br>Consensus algorithms typically arise in the context of replicated state machines.<br>
In this approach, state machines on a collection of servers compute identical copies of the same state<br>
and can continue operating even if some of the servers are down.<br>
Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems.<br>
For example, large-scale systems that have a single cluster leader, such as GFS, HDFS, and RAMCloud,<br>
typically use a separate replicated state machine to manage leader election and store configuration information<br>
that must survive leader crashes.<br>
Examples of replicated state machines include Chubby and ZooKeeper.<br>一致性算法是在复制状态机的背景下产生的。<br>
在这个方法中，服务器集合中的状态机在具有相同状态的完全一致的副本上进行计算，并且即使一些服务器已经宕机也能够持续的工作。<br>
复制状态机被用于在分布式系统中解决一系列的容错问题。<br>
举个例子，有着一个单独集群leader的大规模系统，例如GFS，HDFS以及RAMCloud，通常使用一个单独的复制状态机来管理leader选举和存储在leader崩溃后所必须的配置信息。<br>
复制状态机的例子包括Chubby和ZooKeeper。<br><img alt="Pasted image 20240725172940.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172940.png"><br>Figure 1: Replicated state machine architecture.<br>
The consensus algorithm manages a replicated log containing state machine commands from clients.<br>
The state machines process identical sequences of commands from the logs, so they produce the same outputs.<br>图1：复制状态机的架构。<br>
一致性算法管理包含了来自客户端的状态机指令的复制日志。<br>
状态机以完全相同的顺序处理来自日志的指令，因此它们会产生相同的输出。<br>Replicated state machines are typically implemented using a replicated log, as shown in Figure 1.<br>
Each server stores a log containing a series of commands, which its state machine executes in order.<br>
Each log contains the same commands in the same order, so each state machine processes the same sequence of commands.<br>
Since the state machines are deterministic, each computes the same state and the same sequence of outputs.<br>复制状态机通常使用复制log(replicated log)来实现，如图1所示。<br>
每个服务器存储着一个包含一系列指令的日志，这些指令在状态机上被顺序执行。<br>
每个日志中包含了以相同顺序排布的相同的指令，因此每个状态机都处理相同的指令序列。<br>
因为状态机是确定性的，每一个状态机都计算出相同的状态以及有着相同的输出序列。<br>Keeping the replicated log consistent is the job of the consensus algorithm.<br>
The consensus module on a server receives commands from clients and adds them to its log.<br>
It communicates with the consensus modules on other servers to ensure that every log eventually contains<br>
the same requests in the same order, even if some servers fail.<br>
Once commands are properly replicated, each server’s state machine processes them in log order,<br>
and the outputs are returned to clients.<br>
As a result, the servers appear to form a single, highly reliable state machine.<br>保持复制日志的一致性是一致性算法的工作。<br>
服务器中的一致性模块接受来自客户端的指令并且将其加入日志。<br>
它与其它服务器的一致性模块进行通信以确保每一个日志最终以同样的顺序包含同样的请求，即使其中一些服务器故障了。<br>
一旦指令被正确的复制，每一个服务器的状态机都按照日志中的顺序处理这些指令，并将输出返回给客户端。<br>
因此，服务器的集合似乎形成了一个单独的，高度可靠的状态机。<br>Consensus algorithms for practical systems typically have the following properties:<br>
<br>They ensure safety (never returning an incorrect result) under all non-Byzantine conditions,<br>
including network delays, partitions, and packet loss, duplication, and reordering.
<br>They are fully functional (available) as long as any majority of the servers are operational<br>
and can communicate with each other and with clients.<br>
Thus, a typical cluster of five servers can tolerate the failure of any two servers.<br>
Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster.
<br>They do not depend on timing to ensure the consistency of the logs:<br>
faulty clocks and extreme message delays can, at worst, cause availability problems.
<br>In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls;<br>
a minority of slow servers need not impact overall system performance.
<br>实际系统中的一致性算法通常具有以下属性：<br>
<br>它们确保在所有非拜占庭条件下的安全性(永远不返回错误结果)，(非拜占庭条件)包括网络延迟，分区，和丢包，重复以及重新排序。
<br>只要大多数服务器能够正常工作并且能够与其它服务器以及客户端互相通信，一致性算法就能发挥其全部的功能(可用性)。<br>
因此，一个典型的有着5台服务器组成的集群能够容忍任意两台服务器出现故障。<br>
假设服务器因为故障而停机；他们可以稍后从稳定的存储状态中恢复并重新加入集群。
<br>他们不依赖时间来确保日志的一致性：错误的时钟和极端的消息延迟在最坏的情况下会造成可用性问题。
<br>通常情况下，只要集群中的大多数对单轮的远过程调用做出了响应，命令就可以完成。占少数的慢速服务器不会对系统整体性能造成影响。
<br><br>Over the last ten years, Leslie Lamport’s Paxos protocol has become almost synonymous with consensus:<br>
it is the protocol most commonly taught in courses, and most implementations of consensus use it as a starting point.<br>
Paxos first defines a protocol capable of reaching agreement on a single decision, such as a single replicated log entry.<br>
We refer to this subset as single-decree Paxos.<br>
Paxos then combines multiple instances of this protocol to facilitate a series of decisions such as a log (multi-Paxos).<br>
Paxos ensures both safety and liveness, and it supports changes in cluster membership.<br>
Its correctness has been proven, and it is efficient in the normal case.<br>在过去的十年中，Leslie Lamport的Paxos协议几乎已经成为了一致性算法的代名词：<br>
它是课堂教学中最常用的协议，大多数的一致性算法也将其作为起点。<br>
Paxos首先定义了一个协议，其能够就单个决定达成一致，例如单个日志条目的复制。<br>
我们将这一自己称为single-decree Paxos。<br>
然后Paxos将该协议的多个实例组合起来以达成一系列的决定，例如日志(multi-Paxos)。<br>
Paxos同时保证了安全性和活性，并且支持集群成员的变更。<br>
其正确性已经得到证明，并且在通常情况下是高效的。<br>Unfortunately, Paxos has two significant drawbacks.<br>
The first drawback is that Paxos is exceptionally difficult to understand.<br>
The full explanation is notoriously opaque; few people succeed in understanding it, and only with great effort.<br>
As a result, there have been several attempts to explain Paxos in simpler terms.<br>
These explanations focus on the single-decree subset, yet they are still challenging.<br>
In an informal survey of attendees at NSDI 2012, we found few people who were comfortable with Paxos, even among seasoned researchers.<br>
We struggled with Paxos ourselves;<br>
we were not able to understand the complete protocol until after reading several simplified explanations<br>
and designing our own alternative protocol, a process that took almost a year.<br>不幸的是，Paxos有着两个明显的缺点。<br>
第一个缺点是Paxos异乎寻常的难理解。<br>
Paxos出了名的难理解，即使在付出了巨大努力的情况下，也很少有人能成功的理解它。<br>
因此，有一些人尝试着用更简单的方式来理解Paxos。<br>
这些解释聚焦于single-decree这一子集，但这仍具有挑战性。<br>
在一项针对NSDI 2012与会者的非正式调查中，我们发现很少有人对Paxos感到满意，即使对于经验丰富的研究员来说也是如此。<br>
我们也与Paxos进行了艰难的斗争；直到阅读了几个简化的解释并设计了我们自己的替代方案后我们才能够理解完整的协议，而这个过程花费了将近一年的时间。<br>We hypothesize that Paxos’ opaqueness derives from its choice of the single-decree subset as its foundation.<br>
Single-decree Paxos is dense and subtle:<br>
it is divided into two stages that do not have simple intuitive explanations and cannot be understood independently.<br>
Because of this, it is difficult to develop intuitions about why the single-decree protocol works.<br>
The composition rules for multi-Paxos add significant additional complexity and subtlety.<br>
We believe that the overall problem of reaching consensus on multiple decisions (i.e., a log instead of a single entry)<br>
can be decomposed in other ways that are more direct and obvious.<br>我们猜定Paxos晦涩难懂的原因在于作者选择以single-decree这一子集作为Paxos的基础。<br>
Single-decree Paxos是难理解和精巧的：<br>
它被分为了两个阶段，并且没有简单直接的说明，每一阶段也无法单独的理解。<br>
正因如此，很难凭借直觉的理解single-decree协议为什么能够工作。<br>
multi-Paxos的组合规则也显著的增加了复杂性和微妙之处。<br>
我们认为，就多个决定达成一致的总体问题(例如，使用日志而不是单个的entry)能够被分解为其它更直接和更容易理解的方式。<br>The second problem with Paxos is that it does not provide a good foundation for building practical implementations.<br>
One reason is that there is no widely agreed-upon algorithm for multi-Paxos.<br>
Lamport’s descriptions are mostly about single-decree Paxos;<br>
he sketched possible approaches to multi-Paxos, but many details are missing.<br>
There have been several attempts to flesh out and optimize Paxos, such as [26], [39], and [13],<br>
but these differ from each other and from Lamport’s sketches.<br>
Systems such as Chubby [4] have implemented Paxos-like algorithms, but in most cases their details have not been published.<br>Paxos的第二个问题是它没有为构建实际可行的实现提供一个好的基础。<br>
其中一个原因是对于multi-Paxos没有一个被广泛认同的算法。<br>
Lamport的描述大多数都是关于single-decree Paxos的；他简要的概述了实现multi-Paxos的可行的方法，但缺失了很多的细节。<br>
已经有几个(团队)试图去具体化和优化Paxos，例如[26],[39]和[13],但这些尝试彼此间不同且也不同于Lamport的概述。<br>
像Chubby系统已经实现了类似Paxos的算法，但大多数情况下的细节并没有被公开。<br>Furthermore, the Paxos architecture is a poor one for building practical systems;<br>
this is another consequence of the single-decree decomposition.<br>
For example, there is little benefit to choosing a collection of log entries independently and then melding them into a sequential log;<br>
this just adds complexity.<br>
It is simpler and more efficient to design a system around a log,<br>
where new entries are appended sequentially in a constrained order.<br>
Another problem is that Paxos uses a symmetric peer-to-peer approach at its core<br>
(though it eventually suggests a weak form of leadership as a performance optimization).<br>
This makes sense in a simplified world where only one decision will be made, but few practical systems use this approach.<br>
If a series of decisions must be made, it is simpler and faster to first elect a leader, then have the leader coordinate the decisions.<br>此外，Paxos的架构在构建实际的系统时表现不佳；这是对single-decree进行分解的另一个结果。<br>
例如，选择一组独立的日志集合并将其合并到一个顺序日志中几乎没有带来什么好处；这只会增加复杂性。<br>
围绕日志来设计系统会更简单和更高效，其中新的日志条目以受约束的顺序追加。<br>
另一个问题是，Paxos使用了一种对称的点对点(P2P)方法作为其核心(尽管最后提出了一种更弱形式的leadership作为性能优化)。<br>
在一个只需要做一次决定的，被简化的世界中这样是行得通的，但很少有实际的系统使用这个方式。<br>
如果有一系列的决定必须要做，首先选举出一个leader，然后leader来协调决策会更简单和更快速。<br>As a result, practical systems bear little resemblance to Paxos.<br>
Each implementation begins with Paxos, discovers the difficulties in implementing it,<br>
and then develops a significantly different architecture.<br>
This is time-consuming and error-prone, and the difficulties of understanding Paxos exacerbate the problem.<br>
Paxos’ formulation may be a good one for proving theorems about its correctness,<br>
but real implementations are so different from Paxos that the proofs have little value.<br>The following comment from the Chubby implementers is typical:<br>
There are significant gaps between the description of the Paxos algorithm<br>
and the needs of a real-world system. . . . the final system will be based on an unproven protocol [4].<br>因此，实际的系统与Paxos几乎没有相似之处。<br>
每一个实现都从Paxos出发，发现实现Paxos的困难之处，然后开发出一个与之截然不同的架构。<br>
这既耗费时间又容易出错，并且Paxos的晦涩难懂加剧了这一问题。<br>
Paxos的公式可能可以很好的证明其正确性，但是实际的实现与Paxos是如此的不同，以至于这些证明几乎毫无价值。<br>以下Chubby实现者的评论是具有代表性的：<br>
Paxos算法的描述与现实世界系统的需求之间有着巨大的鸿沟....最终的系统将建立在一个未被证明的协议之上。<br>Because of these problems, we concluded that Paxos does not provide a good foundation either for system building or for education.<br>
Given the importance of consensus in large-scale software systems,<br>
we decided to see if we could design an alternative consensus algorithm with better properties than Paxos.<br>
Raft is the result of that experiment.<br>由于这些问题，我们的结论是Paxos并没有为构建系统或是进行教育提供一个好的基础。<br>
考虑到一致性在大规模软件系统中的重要性，我们决定看看我们是否可以设计出一个比起Paxos有着更好特性的一致性算法。<br>
Raft正是这一实验的成果。<br><br>We had several goals in designing Raft: it must provide a complete and practical foundation for system building,<br>
so that it significantly reduces the amount of design work required of developers;<br>
it must be safe under all conditions and available under typical operating conditions; and it must be efficient for common operations.<br>
But our most important goal—and most difficult challenge—was understandability.<br>
It must be possible for a large audience to understand the algorithm comfortably.<br>
In addition, it must be possible to develop intuitions about the algorithm,<br>
so that system builders can make the extensions that are inevitable in real-world implementations.<br>我们在设计Raft时有几个目标：它必须为构建系统提供一个完整的和实际的基础，从而显著的减少开发者设计时所需的工作；<br>
它必须在任何条件下都是安全的并且在典型的工作状态下是可用的；同时它必须在通常工作状态下是高效的。<br>
但我们最重要的目标也是最困难的挑战是使得Raft通俗易懂。<br>
必须尽可能的使大多数人能够轻松的理解该算法。<br>
这样系统构建者才能够在现实世界的实现中进行不可避免的拓展。<br>There were numerous points in the design of Raft where we had to choose among alternative approaches.<br>
In these situations we evaluated the alternatives based on understandability:<br>
how hard is it to explain each alternative (for example, how complex is its state space,<br>
and does it have subtle implications?), and how easy will it be for a reader to completely understand the approach and its implications?<br>在设计Raft时有很多要点都必须在多个可选方案中抉择。<br>
在这些情况下，我们基于易懂性来评估这些可选方案：<br>
对于每一个可选方案解释起来有多困难(例如，状态空间有多复杂以及是否有微妙的含义？)，以及对于一个读者来说完全理解这个方法和其含义有多容易？<br>We recognize that there is a high degree of subjectivity in such analysis; nonetheless, we used two techniques that are generally applicable.<br>
The first technique is the well-known approach of problem decomposition:<br>
wherever possible, we divided problems into separate pieces that could be solved, explained, and understood relatively independently.<br>
For example, in Raft we separated leader election, log replication, safety, and membership changes.<br>我们意识到这一分析方式具有高度的主观性；尽管如此，但我们还是使用了两种可行的通用技术。<br>
第一个技术是众所周知的问题分解方法：<br>
在可能的情况下，我们将问题分解为几个部分，使得每一部分都可以被相对独立的解决，解释和理解。<br>
例如，我们将Raft分解为leader选举，日志复制，安全性和成员变更这几个部分。<br>Our second approach was to simplify the state space by reducing the number of states to consider,<br>
making the system more coherent and eliminating nondeterminism where possible.<br>
Specifically, logs are not allowed to have holes, and Raft limits the ways in which logs can become inconsistent with each other.<br>
Although in most cases we tried to eliminate nondeterminism, there are some situations where nondeterminism actually improves understandability.<br>
In particular, randomized approaches introduce nondeterminism,<br>
but they tend to reduce the state space by handling all possible choices in a similar fashion(“choose any; it doesn’t matter”).<br>
We used randomization to simplify the Raft leader election algorithm.<br>我们的第二种方法是通过减少需要考虑的状态数量以简化状态空间，使系统变得更加连贯并尽可能的消除不确定性。<br>
特别的，日志是不允许存在空洞的，并且Raft限制了使得日志间变得彼此不一致的方式。<br>
尽管在大多数情况下我们试图消除不确定性，但在一些条件下不确定性实际上能提高可理解性。<br>
特别的，随机化方法引入了不确定性，但它们倾向于通过用相似的方式来处理所有可能的选择以减少状态空间("选择任意一个;具体是哪一个则无关紧要")。<br>
我们使用随机化来简化Raft中的leader选举算法。<br><br>Raft is an algorithm for managing a replicated log of the form described in Section 2.<br>
Figure 2 summarizes the algorithm in condensed form for reference, and Figure 3 lists key properties of the algorithm;<br>
the elements of these figures are discussed piecewise over the rest of this section.<br>Raft是一种管理如第二节所述的复制日志的算法。<br>
图2以简明扼要的总结了算法以供参考，图3列举出了算法的关键特性；这些图中的元素将在本节剩余的部分中进行讨论。<br>Raft implements consensus by first electing a distinguished leader,<br>
then giving the leader complete responsibility for managing the replicated log.<br>
The leader accepts log entries from clients,<br>
replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines.<br>
Having a leader simplifies the management of the replicated log.<br>
For example, the leader can decide where to place new entries in the log without consulting other servers,<br>
and data flows in a simple fashion from the leader to other servers.<br>
A leader can fail or become disconnected from the other servers, in which case a new leader is elected.<br>Raft通过受限选举出一位distinguished leader，然后让它全权的管理复制日志以实现一致性。<br>
这个leader接受来自客户端的日志条目，将其复制到其它服务器中，并且在日志条目可以被安全的应用在它们的状态机上时通知这些服务器。<br>
拥有一个leader可以简化对复制日志的管理。<br>
例如，leader可以决定新日志条目的位置而无需咨询其它服务器，并且数据流以一种简单的形式由leader流向其它服务器。<br>
leader可能会故障或者与其它服务器失联，这种情况下一位新的leader将会被选举出来。<br>Given the leader approach, Raft decomposes the consensus problem into three relatively independent sub-problems,<br>
which are discussed in the subsections that follow:<br>
<br>Leader election: a new leader must be chosen when an existing leader fails (Section 5.2).
<br>Log replication: the leader must accept log entries from clients and replicate them across the cluster,<br>
forcing the other logs to agree with its own (Section 5.3).
<br>Safety: the key safety property for Raft is the State Machine Safety Property in Figure 3:<br>
if any server has applied a particular log entry to its state machine,<br>
then no other server may apply a different command for the same log index.<br>
Section 5.4 describes how Raft ensures this property;<br>
the solution involves an additional restriction on the election mechanism described in Section 5.2.
<br>After presenting the consensus algorithm, this section discusses the issue of availability and the role of timing in the system.<br>通过引入leader的方法，Raft将一致性问题分解为3个相对独立的子问题，这些子问题将在以下子章节中被讨论：<br>
<br>leader选举： 当一位现存的leader故障时必须选出一位新的leader(5.2节)。
<br>日志复制： leader必须从客户端接收日志条目并且在集群中复制它们，并且强制其它节点的日志与leader保持一致(5.3节)。
<br>安全性： Raft的关键安全特性就是图3中的状态机的安全特性：如果任一服务器已经将一个特定的日志条目作用于它的状态机，则没有任何服务器可以对相同的日志索引应用不同的指令。<br>
5.4节描述了Raft是如何确保这一特性的；这一解决方案涉及到对5.2节中所描述的选举机制的额外限制。
<br>在展示了一致性算法后，本章节还将讨论可用性问题以及时序在系统中起到的作用。<br><img alt="Pasted image 20240725172955.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172955.png"><br>A condensed summary of the Raft consensus algorithm (excluding membership changes and log compaction).<br>
The server behavior in the upper-left box is described as a set of rules that trigger independently and repeatedly.<br>
Section numbers such as §5.2 indicate where particular features are discussed.<br>
A formal specification [31] describes the algorithm more precisely.<br>关于Raft一致性算法的精简摘要(不包括成员变更和日志压缩)。<br>
左上方框内所描述的服务器行为被描述为一系列独立和重复触发的规则。<br>
章节编号例如§5.2标识了具体讨论该特定功能的章节。<br>
形式化规约以更精确的方式描述该算法。<br><img alt="Pasted image 20240725173009.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173009.png"><br>Figure 3: Raft guarantees that each of these properties is true at all times.<br>
The section numbers indicate where each property is discussed.<br>图3：Raft保证每一个特性在任何时候都是成立的、名副其实的。<br>
章节号标识着每一个特性被讨论的具体章节。<br><br>A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures.<br>
At any given time each server is in one of three states: leader, follower, or candidate.<br>
In normal operation there is exactly one leader and all of the other servers are followers.<br>
Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates.<br>
The leader handles all client requests (if a client contacts a follower, the follower redirects it to the leader).<br>
The third state, candidate, is used to elect a new leader as described in Section 5.2.<br>
Figure 4 shows the states and their transitions; the transitions are discussed below.<br>一个Raft的集群包含几个服务器;通常是5个节点，这样的系统能容忍系统中的2个节点出现故障。<br>
在任一给定的时间内，每个服务器只会处于3种状态中的一种：领导者(leader),追随者(follower)，或者候选者(candidate)。<br>
在通常情况下，只会有1个leader并且其它的服务器都是follower。<br>
Follower都是被动的: 它们自己不会提出请求而只会简单的响应来自leader和candidate的请求。<br>
leader处理所有来自客户端的请求(如果一个客户端与follower进行联络，follower会将其重定向到leader)。<br>
第三种状态，candidate，用于选举出一个如5.2章节所描述的新leader。<br>
图4展示了状态以及状态间的转换关系；转换关系将在下文被讨论。<br>Raft divides time into terms of arbitrary length, as shown in Figure 5.<br>
Terms are numbered with consecutive integers.<br>
Each term begins with an election, in which one or more candidates attempt to become leader as described in Section 5.2.<br>
If a candidate wins the election, then it serves as leader for the rest of the term.<br>
In some situations an election will result in a split vote.<br>
In this case the term will end with no leader; a new term (with a new election) will begin shortly.<br>
Raft ensures that there is at most one leader in a given term.<br>Raft将时间分割为任意长度的任期(term)，如图5所示。<br>
任期由连续的整数进行编号。<br>
每一个任期都以一次选举开始，其中一个或更多的candidate试图成为leader(如5.2节中所描述的)。<br>
如果一个candidate赢得了选举，然后它将在余下的任期中作为leader。<br>
在一些情况下一次选举可能会导致分裂的投票结果。<br>
在这种情况下，任期将在没有leader的情况下结束; 一个新的任期(伴随者一个新的选举)将很快开始。<br>
Raft保证了在一个给定的任期内最多只会有一个leader。<br><img src="https://img2023.cnblogs.com/blog/1506329/202307/1506329-20230713200905466-706345890.png" referrerpolicy="no-referrer"><br>Figure 4: Server states. Followers only respond to requests from other servers.<br>
If a follower receives no communication, it becomes a candidate and initiates an election.<br>
A candidate that receives votes from a majority of the full cluster becomes the new leader.<br>
Leaders typically operate until they fail.<br>图4：服务器状态。<br>
follower只能响应来自其它服务器的请求。<br>
如果follower没有收到通信，它将成为一名candidate并且初始化一场选举。<br>
一位candidate收到了来自整个集群中的大多数投票则成为新的leader。<br>
leader通常持续工作直到它们发生故障。<br><img alt="Pasted image 20240725173021.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173021.png"><br>Figure 5: Time is divided into terms, and each term begins with an election.<br>
After a successful election, a single leader manages the cluster until the end of the term.<br>
Some elections fail, in which case the term ends without choosing a leader.<br>
The transitions between terms may be observed at different times on different servers.<br>图5：时间以任期进行划分，每一个任期都以一次选举开始。<br>
在成功的选举之后，一个leader管理集群直到任期结束。<br>
有些选举失败了，在这种情况下任期结束时并没有选出一个leader。<br>
可以在不同服务器的不同时间上观察到任期的转换。<br>Different servers may observe the transitions between terms at different times,<br>
and in some situations a server may not observe an election or even entire terms.<br>
Terms act as a logical clock [14] in Raft, and they allow servers to detect obsolete information such as stale leaders.<br>
Each server stores a current term number, which increases monotonically over time.<br>
Current terms are exchanged whenever servers communicate;<br>
if one server’s current term is smaller than the other’s, then it updates its current term to the larger value.<br>
If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state.<br>
If a server receives a request with a stale term number, it rejects the request.<br>不同服务器可能会在不同的时间上观察到任期之间的状态转换，并且在一些情况下一个服务器可能不会观察到一次选举甚至整个任期。<br>
任期在Raft中充当逻辑时钟，并且它们允许服务器检测到过时的信息比如之前的、老leader。<br>
每一个服务器存储了一个当前任期的编号，其随着时间单调增加。<br>
每当服务器之间互相通信时，它们都会互相交换当前的任期(编号);如果一个服务器的当前任期(编号)小于其它的服务器，则其将会将当前的任期(编号)更新为那个更大的值。<br>
如果一个candidate或者leader发现它们的任期(编号)已经过时，它将立即将自己恢复为follower的状态。<br>
如果一个服务器接受到一个带有过时任期编号的请求，它将拒绝这一请求。<br>Raft servers communicate using remote procedure calls(RPCs), and the basic consensus algorithm requires only two types of RPCs.<br>
RequestVote RPCs are initiated by candidates during elections (Section 5.2),<br>
and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat (Section 5.3).<br>
Section 7 adds a third RPC for transferring snapshots between servers.<br>
Servers retry RPCs if they do not receive a response in a timely manner, and they issue RPCs in parallel for best performance.<br>Raft服务器使用远过程调用(RPC)进行通信，并且基本的一致性算法只需要两种类型的RPC。<br>
请求投票的RPC由candidate在选举期间发起(第5.2节)，拓展条目的RPC由leader发起，用于日志条目的复制以及提供心跳机制(第5.3节)。<br>
第7节加入了第三种RPC用于在服务器间传输快照。<br>
如果服务器在给定的时间内没有收到响应，则会对RPC进行重试，并且它们会发起并行的rpc以获得最好的性能。<br><br>Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers.<br>
A server remains in follower state as long as it receives valid RPCs from a leader or candidate.<br>
Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority.<br>
If a follower receives no communication over a period of time called the election timeout,<br>
then it assumes there is no viable leader and begins an election to choose a new leader.<br>Raft使用心跳机制来触发leader选举。当服务器启动时，它们会成为follower。<br>
只要服务器能从leader或者candidate处接收到有效的RPC请求，它们就将保持follower状态。<br>
leader向所有follower发送周期性的心跳(不携带日志条目的AppendEntries RPC)来维持它的权威性。<br>
如果一个follower在一段被成为选举超时的时间段内未接收到任何通信，则它假设当前没有可用的leader并且发起选举来选择一个新的leader。<br>To begin an election, a follower increments its current term and transitions to candidate state.<br>
It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster.<br>
A candidate continues in this state until one of three things happens:<br>
(a) it wins the election,<br>
(b) another server establishes itself as leader, or<br>
(c) a period of time goes by with no winner.<br>
These outcomes are discussed separately in the paragraphs below.<br>为了开始一轮选举，follower增加它当前的任期值并且转换为candidate状态。<br>
然后它将选票投给它自己并且向集群中的其它服务器并行的发起请求投票的RPC(RequestVote RPCs)。<br>
一个candidate会一直保持这种状态直到以下三种情况之一发生：<br>
(a) 它赢得此次选举 (b) 另一个服务器将自己确认为leader，或者 (c) 一段时间后没有产生胜利者。<br>
下文中的各个段落将分别讨论这些结果。<br>A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term.<br>
Each server will vote for at most one candidate in a given term, on a first-come-first-served basis<br>
(note: Section 5.4 adds an additional restriction on votes).<br>
The majority rule ensures that at most one candidate can win the election for a particular term (the Election Safety Property in Figure 3).<br>
Once a candidate wins an election, it becomes leader.<br>
It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.<br>如果一个candidate在同一个任期内接收到了整个集群中大多数服务器的投票，其将赢得这次选举。<br>
每个服务器在给定的某一任期内将会基于先来先服务的原则(first-come-first-served)投票给至多一位candidate(第5.4节对投票增加了额外的限制)。<br>
多数规则确保了对于一个特定的任期，最多只会有一名candidate能够赢得选举(图3中选举的安全特性)。<br>
一旦一个candidate赢得了一次选举，它将成为leader。<br>
然后它向其它服务器发送心跳信息以建立权威并且阻止新的选举。<br>While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader.<br>
If the leader’s term (included in its RPC) is at least as large as the candidate’s current term,<br>
then the candidate recognizes the leader as legitimate and returns to follower state.<br>
If the term in the RPC is smaller than the candidate’s current term, then the candidate rejects the RPC and continues in candidate state.<br>在等待投票时，一个candidate可能会接受到来自自称是leader的其它服务器的AppendEntries RPC。<br>
如果leader的任期(包含在它的RPC中)大于或等于candidate的当前任期，那么candidate承认该leader是合法的并且返回到follower状态。<br>
如果RPC中的任期小于candidate的当前任期，candidate将会拒绝这一RPC并且继续保持candidate的状态。<br>The third possible outcome is that a candidate neither wins nor loses the election:<br>
if many followers become candidates at the same time, votes could be split so that no candidate obtains a majority.<br>
When this happens, each candidate will time out and start a new election by incrementing its term<br>
and initiating another round of RequestVote RPCs.<br>
However, without extra measures split votes could repeat indefinitely.<br>第三种可能的结果是一个candidate既没有赢得选举也没有输掉选举：<br>
如果许多follower都在同一时间成为了candidate，投票可能会被瓜分导致没有candidate获得大多数的选票。<br>
当这种情况发生时，每一个candidate都将会超时并且通过增加它的任期值并且初始化另一轮的RequestVote RPCs以开始一轮新的选举。<br>
然而，如果不采取额外的措施，分裂的投票可能会无限的重复。<br>Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly.<br>
To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms).<br>
This spreads out the servers so that in most cases only a single server will time out;<br>
it wins the election and sends heartbeats before any other servers time out.<br>
The same mechanism is used to handle split votes.<br>
Each candidate restarts its randomized election timeout at the start of an election, and it waits for that timeout to elapse before<br>
starting the next election; this reduces the likelihood of another split vote in the new election.<br>
Section 9.3 shows that this approach elects a leader rapidly.<br>Raft使用随机化的选举超时时间来确保分裂的投票很少会发生并使得它们能够被迅速的解决。<br>
为了防止一开始就出现分裂的投票，选举的超时时间是从一个固定的间隔中被随机选取的(例如150-300ms)。<br>
这打散了服务器使得在大多数情况下只有单独一个服务器将会超时；它赢得选举并且在其它服务器超时之前发送心跳(译者注：超时后自己就会在别的服务器没反应过来前发起新一轮任期更大的投票，让别人都投给它来赢得选举)。<br>
同样的机制也被用于解决分裂的投票。<br>
每个candidate在一轮选举开始时会重新随机的设置其选举超时时间，并且在下一轮选举前等待直到超时；这减少了在新的选举中再一次出现分裂投票的可能性。<br>
第9.3节展示了该方法能迅速的选举出一个leader。<br>Elections are an example of how understandability guided our choice between design alternatives.<br>
Initially we planned to use a ranking system: each candidate was assigned a unique rank, which was used to select between competing candidates.<br>
If a candidate discovered another candidate with higher rank,<br>
it would return to follower state so that the higher ranking candidate could more easily win the next election.<br>
We found that this approach created subtle issues around availability<br>
(a lower-ranked server might need to time out and become a candidate again if a higher-ranked server fails,<br>
but if it does so too soon, it can reset progress towards electing a leader).<br>
We made adjustments to the algorithm several times, but after each adjustment new corner cases appeared.<br>
Eventually we concluded that the randomized retry approach is more obvious and understandable.<br>选举是一个可理解性如何指导我们在可选设计间进行选择的例子。<br>
最初，我们计划使用等级系统(ranking system)：每一个candidate都被分配一个唯一的等级，其用于在彼此竞争的candidate做选择。<br>
如果一个candidate发现了一个具有更高等级的candidate，它将返回到follower状态因此更好等级的candidate将更容易赢得下一次选举。<br>
但我们发现这个方法在可用性方面存在微妙的问题(如果一个高等级的服务器故障了，则一个低等级的服务器可能需要超时并再次成为candidate，但如果这样做的太早，它将会重置选举leader的进度)。<br>
我们对算法进行了数次调整，但每次调整后都出现了新的困境。<br>
最终我们得出结论，随机化重试的方法更直观且更容易被理解。<br><br>Once a leader has been elected, it begins servicing client requests.<br>
Each client request contains a command to be executed by the replicated state machines.<br>
The leader appends the command to its log as a new entry,<br>
then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry.<br>
When the entry has been safely replicated (as described below),<br>
the leader applies the entry to its state machine and returns the result of that execution to the client.<br>
If followers crash or run slowly, or if network packets are lost,<br>
the leader retries AppendEntries RPCs indefinitely (even after it has responded to the client)<br>
until all followers eventually store all log entries.<br>一旦一个leader被选举出来，它将开始服务于客户端的请求。<br>
每一个客户端的请求都包含了一个被用于在复制状态机上执行的指令。<br>
leader将指令作为一个新的条目追加到其日志中，然后向其它的每个服务器发起并行的AppendEntries RPC令它们复制这一条目。<br>
当条目已被安全的被复制(如下所述)，leader在它的状态机上应用这一条目并且将执行的结果返回给客户端。<br>
如果follower崩溃了或者运行的很慢，或者网络丢包，leader会无限的重试AppendEntries RPC(即使在响应了客户端的请求之后)，<br>
直到所有的follower最终都存储了所有的日志条目。<br><img alt="Pasted image 20240725173038.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173038.png"><br>Figure 6: Logs are composed of entries, which are numbered sequentially.<br>
Each entry contains the term in which it was created (the number in each box) and a command for the state machine.<br>
An entry is considered committed if it is safe for that entry to be applied to state machines.<br>图6：日志由按照顺序编号的条目组成。<br>
每一个条目都包含它被创建时的任期(框中的数字)以及用于状态机的指令。<br>
如果条目已经安全的被作用于状态机，则该条目被视为已提交。<br>Logs are organized as shown in Figure 6.<br>
Each log entry stores a state machine command along with the term number when the entry was received by the leader.<br>
The term numbers in log entries are used to detect inconsistencies between logs and to ensure some of the properties in Figure 3.<br>
Each log entry also has an integer index identifying its position in the log.<br>日志如图6所示的方式被组织。<br>
每一个日志条目存储了一个状态机的指令，以及从leader处接受条目时的任期编号。<br>
日志条目中的任期编号被用于检测日志间的不一致，并且用于保证图3中的一些特性。<br>
每个日志条目也有一个整数的索引标识其在日志中的位置。<br>The leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed.<br>
Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines.<br>
A log entry is committed once the leader that created the entry has replicated it on a majority of the servers (e.g., entry 7 in Figure 6).<br>
This also commits all preceding entries in the leader’s log, including entries created by previous leaders.<br>
Section 5.4 discusses some subtleties when applying this rule after leader changes,<br>
and it also shows that this definition of commitment is safe.<br>
The leader keeps track of the highest index it knows to be committed,<br>
and it includes that index in future AppendEntries RPCs (including heartbeats) so that the other servers eventually find out.<br>
Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order).<br>leader决定何时能安全的在状态机上应用日志条目；这样的条目被称作已提交的日志。<br>
Raft保证已提交的条目都会被持久化并且最终将会在所有可用的状态机上被执行。<br>
一旦被创建的条目被大多数服务器所复制，leader就会将其提交(例如，图6中的条目7)。<br>
同时也会提交leader日志中更早之前的所有条目，其中包括被前任leader们所创建的条目。<br>
第5.4节讨论了在leader变更时应用这一规则的微妙之处，同时它也证明了所承诺的定义是安全的。<br>
leader持续的跟踪它已知的被提交日志的最大索引值，并且将索引值包含在未来的AppendEntries RPC中(包括心跳)，以便其它的服务器最终能知道(最大编号的已提交索引)。<br>
一旦一个follower知道一个日志条目已被提交，它便将这一条目应用于本地的状态机(基于日志的顺序)。<br>We designed the Raft log mechanism to maintain a high level of coherency between the logs on different servers.<br>
Not only does this simplify the system’s behavior and make it more predictable, but it is an important component of ensuring safety.<br>
Raft maintains the following properties, which together constitute the Log Matching Property in Figure 3:<br>
<br>If two entries in different logs have the same index and term, then they store the same command.
<br>If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.
<br>我们设计了Raft日志机制，其用于维持不同服务器之间日志的高度一致。<br>
其不仅仅简化了系统的行为，还使得它更加的可预测，同时这也是确保安全性的重要部分。<br>
Raft维护着以下特性，这些特性一并组成了图3中的日志匹配特性(Log Matching Property)：<br>
<br>如果不同日志中的两个条目有着相同的索引值和任期，则它们存储着相同的指令。
<br>如果不同日志中的两个条目有着相同的索引值和任期，则该日志之前的所有条目也都是完全相同的。
<br>The first property follows from the fact that a leader creates at most one entry with a given log index in a given term,<br>
and log entries never change their position in the log.<br>
The second property is guaranteed by a simple consistency check performed by AppendEntries.<br>
When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries.<br>
If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.<br>
The consistency check acts as an induction step: the initial empty state of the logs satisfies the Log Matching Property,<br>
and the consistency check preserves the Log Matching Property whenever logs are extended.<br>
As a result, whenever AppendEntries returns successfully,<br>
the leader knows that the follower’s log is identical to its own log up through the new entries.<br>第一个特性源自这样一个事实，即一个leader只会在特定任期内的某一索引值下最多只会创建一个条目，并且日志条目在日志中的位置是永远不会改变的。<br>
第二个特性则由AppendEntries执行一个简单的一致性检查来保证。<br>
当发送AppendEntries RPC时，leader将前一个条目的索引和任期包含在新条目中。<br>
如果follower没有找到一个具有相同索引值和任期的日志条目，则它将拒绝这一新条目。<br>
一致性检查就像一个归纳的步骤:初始化时的空状态满足日志匹配的特性(Log Matching Property)，并且每当扩展日志时，一致性检查都会维持日志匹配的特性。<br>
因此，每当AppendEntries返回成功时，通过新的条目leader就知道follower的日志与leader自己的是完全一致的，<br>During normal operation, the logs of the leader and followers stay consistent,<br>
so the AppendEntries consistency check never fails.<br>
However, leader crashes can leave the logs inconsistent (the old leader may not have fully replicated all of the entries in its log).<br>
These inconsistencies can compound over a series of leader and follower crashes.<br>
Figure 7 illustrates the ways in which followers’ logs may differ from that of a new leader.<br>
A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader, or both.<br>
Missing and extraneous entries in a log may span multiple terms.<br>在正常操作期间，leader和follower的日志始终保持一致，因此AppendEntries的一致性检查从来不会失败。<br>
然而，leader奔溃会导致日志的不一致(老的leader可能没有将它所有的日志条目完全复制完成)。<br>
这些不一致可能会随着一系列的leader和follower的崩溃而加剧。<br>
图7说明了follower日志可能与新leader不同的方式。<br>
一个Follower可能缺少了之前leader中才有的条目，也可能拥有一些在新leader中不存在的额外的条目，或者这两种方式皆有。<br>
缺失的或者额外多出的条目可能涉及到多个任期。<br><img alt="Pasted image 20240725173049.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173049.png"><br>Figure 7: When the leader at the top comes to power, it is possible that any of scenarios (a–f) could occur in follower logs.<br>
Each box represents one log entry; the number in the box is its term.<br>
A follower may be missing entries (a–b), may have extra uncommitted entries (c–d), or both (e–f).<br>
For example, scenario (f) could occur if that server was the leader for term 2, added several entries to its log,<br>
then crashed before committing any of them; it restarted quickly, became leader for term 3, and added a few more entries to its log;<br>
before any of the entries in either term 2 or term 3 were committed, the server crashed again and remained down for several terms.<br>图7：当leader获得最高权力上台时，以下任何一种情况(a-f)都可能出现在follower的日志中。<br>
每一个框表示一个日志条目；框中的数字是它的任期。<br>
follower可能会缺少一些条目(a-b)，可能有一些额外的未提交的条目(c-d),或者两种情况皆有(e-f)。<br>
例如，如果一个服务器是任期2的leader，其增加了一些条目到它们的日志中，然后在提交这些日志条目之前崩溃了;<br>
它很快重新启动，成为了任期3的leader，并且增加了几个条目到它的日志中，在提交任期2或者任期3中的任何一个条目之前，这个服务器再次崩溃并且在后几个任期内一直处于停机状态，<br>
则会发生情况(f);<br>In Raft, the leader handles inconsistencies by forcing the followers’ logs to duplicate its own.<br>
This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log.<br>
Section 5.4 will show that this is safe when coupled with one more restriction.<br>在Raft中，leader通过强制follower复制它的日志来处理不一致问题。<br>
这意味着follower中存在冲突的日志条目将会被来自leader的日志给覆盖。<br>
第5.4节将展示在加上一个限制时，这将会是安全的。<br>To bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree,<br>
delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point.<br>
All of these actions happen in response to the consistency check performed by AppendEntries RPCs.<br>
The leader maintains a nextIndex for each follower, which is the index of the next log entry the leader will send to that follower.<br>
When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log (11 in Figure 7).<br>
If a follower’s log is inconsistent with the leader’s,<br>
the AppendEntries consistency check will fail in the next AppendEntries RPC.<br>
After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC.<br>
Eventually nextIndex will reach a point where the leader and follower logs match.<br>
When this happens, AppendEntries will succeed,<br>
which removes any conflicting entries in the follower’s log and appends entries from the leader’s log (if any).<br>
Once AppendEntries succeeds, the follower’s log is consistent with the leader’s, and it will remain that way for the rest of the term.<br>为了使得follower的日志与自己的保持一致，leader必须找到两个日志中一致的条目中最新的那个，<br>
删除follower日志中位于该点位之后的所有条目，并且将leader在该点位后的所有条目发送给follower。<br>
所有的这些动作都发生在对AppendEntries RPC的一致性检查工作的响应中。<br>
leader为每一个follower维护了一个nextIndex,这是leader将发送给follower的下一个日志条目的索引编号。<br>
当leader第一次掌权时，其将所有的nextIndex的值初始化为其最后一个日志索引值再加1(图7中的11)。<br>
如果follower的日志与leader的不一致，AppendEntries的一致性检查将会在下一次AppendEntries RPC中失败。<br>
在一次拒绝后，leader将会递减nextIndex并且重试AppendEntries RPC。<br>
最终nextIndex将会到达一个leader与follower的日志想匹配的点位。<br>
当这一情况发生时，AppendEntries将会成功，其将删除follower日志中的所有冲突的条目并且追加来自leader日志中的条目(如果需要的话)。<br>
一旦AppendEntries成功，follower的日志将会与leader一致，并且在本任期内接下来的时间内保持一致。<br>If desired, the protocol can be optimized to reduce the number of rejected AppendEntries RPCs.<br>
For example, when rejecting an AppendEntries request,<br>
the follower can include the term of the conflicting entry and the first index it stores for that term.<br>
With this information, the leader can decrement nextIndex to bypass all of the conflicting entries in that term;<br>
one AppendEntries RPC will be required for each term with conflicting entries, rather than one RPC per entry.<br>
In practice, we doubt this optimization is necessary,<br>
since failures happen infrequently and it is unlikely that there will be many inconsistent entries.<br>如果有需要的话，协议可以通过减少被拒绝的AppendEntries RPCs数量来进行优化。<br>
例如，当一次AppendEntries请求被拒绝时，follower可以将包含对应任期的冲突条目和存储了对应任期的第一个索引值返回给leader。<br>
有了这些信息，leader递减nextIndex来避开对应任期内的所有冲突的条目;对于每一个任期的冲突条目，将只需要一次AppendEntries RPC，而不是一次RPC(处理)一个条目。<br>
在实践中，我们怀疑这一优化是否是必要的，因为很少发生故障并且不太可能有很多不一致的条目。<br>With this mechanism, a leader does not need to take any special actions to restore log consistency when it comes to power.<br>
It just begins normal operation, and the logs automatically converge in response to failures of the AppendEntries consistency check.<br>
A leader never overwrites or deletes entries in its own log (the Leader Append-Only Property in Figure 3).<br>有了这一机制，leader将不需要在掌权时使用任何特别的方法来恢复日志的一致性。<br>
它只是开始进行正常的操作，日志便会在响应AppendEntries的一致性检查时自动的趋于一致。<br>
leader从来不会覆盖或者删除它自己的日志(图3中leader的Append-Only特性)。<br>This log replication mechanism exhibits the desirable consensus properties described in Section 2:<br>
Raft can accept, replicate, and apply new log entries as long as a majority of the servers are up;<br>
in the normal case a new entry can be replicated with a single round of RPCs to a majority of the cluster;<br>
and a single slow follower will not impact performance.<br>这一日志复制机制展示了第2节中所描述的理想的一致性特性。<br>
只要大多数服务器是在线的，Raft便能接收，复制并且应用新的日志条目；<br>
在正常情况下一个新的条目可以通过单轮的RPC复制到集群中的大多数服务器上;并且单独的慢速的follower将不会影响性能。<br><br>The previous sections described how Raft elects leaders and replicates log entries.<br>
However, the mechanisms described so far are not quite sufficient to ensure<br>
that each state machine executes exactly the same commands in the same order.<br>
For example, a follower might be unavailable while the leader commits several log entries,<br>
then it could be elected leader and overwrite these entries with new ones;<br>
as a result, different state machines might execute different command sequences.<br>前面的章节描述了Raft是如何选举leader和复制日志条目的。<br>
然而，目前为止已描述的机制还不足以确保每一个状态机以相同的顺序准确地执行相同的指令。<br>
例如，当leader提交了几个日志条目后一个follower可能会变得不可用，随后follower可以被选举为leader并且用新的条目覆盖这些条目；<br>
因此，不同的状态机可能会执行不同的指令序列。<br>This section completes the Raft algorithm by adding a restriction on which servers may be elected leader.<br>
The restriction ensures that the leader for any given term contains all of the entries committed in previous terms<br>
(the Leader Completeness Property from Figure 3).<br>
Given the election restriction, we then make the rules for commitment more precise.<br>
Finally, we present a proof sketch for the Leader Completeness Property<br>
and show how it leads to correct behavior of the replicated state machine.<br>这一节通过增加一个对哪些服务器可以被选举为leader的限制来完善Raft算法。<br>
该限制确保leader对于给定的任期，其包含了所有之前任期的已提交条目（图3中的leader Completeness特性）。<br>
有了选举的限制，我们也使得关于提交的规则变得更加清晰。<br>
最后，我们给出了关于Leader Completeness的简要证明，并且展示了它是如何让复制状态机执行正确行为的。<br><br>In any leader-based consensus algorithm, the leader must eventually store all of the committed log entries.<br>
In some consensus algorithms, such as Viewstamped Replication [22],<br>
a leader can be elected even if it doesn’t initially contain all of the committed entries.<br>
These algorithms contain additional mechanisms to identify the missing entries and transmit them to the new leader,<br>
either during the election process or shortly afterwards.<br>
Unfortunately, this results in considerable additional mechanism and complexity.<br>
Raft uses a simpler approach where it guarantees<br>
that all the committed entries from previous terms are present on each new leader from the moment of its election,<br>
without the need to transfer those entries to the leader.<br>
This means that log entries only flow in one direction, from leaders to followers,<br>
and leaders never overwrite existing entries in their logs.<br>在任何基于leader的一致性算法中，leader必须最终存储所有已提交的日志条目。<br>
在一些一致性算法中，例如Viewstamped Replication，一个leader即使最初不包含所有已提交的条目也能被选举为leader。<br>
这些算法包含了额外的机制来识别缺失的条目并在选举过程中或选举后不久将其传输给新的leader。<br>
不幸的是，这带来了非常多的额外机制和复杂性。<br>
Raft使用了一种更简单的方法来确保每一个新的leader当选时都拥有之前任期的所有已提交的条目，而无需传输这些条目给leader。<br>
这意味着日志条目只会单方向的从leader向follower流动，并且leader从不覆盖它们已存在的条目。<br>Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries.<br>
A candidate must contact a majority of the cluster in order to be elected,<br>
which means that every committed entry must be present in at least one of those servers.<br>
If the candidate’s log is at least as up-to-date as any other log in that majority<br>
(where “up-to-date” is defined precisely below), then it will hold all the committed entries.<br>
The RequestVote RPC implements this restriction: the RPC includes information about the candidate’s log,<br>
and the voter denies its vote if its own log is more up-to-date than that of the candidate.<br>Raft使用投票机制来防止不包含所有已提交条目的candidate赢得选举。<br>
一个candidate必须与集群中的大多数成员联系后才能当选，这意味着每个提交的条目必须至少存在于其中的至少一个服务器中。<br>
如果candidate的日志至少和其它大多数的日志一样新(何为"最新"(up-to-date)将在下面被定义)，则它将持有所有已提交的条目。<br>
RequestVote RPC中实现了之一限制：RPC包括了candidate的日志信息，并且如果candidate的日志不如投票人(voter)的日志新，则voter将拒绝投票给该candidate。<br>Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs.<br>
If the logs have last entries with different terms, then the log with the later term is more up-to-date.<br>
If the logs end with the same term, then whichever log is longer is more up-to-date.<br>Raft通过比较两个日志中最后一个条目的索引和任期来决定谁是最新的。<br>
如果两个日志中最后的条目有着不同的任期，则任期较后的日志是更新的。<br>
如果两个日志中最后的条目有着相同的任期，则较长的(注：索引值更大的)那个日志是更新的。<br><br>As described in Section 5.3, a leader knows that an entry from its current term is committed once<br>
that entry is stored on a majority of the servers.<br>
If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry.<br>
However, a leader cannot immediately conclude that an entry from a previous term is committed once it is stored on a majority of servers.<br>
Figure 8 illustrates a situation where an old log entry is stored on a majority of servers,<br>
yet can still be overwritten by a future leader.<br>如5.3节所描述的那样，leader一旦知道当前任期内的一个条目被存储在了大多数的服务器中，就会将其提交。<br>
如果leader在提交一个条目前崩溃了，未来的leader将试图去完成该条目的复制。<br>
然而，leader无法立即得出结论，即一个来自之前任期的条目一旦被大多数服务器所存储就是已被提交的。<br>
图8展示了这样一种情况，一个老的日志条目被存储在了大多数的服务器上，但任然被未来的leader覆盖掉了。<br><img alt="Pasted image 20240725173101.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173101.png"><br>A time sequence showing why a leader cannot determine commitment using log entries from older terms.<br>
In(a) S1 is leader and partially replicates the log entry at index2.<br>
In (b) S1 crashes; S5 is elected leader for term 3 with votes from S3, S4, and itself, and accepts a different entry at log index 2.<br>
In (c) S5 crashes; S1 restarts, is elected leader, and continues replication.<br>
At this point, the log entry from term 2 has been replicated on a majority of the servers, but it is not committed.<br>
If S1 crashes as in (d), S5 could be elected leader (with votes from S2, S3, and S4) and overwrite the entry with its own entry from term 3.<br>
However, if S1 replicates an entry from its current term on a majority of the servers before crashing, as in (e),<br>
then this entry is committed (S5 cannot win an election).<br>
At this point all preceding entries in the log are committed as well.<br>一个时间序列，展示了为什么leader不能使用来自旧任期的日志条目来决定是否已提交。(注：S1-S5是集群中的5台服务器，a-e是时间序列)<br>
在(a)中S1是leader并且部分的复制了位于index2的日志条目。<br>
在(b)中S1崩溃了;S5通过任期3中来自S3，S4和它自己的投票而被选举为leader，并且接受了一个不同的条目在日志index2。<br>
在(c)中S5崩溃了;S1重新启动，被选举为了leader，并且继续复制。<br>
在这个时间点，来自任期2的日志条目已经被复制到了大多数服务器中，但还没有被提交。<br>
如果S1像(d)中那样崩溃了，S5可以被选举为leader(通过来自S2，S3,和S4的投票)并且用它自己的来自任期3的条目进行覆盖。<br>
然而，如果S1在崩溃前复制了来自它当前任期的条目在大多数服务器中，就像(e),则这一条目是已提交的(S5不能赢得选举)。<br>
此时日志中所有之前的条目都已经被提交。<br><img alt="Pasted image 20240725173109.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173109.png"><br>Figure 9: If S1 (leader for term T) commits a new log entry from its term, and S5 is elected leader for a later term U,<br>
then there must be at least one server (S3) that accepted the log entry and also voted for S5.<br>图9：如果S1(任期T的leader)提交了已给来自它任期的新日志条目，并且S5在后面的任期U被选举为leader,<br>
则至少有一个服务器(S3)能够接收该日志条目并且也投票给S5。<br>To eliminate problems like the one in Figure 8, Raft never commits log entries from previous terms by counting replicas.<br>
Only log entries from the leader’s current term are committed by counting replicas;<br>
once an entry from the current term has been committed in this way,<br>
then all prior entries are committed indirectly because of the Log Matching Property.<br>
There are some situations where a leader could safely conclude that an older log entry is committed<br>
(for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity.<br>为了消除像图8中那样的问题，Raft从来不基于副本数量来提交来自之前任期的日志条目。<br>
只有来自leader当前任期的日志条目才基于副本数量被提交，一旦一个来自当前任期的条目以这种方式被提交，则所有之前的条目都将由于Log Matching特性而间接的被提交。<br>
在一些情况下，leader可以安全的断定一个之前的log已经被提交(比如，如果一个entry已经被存储在每一个服务器上了)，但为了简单起见，Raft采取了一种更保守的方法。<br>Raft incurs this extra complexity in the commitment rules because log entries retain their original term numbers<br>
when a leader replicates entries from previous terms.<br>
In other consensus algorithms, if a new leader re-replicates entries from prior “terms,” it must do so with its new “term number.”<br>
Raft’s approach makes it easier to reason about log entries, since they maintain the same term number over time and across logs.<br>
In addition, new leaders in Raft send fewer log entries from previous terms<br>
than in other algorithms (other algorithms must send redundant log entries to renumber them before they can be committed).<br>Raft向提交规则中引入了额外的复杂性，因为当leader复制来自之前任期的条目时，这些日志条目会保留它原始的任期编号。<br>
在其它一致性算法中，如果一个新的leader要重新复制来自之前任期的条目，它必须使用新的任期编号。<br>
Raft的方法使得更容易理解日志条目，因为它们在不同服务器的日志中自始至终保留了相同的任期编号。<br>
额外的，相比其它算法，raft中的新leader会发送更少的来自之前任期的日志条目(其它算法必须发送冗余的日志条目以对让对应的日志条目在提交前重新进行编号)。<br><br>Given the complete Raft algorithm,<br>
we can now argue more precisely that the Leader Completeness Property holds (this argument is based on the safety proof; see Section 9.2).<br>
We assume that the Leader Completeness Property does not hold, then we prove a contradiction.<br>
Suppose the leader for term T (leaderT) commits a log entry from its term, but that log entry is not stored by the leader of some future term.<br>
Consider the smallest term U &gt; T whose leader (leaderU) does not store the entry.<br>在给出了完整的Raft算法后，我们可以更加准确的讨论leader的完整性(Completeness)特性是否成立了(这一讨论基于9.2节的安全性证明)。<br>
我们假设leader的Completeness特性不成立，则我们可以推到出矛盾来。<br>
假设任期T的leader(leaderT)提交了一个来自当前任期的日志条目，但该日志条目没有被未来某些任期的leader所存储。<br>
考虑一个大于T的最小任期U，其leader(leaderU)没有存储这个条目。<br>
<br>
The committed entry must have been absent from leaderU’s log at the time of its election (leaders never delete or overwrite entries).

<br>
leaderT replicated the entry on a majority of the cluster, and leaderU received votes from a majority of the cluster.<br>
Thus, at least one server (“the voter”) both accepted the entry from leaderT and voted for leaderU, as shown in Figure 9.<br>
The voter is key to reaching a contradiction.

<br>
The voter must have accepted the committed entry from leaderT before voting for leaderU;<br>
otherwise it would have rejected the AppendEntries request from leaderT (its current term would have been higher than T).

<br>
The voter still stored the entry when it voted for leaderU, since every intervening leader contained the entry (by assumption),<br>
leaders never remove entries, and followers only remove entries if they conflict with the leader.

<br>
The voter granted its vote to leaderU, so leaderU’s log must have been as up-to-date as the voter’s.<br>
This leads to one of two contradictions.

<br>
First, if the voter and leaderU shared the same last log term,<br>
then leaderU’s log must have been at least as long as the voter’s, so its log contained every entry in the voter’s log.<br>
This is a contradiction, since the voter contained the committed entry and leaderU was assumed not to.

<br>
Otherwise, leaderU’s last log term must have been larger than the voter’s.<br>
Moreover, it was larger than T, since the voter’s last log term was at least T (it contains the committed entry from term T).<br>
The earlier leader that created leaderU’s last log entry must have contained the committed entry in its log (by assumption).<br>
Then, by the Log Matching Property, leaderU’s log must also contain the committed entry, which is a contradiction.

<br>
This completes the contradiction. Thus, the leaders of all terms greater than T must contain all entries from term T<br>
that are committed in term T.

<br>
The Log Matching Property guarantees that future leaders will also contain entries that are committed indirectly,<br>
such as index 2 in Figure 8(d).

<br>
已提交的条目在leaderU当选时，必须不在leaderU的日志中(leader从来不会删除或者覆盖条目)。

<br>
leaderT将对应条目复制到了集群中的大多数(服务器)中,并且leaderU获得了来自集群中的大多的选票。<br>
因此，至少有一个服务器(作为voter)同时接收到了来自leaderT的条目并且投票给了leaderU.如图9所示。该voter是达成矛盾的关键所在。

<br>
voter必须在投票给leaderU之前接受来自leaderT的已提交的条目;<br>
否则其将拒绝来自leaderT的AppendEntries request(它当前的任期将已经高于T)。

<br>
voter在投票给leaderU时依然存储了该条目，因为每一个介于其中的leader(任期位于T和U之间)都包含了该条目，<br>
leader从不移除条目，并且follower只移除与leader相冲突的条目。

<br>
voter同意投票给leaderU,因此leaderU的日志必须至少与voter是一样新的。这带来了以下两个矛盾中的一个。

<br>
首先，如果voter和leaderU的最后一个日志有着相同的任期，则leaderU的日志必须至少与voter一样长，<br>
因此leaderU的日志包含了voter日志中的每一个条目。<br>
这是矛盾的，因为voter包含了已提交的条目而leaderU被假设为没有包含。

<br>
否则leaderU的最后一个日志的任期就必须比voter要大了。<br>
此外，任期的值也大于T，因为voter的最后一个日志的任期至少是T(其包含了来自任期T的所有已提交条目)。<br>
创建leaderU最后一个日志条目的更早的leader也必须包含这个日志(假设)。<br>
然后，基于Log Matching特性，leaderU的日志必须也包含已提交的条目，这是一个矛盾。

<br>
这就终结了矛盾。因此，所有任期大于T的leader必须包含所有的任期T内的已提交条目。

<br>
Log Matching特性保证了未来的leader也包含间接提交的日志，就像图8中的索引2。

<br>Given the Leader Completeness Property, we can prove the State Machine Safety Property from Figure 3,<br>
which states that if a server has applied a log entry at a given index to its state machine,<br>
no other server will ever apply a different log entry for the same index.<br>
At the time a server applies a log entry to its state machine,<br>
its log must be identical to the leader’s log up through that entry and the entry must be committed.<br>
Now consider the lowest term in which any server applies a given log index;<br>
the Log Completeness Property guarantees that the leaders for all higher terms will store that same log entry,<br>
so servers that apply the index in later terms will apply the same value.<br>
Thus, the State Machine Safety Property holds.<br>通过Leader Completeness特性，我们可以证明来自图3的State Machine Safety(安全状态机)特性，<br>
如果服务器将给定索引日志条目作用于状态机，其它的服务器将不能在相同的索引处应用不同的日志条目。<br>
一旦服务器应用了一个日志条目到其状态机上，其日志必须与传递该条目的leader的日志完全一样，并且这个条目必须被提交。<br>
现在考虑任一服务器应用给定日志索引的最小任期，Log Completeness特性保证了所有更高任期的leader将存储相同的日志条目，所以服务器在最晚任期所应用的索引将作用于相同的值。<br>
因此，State Machine Safety特性是成立的。<br>Finally, Raft requires servers to apply entries in log index order.<br>
Combined with the State Machine Safety Property,<br>
this means that all servers will apply exactly the same set of log entries to their state machines, in the same order.<br>最后，Raft要求服务器按照日志索引的顺序应用日志条目。<br>
结合State Machine Safety特性，这意味着所有的服务器将精确的以相同的顺序为它们的状态机应用一个相同的日志条目集合。<br><br>Until this point we have focused on leader failures.<br>
Follower and candidate crashes are much simpler to handle than leader crashes, and they are both handled in the same way.<br>
If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail.<br>
Raft handles these failures by retrying indefinitely; if the crashed server restarts, then the RPC will complete successfully.<br>
If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts.<br>
Raft RPCs are idempotent, so this causes no harm.<br>
For example, if a follower receives an AppendEntries request that includes log entries already present in its log,<br>
it ignores those entries in the new request.<br>在此之前我们一直聚焦于leader出故障的情况。<br>
follower和candidate的崩溃比起leader的崩溃会更加容易处理，并且它们都以相同的方式被处理。<br>
如果一个follower或者candidate崩溃了，则未来发送给它的投票请求(RequestVote)和AppendEntries RPC的发送将会失败。<br>
Raft通过无限的重试来处理这些失败，如果已崩溃的服务器重启了，则RPC将会成功的完成。<br>
如果服务器在完成了一个RPC但是在进行响应之前崩溃了，则它将会在重启后再一次接受到相同的RPC。<br>
Raft的RPC是幂等的，所以这不会有问题。<br>
例如，如果一个follower接受到的一个AppendEntries请求中包含的日志条目已经在它自己的日志中了，该follower就会在这次新的请求中忽略掉这些条目。<br><br>One of our requirements for Raft is that safety must not depend on timing:<br>
the system must not produce incorrect results just because some event happens more quickly or slowly than expected.<br>
However, availability (the ability of the system to respond to clients in a timely manner) must inevitably depend on timing.<br>
For example, if message exchanges take longer than the typical time between server crashes,<br>
candidates will not stay up long enough to win an election; without a steady leader, Raft cannot make progress.<br>我们对Raft的要求之一是安全性不得依赖时间：系统不能因为一些事件比所期望的更快或更慢发生而产生不正确的结果。<br>
然而，可用性(系统及时响应客户端的能力)一定不可避免的依赖于时间。<br>
例如，如果消息交换所花费的时间比服务器崩溃时所花费的时间还长，candidates将无法一直等待以赢得一场选举；没有一个稳定的leader，Raft就无法工作。<br>Leader election is the aspect of Raft where timing is most critical.<br>
Raft will be able to elect and maintain a steady leader as long as the system satisfies the following timing requirement:<br>
broadcastTime ≪ electionTimeout ≪ MTBF<br>leader选举是Raft关于时间的最关键的方面。<br>
只要系统能满足以下时间的需求，Raft将能够选出并且维持一个稳定的leader：<br>
广播时间(broadcastTime) ≪ 选举超时时间(electionTimeout) ≪ 平均故障间隔时间(MTBF: Mean Time between Failures)<br>In this inequality broadcastTime is the average time it takes a server to send RPCs in parallel to every server<br>
in the cluster and receive their responses;<br>
electionTimeout is the election timeout described in Section 5.2;<br>
and MTBF is the average time between failures for a single server.<br>
The broadcast time should be an order of magnitude less than the election timeout so<br>
that leaders can reliably send the heartbeat messages required to keep followers from starting elections;<br>
given the randomized approach used for election timeouts, this inequality also makes split votes unlikely.<br>
The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.<br>
When the leader crashes, the system will be unavailable for roughly the election timeout;<br>
we would like this to represent only a small fraction of overall time.<br>在这个不等式中，广播时间是服务器并行发送RPC给集群中的每一个服务器并且接受到它们的响应所花费的时间；<br>
选举超时时间是在5.2节中所描述的选举超时时间；同时MTBF是对于单一服务器在两次故障间隔的平均时间。<br>
广播时间应该比选举超时时间小一个数量级因此leader可以可靠的发送所需的心跳信息来阻止follower开始选举；<br>
考虑到用于选举超时的随机化方法，这个不等式也使得不太可能出现投票分裂。<br>
选举超时时间必须比MTBF低几个数量级才能使得系统能稳定的运行。<br>
当leader崩溃时，系统将有大致等于选举超时时间左右的不可用时间，我们希望这只占用整个(工作)时间的一小部分。<br>The broadcast time and MTBF are properties of the underlying system, while the election timeout is something we must choose.<br>
Raft’s RPCs typically require the recipient to persist information to stable storage,<br>
so the broadcast time may range from 0.5ms to 20ms, depending on storage technology.<br>
As a result, the election timeout is likely to be somewhere between 10ms and 500ms.<br>
Typical server MTBFs are several months or more, which easily satisfies the timing requirement.<br>广播时间和平均故障间隔时间是底层系统的特性，只有选举超时时间是我们必须选择的。<br>
Raft的RPC通常需要接收方将信息持久化到稳定的存储介质中，所以广播时间可能在0.5ms到20ms之间，这取决于存储技术。<br>
因此，选举时间可能在10ms到500ms之间。<br>
典型的服务器平均故障间隔时间是几个月或者更多，因此对这一时间(的要求)很容易满足。<br><img alt="Pasted image 20240725173120.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173120.png"><br>Figure 10: Switching directly from one configuration to another is unsafe because different servers will switch at different times.<br>
In this example, the cluster grows from three servers to five.<br>
Unfortunately, there is a point in time where two different leaders can be elected for the same term,<br>
one with a majority of the old configuration (Cold) and another with a majority of the new configuration (Cnew).<br>图10：直接将一种配置切换到另一种配置是不安全的因为不同的服务器将会在不同的时间点进行切换。<br>
在这个例子中，集群从3台服务器增长到5台。<br>
不幸的是，这个时间点将会在相同的任期内选举出两个不同的leader，其中之一获得了旧配置中的大多数(Cold)同时另一个获得了新配置中的大多数(Cnew)。<br><br>Up until now we have assumed that the cluster configuration (the set of servers participating in the consensus algorithm) is fixed.<br>
In practice, it will occasionally be necessary to change the configuration,<br>
for example to replace servers when they fail or to change the degree of replication.<br>
Although this can be done by taking the entire cluster off-line, updating configuration files,<br>
and then restarting the cluster, this would leave the cluster unavailable during the changeover.<br>
In addition, if there are any manual steps, they risk operator error.<br>
In order to avoid these issues, we decided to automate configuration changes and incorporate them into the Raft consensus algorithm.<br>到目前为止，我们已经假设集群的配置(参与一致性算法的服务器集合)是固定的。<br>
在实践中，偶尔的改变配置是必须的，例如在服务器发生故障时进行替换或者改变复制的程度。<br>
尽管这可以通过使整个集群离线，更新配置文件并且随后重启集群来实现，但这也使得集群在转换过程中变得不可用。<br>
另外，如果有任何的手工步骤，则有管理员操作失误的风险。<br>
为了避免这些问题，我们决定将配置的变更自动化并且将其纳入到Raft一致性算法中。<br>For the configuration change mechanism to be safe,<br>
there must be no point during the transition where it is possible for two leaders to be elected for the same term.<br>
Unfortunately, any approach where servers switch directly from the old configuration to the new configuration is unsafe.<br>
It isn’t possible to atomically switch all of the servers at once,<br>
so the cluster can potentially split into two independent majorities during the transition (see Figure 10).<br>为了使得配置变更的过程是安全的，在转换的过程中必须保证不能在同一个任期内选举出两个leader。<br>
不幸的是，任何将旧配置直接切换到新配置的方法都是不安全的。<br>
不可能原子性的一次性切换所有的服务器，因此服务器可能在转换期间被切分为两个独立的多数(如图10所示)。<br>In order to ensure safety, configuration changes must use a two-phase approach.<br>
There are a variety of ways to implement the two phases.<br>
For example, some systems(e.g., [22]) use the first phase to disable the old configuration so it cannot process client requests;<br>
then the second phase enables the new configuration.<br>
In Raft the cluster first switches to a transitional configuration we call joint consensus;<br>
once the joint consensus has been committed, the system then transitions to the new configuration.<br>
The joint consensus combines both the old and new configurations:<br>
<br>Log entries are replicated to all servers in both configurations.
<br>Any server from either configuration may serve as leader
<br>Agreement (for elections and entry commitment) requires separate majorities from both the old and new configurations.
<br><img alt="Pasted image 20240725173130.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173130.png"><br>Figure 11: Timeline for a configuration change.<br>
Dashed lines show configuration entries that have been created but not committed, and solid lines show the latest committed configuration entry.<br>
The leader first creates the Cold,new configuration entry in its log and commits it to Cold,new<br>
(a majority of Cold and a majority of Cnew).<br>
Then it creates the Cnew entry and commits it to a majority of Cnew.<br>
There is no point in time in which Cold and Cnew can both make decisions independently.<br>图11：配置变更的时间线。<br>
虚线标识配置条目已经被创建但还未被提交，而实现标识最新的已提交的配置条目。<br>
leader首先在它的日志中创建Cold,new的配置条目并且向Cold,new(Cold中的大多数以及Cnew中的大多数)提交这一日志。<br>
然后它创建Cnew条目并且向Cnew中的大多数提交这一条目。<br>
没有任何一个时间点可以让Cold和Cnew都能同时独立的做出决定。<br>为了确保安全，配置的变更必须使用一种两阶段的方法。<br>
有很多方法可以实现两阶段。<br>
例如，一些系统通过在一阶段禁用旧的配置因此其无法处理客户端请求，然后二阶段则启用新的配置。<br>
在Raft的集群首先切换到我们成为联合一致(joint consensus)的过渡配置;一旦联合一致已被提交，系统便过度到新的配置。<br>
联合一致结合了旧的和新的配置：<br>
<br>日志条目都会被复制到在这两种配置中所有的服务器上。
<br>新、旧配置中的任一服务器都可以作为leader。
<br>(对于选举和条目提交)达成一致需要在新的和旧的配置中分别获得大多数服务器的同意。
<br>The joint consensus allows individual servers to transition between configurations at different times without compromising safety.<br>
Furthermore, joint consensus allows the cluster to continue servicing client requests throughout the configuration change.<br>联合一致允许单独的服务器在不同的时间内转换配合而不会在安全性上有所妥协。<br>
此外，联合一致允许集群在配置变更的过程中持续的为客户端的请求提供服务。<br>Cluster configurations are stored and communicated using special entries in the replicated log;<br>
Figure 11 illustrates the configuration change process.<br>
When the leader receives a request to change the configuration from Cold to Cnew,<br>
it stores the configuration for joint consensus(Cold,new in the figure) as a log entry and replicates that<br>
entry using the mechanisms described previously.<br>
Once a given server adds the new configuration entry to its log, it uses that configuration for all future decisions<br>
(a server always uses the latest configuration in its log, regardless of whether the entry is committed).<br>
This means that the leader will use the rules of Cold,new to determine when the log entry for Cold,new is committed.<br>
If the leader crashes, a new leader may be chosen under either Cold or Cold,new,<br>
depending on whether the winning candidate has received Cold,new.<br>
In any case, Cnew cannot make unilateral decisions during this period.<br>集群配置通过复制日志中特殊的条目进行存储和通信；图11展示了配置变更的过程。<br>
当leader接受到令配置从Cold(旧配置)到Cnew(新配置)的请求时，<br>
它为了联合一致以一个日志条目的形式存储这个配置(图中的Cold,new)并且使用之前所描述的机制复制这个条目。<br>
一旦给定的服务器将新的配置条目加入了它的日志，它将使用这些配置来指定未来所有的决定(一个服务器总是使用它日志中最后的配置，无论该条目是否是已提交的)。<br>
这意味着leader将使用规则Cold,new来决定何时提交关于Cold,new的日志条目。<br>
如果leader崩溃了，新的leader可能是在Cold或者是Cold,new下选择出来的，这取决于获胜的candidate是否已经收到了Cold,new。<br>
无论如何，Cnew都不能在这个阶段单独的做出决定。<br>Once Cold,new has been committed, neither Cold nor Cnew can make decisions without approval of the other,<br>
and the Leader Completeness Property ensures that only servers with the Cold,new log entry can be elected as leader.<br>
It is now safe for the leader to create a log entry describing Cnew and replicate it to the cluster.<br>
Again, this configuration will take effect on each server as soon as it is seen.<br>
When the new configuration has been committed under the rules of Cnew,<br>
the old configuration is irrelevant and servers not in the new configuration can be shut down.<br>
As shown in Figure 11, there is no time when Cold and Cnew can both make unilateral decisions; this guarantees safety.<br>一旦Cold,new已经提交，Cold或者Cnew都不能在没有另一方同意的情况下做出决定，<br>
并且Leader Completeness特性确保只有拥有Cold,new日志条目的服务器才能被选举为leader。<br>
现在leader可以安全的创建一个描述了Cnew的日志条目并将其在集群中进行复制。<br>
同样的，该配置将在每一个服务器看到其后立即生效。<br>
当新的配置在Cnew的规则下被提交，旧的配置将变得无关紧要并且没有在新配置中的服务器将可以被关闭。<br>
如图11所示，Cold和Cnew不能同时做出单独的决定；这保证了安全性。<br>There are three more issues to address for reconfiguration.<br>
The first issue is that new servers may not initially store any log entries.<br>
If they are added to the cluster in this state, it could take quite a while for them to catch up,<br>
during which time it might not be possible to commit new log entries.<br>
In order to avoid availability gaps, Raft introduces an additional phase before the configuration change,<br>
in which the new servers join the cluster as non-voting members<br>
(the leader replicates log entries to them, but they are not considered for majorities).<br>
Once the new servers have caught up with the rest of the cluster, the reconfiguration can proceed as described above.<br>关于配置变更还存在三个问题需要解决。<br>
第一个问题是，新的服务器可能在初始化时没有存储任何的日志条目。<br>
如果在这种状态下被加入到集群，它可能需要花费很长一段时间才能赶上，在这段时间内都无法提交新的日志条目。<br>
为了避免可用性的差距，Raft在配置变更前引入了一个额外的阶段，新的服务器以无投票权成员(non-voting members)的身份加入集群<br>
(leader复制日志条目给它们，但它们不被认为是大多数的一份子)。<br>
一旦新的服务器能够追上集群中的其它机器，就可以向上述那般执行配置变更。<br>The second issue is that the cluster leader may not be part of the new configuration.<br>
In this case, the leader steps down (returns to follower state) once it has committed the Cnew log entry.<br>
This means that there will be a period of time (while it is committing Cnew)<br>
when the leader is managing a cluster that does not include itself; it replicates log entries but does not count itself in majorities.<br>
The leader transition occurs when Cnew is committed<br>
because this is the first point when the new configuration can operate independently (it will always be possible to choose a leader from Cnew).<br>
Before this point, it may be the case that only a server from Cold can be elected leader.<br>第二个问题是，集群的leader可能不是新配置中的一员。<br>
在这种情况下，一旦Cnew日志条目被提交，leader将会退下(返回到follower状态)。<br>
这意味着存在一段时间(在提交Cnew时)，其中leader管理者一个不包含自己的集群；它复制着日志条目但不把它自己算作大多数中的一员。<br>
当Cnew被提交时将会发生leader的切换，因为这是新配置可以进行独立操作的第一个点位(总是可以在Cnew中选择出一个leader)。<br>
在此之前，只有来自Cold的服务器才有可能被选举为leader。<br>The third issue is that removed servers (those not in Cnew) can disrupt the cluster.<br>
These servers will not receive heartbeats, so they will time out and start new elections.<br>
They will then send RequestVote RPCs with new term numbers, and this will cause the current leader to revert to follower state.<br>
A new leader will eventually be elected, but the removed servers will time out again and the process will repeat,<br>
resulting in poor availability.<br>第三个问题是被移除的服务器(不在Cnew中)可能会中断集群。<br>
这些服务器将不再接收到心跳，所以它们将会超时而启动新的选举。<br>
然后它们将发送有着新任期编号的RequestVote RPC，并且这将导致当前的leader恢复为follower状态。<br>
最终将会有一名新的leader被选举出来，但是被移除的服务器将会再次超时并且重复这一过程，这将导致系统有着较差的可用性。<br>To prevent this problem, servers disregard RequestVote RPCs when they believe a current leader exists.<br>
Specifically, if a server receives a RequestVote RPC within the minimum election timeout of hearing from a current leader,<br>
it does not update its term or grant its vote.<br>
This does not affect normal elections, where each server waits at least a minimum election timeout before starting an election.<br>
However, it helps avoid disruptions from removed servers:<br>
if a leader is able to get heartbeats to its cluster, then it will not be deposed by larger term numbers.<br>为了避免这一问题，服务器将会在它们认为当前leader存在时忽略掉RequestVote RPC。<br>
特别的，如果一个服务器在当前leader最小的选举超时时间内接收到一个RequestVote RPC，它将不会更新它的任期或者发起投票。<br>
这不会影响正常的选举，即每一个服务器在开始一轮选举之前至少等待一个最小的选举超时时间。<br>
然而，它有助于避免移除服务器时的混乱：如果一个leader能够提供集群中的心跳，则它将不会被一个更大的任期编号给取代。<br><br>Raft’s log grows during normal operation to incorporate more client requests, but in a practical system, it cannot grow without bound.<br>
As the log grows longer, it occupies more space and takes more time to replay.<br>
This will eventually cause availability problems without some mechanism to discard obsolete information that has accumulated in the log.<br>Raft的日志在正常操作期间不断增长以满足更多的客户端请求，但是在实际的系统中，日志不能不加限制的增长。<br>
随着日志不断变长，它将占用更多的空间并且花费更长的事件来进行回放。<br>
如果没有一些机制来剔除日志中所累积的过时的信息，这终将造成可用性问题。<br>Snapshotting is the simplest approach to compaction.<br>
In snapshotting, the entire current system state is written to a snapshot on stable storage,<br>
then the entire log up to that point is discarded.<br>
Snapshotting is used in Chubby and ZooKeeper, and the remainder of this section describes snapshotting in Raft.<br>快照是最简单的压缩方法。<br>
在快照中，完整的当前系统状态以快照的形式写入稳定的存储中，然后在这个点位之前的整个日志会被丢弃。<br>
快照被用于Chubby和ZooKeeper中，本届的剩余部分将用于描述Raft中的快照。<br><img alt="Pasted image 20240725173140.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173140.png"><br>Figure 12: A server replaces the committed entries in its log(indexes 1 through 5) with a new snapshot,<br>
which stores just the current state (variables x and y in this example).<br>
The snapshot’s last included index and term serve to position the snapshot in the log preceding entry 6.<br>图12：服务器用新的快照代替其日志中已提交的条目(索引1到5)，该快照只存储了当前的状态(本例中的变量x和y)。<br>
快照中的last included index和term用于定位快照中的条目6之前的日志。<br>Incremental approaches to compaction, such as log cleaning [36] and log-structured merge trees [30, 5], are also possible.<br>
These operate on a fraction of the data at once, so they spread the load of compaction more evenly over time.<br>
They first select a region of data that has accumulated many deleted and overwritten objects,<br>
then they rewrite the live objects from that region more compactly and free the region.<br>
This requires significant additional mechanism and complexity compared to snapshotting,<br>
which simplifies the problem by always operating on the entire data set.<br>
While log cleaning would require modifications to Raft, state machines can implement LSM trees using the same interface as snapshotting.<br>基于增量的压缩方法，例如日志清理和日志结构合并树(LSM tree)也是可行的。<br>
这些操作一次只操作少量的数据，因此它们能随着时间的退役均摊负载。<br>
它们首先选择一片数据区域，其已经积累了很多的被删除和覆盖的对象，然后它们以更加紧凑的方式重写来自这一片区域的存活对象(live objects)并释放这一区域。<br>
与快照压缩相比这显著的引入了额外的机制和复杂度，快照通过始终操作整个数据集合来简化这一问题。<br>
虽然日志清理需要对Raft进行修改，但状态机可以使用与快照相同的接口来实现LSM树。<br>Figure 12 shows the basic idea of snapshotting in Raft.<br>
Each server takes snapshots independently, covering just the committed entries in its log.<br>
Most of the work consists of the state machine writing its current state to the snapshot.<br>
Raft also includes a small amount of metadata in the snapshot:<br>
the last included index is the index of the last entry in the log that the snapshot replaces<br>
(the last entry the state machine had applied), and the last included term is the term of this entry.<br>
These are preserved to support the AppendEntries consistency check for the first log entry following the snapshot,<br>
since that entry needs a previous log index and term.<br>
To enable cluster membership changes (Section 6), the snapshot also includes the latest configuration in the log as of last included index.<br>
Once a server completes writing a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.<br>图12展示了Raft中关于快照的基础思想。<br>
每一个服务器都独立的获得快照，只覆盖它已提交的日志条目。<br>
大部分的工作主要由状态机以快照形式写入它的当前状态组成。<br>
Raft还将少量的元数据包括在了快照中：<br>
last included index是快照代替的日志中的最后一个条目的索引值(状态机已应用的最后一个条目)，并且last included term是这个条目的任期值。<br>
保留这些条目是为了支持快照后面第一个条目的AppendEntries一致性检查，因为这个条目需要前一个日志的索引值和任期值。<br>
要启用集群变更(第6节)，快照还要包括含有last included index日志的最后配置。<br>
一旦一个服务器完成了一个快照的写入，它可能会删除包含last included index之前的所有日志条目，以及之前的任何快照。<br>Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind.<br>
This happens when the leader has already discarded the next log entry that it needs to send to a follower.<br>
Fortunately, this situation is unlikely in normal operation: a follower that has kept up with the leader would already have this entry.<br>
However, an exceptionally slow follower or a new server joining the cluster(Section 6) would not.<br>
The way to bring such a follower up-to-date is for the leader to send it a snapshot over the network.<br><img alt="Pasted image 20240725173149.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173149.png"><br>Figure 13: A summary of the InstallSnapshot RPC.<br>
Snapshots are split into chunks for transmission; this gives the follower a sign of life with each chunk, so it can reset its election timer.<br>图13：InstallSnapshot RPC的快照。<br>
快照被分割为块进行传输；每一块都带给了follower其存活的标识，因此follower可以重置其选举计时器。<br>尽管服务器通常独立的生成快照，但leader必须偶尔的向落后的follower发送快照。<br>
当leader已经丢弃了需要发送给follower的下一个日志条目时就会发生这种情况。<br>
幸运的是，这种情况不太可能在正常操作中出现：一个跟上了leader的follower已经有了这个条目了。<br>
然而，一个异常慢的follower或者一个新加入集群的服务器(第6节)将没有这个条目。<br>
让这样的一个follower的日志和leader一样新的方法就是通过网络向它发送一个快照。<br>The leader uses a new RPC called InstallSnapshot to send snapshots to followers that are too far behind; see Figure 13.<br>
When a follower receives a snapshot with this RPC, it must decide what to do with its existing log entries.<br>
Usually the snapshot will contain new information not already in the recipient’s log.<br>
In this case, the follower discards its entire log; it is all superseded by the snapshot<br>
and may possibly have uncommitted entries that conflict with the snapshot.<br>
If instead the follower receives a snapshot that describes a prefix of its log (due to retransmission or by mistake),<br>
then log entries covered by the snapshot are deleted but entries following the snapshot are still valid and must be retained.<br>leader使用一种新的被成为InstallSnapshot的RPC向落后太多的follower发送快照;如图13所示。<br>
当一个follower使用这个RPC接受到一个快照时，它必须决定如何处理它目前已存在的日志条目。<br>
通常，这个快照将包含目前还不在接受者日志中的新信息。<br>
这种情况下，follower将丢弃它全部的日志;其全部被快照所取代，并且被丢弃的日志中可能有着与快照相冲突的但还未提交的条目。<br>
相反，如果follower接受到的快照是它当前日志的前面一部分(由于重传或者出错了)，则被快照所覆盖的日志条目将会被删除但是快照后面的条目依然是有效的并且必须被保留。<br>This snapshotting approach departs from Raft’s strong leader principle, since followers can take snapshots without the knowledge of the leader.<br>
However, we think this departure is justified.<br>
While having a leader helps avoid conflicting decisions in reaching consensus,<br>
consensus has already been reached when snapshotting, so no decisions conflict.<br>
Data still only flows from leaders to followers, just followers can now reorganize their data.<br>这种快照的方式背离了Raft的强leader原则，因为follower可以在leader不知情的情况下生成快照。<br>
然而，我们认为这种背离是值得的。<br>
虽然由一个leader有助于避免在达成一致时产生决策冲突，但生成快照时是已经达成了一致的，所以不会有决策冲突。<br>
数据依然是仅由leader流向follower，但follower现在可以重新组织它们的数据。<br>We considered an alternative leader-based approach in which only the leader would create a snapshot,<br>
then it would send this snapshot to each of its followers.<br>
However, this has two disadvantages.<br>
First, sending the snapshot to each follower would waste network bandwidth and slow the snapshotting process.<br>
Each follower already has the information needed to produce its own snapshots,<br>
and it is typically much cheaper for a server to produce a snapshot from its local state than it is to send and receive one over the network.<br>
Second, the leader’s implementation would be more complex.<br>
For example, the leader would need to send snapshots to followers in parallel with replicating new log entries to them,<br>
so as not to block new client requests.<br>我们考虑过另一种基于leader的方法，其只有leader可以创建快照，然后leader将发送快照给每一个follower。<br>
然而，这样做有两个缺点。<br>
首先，发送快照给每一个follower将浪费网络带宽并且减慢快照的处理。<br>
每一个follower已经有了生成它们自己快照所需要的信息，并且通常基于服务器本地状态来生成快照要比它们通过从网络发送和接收快照的开销要更低。<br>
其次，leader也会被实现的更加复杂。<br>
比如，leader将需要并行的发送快照给follower的同时还要令它们复制新的日志条目，以避免阻塞新的客户端请求。<br>There are two more issues that impact snapshotting performance.<br>
First, servers must decide when to snapshot.<br>
If a server snapshots too often, it wastes disk bandwidth and energy; if it snapshots too infrequently,<br>
it risks exhausting its storage capacity, and it increases the time required to replay the log during restarts.<br>
One simple strategy is to take a snapshot when the log reaches a fixed size in bytes.<br>
If this size is set to be significantly larger than the expected size of a snapshot,<br>
then the disk bandwidth overhead for snapshotting will be small.<br>还有两个问题会影响快照的性能。<br>
首先，服务器必须决定何时生成快照。<br>
如果服务器生成快照太频繁，则将浪费磁盘带宽和能源；如果生成快照太不频繁，则存在耗尽磁盘空间的风险，并且增加重启时回放日志所需的时间。<br>
一种简单的策略时当日志到达一个固定的字节数时生成一个快照。<br>
如果这个大小设置为一个明显大于快照预期大小的值，则用于快照生成的磁盘带宽开销将会很小。<br>The second performance issue is that writing a snapshot can take a significant amount of time,<br>
and we do not want this to delay normal operations.<br>
The solution is to use copy-on-write techniques so that new updates can be accepted without impacting the snapshot being written.<br>
For example, state machines built with functional data structures naturally support this.<br>
Alternatively, the operating system’s copy-on-write support (e.g., fork on Linux)<br>
can be used to create an in-memory snapshot of the entire state machine (our implementation uses this approach).<br>第二个问题是写入一个快照会花费非常多的时间，并且我们不希望这会延迟正常操作。<br>
解决的方案是使用写时复制(copy-on-write)技术,以便可以在不影响快照的写入的同时接受新的更新。<br>
例如，使用函数式数据结构(functional data structures)构建的状态机能自然的支持这一点。<br>
或者，操作系统的写时复制支持(例如，linux中的fork)可以被用于创建整个状态机的内存快照(我们的实现使用了这个方法)。<br><br>This section describes how clients interact with Raft,<br>
including how clients find the cluster leader and how Raft supports linearizable semantics [10].<br>
These issues apply to all consensus-based systems, and Raft’s solutions are similar to other systems.<br>本节描述了客户端如何与Raft交互，包括客户端如何找到集群leader以及Raft是如何支持线性化语义的。<br>
这些问题适用于所有的基于一致性的系统，同时Raft的解决方案也与其它系统是类似的。<br>Clients of Raft send all of their requests to the leader.<br>
When a client first starts up, it connects to a randomly-chosen server.<br>
If the client’s first choice is not the leader,<br>
that server will reject the client’s request and supply information about the most recent leader it has heard from<br>
(AppendEntries requests include the network address of the leader).<br>
If the leader crashes, client requests will time out; clients then try again with randomly-chosen servers.<br>Raft的客户端将它们的所有请求发送给leader。<br>
当客户端第一次启动时，它会随机选择一台服务器并进行连接。<br>
如果客户端第一次选择的不是leader，则服务器将会拒绝客户端的请求并且提供关于它听到的最近的leader的信息(AppendEntries的请求中包括了leader的网络地址)。<br>
如果leader崩溃了，客户端的请求将会超时;客户端则会再一次随机选择一台服务器。<br>Our goal for Raft is to implement linearizable semantics (each operation appears to execute instantaneously,<br>
exactly once, at some point between its invocation and its response).<br>
However, as described so far Raft can execute a command multiple times:<br>
for example, if the leader crashes after committing the log entry but before responding to the client,<br>
the client will retry the command with a new leader, causing it to be executed a second time.<br>
The solution is for clients to assign unique serial numbers to every command.<br>
Then, the state machine tracks the latest serial number processed for each client, along with the associated response.<br>
If it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.<br>我们对于Raft的目标是实现可线性化的语义(每一个操作会立即执行，执行且只执行一次，执行的时机位于请求和响应之间)。<br>
然而，如上所述Raft可以执行执行一条指令多次：例如，如果leader在提交日志条目后但响应客户端之前崩溃了，客户端将会与新的leader重试这条指令，使得该指令被执行了两次。<br>
解决方案是让客户端为每一个指令分配一个唯一的序列号。<br>
然后，状态机追踪为每一个客户端处理的最后的序列号，以及相关的响应。<br>
如果它接受到了一个指令其序列号是已经被执行了的，它将立即返回而不会重新执行该请求。<br>Read-only operations can be handled without writing anything into the log.<br>
However, with no additional measures, this would run the risk of returning stale data,<br>
since the leader responding to the request might have been superseded by a newer leader of which it is unaware.<br>
Linearizable reads must not return stale data, and Raft needs two extra precautions to guarantee this without using the log.<br>
First, a leader must have the latest information on which entries are committed.<br>
The Leader Completeness Property guarantees that a leader has all committed entries,<br>
but at the start of its term, it may not know which those are.<br>
To find out, it needs to commit an entry from its term.<br>
Raft handles this by having each leader commit a blank no-op entry into the log at the start of its term.<br>
Second, a leader must check whether it has been deposed before processing a read-only request<br>
(its information may be stale if a more recent leader has been elected).<br>
Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests.<br>
Alternatively, the leader could rely on the heartbeat mechanism to provide a form of lease [9],<br>
but this would rely on timing for safety (it assumes bounded clock skew).<br>只读操作可以直接被处理而不需要向日志写入任何东西。<br>
然而，如果没有额外的机制，将会有返回过时数据的风险，因为响应请求的leader可能已经被一个新的leader取代了但它自己却没感知到。<br>
线性化的读必须不返回过时数据，并且Raft需要两个额外的预防措施在不使用日志的前提下保证这一点。<br>
首先，leader必须掌握已提交日志条目的最新信息。<br>
leader完整性属性保证了leader有着所有已提交的条目，但在它任期的开始时，它不知道哪些是已提交的条目。<br>
为了找到哪些是已提交的条目，它需要提交一个来自它自己任期的条目。<br>
Raft通过在leader开始其任期时，让每一个leader提交一个空白的no-op条目来处理这一问题。<br>
其次，leader在处理只读请求时必须检查它是否已经被罢黜退位了(如果最新的leader已经被选出，则它的信息可能已经过时了)。<br>
Raft通过让leader在响应只读请求之前与集群中的大多数交换心跳信息来解决这一问题。<br>
或者，leader可以依赖心跳机制来提供一种租约的形式，但这将会依赖于时钟的安全性(假设时间误差是有限的)。<br><br>We have implemented Raft as part of a replicated state machine that stores configuration information for RAMCloud [33]<br>
and assists in failover of the RAMCloud coordinator.<br>
The Raft implementation contains roughly 2000 lines of C++ code, not including tests, comments, or blank lines.<br>
The source code is freely available [23].<br>
There are also about 25 independent third-party open source implementations [34] of Raft in various stages of development,<br>
based on drafts of this paper.<br>
Also, various companies are deploying Raft-based systems [34].<br>我们已经将Raft实现为复制状态机的一部分，其存储RAMCloud的配置信息并且协助RAMCloud协调者进行故障恢复。<br>
Raft的实现包含了大概2000行的C++代码，不包括测试，备注或者空行。<br>
源代码是免费提供的。<br>
基于本论文的草稿，有大约25个独立的、处于不同开发阶段的Raft三方开源实现。<br>
此外，很多公司也部署了基于Raft的系统。<br>The remainder of this section evaluates Raft using three criteria: understandability, correctness, and performance.<br>本节的剩余部分用于在三个方面评估Raft: 可理解性，正确性和性能。<br><br>To measure Raft’s understandability relative to Paxos,<br>
we conducted an experimental study using upper-level undergraduate and graduate students in<br>
an Advanced Operating Systems course at Stanford University and a Distributed Computing course at U.C. Berkeley.<br>
We recorded a video lecture of Raft and another of Paxos, and created corresponding quizzes.<br>
The Raft lecture covered the content of this paper except for log compaction;<br>
the Paxos lecture covered enough material to create an equivalent replicated state machine, including single-decree Paxos,<br>
multi-decree Paxos, reconfiguration, and a few optimizations needed in practice (such as leader election).<br>
The quizzes tested basic understanding of the algorithms and also required students to reason about corner cases.<br>
Each student watched one video, took the corresponding quiz, watched the second video, and took the second quiz.<br>
About half of the participants did the Paxos portion first<br>
and the other half did the Raft portion first in order to account for both individual differences in performance<br>
and experience gained from the first portion of the study.<br>
We compared participants’ scores on each quiz to determine whether participants showed a better understanding of Raft.<br><img alt="Pasted image 20240725173202.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173202.png"><br>Figure 14: A scatter plot comparing 43 participants’ performance on the Raft and Paxos quizzes.<br>
Points above the diagonal (33) represent participants who scored higher for Raft.<br>图14：比较43名参与者在Raft和Paxos测验中表现的散点图。<br>
位于对角线之上的(33个)参与者是Raft分数更高的。<br>为了测量Raft相对于Paxos的可理解性，我们对来自斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式系统课程的高水平本科生和研究生组织了一场学习实验。<br>
我们录制了Raft和Paxos的视频讲座，并且制作了相对应的测验。<br>
Raft的讲座覆盖了本文除日志压缩以外的内容，Paxos的讲座覆盖了相当于创建一个等效的复制状态机的足够多的材料，包括single-decree Paxos，multi-decree Paxos，<br>
刷新配置，以及实践中所需要的一小部分优化(例如leader选举)。<br>
测验测试了学生对算法的基础理解同时也需要学生能推理出极端的case。<br>
每个学生观看第一个视频，然后做相应的测验，再看第二个视频，然后再做第二个视频对应的测验。<br>
为解释本实验第一次学习时获得的经验和表现上的差异，大约一般的实验者先做Paxos那部分的而另一半实验者则先做Raft的那部分。<br>
我们比较了实验者在每一次测验中的分数来确定实现者是否展现出了对Raft有着更好的裂解。<br>We tried to make the comparison between Paxos and Raft as fair as possible.<br>
The experiment favored Paxos in two ways: 15 of the 43 participants reported having some prior experience with Paxos,<br>
and the Paxos video is 14% longer than the Raft video.<br>
As summarized in Table 1, we have taken steps to mitigate potential sources of bias.<br>
All of our materials are available for review [28, 31].<br>我们尝试着使得Paxos和Raft之间的比较尽可能的公平。<br>
该实验在两方面有利于Paxos：43名实验者中的15名报告说曾经有着一些关于Paxos的经验，同时Paxos的视频比Raft的视频要长14%。<br>
如表1所示，我们已经采取措施来减少潜在的来源偏差。<br>
我们所有的材料都是可以审查的。<br>On average, participants scored 4.9 points higher on the Raft quiz than on the Paxos quiz<br>
(out of a possible 60 points, the mean Raft score was 25.7 and the mean Paxos score was 20.8);<br>
Figure 14 shows their individual scores.<br>
A paired t-test states that, with 95% confidence,<br>
the true distribution of Raft scores has a mean at least 2.5 points larger than the true distribution of Paxos scores.<br>平均而言，参与者在Raft测验中的得分要比Paxos的测验中的得分要高4.9分(换算成60分制，意味着Raft的测验分数为25.7同时Paxos的测验分数为20.8)<br>
图14展示了它们各自的分数。<br>
配队t-test表名，有95%的置信度下，Raft的真实分数分布比Paxos的真实分数分布至少要高2.5分。<br>We also created a linear regression model that predicts a new student’s quiz scores based on three factors:<br>
which quiz they took, their degree of prior Paxos experience, and the order in which they learned the algorithms.<br>
The model predicts that the choice of quiz produces a 12.5-point difference in favor of Raft.<br>
This is significantly higher than the observed difference of 4.9 points, because many of the actual students had prior Paxos experience,<br>
which helped Paxos considerably, whereas it helped Raft slightly less.<br>
Curiously, the model also predicts scores 6.3 points lower on Raft for people that have already taken the Paxos quiz;<br>
although we don’t know why, this does appear to be statistically significant.<br><img alt="Pasted image 20240725173212.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173212.png"><br>Figure 15: Using a 5-point scale, participants were asked(left) which algorithm they felt<br>
would be easier to implement in a functioning, correct, and efficient system,<br>
and (right) which would be easier to explain to a CS graduate student.<br>图15：使用5分支，参与者被问到(左侧)他们感觉使用哪种算法更容易去实现一个正常工作的，正确的，和有效的系统，<br>
同时(右侧)是哪种算法对于计算机科学(CS)的研究生来说会更容易解释。<br>我们还创建了一个线性回归模型，其用于预测新生基于三个要素的测验成绩：分别是它们参加的测验，它们之前关于Paxos的经验，以及它们学习算法的顺序。<br>
这个模型预测选择的测验中Raft要比Paxos高12.5分。<br>
这明显高于观察到的4.9分的差异，因为实际上很多学生之前有过Paxos的经验，这有助于对Paxos的理解，而对于Raft的帮助则少很多。<br>
奇怪的是，模型还预测已经参加过Paxos测验的人在Raft的实验上将会低6.3分；即使我们不知道为什么，但这似乎具有统计学的意义。<br>We also surveyed participants after their quizzes to see which algorithm they felt would be easier to implement or explain;<br>
these results are shown in Figure 15.<br>
An overwhelming majority of participants reported Raft would be easier to implement and explain (33 of 41 for each question).<br>
However, these self-reported feelings may be less reliable than participants’ quiz scores,<br>
and participants may have been biased by knowledge of our hypothesis that Raft is easier to understand.<br>我们还在参与者测验后对其进行了调查，询问它们感觉哪种算法更加容易实现或解释；结果如图15所示。<br>
绝大多数参与者表示Raft要更加容易实现和解释(41个被提问者中的33个)<br>
然而，这些自我报告的感受可能不如参与者的测验分数更加可靠，并且参与者可能由于我们假设了Raft更加容易理解而产生偏见。<br>A detailed discussion of the Raft user study is available at [31].<br>有关Raft用户研究的详细讨论，请参见[31]。<br><img alt="Pasted image 20240725173219.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173219.png"><br><br>We have developed a formal specification and a proof of safety for the consensus mechanism described in Section 5.<br>
The formal specification [31] makes the information summarized in Figure 2 completely precise using the TLA+ specification language [17].<br>
It is about 400 lines long and serves as the subject of the proof.<br>
It is also useful on its own for anyone implementing Raft.<br>
We have mechanically proven the Log Completeness Property using the TLA proof system [7].<br>
However, this proof relies on invariants that have not been mechanically checked<br>
(for example, we have not proven the type safety of the specification).<br>
Furthermore, we have written an informal proof [31] of the State Machine Safety property which is complete<br>
(it relies on the specification alone) and rela tively precise (it is about 3500 words long).<br><img alt="Pasted image 20240725173227.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173227.png"><br>Figure 16: The time to detect and replace a crashed leader.<br>
The top graph varies the amount of randomness in election timeouts, and the bottom graph scales the minimum election timeout.<br>
Each line represents 1000 trials (except for 100 trials for “150–150ms”) and corresponds to a particular choice of election timeouts;<br>
for example, “150–155ms” means that election timeouts were chosen randomly and uniformly between 150ms and 155ms.<br>
The measurements were taken on a cluster of five servers with a broadcast time of roughly 15ms.<br>
Results for a cluster of nine servers are similar.<br>图16：检测和替代一个已崩溃leader的时间。<br>
上图是一系列随机化的选举超时时间，下图是缩放后的最小选举超时时间。<br>
每一行代表了对应于一个特定选举超时时间的1000次实验(除了“150-150ms”的100次实验)；<br>
例如，“150-155ms”意味着选举超时时间是在150ms到155ms间随机且均匀选择的。<br>
测量是在一个有着5台机器的集群上进行的，其广播时间大约为15ms。<br>
由9台服务器组成的集群的结果是类似的。<br>我们已经为第5节所描述的一致性机制提供了形式化规约和安全性证明。<br>
形式化规约使用TLA+规约语言精确的使用了如图2摘要中的信息。<br>
大约由400行长并且可以作为证明的主体来使用。<br>
对于任何一个想实现Raft的人来说也是有用的。<br>
我们已经使用TLA证明系统机械地证明了Log Completeness特性。<br>
然而，这一证明依赖于尚未被机械地检查地不变量(例如，我们还没有证明规约的类型安全性)。<br>
此外，我们也编写了关于状态机安全特性(State Machine Safety property)的非正式证明，该证明是完整的(仅依赖于规约)并且是相对精确的(大约长3500字)。<br><br>Raft’s performance is similar to other consensus algorithms such as Paxos.<br>
The most important case for performance is when an established leader is replicating new log entries.<br>
Raft achieves this using the minimal number of messages (a single round-trip from the leader to half the cluster).<br>
It is also possible to further improve Raft’s performance.<br>
For example, it easily supports batching and pipelining requests for higher throughput and lower latency.<br>
Various optimizations have been proposed in the literature for other algorithms; many of these could be applied to Raft,<br>
but we leave this to future work.<br>Raft的性能与Paxos等其它一致性算法的性能相差无几。<br>
对于性能而言最重要的方面是一个已被选出的leader复制新的日志条目。<br>
Raft通过使用最少数量的消息来实现这一点(从leader到集群中半数机器的单轮往返)。<br>
这也可能进一步的提升Raft的性能。<br>
例如，它可以轻松的支持批处理和流水线(pipelining)请求来获得更高的吞吐量和更低的延迟。<br>
在资料中已经针对其它算法提出了一系列的优化;其中有很多优化也能应用在Raft中，但我们将此留给未来的工作。<br>We used our Raft implementation to measure the performance of Raft’s leader election algorithm and answer two questions.<br>
First, does the election process converge quickly?<br>
Second, what is the minimum downtime that can be achieved after leader crashes?<br>我们使用我们自己的Raft实现来衡量Raft的leader选举算法性能，并且回答两个问题。<br>
第一，选举过程是否迅速的收敛？<br>
第二，在leader崩溃后可以实现的停机宕机时间是多少？<br>To measure leader election, we repeatedly crashed the leader of a cluster of five servers<br>
and timed how long it took to detect the crash and elect a new leader (see Figure 16).<br>
To generate a worst-case scenario, the servers in each trial had different log lengths, so some candidates were not eligible to become leader.<br>
Furthermore, to encourage split votes, our test script triggered<br>
a synchronized broadcast of heartbeat RPCs from the leader before terminating its process<br>
(this approximates the behavior of the leader replicating a new log entry prior to crashing).<br>
The leader was crashed uniformly randomly within its heartbeat interval, which was half of the minimum election timeout for all tests.<br>
Thus, the smallest possible downtime was about half of the minimum election timeout.<br>为了测量leader选举的性能，我们反复的令一个五节点集群中的leader崩溃，并且测量集群多久能检测到崩溃并选举出一个新的leader(见图16)。<br>
为了生成最坏的情况，在每次实验中服务器都有着不同的日志长度，因此一些candidate将没有资格成为leader。<br>
此外，为了促进分裂投票的产生，我们的测试脚本在终止leader进程前触发了一次leader的同步RPC心跳广播(这类似于leader在崩溃前复制新的日志条目的行为)。<br>The top graph in Figure 16 shows that a small amount of randomization in the election timeout is enough to avoid split votes in elections.<br>
In the absence of randomness, leader election consistently took longer than 10 seconds in our tests due to many split votes.<br>
Adding just 5ms of randomness helps significantly, resulting in a median downtime of 287ms.<br>
Using more randomness improves worst-case behavior: with 50ms of randomness the worstcase completion time (over 1000 trials) was 513ms.<br>图16中的上图显示了，只要少量的随机化选举超时时间就足够避免选举时的投票分裂。<br>
在缺乏随机性的情况下，由于许多分裂的投票，在我们的测试中leader选举一直持续了超过10秒钟。<br>
只要增加5ms的随机性就能有显著的帮助，平均的停机时间中位数为287ms。<br>
使用更多的随机性可以改善最坏情况下的行为：有着50ms的随机性时最坏情况下(超过1000次实验)的选举完成时间为513ms。<br>The bottom graph in Figure 16 shows that downtime can be reduced by reducing the election timeout.<br>
With an election timeout of 12–24ms, it takes only 35ms on average to elect a leader (the longest trial took 152ms).<br>
However, lowering the timeouts beyond this point violates Raft’s timing requirement:<br>
leaders have difficulty broadcasting heartbeats before other servers start new elections.<br>
This can cause unnecessary leader changes and lower overall system availability.<br>
We recommend using a conservative election timeout such as 150–300ms;<br>
such timeouts are unlikely to cause unnecessary leader changes and will still provide good availability.<br>图16中的下图显示，可以通过减少选举超时时间来减少停机时间。<br>
由于选举超时时间为12-24ms，选举出一个leader的平均耗时只需要35ms(最长的一次实验花费了152ms)。<br>
然而，超时时间低于这一位点以下会违反Raft的时间需求：leader很难在其它leader发起新一轮选举前进行心跳广播。<br>
这可能会导致不必要的leader变更以及更低的整体系统可用性。<br>
我们推荐使用一个保守的选举超时时间比如150-300ms；这一超时时间不太可能造成不必要的leader变更并且将仍然提供良好的可用性。<br><br>There have been numerous publications related to consensus algorithms, many of which fall into one of the following categories:<br>
<br>Lamport’s original description of Paxos [15], and attempts to explain it more clearly [16, 20, 21].
<br>Elaborations of Paxos, which fill in missing details and modify the algorithm to provide a better foundation for implementation [26, 39, 13].
<br>Systems that implement consensus algorithms, such as Chubby [2, 4], ZooKeeper [11, 12], and Spanner [6].<br>
The algorithms for Chubby and Spanner have not been published in detail, though both claim to be based on Paxos.<br>
ZooKeeper’s algorithm has been published in more detail, but it is quite different from Paxos.
<br>Performance optimizations that can be applied to Paxos [18, 19, 3, 25, 1, 27].
<br>Oki and Liskov’s View-stamped Replication (VR), an alternative approach to consensus developed around the same time as Paxos.<br>
The original description [29] was intertwined with a protocol for distributed transactions,<br>
but the core consensus protocol has been separated in a recent update [22].<br>
VR uses a leader-based approach with many similarities to Raft.
<br>已经有许多与一致性算法有关的出版物了，其中很多都属于以下类目中的一个：<br>
<br>Lamport对于Paxos的原始描述，并且试图更加清晰的进行解释。
<br>对Paxos的细化，其填充了确实的细节并且修改了算法以为实现Paxos提供了一个更好的基础。
<br>实现了共识算法的系统，例如Chubby，ZooKeeper和Spanner。<br>
Chubby和Spanner的算法并没有公布细节，即使它们都声称其基于Paxos。<br>
ZooKeeper的算法公布了更多的细节，但其与Paxos截然不同。
<br>可用于Paxos的性能优化。
<br>Oki和Liskov的Viewstamped Replication (VR)算法，另一种共识算法其被开发的时间点与Paxos相同。<br>
最初的描述与分布式事务的协议混在了一起，但最近的更新中其核心的共识协议已经被分离出来了。<br>
VR采用了一种基于leader的方法,其与Raft存在很多相似之处。
<br>The greatest difference between Raft and Paxos is Raft’s strong leadership:<br>
Raft uses leader election as an essential part of the consensus protocol, and it concentrates as much functionality as possible in the leader.<br>
This approach results in a simpler algorithm that is easier to understand.<br>
For example, in Paxos, leader election is orthogonal to the basic consensus protocol:<br>
it serves only as a performance optimization and is not required for achieving consensus.<br>
However, this results in additional mechanism:<br>
Paxos includes both a two-phase protocol for basic consensus and a separate mechanism for leader election.<br>
In contrast, Raft incorporates leader election directly into the consensus algorithm and uses it as the first of the two phases of consensus.<br>
This results in less mechanism than in Paxos.<br>Raft和Paxos最大的区别在于Raft的强领导性：<br>
Raft将leader选举作为共识协议中必要的组成部分，并尽可能的将很多功能集中在leader身上。<br>
这一策略使得Raft成为了一个更简单的算法，其更容易被理解。<br>
例如，在Paxos中，leader选举基本上与基础的一致性协议无关：<br>
其仅仅用于性能优化并且不是实现共识所必须的。<br>
然而，这产生了额外的机制：<br>
Paxos包括了一个用于基础一致性的两阶段协议以及一个单独的用于leader选举的机制。<br>
相比之下，Raft将leader选举直接纳入共识算法中并且将leader选举作为共识的两阶段中的第一个阶段。<br>
这使得Raft比起Paxos有着更少的机制。<br>Like Raft, VR and ZooKeeper are leader-based and therefore share many of Raft’s advantages over Paxos.<br>
However, Raft has less mechanism that VR or ZooKeeper because it minimizes the functionality in non-leaders.<br>
For example, log entries in Raft flow in only one direction: outward from the leader in AppendEntries RPCs.<br>
In VR log entries flow in both directions (leaders can receive log entries during the election process);<br>
this results in additional mechanism and complexity.<br>
The published description of ZooKeeper also transfers log entries both to and from the leader,<br>
but the implementation is apparently more like Raft [35].<br>与Raft一样，VR和ZooKeeper都是基于leader的并且也共享着很多Raft相对于Paxos的优点。<br>
然而，Raft比起VR或者Zookeeper有着更少的机制，因为它最小化了非leader(non-leaders)的功能。<br>
例如，Raft中的日志条目流向只有一个方向：从leader向外流出的AppendEntries RPC。<br>
在VR中日志条目是双向流动的(leader也可以在选举期间接受日志条目);这引入了额外的机制和复杂度。<br>
ZooKeeper的已公布的描述中也允许日志条目在leader中双向的传输，但其实现明显与Raft更加相似。<br>Raft has fewer message types than any other algorithm for consensus-based log replication that we are aware of.<br>
For example, we counted the message types VR and ZooKeeper use for basic consensus and membership<br>
changes (excluding log compaction and client interaction, as these are nearly independent of the algorithms).<br>
VR and ZooKeeper each define 10 different message types, while Raft has only 4 message types (two RPC requests and their responses).<br>
Raft’s messages are a bit more dense than the other algorithms’, but they are simpler collectively.<br>
In addition, VR and ZooKeeper are described in terms of transmitting entire logs during leader changes;<br>
additional message types will be required to optimize these mechanisms so that they are practical.<br>Raft的消息类型比任何其它已知的、基于日志复制的共识算法都要少。<br>
例如，我们统计了VR和ZooKeeper用于基础共识和成员变更的消息类型数(不包括日志压缩和客户端交互，因为这些与算法几乎是独立的)。<br>
VR和ZooKeeper都定义了10种不同的消息类型，而Raft只有4种(2种RPC的请求以及它们的响应)。<br>
Raft的消息比其它算法的要稍微紧密一些，但总体上更加简单。<br>
此外，VR和ZooKeeper所描述的在任期转换时需要传输完整的日志;所以在实践中需要额外的消息类型来优化这些机制。<br>Raft’s strong leadership approach simplifies the algorithm, but it precludes some performance optimizations.<br>
For example, Egalitarian Paxos (EPaxos) can achieve higher performance under some conditions with a leaderless approach [27].<br>
EPaxos exploits commutativity in state machine commands.<br>
Any server can commit a command with just one round of communication as long as other commands that are proposed concurrently commute with it.<br>
However, if commands that are proposed concurrently do not commute with each other, EPaxos requires an additional round of communication.<br>
Because any server may commit commands, EPaxos balances load well between servers and is able to achieve lower latency than Raft in WAN settings.<br>
However, it adds significant complexity to Paxos.<br>Raft的强领导力方法简化了算法，但是也排除了一些性能优化。<br>
例如，Egalitarian Paxos(EPaxos)可以通过无leader的方法在某些条件下可以获得更高的性能。<br>
EPaxos利用了状态机指令的交换性。<br>
只要同时提出的其它指令能够与之交换，任何服务器都可以仅在一轮通信中提交指令。<br>
然而，如果同时发出的指令不能相互交换，则EPaxos需要额外的一轮通信。<br>
因为任何服务器都能够提交指令，EPaxos能够更好的平衡服务器间的负载并且能够达到比Raft的WAN设置更低的延迟。<br>
然而，这显著的增加了Paxos的复杂性。<br>Several different approaches for cluster membership changes have been proposed or implemented in other work,<br>
including Lamport’s original proposal [15], VR [22], and SMART [24].<br>
We chose the joint consensus approach for Raft because it leverages the rest of the consensus protocol,<br>
so that very little additional mechanism is required for membership changes.<br>
Lamport’s α-based approach was not an option for Raft because it assumes consensus can be reached without a leader.<br>
In comparison to VR and SMART, Raft’s reconfiguration algorithm has the advantage<br>
that membership changes can occur without limiting the processing of normal requests;<br>
in contrast, VR stops all normal processing during configuration changes,<br>
and SMART imposes an α-like limit on the number of outstanding requests.<br>
Raft’s approach also adds less mechanism than either VR or SMART.<br>在其它工作中，几种不同的用于集群成员变更的方法已经被提出或被实现，包括Lamport的原始提案，VR以及SMART。<br>
我们为Raft选择了联合一致的方法，因为它利用了一致性协议的其余部分，因此成员变更只需要增加非常少的额外机制。<br>
Lamport的α-based方法没有被Raft选中，因为它假设可以在没有leader的情况下达成共识。<br>
与VR和SMART相比，Raft的刷新配置的算法有一个优点是可以再不限制正常请求的情况下进行成员变更；<br>
相比之下，VR在配置变更期间停止所有正常的请求处理并且SMART对未完成的请求施加了α-like限制。<br>
Raft的方法相比VR或者SMART也增加了最少的机制。<br><br>Algorithms are often designed with correctness, efficiency, and/or conciseness as the primary goals.<br>
Although these are all worthy goals, we believe that understandability is just as important.<br>
None of the other goals can be achieved until developers render the algorithm into a practical implementation,<br>
which will inevitably deviate from and expand upon the published form.<br>
Unless developers have a deep understanding of the algorithm and can create intuitions about it,<br>
it will be difficult for them to retain its desirable properties in their implementation.<br>算法的设计通常以正确性，效率和/或间接性为主要目标。<br>
尽管这些都是有价值的目标，我们认为可理解性同样重要。<br>
在开发人员将算法转化为一个可行的实现前无法达成任何其它的目标，而实际实现将不可避免的偏离和拓展已发布的形式。<br>
除非开发人员对算法有着很深的理解并且对其产生直觉，否则其将很难在他们的实现中保留理想的特性。<br>In this paper we addressed the issue of distributed consensus, where a widely accepted but impenetrable algorithm,<br>
Paxos, has challenged students and developers for many years.<br>
We developed a new algorithm, Raft, which we have shown to be more understandable than Paxos.<br>
We also believe that Raft provides a better foundation for system building.<br>
Using understandability as the primary design goal changed the way we approached the design of Raft;<br>
as the design progressed we found ourselves reusing a few techniques repeatedly,<br>
such as decomposing the problem and simplifying the state space.<br>
These techniques not only improved the understandability of Raft but also made it easier to convince ourselves of its correctness.<br>在本文中我们讨论了分布式一致性的问题，一种被广泛接受但难于实现的算法Paxos，在多年来一直在挑战着学生和开发者。<br>
我们开发了一种新的算法，Raft，我们已经展示了其比Paxos更加容易理解。<br>
我们也认为Raft为构建系统提供了一个更好的基础。<br>
以可理解性作为主要实现目标改变了我们设计Raft时的方法；随着设计的近战我们发现我们重复的复用了少量技术，例如分解问题和简化状态空间。<br>
这些技术不仅提高了Raft的可理解性也使得我们更容易相信它的正确性。<br><br>The user study would not have been possible without the support of Ali Ghodsi, David Mazieres,<br>
and the students of CS 294-91 at Berkeley and CS 240 at Stanford.<br>
Scott Klemmer helped us design the user study, and Nelson Ray advised us on statistical analysis.<br>
The Paxos slides for the user study borrowed heavily from a slide deck originally created by Lorenzo Alvisi.<br>
Special thanks go to David Mazi`eres and Ezra Hoch for finding subtle bugs in Raft.<br>
Many people provided helpful feedback on the paper and user study materials,<br>
including Ed Bugnion, Michael Chan, Hugues Evrard,Daniel Giffin, Arjun Gopalan, Jon Howell, Vimalkumar Jeyakumar, Ankita Kejriwal,<br>
Aleksandar Kracun, Amit Levy, Joel Martin, Satoshi Matsushita, Oleg Pesok, David Ramos,<br>
Robbert van Renesse, Mendel Rosenblum, Nicolas Schiper, Deian Stefan, Andrew Stone, Ryan Stutsman,<br>
David Terei, Stephen Yang, Matei Zaharia, 24 anonymous conference reviewers (with duplicates), and especially our shepherd Eddie Kohler.<br>
Werner Vogels tweeted a link to an earlier draft, which gave Raft significant exposure.<br>
This work was supported by the Gigascale Systems Research Center and the Multiscale Systems Center,<br>
two of six research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program,<br>
by STAR net, a Semiconductor Research Corporation program sponsored by MARCO and DARPA,<br>
by the National Science Foundation under Grant No. 0963859, and by grants from Facebook, Google, Mellanox, NEC, NetApp, SAP,<br>
and Samsung. Diego Ongaro is supported by The Junglee Corporation Stanford Graduate Fellowship.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/in-search-of-an-understandable-consensus-algorithm(extended-version).html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft 原始论文中英/In Search of an Understandable Consensus Algorithm(Extended Version).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:46:27 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172940.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172940.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Raft实现 - 一致性算法介绍]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:raft" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#raft</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:raft" class="tag" target="_blank" rel="noopener nofollow">#raft</a><br><br><br>
<br>对可靠性有很高要求的系统，通常都会额外部署1至多个机器为备用副本组成主备集群，避免出现单点故障。<br>
有状态的系统需要主节点与备用副本间以某种方式进行数据复制，这样主节点出现故障时就能快速的令备用机器接管系统以达到高可用的目的。
<br>常见的主备复制方式是异步、弱一致性的，例如DNS系统，mysql、redis(7.0之前)等数据库的主备复制，或者通过某种消息中间件来进行解耦，即在CAP中选择了AP(高可用、分区容错)而舍弃了C(强一致性)。<br>
弱一致性的AP相比强一致CP的复制有着许多优点：效率高(多个单次操作可以批量处理)，耦合性低(备份节点挂了也不影响主节点工作)，实现相对简单等等。<br>
但AP复制最大的缺点就是丧失了强一致性，主节点在操作完成响应客户端后，但还未成功同步到备份节点前宕机，对应的变更存在着丢失的风险，因此AP的方案不适用于对一致性有苛刻要求的场合。
<br>最原始的强一致性主备同步，即主节点在每一个备份节点同步完成后才能响应客户端成功的方案效率太低，可用性太差(任意一个备份节点故障就会使得集群不可用)。<br>
因此基于多数派的分布式强一致算法被发明了出来，其中最早被提出的便是Paxos算法。但Paxos算法过于复杂，在分布式环境下有大量的case需要得到正确的实现，因此时至今日也没有多少系统真正的将Paxos落地。
<br><br>
<br>由于Paxos过于复杂的原因，Raft算法被发明了出来。Raft算法在设计时大量参考了Paxos，也是一个基于日志和多数派的一致性算法，但在很多细节上相比Paxos做了许多简化。
<br>因为Raft比Paxos要简单很多，更容易被开发人员理解并最终用于构建实际的系统。因此即使raft算法的性能相比Paxos要差一点，但目前流行的强一致分布式系统基本都是基于Raft算法的。
<br><a data-tooltip-position="top" aria-label="https://raft.github.io/raft.pdf" rel="noopener nofollow" class="external-link" href="https://raft.github.io/raft.pdf" target="_blank">raft的论文</a> 中将raft算法的功能分解为4个模块：<br>
<br>leader选举
<br>日志复制
<br>日志压缩
<br>集群成员动态变更
<br>其中前两项“leader选举”和“日志复制”是raft算法的基础，而后两项“日志压缩”和“集群成员动态变更”属于raft算法在功能上的重要优化。<br><a data-tooltip-position="top" aria-label="https://www.cnblogs.com/xiaoxiongcanguan/p/17552027.html" rel="noopener nofollow" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17552027.html" target="_blank">raft论文中英翻译</a><br><br>通过raft的论文或者其它相关资料，读者基本能大致理解raft的工作原理。<br>
但纸上得来终觉浅，绝知此事要躬行，亲手实践才能更好的把握raft中的精巧细节，加深对raft算法的理解，更有效的阅读基于raft或其它一致性协议的开源项目源码。<br><br>在这个系列博客中会带领读者一步步实现一个基于raft算法的简易KV数据库,即MyRaft。MyRaft的实现基于原始的raft算法，没有额外的优化，目的是为了保证实现的简单性。<br>
MyRaft实现了raft论文中提到的三个功能，即”leader选举“、”日志复制“和”日志压缩“（在实践中发现“集群成员动态变更”对原有逻辑有较大改动而大幅增加了复杂度，限于个人水平暂不实现）。<br>
三个功能会通过三次迭代实验逐步完成，其中每个迭代都会以博客的形式分享出来。<br><br><br>
<br>由于是MyRaft的第一个迭代，在这个迭代中需要先搭好MyRaft的基础骨架。<br>
raft中的每个节点本质上是一个rpc服务器，同时也是一个rpc的消费者，节点之间通过rpc的方式互相通信。
<br>MyRaft使用的rpc框架是上一个实验中自己实现的MyRpc框架：<br>
博客地址: <a rel="noopener nofollow" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17506728.html" target="_blank">https://www.cnblogs.com/xiaoxiongcanguan/p/17506728.html</a><br>
github地址：<a rel="noopener nofollow" class="external-link" href="https://github.com/1399852153/MyRpc" target="_blank">https://github.com/1399852153/MyRpc</a> (main分支)
<br><br>
<br>因为lab1中只实现leader选举，简单起见只定义当前所需的api接口，接口参数相比最终的实现也省去了大量当前用不上的字段，后续有需要再进行拓展。
<br>public interface RaftService {

    /**
     * 请求投票 requestVote
     *
     * Receiver implementation:
     * 1. Reply false if term &lt; currentTerm (§5.1)
     * 2. If votedFor is null or candidateId, and candidate’s log is at
     * least as up-to-date as receiver’s log, grant vote (§5.2, §5.4)
     *
     * 接受者需要实现以下功能：
     * 1. 如果参数中的任期值term小于当前自己的任期值currentTerm，则返回false不同意投票给调用者
     * 2. 如果自己还没有投票(FIFO)或者已经投票给了candidateId对应的节点(幂等)，
     *    并且候选人的日志至少与被调用者的日志一样新(比较日志的任期值和索引值)，则投票给调用者(返回值里voteGranted为true)
     * */
    RequestVoteRpcResult requestVote(RequestVoteRpcParam requestVoteRpcParam);

    /**
     * 追加日志条目 AppendEntries
     * */
    AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam);
}
<br>/**
 * 请求投票的RPC接口参数对象
 */
public class RequestVoteRpcParam implements Serializable {

    /**
     * 候选人的任期编号
     * */
    private int term;

    /**
     * 候选人的Id
     * */
    private String candidateId;

    /**
     * 候选人最新日志的索引编号
     * */
    private long lastLogIndex;

    /**
     * 候选人最新日志对应的任期编号
     * */
    private int lastLogTerm;
}
<br>/**
 * 请求投票的RPC接口响应对象
 * */
public class RequestVoteRpcResult implements Serializable {

    /**
     * 被调用者当前的任期值
     * */
    private int term;

    /**
     * 是否同意投票给调用者
     * */
    private boolean voteGranted;
}
<br>/**
 * 追加日志条目的RPC接口参数对象
 * */
public class AppendEntriesRpcParam implements Serializable {

    /**
     * 当前leader的任期值
     * */
    private int term;

    /**
     * leader的id
     * */
    private String leaderId;
}
<br>/**
 * 追加日志条目的RPC接口响应对象
 * */
public class AppendEntriesRpcResult implements Serializable {

    /**
     * 被调用者当前的任期值
     * */
    private int term;

    /**
     * 是否处理成功
     * */
    private boolean success;
}
<br><br>/**
 * raft的rpc服务
 * */
public class RaftRpcServer extends RaftServer {

    private final Registry registry;
    private final RaftNodeConfig currentNodeConfig;

    public RaftRpcServer(RaftConfig raftConfig, Registry registry){
        super(raftConfig);

        this.currentNodeConfig = raftConfig.getCurrentNodeConfig();
        this.registry = registry;
    }

    @Override
    public void init(List&lt;RaftService&gt; otherNodeInCluster) {
        // 先初始化内部模块
        super.init(otherNodeInCluster);

        // 初始化内部的模块后，启动rpc
        initRpcServer();
    }

    public List&lt;RaftService&gt; getRpcProxyList(List&lt;RaftNodeConfig&gt; otherNodeInCluster){
        return initRpcConsumer(otherNodeInCluster);
    }

    private List&lt;RaftService&gt; initRpcConsumer(List&lt;RaftNodeConfig&gt; otherNodeInCluster){
        ConsumerBootstrap consumerBootstrap = new ConsumerBootstrap()
            .registry(registry)
            .loadBalance(new SimpleRoundRobinBalance());

        // 注册消费者
        Consumer&lt;RaftService&gt; consumer = consumerBootstrap.registerConsumer(RaftService.class,new FastFailInvoker());
        RaftService raftServiceProxy = consumer.getProxy();

        List&lt;RaftService&gt; raftRpcConsumerList = new ArrayList&lt;&gt;();
        for(RaftNodeConfig raftNodeConfig : otherNodeInCluster){
            // 使用rpc代理的客户端
            raftRpcConsumerList.add(new RaftRpcConsumer(raftNodeConfig,raftServiceProxy));
        }

        return raftRpcConsumerList;
    }

    private void initRpcServer(){
        URLAddress providerURLAddress = new URLAddress(currentNodeConfig.getIp(),currentNodeConfig.getPort());
        Provider&lt;RaftService&gt; provider = new Provider&lt;&gt;();
        provider.setInterfaceClass(RaftService.class);
        provider.setRef(this);
        provider.setUrlAddress(providerURLAddress);
        provider.setRegistry(registry);
        provider.export();

        NettyServer nettyServer = new NettyServer(providerURLAddress);
        nettyServer.init();
    }
}
<br>public class RaftRpcConsumer implements RaftService {

    private static final Logger logger = LoggerFactory.getLogger(RaftRpcConsumer.class);

    private final RaftNodeConfig targetNodeConfig;
    private final RaftService raftServiceProxy;

    public RaftRpcConsumer(RaftNodeConfig targetNodeConfig, RaftService proxyRaftService) {
        this.targetNodeConfig = targetNodeConfig;
        this.raftServiceProxy = proxyRaftService;
    }

    @Override
    public RequestVoteRpcResult requestVote(RequestVoteRpcParam requestVoteRpcParam) {
        // 强制指定rpc目标的ip/port
        setTargetProviderUrl();
        RequestVoteRpcResult result = raftServiceProxy.requestVote(requestVoteRpcParam);
        return result;
    }

    @Override
    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        // 强制指定rpc目标的ip/port
        setTargetProviderUrl();
        AppendEntriesRpcResult result = raftServiceProxy.appendEntries(appendEntriesRpcParam);
        return result;
    }

    private void setTargetProviderUrl(){
        ConsumerRpcContext consumerRpcContext = ConsumerRpcContextHolder.getConsumerRpcContext();
        consumerRpcContext.setTargetProviderAddress(
            new URLAddress(targetNodeConfig.getIp(),targetNodeConfig.getPort()));
    }
}
<br><br>
<br>raft的论文中提到raft服务中需要持久化的三个要素：currentTerm（当前服务器的任期值）、votedFor(当前服务器在此之前投票给了谁)和logs(raft的操作日志，与本篇博客无关在lab2中才会引入)。
<br>currentTerm和votedFor需要持久化的原因是为了避免raft节点在完成leader选举的投票后宕机，重启恢复后如果这两个数据丢失了就很容易在同一任期内投票给多个候选人而出现集群脑裂(即多个合法leader)。
<br>MyRaft用磁盘文件进行持久化，简单起见在currentTerm或votedFor更新时加写锁，通过原子性的整体刷盘来完成持久化。
<br>public class RaftServerMetaData {

    /**
     * 当前服务器的任期值
     * */
    private int currentTerm;

    /**
     * 当前服务器在此之前投票给了谁？
     * (候选者的serverId，如果还没有投递就是null)
     * */
    private String votedFor;
}
<br>public class RaftServerMetaDataPersistentModule {

    /**
     * 当前服务器的任期值
     * */
    private volatile int currentTerm;

    /**
     * 当前服务器在此之前投票给了谁？
     * (候选者的serverId，如果还没有投递就是null)
     * */
    private volatile String votedFor;

    private final File persistenceFile;

    private final ReentrantReadWriteLock reentrantLock = new ReentrantReadWriteLock();
    private final ReentrantReadWriteLock.WriteLock writeLock = reentrantLock.writeLock();
    private final ReentrantReadWriteLock.ReadLock readLock = reentrantLock.readLock();

    public RaftServerMetaDataPersistentModule(String serverId) {
        String userPath = System.getProperty("user.dir") + File.separator + serverId;

        this.persistenceFile = new File(userPath + File.separator + "raftServerMetaData-" + serverId + ".txt");
        MyRaftFileUtil.createFile(persistenceFile);

        // 读取持久化在磁盘中的数据
        RaftServerMetaData raftServerMetaData = readRaftServerMetaData(persistenceFile);
        this.currentTerm = raftServerMetaData.getCurrentTerm();
        this.votedFor = raftServerMetaData.getVotedFor();
    }

    public int getCurrentTerm() {
        readLock.lock();
        try {
            return currentTerm;
        }finally {
            readLock.unlock();
        }
    }

    public void setCurrentTerm(int currentTerm) {
        writeLock.lock();
        try {
            this.currentTerm = currentTerm;

            // 更新后数据落盘
            persistentRaftServerMetaData(new RaftServerMetaData(this.currentTerm,this.votedFor),persistenceFile);
        }finally {
            writeLock.unlock();
        }
    }

    public String getVotedFor() {
        readLock.lock();
        try {
            return votedFor;
        }finally {
            readLock.unlock();
        }
    }

    public void setVotedFor(String votedFor) {
        writeLock.lock();
        try {
            if(Objects.equals(this.votedFor,votedFor)){
                // 相等的话就不刷新了
                return;
            }
            
            this.votedFor = votedFor;

            // 更新后数据落盘
            persistentRaftServerMetaData(new RaftServerMetaData(this.currentTerm,this.votedFor),persistenceFile);
        }finally {
            writeLock.unlock();
        }
    }

    private static RaftServerMetaData readRaftServerMetaData(File persistenceFile){
        String content = MyRaftFileUtil.getFileContent(persistenceFile);
        if(StringUtils.hasText(content)){
            return JsonUtil.json2Obj(content,RaftServerMetaData.class);
        }else{
            return RaftServerMetaData.getDefault();
        }
    }

    private static void persistentRaftServerMetaData(RaftServerMetaData raftServerMetaData, File persistenceFile){
        String content = JsonUtil.obj2Str(raftServerMetaData);

        MyRaftFileUtil.writeInFile(persistenceFile,content);
    }
}
<br><br><br>raft的leader选举在论文中有较详细的描述，这里说一下我认为的关键细节。<br>
<br>Raft算法中leader扮演着绝对核心的角色，leader负责处理客户端的请求、将操作日志同步给其它的follower节点以及通知follower提交日志等等。<br>
因此Raft集群必须基于多数原则选举出一个存活的leader才能对外提供服务，并且一个任期内只能有一个基于多数票选出的leader。
<br>raft是非拜占庭容错共识算法，rpc通信时交互的双方的请求和响应都是可信的，不会作假，节点运行的行为也符合raft算法的规定。
<br>raft中存在任期term的概念，任期值只会单向递增，可以理解为一个虚拟的时间，是raft实现线性一致性关键的一环。过去的leader(term值更小的)需要服从、追随现任的leader(term值更大的)。
<br>在raft节点刚启动时处于follower追随者状态。如果一段时间内raft节点没有接受到来自leader的定时心跳rpc(logEntry为空的appendEntries)通知时就会发起一轮新的选举。<br>
产生这个现象的原因有很多，比如集群刚刚启动还没有leader；或者之前的leader因为某种原因宕机或与follower的网络通信出现故障等。
<br>发起请求的follower会转变为candidate候选人状态，并首先投票给自己。同时并行的向集群中的其它节点发起请求投票的rpc请求(requestVote),可以理解为给自己拉票。<br>
接收到requestVote请求的节点会根据自身的状态等信息决定是否投票给发起投票的节点。<br>
当candidate获得了集群中超过半数的投票(即包括自己在内的1票加上requestVote返回投票成功的数量超过半数(比如5节点得到3票，6节点得到4票))，则candidate成为当前任期的leader。<br>
如果没有任何一个candidate获得多数选票(没选出leader，可能是分票了，也可能是网络波动等等)，则candidate会将当前任期自增1，则下一次选举超时时会再触发一轮新的选举，循环往复直至选出leader。<br>
<img alt="Pasted image 20240725172248.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172248.png">
<br>leader当选后需要立即向其它的节点发送心跳rpc(logEntry为空的appendEntries)，昭告新leader的产生以抑制其它节点发起新的选举。<br>
心跳rpc必须以一定频率的定时向所有follower发送，发送的时间间隔需要小于设置的选举超时时间。
<br>由于处理requestVote是先到先得的，同一任期内先发起投票请求的candidate会收到票，后发送的会被拒绝。<br>
假如leader宕机了，则每个follower都会在一段时间后触发新一轮选举，如果没有额外的限制，则每个节点并发的发起选举很容易导致分票，即自己投给自己，则难以达成共识(取得多数票)。<br>
Raft的论文中提出了随机化选举超时时间的方案，即每个follower节点的选举超时时间是一个固定值再加上一个随机化的值得到的，这样很难在同一瞬间都触发选举。<br>
随机超时时间更短的follower能够最先发起选举，更快的得到其它节点的投票从而避免分票的情况。<br>
虽然无法完全避免分票，但实践中发现效果很好，随机超时时间下通常少数的几次分票后就能收敛而选出leader来。
<br>注意：raft论文在5.4安全性一节中提到，leader选举对于candidate的日志状态有一定的要求(因为只有拥有完整日志的节点才有资格成为leader，确保leader更替时日志不会丢失)，<br>
但lab1中不支持日志复制，所以MyRaft在lab1的requestVote实现中省略了相关逻辑。
<br><br>下面基于源码展开介绍MyRaft是如何实现raft领导者选举的。<br>大致分为以下几部分：<br>
<br>raft节点配置
<br>raft节点定时选举超时检查
<br>candidate发起选举
<br>leader定时发起心跳广播
<br>raft节点处理requestVote请求
<br><br>
<br>raft服务中有很多参数需要配置，比如服务的ip/port，或者raft相关的选举超时时间、心跳间隔时间等等。
<br>MyRaft中统一放到一个叫RaftConfig的类里维护，后续的功能实现时也会在这个类中进行拓展。
<br>public class RaftConfig {
    
    /**
     * 当前服务节点的id(集群内全局唯一)
     * */
    private final String serverId;

    /**
     * 自己节点的配置
     * */
    private final RaftNodeConfig currentNodeConfig;

    /**
     * 整个集群所有的服务节点的id集合
     * */
    private final List&lt;RaftNodeConfig&gt; raftNodeConfigList;

    /**
     * 集群中多数的值(例如：5节点majorityNum=3,6节点majorityNum=4)
     * */
    private final int majorityNum;

    /**
     * 选举超时时间 单位:秒
     * */
    private int electionTimeout;
    
    /**
     * 选举超时时间的随机化区间 单位：毫秒
     * */
    private Range&lt;Integer&gt; electionTimeoutRandomRange;

    /**
     * 心跳间隔时间 单位：秒
     * */
    private int HeartbeatInternal;
}
<br>public class RaftNodeConfig {

    private String serverId;
    private String ip;
    private int port;
}
<br><br>
<br>MyRaft中将leader选举相关的主要逻辑都集中维护在RaftLeaderElectionModule类中。<br>
lastHeartbeatTime属性用于存储最后一次收到leader心跳的绝对时间，如果当前节点状态不是leader，并且发现lastHeartbeatTime距离当前时间已经超过了指定的选举超时时间则触发选举。
<br>心跳检查的超时逻辑集中在HeartbeatTimeoutCheckTask中。<br>
由于需要引入随机化的心跳超时时间，因此无法使用ScheduledExecutorService的scheduleAtFixedRate方法，改为在每个任务执行完成时再添加一个新任务回去的方式来实现。
<br>/**
 * Raft服务器的leader选举模块
 * */
public class RaftLeaderElectionModule {

    private static final Logger logger = LoggerFactory.getLogger(RaftLeaderElectionModule.class);

    private final RaftServer currentServer;

    /**
     * 最近一次接受到心跳的时间
     * */
    private volatile Date lastHeartbeatTime;

    private final ScheduledExecutorService scheduledExecutorService;

    private final ExecutorService rpcThreadPool;

    public RaftLeaderElectionModule(RaftServer currentServer) {
        this.currentServer = currentServer;
        this.lastHeartbeatTime = new Date();
        this.scheduledExecutorService = Executors.newScheduledThreadPool(3);
        this.rpcThreadPool = Executors.newFixedThreadPool(
                Math.max(currentServer.getOtherNodeInCluster().size() * 2, 1));

        registerHeartbeatTimeoutCheckTaskWithRandomTimeout();
    }

    /**
     * 提交新的延迟任务(带有随机化的超时时间)
     * */
    public void registerHeartbeatTimeoutCheckTaskWithRandomTimeout(){
        int electionTimeout = currentServer.getRaftConfig().getElectionTimeout();
        if(currentServer.getCurrentTerm() &gt; 0 &amp;&amp; currentServer.getRaftConfig().getDebugElectionTimeout() != null){
            // debug的时候多等待一些时间
            electionTimeout = currentServer.getRaftConfig().getDebugElectionTimeout();
        }

        long randomElectionTimeout = getRandomElectionTimeout();
        // 选举超时时间的基础上，加上一个随机化的时间
        long delayTime = randomElectionTimeout + electionTimeout * 1000L;
        logger.debug("registerHeartbeatTimeoutCheckTaskWithRandomTimeout delayTime={}",delayTime);
        scheduledExecutorService.schedule(
            new HeartbeatTimeoutCheckTask(currentServer,this),delayTime,TimeUnit.MILLISECONDS);
    }

    /**
     * 处理投票请求
     * 注意：synchronized修饰防止不同candidate并发的投票申请处理，以FIFO的方式处理
     * */
    public synchronized RequestVoteRpcResult requestVoteProcess(RequestVoteRpcParam requestVoteRpcParam){
        if(this.currentServer.getCurrentTerm() &gt; requestVoteRpcParam.getTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 发起投票的candidate任期小于当前服务器任期，拒绝投票给它
            logger.info("reject requestVoteProcess! term &lt; currentTerm, currentServerId={}",currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 发起投票的节点任期高于当前节点，无条件投票给它(任期高的说了算)
        if(this.currentServer.getCurrentTerm() &lt; requestVoteRpcParam.getTerm()){
            // 刷新元数据
            this.currentServer.refreshRaftServerMetaData(
                new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
            // 任期没它高，自己转为follower
            this.currentServer.setServerStatusEnum(ServerStatusEnum.FOLLOWER);
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
        }

        // term任期值相同，需要避免同一任期内投票给不同的节点而脑裂
        if(this.currentServer.getVotedFor() != null &amp;&amp; !this.currentServer.getVotedFor().equals(requestVoteRpcParam.getCandidateId())){
            // If votedFor is null or candidateId（取反的卫语句）
            // 当前服务器已经把票投给了别人,拒绝投票给发起投票的candidate
            logger.info("reject requestVoteProcess! votedFor={},currentServerId={}",
                currentServer.getVotedFor(),currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 投票校验通过,刷新元数据
        this.currentServer.refreshRaftServerMetaData(
            new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
        this.currentServer.processCommunicationHigherTerm(requestVoteRpcParam.getTerm());
        return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
    }

    public void refreshLastHeartbeatTime(){
        // 刷新最新的接受到心跳的时间
        this.lastHeartbeatTime = new Date();
        // 接受新的心跳,说明现在leader是存活的，清理掉之前的投票信息
        this.currentServer.cleanVotedFor();
    }
    
    private long getRandomElectionTimeout(){
        long min = currentServer.getRaftConfig().getElectionTimeoutRandomRange().getLeft();
        long max = currentServer.getRaftConfig().getElectionTimeoutRandomRange().getRight();

        // 生成[min,max]范围内随机整数的通用公式为：n=rand.nextInt(max-min+1)+min。
        return ThreadLocalRandom.current().nextLong(max-min+1) + min;
    }
}
<br>/**
 * 心跳超时检查任务
 * */
public class HeartbeatTimeoutCheckTask implements Runnable{

    private static final Logger logger = LoggerFactory.getLogger(HeartbeatTimeoutCheckTask.class);

    private final RaftServer currentServer;
    private final RaftLeaderElectionModule raftLeaderElectionModule;

    public HeartbeatTimeoutCheckTask(RaftServer currentServer, RaftLeaderElectionModule raftLeaderElectionModule) {
        this.currentServer = currentServer;
        this.raftLeaderElectionModule = raftLeaderElectionModule;
    }

    @Override
    public void run() {
        if(currentServer.getServerStatusEnum() == ServerStatusEnum.LEADER){
            // leader是不需要处理心跳超时的
            // 注册下一个心跳检查任务
            raftLeaderElectionModule.registerHeartbeatTimeoutCheckTaskWithRandomTimeout();
        }else{
            try {
                doTask();
            }catch (Exception e){
                logger.info("do HeartbeatTimeoutCheckTask error! ignore",e);
            }

            // 注册下一个心跳检查任务
            raftLeaderElectionModule.registerHeartbeatTimeoutCheckTaskWithRandomTimeout();
        }
    }

    private void doTask(){
        logger.debug("do HeartbeatTimeoutCheck start {}",currentServer.getServerId());

        int electionTimeout = currentServer.getRaftConfig().getElectionTimeout();

        // 当前时间
        Date currentDate = new Date();
        Date lastHeartbeatTime = raftLeaderElectionModule.getLastHeartbeatTime();
        long diffTime = currentDate.getTime() - lastHeartbeatTime.getTime();

        logger.debug("currentDate={}, lastHeartbeatTime={}, diffTime={}, serverId={}",
            currentDate,lastHeartbeatTime,diffTime,currentServer.getServerId());
        // 心跳超时判断
        if(diffTime &gt; (electionTimeout * 1000L)){
            logger.info("HeartbeatTimeoutCheck check fail, trigger new election! serverId={}",currentServer.getServerId());

            // 触发新的一轮选举
            triggerNewElection();
        }else{
            // 认定为心跳正常，无事发生
            logger.debug("HeartbeatTimeoutCheck check success {}",currentServer.getServerId());
        }

        logger.debug("do HeartbeatTimeoutCheck end {}",currentServer.getServerId());
    }

    private void triggerNewElection(){
        logger.info("HeartbeatTimeoutCheck check fail, trigger new election! serverId={}",currentServer.getServerId());

        // 距离最近一次接到心跳已经超过了选举超时时间，触发新一轮选举

        // 当前服务器节点当前任期自增1
        currentServer.setCurrentTerm(currentServer.getCurrentTerm()+1);
        // 自己发起选举，先投票给自己
        currentServer.setVotedFor(currentServer.getServerId());
        // 角色转变为CANDIDATE候选者
        currentServer.setServerStatusEnum(ServerStatusEnum.CANDIDATE);

        // 并行的发送请求投票的rpc给集群中的其它节点
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();
        List&lt;Future&lt;RequestVoteRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        // 构造请求参数
        RequestVoteRpcParam requestVoteRpcParam = new RequestVoteRpcParam();
        requestVoteRpcParam.setTerm(currentServer.getCurrentTerm());
        requestVoteRpcParam.setCandidateId(currentServer.getServerId());

        for(RaftService node : otherNodeInCluster){
            Future&lt;RequestVoteRpcResult&gt; future = raftLeaderElectionModule.getRpcThreadPool().submit(
                ()-&gt; {
                    RequestVoteRpcResult rpcResult = node.requestVote(requestVoteRpcParam);
                    // 收到更高任期的处理
                    currentServer.processCommunicationHigherTerm(rpcResult.getTerm());
                    return rpcResult;
                }
            );

            futureList.add(future);
        }

        List&lt;RequestVoteRpcResult&gt; requestVoteRpcResultList = CommonUtil.concurrentGetRpcFutureResult(
            "requestVote", futureList,
            raftLeaderElectionModule.getRpcThreadPool(),1,TimeUnit.SECONDS);

        // 获得rpc响应中决定投票给自己的总票数（算上自己的1票）
        int getRpcVoted = (int) requestVoteRpcResultList.stream().filter(RequestVoteRpcResult::isVoteGranted).count()+1;
        logger.info("HeartbeatTimeoutCheck election, getRpcVoted={}, currentServerId={}",getRpcVoted,currentServer.getServerId());

        // 是否获得大多数的投票
        boolean majorVoted = getRpcVoted &gt;= this.currentServer.getRaftConfig().getMajorityNum();
        if(majorVoted){
            logger.info("HeartbeatTimeoutCheck election result: become a leader! {}, currentTerm={}",currentServer.getServerId(),currentServer.getCurrentTerm());

            // 票数过半成功当选为leader
            currentServer.setServerStatusEnum(ServerStatusEnum.LEADER);
            currentServer.setCurrentLeader(currentServer.getServerId());

            // 成为leader后立马发送一次心跳,抑制其它节点发起新的一轮选举
            // Upon election: send initial empty AppendEntries RPCs (heartbeat) to each server;
            // repeat during idle periods to prevent election timeouts (§5.2)
            HeartbeatBroadcastTask.doHeartbeatBroadcast(currentServer);
        }else{
            // 票数不过半，无法成为leader
            logger.info("HeartbeatTimeoutCheck election result: not become a leader! {}",currentServer.getServerId());
        }

        this.currentServer.cleanVotedFor();
    }
}
<br><br>
<br>在上一节介绍的HeartbeatTimeoutCheckTask中，如果发现有一段时间没有收到心跳后当前节点便会触发新一轮的选举，主要逻辑在triggerNewElection方法中。<br>
triggerNewElection中通过首先令当前term值自增1并投票给自己，然后并行的向集群中的其它节点发送requestVote的rpc请求。
<br>并行处理逻辑通过CommonUtil中的concurrentGetRpcFutureResult方法收集所有的响应结果。<br>
通过future.get设置超时时间，超时则认为是投票失败。
<br>在超时时间内获得所有响应结果后，计算所得到的的票数是否大于半数(&gt;majorityNum)。<br>
如果超过半数则认为选举成功，自己成为合法的leader。当前节点刷新相关的状态数据，同时立即发起一次心跳广播以抑制其它节点发起新的选举。
<br>public class CommonUtil {

    private static final Logger logger = LoggerFactory.getLogger(CommonUtil.class);

    /**
     * 并发的获得future列表的结果
     * */
    public static &lt;T&gt; List&lt;T&gt; concurrentGetRpcFutureResult(
            String info, List&lt;Future&lt;T&gt;&gt; futureList, ExecutorService threadPool, long timeout, TimeUnit timeUnit){
        CountDownLatch countDownLatch = new CountDownLatch(futureList.size());

        List&lt;T&gt; resultList = new ArrayList&lt;&gt;(futureList.size());

        for(Future&lt;T&gt; futureItem : futureList){
            threadPool.execute(()-&gt;{
                try {
                    logger.debug(info + " concurrentGetRpcFutureResult start!");
                    T result = futureItem.get(timeout,timeUnit);
                    logger.debug(info + " concurrentGetRpcFutureResult end!");

                    resultList.add(result);
                } catch (Exception e) {
                    // rpc异常不考虑
                    logger.error( "{} getFutureResult error!",info,e);
                } finally {
                    countDownLatch.countDown();
                }
            });
        }

        try {
            countDownLatch.await();
        } catch (InterruptedException e) {
            throw new MyRaftException("getFutureResult error!",e);
        }

        return resultList;
    }
}
<br><br>
<br>MyRaft中将leader心跳广播相关的逻辑都集中在了RaftHeartbeatBroadcastModule类中，心跳广播任务以RaftConfig中设置的频率定期执行。<br>
需要注意的是，心跳的时间间隔必须配置为明显小于配置的选举超时时间的值(考虑到rpc网络请求延迟以及follower实际处理心跳的耗时)，否则leader心跳将无法抑制follower触发选举。
<br>leader心跳广播的逻辑集中在HeartbeatBroadcastTask中。和发起投票类似，当前节点通过future并发的向集群中的所有节点发送logEntry为空的appendEntries(因为lab1中不涉及日志复制，所以直接去掉了logEntry这个参数字段)。<br>
leader原则上可以不关心follower对于心跳的响应结果，但还是需要检查成功响应中返回的term值。如果发现有其它节点返回了更大的term值，说明集群中可能已经选出了新的leader或者正在进行选举，则当前节点需要退回到follower状态。
<br>/**
 * Raft服务器的心跳广播模块
 * */
public class RaftHeartbeatBroadcastModule {

    private final RaftServer currentServer;

    private final ScheduledExecutorService scheduledExecutorService;

    private final ExecutorService rpcThreadPool;

    public RaftHeartbeatBroadcastModule(RaftServer currentServer) {
        this.currentServer = currentServer;

        this.scheduledExecutorService = Executors.newScheduledThreadPool(1);
        this.rpcThreadPool = Executors.newFixedThreadPool(
            Math.max(currentServer.getOtherNodeInCluster().size() * 2, 1));

        int HeartbeatInternal = currentServer.getRaftConfig().getHeartbeatInternal();

        // 心跳广播任务需要以固定频率执行(scheduleAtFixedRate)
        scheduledExecutorService.scheduleAtFixedRate(
            new HeartbeatBroadcastTask(currentServer,this), 0, HeartbeatInternal, TimeUnit.SECONDS);
    }
}
<br>/**
 * leader心跳广播任务
 * */
public class HeartbeatBroadcastTask implements Runnable{

    private static final Logger logger = LoggerFactory.getLogger(HeartbeatBroadcastTask.class);

    private final RaftServer currentServer;
    private final RaftHeartbeatBroadcastModule raftHeartbeatBroadcastModule;

    private int HeartbeatCount = 0;

    public HeartbeatBroadcastTask(RaftServer currentServer, RaftHeartbeatBroadcastModule raftHeartbeatBroadcastModule) {
        this.currentServer = currentServer;
        this.raftHeartbeatBroadcastModule = raftHeartbeatBroadcastModule;
    }

    @Override
    public void run() {
        if(currentServer.getServerStatusEnum() != ServerStatusEnum.LEADER){
            // 只有leader才需要广播心跳
            return;
        }

        // 心跳广播
        doHeartbeatBroadcast(currentServer);
    }

    /**
     * 做心跳广播
     * @return 是否大多数节点依然认为自己是leader
     * */
    public static boolean doHeartbeatBroadcast(RaftServer currentServer){
        logger.info("do HeartbeatBroadcast start {}",currentServer.getServerId());

        // 先刷新自己的心跳时间
        currentServer.getRaftLeaderElectionModule().refreshLastHeartbeatTime();

        // 并行的发送心跳rpc给集群中的其它节点
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();
        List&lt;Future&lt;AppendEntriesRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        // 构造请求参数(心跳rpc，entries为空)
        AppendEntriesRpcParam appendEntriesRpcParam = new AppendEntriesRpcParam();
        appendEntriesRpcParam.setTerm(currentServer.getCurrentTerm());
        appendEntriesRpcParam.setLeaderId(currentServer.getServerId());

        for(RaftService node : otherNodeInCluster){
            Future&lt;AppendEntriesRpcResult&gt; future = currentServer.getRaftHeartbeatBroadcastModule().getRpcThreadPool().submit(
                ()-&gt; {
                    AppendEntriesRpcResult rpcResult = node.appendEntries(appendEntriesRpcParam);
                    // rpc交互时任期高于当前节点任期的处理
                    currentServer.processCommunicationHigherTerm(rpcResult.getTerm());
                    return rpcResult;
                }
            );

            futureList.add(future);
        }

        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = CommonUtil.concurrentGetRpcFutureResult("doHeartbeatBroadcast",futureList,
            currentServer.getRaftHeartbeatBroadcastModule().getRpcThreadPool(),1, TimeUnit.SECONDS);

        // 通知成功的数量(+1包括自己)
        int successResponseCount = (int) (appendEntriesRpcResultList.stream().filter(AppendEntriesRpcResult::isSuccess).count() + 1);
        if(successResponseCount &gt;= currentServer.getRaftConfig().getMajorityNum()
            &amp;&amp; currentServer.getServerStatusEnum() == ServerStatusEnum.LEADER){
            // 大多数节点依然认为自己是leader,并且广播的节点中没有人任期高于当前节点，让当前节点主动让位
            return true;
        }else{
            // 大多数节点不认为自己是leader（包括广播超时等未接到响应的场景，也认为是广播失败）
            return false;
        }
    }
}
<br><br>处理requestVote请求<br>
<br>MyRaft处理requestVote的逻辑在上面提到的RaftLeaderElectionModule的requestVoteProcess方法中。<br>
raft需要保证每个任期都只能选出一个leader，所以对于特定的term任期需要做到只有一个节点能获得超过半数选票。
<br>因此，requestVoteProcess中会对发起投票的candidate和自己本地的term值进行比对，如果term值比自己低就直接拒绝(过去的leader不是leader，只有现在的leader才是leader)。<br>
每个节点只有一票，如果term值相同则需要确保自己在此之前没有投票给其它candidate。
<br>处理心跳请求<br>
<br>MyRaft的处理心跳请求的逻辑在RaftServer类的appendEntries中。由于lab1没有日志复制的功能，所以认为收到的请求都是心跳请求。
<br>appendEntries中，同样需要比对参数中term和本地term的值，尽可能的确保是真正的leader发来的心跳。<br>
如果校验通过了，则将本地的最后接受到的心跳时间刷新为当前时间，来抑制选举超时检查任务中触发新一轮选举。
<br>public class RaftServer implements RaftService {

    private static final Logger logger = LoggerFactory.getLogger(RaftServer.class);

    /**
     * 当前服务节点的id(集群内全局唯一)
     * */
    private final String serverId;

    /**
     * Raft服务端配置
     * */
    private final RaftConfig raftConfig;

    /**
     * 当前服务器的状态
     * */
    private volatile ServerStatusEnum serverStatusEnum;

    /**
     * raft服务器元数据(当前任期值currentTerm、当前投票给了谁votedFor)
     * */
    private final RaftServerMetaDataPersistentModule raftServerMetaDataPersistentModule;

    /**
     * 当前服务认为的leader节点的Id
     * */
    private volatile String currentLeader;

    /**
     * 集群中的其它raft节点服务
     * */
    protected List&lt;RaftService&gt; otherNodeInCluster;

    private RaftLeaderElectionModule raftLeaderElectionModule;
    private RaftHeartbeatBroadcastModule raftHeartbeatBroadcastModule;

    public RaftServer(RaftConfig raftConfig) {
        this.serverId = raftConfig.getServerId();
        this.raftConfig = raftConfig;
        // 初始化时都是follower
        this.serverStatusEnum = ServerStatusEnum.FOLLOWER;

        // 服务器元数据模块
        this.raftServerMetaDataPersistentModule = new RaftServerMetaDataPersistentModule(raftConfig.getServerId());
    }

    public void init(List&lt;RaftService&gt; otherNodeInCluster){
        // 集群中的其它节点服务
        this.otherNodeInCluster = otherNodeInCluster;

        raftLeaderElectionModule = new RaftLeaderElectionModule(this);
        raftHeartbeatBroadcastModule = new RaftHeartbeatBroadcastModule(this);

        logger.info("raft server init end! otherNodeInCluster={}, currentServerId={}",otherNodeInCluster,serverId);
    }

    @Override
    public RequestVoteRpcResult requestVote(RequestVoteRpcParam requestVoteRpcParam) {
        RequestVoteRpcResult requestVoteRpcResult = raftLeaderElectionModule.requestVoteProcess(requestVoteRpcParam);

        processCommunicationHigherTerm(requestVoteRpcParam.getTerm());

        logger.info("do requestVote requestVoteRpcParam={},requestVoteRpcResult={}, currentServerId={}",
            requestVoteRpcParam,requestVoteRpcResult,this.serverId);

        return requestVoteRpcResult;
    }

    @Override
    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        if(appendEntriesRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("doAppendEntries term &lt; currentTerm");
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
        }

        if(appendEntriesRpcParam.getTerm() &gt;= this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // appendEntries请求中任期值如果大于自己，说明已经有一个更新的leader了，自己转为follower，并且以对方更大的任期为准
            this.serverStatusEnum = ServerStatusEnum.FOLLOWER;
            this.currentLeader = appendEntriesRpcParam.getLeaderId();
            this.raftServerMetaDataPersistentModule.setCurrentTerm(appendEntriesRpcParam.getTerm());
        }

        // 来自leader的心跳处理，清理掉之前选举的votedFor
        this.cleanVotedFor();
        // entries为空，说明是心跳请求，刷新一下最近收到心跳的时间
        raftLeaderElectionModule.refreshLastHeartbeatTime();

        // 心跳请求，直接返回
        return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),true);
    }
}
<br><br>在工程的test目录下，可以启动一个5节点的MyRaft的服务集群(用main方法启动即可)，通过修改其中的RaftClusterGlobalConfig类可以修改相关的配置。<br>
<img alt="Pasted image 20240725172328.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172328.png"><br>public class RaftClusterGlobalConfig {
    
    public static Registry registry = RegistryFactory.getRegistry(
        new RegistryConfig(RegistryCenterTypeEnum.FAKE_REGISTRY.getCode(), "127.0.0.1:2181"));

    /**
     * raft的集群配置
     * */
    public static final List&lt;RaftNodeConfig&gt; raftNodeConfigList = Arrays.asList(
        new RaftNodeConfig("raft-1","127.0.0.1",8001)
        ,new RaftNodeConfig("raft-2","127.0.0.1",8002)
        ,new RaftNodeConfig("raft-3","127.0.0.1",8003)
        ,new RaftNodeConfig("raft-4","127.0.0.1",8004)
        ,new RaftNodeConfig("raft-5","127.0.0.1",8005)
    );

    public static final int electionTimeout = 3;

    public static final Integer debugElectionTimeout = null;

    public static final int HeartbeatInterval = 1;

    /**
     * N次心跳后，leader会自动模拟出现故障(退回follow，停止心跳广播)
     * N&lt;=0代表不触发自动模拟故障
     */
    public static final int leaderAutoFailCount = 0;

    /**
     * 随机化的选举超时时间(毫秒)
     * */
    public static final Range&lt;Integer&gt; electionTimeoutRandomRange = new Range&lt;&gt;(150,500);

    public static void initRaftRpcServer(String serverId){
        RaftNodeConfig currentNodeConfig = RaftClusterGlobalConfig.raftNodeConfigList
            .stream().filter(item-&gt;item.getServerId().equals(serverId)).findAny()
            .orElseThrow(() -&gt; new MyRaftException("serverId must in raftNodeConfigList"));

        List&lt;RaftNodeConfig&gt; otherNodeList = RaftClusterGlobalConfig.raftNodeConfigList
            .stream().filter(item-&gt;!item.getServerId().equals(serverId)).collect(Collectors.toList());

        RaftConfig raftConfig = new RaftConfig(
            currentNodeConfig,RaftClusterGlobalConfig.raftNodeConfigList);
        raftConfig.setElectionTimeout(RaftClusterGlobalConfig.electionTimeout);
        raftConfig.setDebugElectionTimeout(RaftClusterGlobalConfig.debugElectionTimeout);

        raftConfig.setHeartbeatInternal(RaftClusterGlobalConfig.HeartbeatInterval);
        raftConfig.setLeaderAutoFailCount(RaftClusterGlobalConfig.leaderAutoFailCount);
        // 随机化选举超时时间的范围
        raftConfig.setElectionTimeoutRandomRange(RaftClusterGlobalConfig.electionTimeoutRandomRange);

        RaftRpcServer raftRpcServer = new RaftRpcServer(raftConfig, RaftClusterGlobalConfig.registry);

        List&lt;RaftService&gt; raftServiceList = raftRpcServer.getRpcProxyList(otherNodeList);
        // raft服务，启动！
        raftRpcServer.init(raftServiceList);
    }
}
<br>验证lab1中MyRaft leader选举实现的正确性，可以通过以下几个case简单的验证下：<br>
<br>启动5个节点，看看是否能够在短时间内选举出一个leader，leader是否能抑制后续的选举(leader定时心跳有日志能观察到)。
<br>将leader杀掉(5节点集群最多能容忍2个节点故障)，看是否在选举超时后触发新一轮选举，并且成功选出新的leader。
<br>将之前杀掉的leader再启动，看能否成功的回到集群中。
<br><br>在原始的raft算法的leader选举中存在一个问题。具体场景举例如下：<br>
<br>一个5节点的raft集群，突然其中2个follower节点与另外三个节点(包含当前leader)之间出现了网络分区，不同网络分区的节点无法正常通信。
<br>此时3节点所在的网络分区是多数分区，因此可以正常工作。而2个节点所在的分区是少数分区，由于无法接到leader心跳而触发新的选举。<br>
raft的论文中提到，发起新选举需要先将自己的任期值term自增1，然后发起并行的requestVote。<br>
但此时2节点所在的少数分区是无法成功获得大多数选票的，因此在这个分区中的节点会不断的发起一轮又一轮的leader选举，term值也会在很短的时间内快速增长。
<br>在一段时间后网络分区问题恢复后，集群中的所有节点又能互相通信了，此时少数分区节点的term值大概率远大于正常工作的多数分区中的节点。<br>
在少数分区节点收到来自多数分区节点的leader的rpc请求时，其会响应一个更大的term值。此时位于多数分区中的leader会因为响应的term值高于自己而主动退位，集群内会发起一轮新的选举。
<br>从本质上来说，这个分区恢复后进行的新选举是无意义的。且由于进行选举会造成集群短暂的不可用，因此最好能避免这个问题。<br>业界给出的解决方法是在真正的选举前先发起一轮预选举(preVote)。<br>
<br>预选举的操作和选举一样，也是并行的发起requestVote请求，主要的区别在于发起预选举的节点并不事先将term值自增，而是维持不变。节点的状态也在candidate的基础上新增了一个preCandidate状态。
<br>发起预选举的节点需要根据预选举中发起的并行requestVote结果来决定是否开启实际的leader选举。<br>
如果预选举中发起的并行requestVote得到了多数票，则可以接着发起实际的选举。而如果没有得到多数票，则不进行实际的选举。
<br>引入了预选举的机制后，上面所说的网络分区发生时，少数分区的节点由于无法在预选举中获得大多数票，因此只会不断的发起一轮又一轮的预选举。<br>
因此，其term值不会不断增加而是一直维持在分区发生时的值。在分区问题恢复后，其term值一定是小于或等于多数分区内leader的term值，而不会进行一轮无效的选举，从而解决上述的问题。<br>
但需要注意的是，引入预选举机制也会增加正常状况下发起正常选举的开销。
<br>MyRaft为了保持实现的简单性，并没有实现预选举机制。但etcd、sofa-jraft等流行的开源raft系统都是实现了预选举优化的，所以在这里还是简单介绍一下。<br><br>
<br>作为手写raft系列博客的第一篇，在博客的第一、二节简单介绍了raft算法和MyRaft，第3节则详细分析了leader选举的关键细节并基于源码详细分析了MyRaft是如何实现leader选举的。
<br>单纯实现Raft的leader选举并没有什么难度。以我个人的实践经验来说，真正的困难之处在于后续功能的叠加。<br>
由于raft的论文中介绍的几个模块彼此之间是紧密关联的。因此后续日志复制、日志压缩以及成员动态变更这几个功能的逐步实现中，每完成一个都会对上个版本的代码在细节上有不小的调整，大大增加了整体的复杂度。
<br>博客中展示的完整代码在我的github上：<a rel="noopener nofollow" class="external-link" href="https://github.com/1399852153/MyRaft" target="_blank">https://github.com/1399852153/MyRaft</a> (release/lab1_leader_election分支)，希望能帮助到对raft算法感兴趣的小伙伴。<br>
内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/raft实现-一致性算法介绍.html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft实现/Raft实现 - 一致性算法介绍.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 02:53:20 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725171642.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725171642.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Raft实现 - 实现日志压缩]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:raft" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#raft</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:raft" class="tag" target="_blank" rel="noopener nofollow">#raft</a><br><br>我们知道raft协议是基于日志复制的协议，日志数据是raft的核心。但随着raft集群的持续工作，raft的日志文件将会维护越来越多的日志，而这会带来以下几个问题。<br>
<br>日志文件过大会占用所在机器过多的本地磁盘空间。
<br>对于新加入集群的follower，leader与该follower之间完成日志同步会非常缓慢。
<br>对于自身不进行持久化的状态机，raft节点重启后回放日志也会非常缓慢。
<br>考虑到绝大多数的状态机中存储的数据并不都是新增，而更多的是对已有数据的更新，则状态机中所存储的数据量通常会远小于raft日志的总大小。例如K/V数据库，对相同key的N次操作(整体更新操作)，只有最后一次操作是实际有效的，而在此之前的针对该key的raft日志其实已经没有保存的必要了。<br>
因此raft的作者在论文的日志压缩一节中提到了几种日志压缩的算法(基于快照的、基于LSM树的)，raft选择了更容易理解和实现的、基于状态机快照的算法作为日志压缩的基础。<br><br>raft日志压缩实现中有以下几个关键点：<br>
<br>raft的各个节点可以按照某种策略独立的生成快照(比如定期检测日志文件大小是否超过阈值)，快照的主要内容是状态机当前瞬间所维护的所有数据的快照。<br>
MyRaft的状态机是一个纯内存的K/V数据库，所以快照就是内存中对应Map数据序列化后的内容。
<br>当前状态机中的快照实际上等同于所有已提交日志的顺序执行的最终结果，快照文件生成后会将所有已提交的日志全部删除以达成压缩的目的。<br>
而在处理appendEntries时，leader需要在参数中设置当前传输日志的前一条日志的index和term值，如果此时leader前一条日志恰好是已提交的并且被压缩到快照里而被删除了，则获取不到这个值了。<br>
相对应的，follower也可能出现类似的情况，即当前所有日志都是已提交的并且由于日志压缩被删除了，进行prevIndex/prevTerm校验时，也需要这个数据。<br>
因此，最终的快照中包含了最后一条已提交日志的index和term值这一关键的元数据。
<br>在leader和日志进度较慢的follower进行通信时，如果follower所需要的日志是很早的，而leader这边对应index的日志又被快照压缩而删除了，没法通过appendEntries进行同步。<br>
raft对此新增加了一个rpc接口installSnapshot专门用于解决这个问题。在leader发现follower所需的日志已经被自己压缩到快照里时，则会通过installSnapshot将自己完整的快照直接复制给follower。<br>
由于快照可能很大，所以installSnapshot一次只会传输少量的日志，通过多次的交互后完成整个快照的安装。当follower侧完成了快照同步后，后续所需要同步的日志就都是leader日志文件中还保留的，后续的日志接着使用appendEntries同步即可。
<br>下面开始结合源码分析MyRaft的日志压缩功能<br><br>
<br>raft快照数据由RaftSnapshot对象承载，除了二进制的状态机状态数据外，还包括了快照最后一条日志的index和term的值。
<br>MyRaft关于快照数据读写的逻辑集中维护在SnapshotModule中，简单起见使用一把全局的读写锁来防止并发而不过多的考虑性能。<br>
在SnapshotModule中通过引入临时文件的方式来解决新快照文件在生成过程中可能突然宕机的问题。
<br>/**
 * raft快照对象
 * */
public class RaftSnapshot {

    /**
     * 快照所包含的最后一条log的索引编号
     * */
    private long lastIncludedIndex;

    /**
     * 快照所包含的最后一条log的任期编号
     * */
    private int lastIncludedTerm;

    /**
     * 快照数据
     * (注意：暂不考虑快照过大导致byte数组放不下的情况)
     * */
    private byte[] snapshotData = new byte[0];
}
<br><img alt="Pasted image 20240725172559.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172559.png"><br>public class SnapshotModule {
    private static final Logger logger = LoggerFactory.getLogger(SnapshotModule.class);

    private final RaftServer currentServer;

    private final File snapshotFile;

    private final ReentrantReadWriteLock reentrantLock = new ReentrantReadWriteLock();
    private final ReentrantReadWriteLock.WriteLock writeLock = reentrantLock.writeLock();
    private final ReentrantReadWriteLock.ReadLock readLock = reentrantLock.readLock();

    private static final String snapshotFileName = "snapshot.txt";
    private static final String snapshotTempFileName = "snapshot-temp.txt";

    /**
     * 存放快照实际数据的偏移量(lastIncludedIndex + lastIncludedTerm 共两个字段后存放快照)
     * */
    private static final int actualDataOffset = 4 + 8;

    public SnapshotModule(RaftServer currentServer) {
        this.currentServer = currentServer;

        // 保证目录是存在的
        String snapshotFileDir = getSnapshotFileDir();
        new File(snapshotFileDir).mkdirs();

        snapshotFile = new File(snapshotFileDir + File.separator + snapshotFileName);

        File snapshotTempFile = new File(snapshotFileDir + File.separator + snapshotTempFileName);

        if(!snapshotFile.exists() &amp;&amp; snapshotTempFile.exists()){
            // 快照文件不存在，但是快照的临时文件存在。说明在写完临时文件并重命名之前宕机了(临时文件是最新的完整快照)

            // 将tempFile重命名为快照文件
            snapshotTempFile.renameTo(snapshotFile);

            logger.info("snapshot-temp file rename to snapshot file success!");
        }
    }

    /**
     * 持久化一个新的快照文件
     * (暂不考虑快照太大的问题)
     * */
    public void persistentNewSnapshotFile(RaftSnapshot raftSnapshot){
        logger.info("do persistentNewSnapshotFile raftSnapshot={}",raftSnapshot);
        writeLock.lock();

        try {
            String userPath = getSnapshotFileDir();

            // 新的文件名是tempFile
            String newSnapshotFilePath = userPath + File.separator + snapshotTempFileName;
            logger.info("do persistentNewSnapshotFile newSnapshotFilePath={}", newSnapshotFilePath);

            File snapshotTempFile = new File(newSnapshotFilePath);
            try (RandomAccessFile newSnapshotFile = new RandomAccessFile(snapshotTempFile, "rw")) {
                MyRaftFileUtil.createFile(snapshotTempFile);

                newSnapshotFile.writeInt(raftSnapshot.getLastIncludedTerm());
                newSnapshotFile.writeLong(raftSnapshot.getLastIncludedIndex());
                newSnapshotFile.write(raftSnapshot.getSnapshotData());

                logger.info("do persistentNewSnapshotFile success! raftSnapshot={}", raftSnapshot);
            } catch (IOException e) {
                throw new MyRaftException("persistentNewSnapshotFile error", e);
            }

            // 先删掉原来的快照文件，然后把临时文件重名名为快照文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
            snapshotFile.delete();
            snapshotTempFile.renameTo(snapshotFile);
        }finally {
            writeLock.unlock();
        }
    }

    /**
     * 安装快照
     * */
    public void appendInstallSnapshot(InstallSnapshotRpcParam installSnapshotRpcParam){
        logger.info("do appendInstallSnapshot installSnapshotRpcParam={}",installSnapshotRpcParam);
        writeLock.lock();

        String userPath = getSnapshotFileDir();

        // 新的文件名是tempFile
        String newSnapshotFilePath = userPath + File.separator + snapshotTempFileName;
        logger.info("do appendInstallSnapshot newSnapshotFilePath={}", newSnapshotFilePath);

        File snapshotTempFile = new File(newSnapshotFilePath);
        try (RandomAccessFile newSnapshotFile = new RandomAccessFile(snapshotTempFile, "rw")) {
            MyRaftFileUtil.createFile(snapshotTempFile);

            if(installSnapshotRpcParam.getOffset() == 0){
                newSnapshotFile.setLength(0);
            }

            newSnapshotFile.seek(0);
            newSnapshotFile.writeInt(installSnapshotRpcParam.getLastIncludedTerm());
            newSnapshotFile.writeLong(installSnapshotRpcParam.getLastIncludedIndex());

            // 文件指针偏移，找到实际应该写入快照数据的地方
            newSnapshotFile.seek(actualDataOffset + installSnapshotRpcParam.getOffset());
            // 写入快照数据
            newSnapshotFile.write(installSnapshotRpcParam.getData());

            logger.info("do appendInstallSnapshot success! installSnapshotRpcParam={}", installSnapshotRpcParam);
        } catch (IOException e) {
            throw new MyRaftException("appendInstallSnapshot error", e);
        } finally {
            writeLock.unlock();
        }

        if(installSnapshotRpcParam.isDone()) {
            writeLock.lock();
            try {
                // 先删掉原来的快照文件，然后把临时文件重名名为快照文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
                snapshotFile.delete();
                snapshotTempFile.renameTo(snapshotFile);
            } finally {
                writeLock.unlock();
            }
        }
    }

    /**
     * 没有实际快照数据，只有元数据
     * */
    public RaftSnapshot readSnapshotMetaData(){
        if(this.snapshotFile.length() == 0){
            return null;
        }

        readLock.lock();

        try(RandomAccessFile latestSnapshotRaFile = new RandomAccessFile(this.snapshotFile, "r")) {
            RaftSnapshot raftSnapshot = new RaftSnapshot();
            raftSnapshot.setLastIncludedTerm(latestSnapshotRaFile.readInt());
            raftSnapshot.setLastIncludedIndex(latestSnapshotRaFile.readLong());

            return raftSnapshot;
        } catch (IOException e) {
            throw new MyRaftException("readSnapshotNoData error",e);
        } finally {
            readLock.unlock();
        }
    }

    public RaftSnapshot readSnapshot(){
        logger.info("do readSnapshot");

        if(this.snapshotFile.length() == 0){
            logger.info("snapshotFile is empty!");
            return null;
        }

        readLock.lock();

        try(RandomAccessFile latestSnapshotRaFile = new RandomAccessFile(this.snapshotFile, "r")) {
            logger.info("do readSnapshot");

            RaftSnapshot latestSnapshot = new RaftSnapshot();
            latestSnapshot.setLastIncludedTerm(latestSnapshotRaFile.readInt());
            latestSnapshot.setLastIncludedIndex(latestSnapshotRaFile.readLong());

            // 读取snapshot的实际数据(简单起见，暂不考虑快照太大内存溢出的问题，可以优化为按照偏移量多次读取)
            byte[] snapshotData = new byte[(int) (this.snapshotFile.length() - actualDataOffset)];
            latestSnapshotRaFile.read(snapshotData);
            latestSnapshot.setSnapshotData(snapshotData);

            logger.info("readSnapshot success! readSnapshot={}",latestSnapshot);
            return latestSnapshot;
        } catch (IOException e) {
            throw new MyRaftException("readSnapshot error",e);
        } finally {
            readLock.unlock();
        }
    }

    private String getSnapshotFileDir(){
        return System.getProperty("user.dir")
            + File.separator + currentServer.getServerId()
            + File.separator + "snapshot";
    }
}
<br><br>
<br>相比lab2，lab3中MyRaft的日志模块新增加了一个定时任务，用于检查当前日志文件的大小是否超过了指定的阈值，如果超过了阈值则会触发生成新日志快照的逻辑。<br>
和快照模块类似，考虑到日志文件压缩时可能宕机的问题，同样采用引入临时文件的方法解决。
<br>生成快照的逻辑里先将新的快照通过SnapshotModule持久化，然后将当前已提交的日志从日志文件中删除掉。<br>
日志文件是从前写到后的，直接操作原日志文件会比较麻烦和危险。因此MyRaft将所有未提交的日志写入一个新的临时日志文件后，再通过一次文件名的切换实现对已提交日志的删除。
<br>   /**
   * 构建快照的检查
   * */
    private void buildSnapshotCheck() {
        try {
            if(!readLock.tryLock(1,TimeUnit.SECONDS)){
                logger.info("buildSnapshotCheck lock fail, quick return!");
                return;
            }
        } catch (InterruptedException e) {
            throw new MyRaftException("buildSnapshotCheck tryLock error!",e);
        }

        try {
            long logFileLength = this.logFile.length();
            long logFileThreshold = currentServer.getRaftConfig().getLogFileThreshold();
            if (logFileLength &lt; logFileThreshold) {
                logger.info("logFileLength not reach threshold, do nothing. logFileLength={},threshold={}", logFileLength, logFileThreshold);
                return;
            }

            logger.info("logFileLength already reach threshold, start buildSnapshot! logFileLength={},threshold={}", logFileLength, logFileThreshold);

            byte[] snapshot = currentServer.getKvReplicationStateMachine().buildSnapshot();
            LogEntry lastCommittedLogEntry = readLocalLog(this.lastCommittedIndex);

            RaftSnapshot raftSnapshot = new RaftSnapshot();
            raftSnapshot.setLastIncludedTerm(lastCommittedLogEntry.getLogTerm());
            raftSnapshot.setLastIncludedIndex(lastCommittedLogEntry.getLogIndex());
            raftSnapshot.setSnapshotData(snapshot);

            // 持久化最新的一份快照
            currentServer.getSnapshotModule().persistentNewSnapshotFile(raftSnapshot);
        }finally {
            readLock.unlock();
        }

        try {
            buildNewLogFileRemoveCommittedLog();
        } catch (IOException e) {
            logger.error("buildNewLogFileRemoveCommittedLog error",e);
        }
    }
<br>   /**
     * 构建一个删除了已提交日志的新日志文件(日志压缩到快照里了)
     * */
    private void buildNewLogFileRemoveCommittedLog() throws IOException {
        long lastCommitted = getLastCommittedIndex();
        long lastIndex = getLastIndex();

        // 暂不考虑读取太多日志造成内存溢出的问题
        List&lt;LocalLogEntry&gt; logEntryList;
        if(lastCommitted == lastIndex){
            // (lastCommitted == lastIndex) 所有日志都提交了，创建一个空的新日志文件
            logEntryList = new ArrayList&lt;&gt;();
        }else{
            // 还有日志没提交，把没提交的记录到新的日志文件中
            logEntryList = readLocalLog(lastCommitted+1,lastIndex);
        }

        File tempLogFile = new File(getLogFileDir() + File.separator + logTempFileName);
        MyRaftFileUtil.createFile(tempLogFile);
        try(RandomAccessFile randomAccessTempLogFile = new RandomAccessFile(tempLogFile,"rw")) {

            long currentOffset = 0;
            for (LogEntry logEntry : logEntryList) {
                // 写入日志
                writeLog(randomAccessTempLogFile, logEntry, currentOffset);

                currentOffset = randomAccessTempLogFile.getFilePointer();
            }

            this.currentOffset = currentOffset;
        }

        File tempLogMeteDataFile = new File(getLogFileDir() + File.separator + logMetaDataTempFileName);
        MyRaftFileUtil.createFile(tempLogMeteDataFile);

        // 临时的日志元数据文件写入数据
        refreshMetadata(tempLogMeteDataFile,currentOffset);

        writeLock.lock();
        try{
            // 先删掉原来的日志文件，然后把临时文件重名名为日志文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
            this.logFile.delete();
            boolean renameLogFileResult = tempLogFile.renameTo(this.logFile);
            if(!renameLogFileResult){
                logger.error("renameLogFile error!");
            }

            // 先删掉原来的日志元数据文件，然后把临时文件重名名为日志元数据文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
            this.logMetaDataFile.delete();
            boolean renameTempLogMeteDataFileResult = tempLogMeteDataFile.renameTo(this.logMetaDataFile);
            if(!renameTempLogMeteDataFileResult){
                logger.error("renameTempLogMeteDataFile error!");
            }
        }finally {
            writeLock.unlock();
        }
    }
<br><br><br>相比lab2，在引入了快照压缩功能后，leader侧的日志复制逻辑需要进行一点小小的拓展。<br>
即当要向follower同步某一条日志时，对应日志可能已经被压缩掉了，因此此时需要改为使用installSnapshotRpc来完成快照的安装。<br><img alt="Pasted image 20240725172616.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172616.png"><br>   /**
     * leader向集群广播，令follower复制新的日志条目
     * */
    public List&lt;AppendEntriesRpcResult&gt; replicationLogEntry(LogEntry lastEntry) {
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();

        List&lt;Future&lt;AppendEntriesRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        for(RaftService node : otherNodeInCluster){
            // 并行发送rpc，要求follower复制日志
            Future&lt;AppendEntriesRpcResult&gt; future = this.rpcThreadPool.submit(()-&gt;{
                logger.info("replicationLogEntry start!");

                long nextIndex = this.currentServer.getNextIndexMap().get(node);

                AppendEntriesRpcResult finallyResult = null;

                // If last log index ≥ nextIndex for a follower: send AppendEntries RPC with log entries starting at nextIndex
                while(lastEntry.getLogIndex() &gt;= nextIndex){
                    AppendEntriesRpcParam appendEntriesRpcParam = new AppendEntriesRpcParam();
                    appendEntriesRpcParam.setLeaderId(currentServer.getServerId());
                    appendEntriesRpcParam.setTerm(currentServer.getCurrentTerm());
                    appendEntriesRpcParam.setLeaderCommit(this.lastCommittedIndex);

                    int appendLogEntryBatchNum = this.currentServer.getRaftConfig().getAppendLogEntryBatchNum();

                    // 要发送的日志最大index值
                    // (追进度的时候，就是nextIndex开始批量发送appendLogEntryBatchNum-1条(左闭右闭区间)；如果进度差不多那就是以lastEntry.index为界限全部发送出去)
                    long logIndexEnd = Math.min(nextIndex+(appendLogEntryBatchNum-1), lastEntry.getLogIndex());
                    // 读取出[nextIndex-1,logIndexEnd]的日志(左闭右闭区间),-1往前一位是为了读取出preLog的信息
                    List&lt;LocalLogEntry&gt; localLogEntryList = this.readLocalLog(nextIndex-1,logIndexEnd);

                    logger.info("replicationLogEntry doing! nextIndex={},logIndexEnd={},LocalLogEntryList={}",
                        nextIndex,logIndexEnd,JsonUtil.obj2Str(localLogEntryList));

                    List&lt;LogEntry&gt; logEntryList = localLogEntryList.stream()
                        .map(LogEntry::toLogEntry)
                        .collect(Collectors.toList());

                    // 索引区间大小
                    long indexRange = (logIndexEnd - nextIndex + 1);

                    // 假设索引区间大小为N，可能有三种情况
                    // 1. 查出N条日志，所需要的日志全都在本地日志文件里没有被压缩
                    // 2. 查出1至N-1条日志，部分日志被压缩到快照里 or 就是只有那么多日志(一次批量查5条，但当前总共只写入了3条)
                    // 3. 查出0条日志，需要的日志全部被压缩了(因为是先落盘再广播，如果既没有日志也没有快照那就是有bug)
                    if(logEntryList.size() == indexRange+1){
                        // 查出了区间内的所有日志(case 1)

                        logger.info("find log size match!");
                        // preLog
                        LogEntry preLogEntry = logEntryList.get(0);
                        // 实际需要传输的log
                        List&lt;LogEntry&gt; needAppendLogList = logEntryList.subList(1,logEntryList.size());
                        appendEntriesRpcParam.setEntries(needAppendLogList);
                        appendEntriesRpcParam.setPrevLogIndex(preLogEntry.getLogIndex());
                        appendEntriesRpcParam.setPrevLogTerm(preLogEntry.getLogTerm());
                    }else if(logEntryList.size() &gt; 0 &amp;&amp; logEntryList.size() &lt;= indexRange){
                        // 查出了部分日志(case 2)
                        // 新增日志压缩功能后，查出来的数据个数小于指定的区间不一定就是查到第一条数据，还有可能是日志被压缩了

                        logger.info("find log size not match!");

                        RaftSnapshot readSnapshotNoData = currentServer.getSnapshotModule().readSnapshotMetaData();
                        if(readSnapshotNoData != null){
                            logger.info("has snapshot! readSnapshotNoData={}",readSnapshotNoData);

                            // 存在快照，使用快照里保存的上一条日志信息
                            appendEntriesRpcParam.setPrevLogIndex(readSnapshotNoData.getLastIncludedIndex());
                            appendEntriesRpcParam.setPrevLogTerm(readSnapshotNoData.getLastIncludedTerm());
                        }else{
                            logger.info("no snapshot! prevLogIndex=-1, prevLogTerm=-1");

                            // 没有快照，说明恰好发送第一条日志记录(比如appendLogEntryBatchNum=5，但一共只有3条日志全查出来了)
                            // 第一条记录的prev的index和term都是-1
                            appendEntriesRpcParam.setPrevLogIndex(-1);
                            appendEntriesRpcParam.setPrevLogTerm(-1);
                        }

                        appendEntriesRpcParam.setEntries(logEntryList);
                    } else if(logEntryList.isEmpty()){
                        // 日志压缩把要同步的日志删除掉了，只能使用installSnapshotRpc了(case 3)
                        logger.info("can not find and log entry，maybe delete for log compress");
                        // 快照压缩导致leader更早的index日志已经不存在了

                        // 应该改为使用installSnapshot来同步进度
                        RaftSnapshot raftSnapshot = currentServer.getSnapshotModule().readSnapshot();
                        doInstallSnapshotRpc(node,raftSnapshot,currentServer);

                        // 走到这里，一般是成功的完成了快照的安装。目标follower目前已经有了包括lastIncludedIndex以及之前的所有日志
                        // 如果是因为成为follower快速返回，则再循环一次就结束了
                        nextIndex = raftSnapshot.getLastIncludedIndex() + 1;
                        continue;
                    } else{
                        // 走到这里不符合预期，日志模块有bug
                        throw new MyRaftException("replicationLogEntry logEntryList size error!" +
                            " nextIndex=" + nextIndex + " logEntryList.size=" + logEntryList.size());
                    }

                    logger.info("leader do appendEntries start, node={}, appendEntriesRpcParam={}",node,appendEntriesRpcParam);
                    AppendEntriesRpcResult appendEntriesRpcResult = node.appendEntries(appendEntriesRpcParam);
                    logger.info("leader do appendEntries end, node={}, appendEntriesRpcResult={}",node,appendEntriesRpcResult);

                    finallyResult = appendEntriesRpcResult;
                    // 收到更高任期的处理
                    boolean beFollower = currentServer.processCommunicationHigherTerm(appendEntriesRpcResult.getTerm());
                    if(beFollower){
                        return appendEntriesRpcResult;
                    }

                    if(appendEntriesRpcResult.isSuccess()){
                        logger.info("appendEntriesRpcResult is success, node={}",node);

                        // If successful: update nextIndex and matchIndex for follower (§5.3)

                        // 同步成功了，nextIndex递增一位
                        this.currentServer.getNextIndexMap().put(node,nextIndex+1);
                        this.currentServer.getMatchIndexMap().put(node,nextIndex);

                        nextIndex++;
                    }else{
                        // 因为日志对不上导致一致性检查没通过，同步没成功，nextIndex往后退一位

                        logger.info("appendEntriesRpcResult is false, node={}",node);

                        // If AppendEntries fails because of log inconsistency: decrement nextIndex and retry (§5.3)
                        nextIndex--;
                        this.currentServer.getNextIndexMap().put(node,nextIndex);
                    }
                }

                if(finallyResult == null){
                    // 说明有bug
                    throw new MyRaftException("replicationLogEntry finallyResult is null!");
                }

                logger.info("finallyResult={},node={}",node,finallyResult);

                return finallyResult;
            });

            futureList.add(future);
        }

        // 获得结果
        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = CommonUtil.concurrentGetRpcFutureResult(
                "do appendEntries", futureList,
                this.rpcThreadPool,2, TimeUnit.SECONDS);

        logger.info("leader replicationLogEntry appendEntriesRpcResultList={}",appendEntriesRpcResultList);

        return appendEntriesRpcResultList;
    }
<br><br>前面提到，follower侧在进行日志一致性校验时，也可能出现恰好前一条日志被压缩到快照里的情况。<br>
因此需要在当前日志不存在时，尝试通过SnapshotModule读取快照数据中的前一条日志信息来进行比对。<br>    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        if(appendEntriesRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("doAppendEntries term &lt; currentTerm");
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
        }

        if(appendEntriesRpcParam.getTerm() &gt;= this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // appendEntries请求中任期值如果大于自己，说明已经有一个更新的leader了，自己转为follower，并且以对方更大的任期为准
            this.serverStatusEnum = ServerStatusEnum.FOLLOWER;
            this.currentLeader = appendEntriesRpcParam.getLeaderId();
            this.raftServerMetaDataPersistentModule.setCurrentTerm(appendEntriesRpcParam.getTerm());
        }

        if(appendEntriesRpcParam.getEntries() == null || appendEntriesRpcParam.getEntries().isEmpty()){
            // 来自leader的心跳处理，清理掉之前选举的votedFor
            this.cleanVotedFor();
            // entries为空，说明是心跳请求，刷新一下最近收到心跳的时间
            raftLeaderElectionModule.refreshLastHeartbeatTime();

            long currentLastCommittedIndex = logModule.getLastCommittedIndex();
            logger.debug("doAppendEntries heartbeat leaderCommit={},currentLastCommittedIndex={}",
                appendEntriesRpcParam.getLeaderCommit(),currentLastCommittedIndex);

            if(appendEntriesRpcParam.getLeaderCommit() &gt; currentLastCommittedIndex) {
                // 心跳处理里，如果leader当前已提交的日志进度超过了当前节点的进度，令当前节点状态机也跟上
                // 如果leaderCommit &gt;= logModule.getLastIndex(),说明当前节点的日志进度不足，但可以把目前已有的日志都提交给状态机去执行
                // 如果leaderCommit &lt; logModule.getLastIndex(),说明当前节点进度比较快，有一些日志是leader已复制但还没提交的，把leader已提交的那一部分作用到状态机就行
                long minNeedCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), logModule.getLastIndex());
                pushStatemachineApply(minNeedCommittedIndex);
            }

            // 心跳请求，直接返回
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),true);
        }

        // logEntries不为空，是真实的日志复制rpc

        logger.info("do real log append! appendEntriesRpcParam={}",appendEntriesRpcParam);
        // AppendEntry可靠性校验，如果prevLogIndex和prevLogTerm不匹配，则需要返回false，让leader发更早的日志过来
        {
            LogEntry localPrevLogEntry = logModule.readLocalLog(appendEntriesRpcParam.getPrevLogIndex());
            if(localPrevLogEntry == null){
                // 没有查到prevLogIndex对应的日志，分两种情况
                RaftSnapshot raftSnapshot = snapshotModule.readSnapshotMetaData();
                localPrevLogEntry = new LogEntry();
                if(raftSnapshot == null){
                    // 当前节点日志条目为空,又没有快照，说明完全没有日志(默认任期为-1，这个是约定)
                    localPrevLogEntry.setLogIndex(-1);
                    localPrevLogEntry.setLogTerm(-1);
                }else{
                    // 日志里没有，但是有快照(把快照里记录的最后一条日志信息与leader的参数比对)
                    localPrevLogEntry.setLogIndex(raftSnapshot.getLastIncludedIndex());
                    localPrevLogEntry.setLogTerm(raftSnapshot.getLastIncludedTerm());
                }
            }

            if (localPrevLogEntry.getLogTerm() != appendEntriesRpcParam.getPrevLogTerm()) {
                //  Reply false if log doesn’t contain an entry at prevLogIndex
                //  whose term matches prevLogTerm (§5.3)
                //  本地日志和参数中的PrevLogIndex和PrevLogTerm对不上(对应日志不存在，或者任期对不上)
                logger.info("doAppendEntries localPrevLogEntry not match, localLogEntry={}",localPrevLogEntry);

                return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
            }
        }

        // 走到这里说明找到了最新的一条匹配的记录
        logger.info("doAppendEntries localEntry is match");

        List&lt;LogEntry&gt; newLogEntryList = appendEntriesRpcParam.getEntries();

        // 1. Append any new entries not already in the log
        // 2. If an existing entry conflicts with a new one (same index but different terms),
        //    delete the existing entry and all that follow it (§5.3)
        // 新日志的复制操作（直接整个覆盖掉prevLogIndex之后的所有日志,以leader发过来的日志为准）
        logModule.writeLocalLog(newLogEntryList, appendEntriesRpcParam.getPrevLogIndex());

        // If leaderCommit &gt; commitIndex, set commitIndex = min(leaderCommit, index of last new entry)
        if(appendEntriesRpcParam.getLeaderCommit() &gt; logModule.getLastCommittedIndex()){
            // 如果leaderCommit更大，说明当前节点的同步进度慢于leader，以新的entry里的index为准(更高的index还没有在本地保存(因为上面的appendEntry有效性检查))
            // 如果index of last new entry更大，说明当前节点的同步进度是和leader相匹配的，commitIndex以leader最新提交的为准

            LogEntry lastNewEntry = newLogEntryList.get(newLogEntryList.size()-1);
            long lastCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), lastNewEntry.getLogIndex());
            pushStatemachineApply(lastCommittedIndex);
        }

        // 返回成功
        return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(), true);
    }
<br><br>
<br>MyRaft中，leader侧安装快照的方法实现的比较简单，未考虑快照可能很大的情况，所以直接一股脑将整个快照文件全部读取到内存中来了(在向多个follower并发安装快照时会占用很多的内存，待优化)。
<br>在将快照读取到内存中后，通过一个循环将快照数据按照配置的block大小逐步的发送给follower。在发送完最后一个block数据后，rpc请求参数的done属性会被设置为true标识为同步完成。
<br>    public static void doInstallSnapshotRpc(RaftService targetNode, RaftSnapshot raftSnapshot, RaftServer currentServer){
        int installSnapshotBlockSize = currentServer.getRaftConfig().getInstallSnapshotBlockSize();
        byte[] completeSnapshotData = raftSnapshot.getSnapshotData();

        int currentOffset = 0;
        while(true){
            InstallSnapshotRpcParam installSnapshotRpcParam = new InstallSnapshotRpcParam();
            installSnapshotRpcParam.setLastIncludedIndex(raftSnapshot.getLastIncludedIndex());
            installSnapshotRpcParam.setTerm(currentServer.getCurrentTerm());
            installSnapshotRpcParam.setLeaderId(currentServer.getServerId());
            installSnapshotRpcParam.setLastIncludedTerm(raftSnapshot.getLastIncludedTerm());
            installSnapshotRpcParam.setOffset(currentOffset);

            // 填充每次传输的数据块
            int blockSize = Math.min(installSnapshotBlockSize,completeSnapshotData.length-currentOffset);
            byte[] block = new byte[blockSize];
            System.arraycopy(completeSnapshotData,currentOffset,block,0,blockSize);
            installSnapshotRpcParam.setData(block);

            currentOffset += installSnapshotBlockSize;
            if(currentOffset &gt;= completeSnapshotData.length){
                installSnapshotRpcParam.setDone(true);
            }else{
                installSnapshotRpcParam.setDone(false);
            }

            InstallSnapshotRpcResult installSnapshotRpcResult = targetNode.installSnapshot(installSnapshotRpcParam);

            boolean beFollower = currentServer.processCommunicationHigherTerm(installSnapshotRpcResult.getTerm());
            if(beFollower){
                // 传输过程中发现自己已经不再是leader了，快速结束
                logger.info("doInstallSnapshotRpc beFollower quick return!");
                return;
            }

            if(installSnapshotRpcParam.isDone()){
                // 快照整体安装完毕
                logger.info("doInstallSnapshotRpc isDone!");
                return;
            }
        }
    }
<br><br>
<br>follower侧处理快照安装rpc的逻辑中，除了必要的对参数term大小的检查，就是简单的通过SnapshotModule完成快照的安装工作。
<br>在快照整体成功安装完成后，通过LogModule.compressLogBySnapshot方法将所有已提交的日志全都删除掉，并将之前安装好的快照整体作用到follower自己本地的状态机中。
<br>    public InstallSnapshotRpcResult installSnapshot(InstallSnapshotRpcParam installSnapshotRpcParam) {
        logger.info("installSnapshot start! serverId={},installSnapshotRpcParam={}",this.serverId,installSnapshotRpcParam);

        if(installSnapshotRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply immediately if term &lt; currentTerm
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("installSnapshot term &lt; currentTerm");
            return new InstallSnapshotRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm());
        }

        // 安装快照
        this.snapshotModule.appendInstallSnapshot(installSnapshotRpcParam);

        // 快照已经完全安装好了
        if(installSnapshotRpcParam.isDone()){
            // discard any existing or partial snapshot with a smaller index
            // 快照整体安装完毕，清理掉index小于等于快照中lastIncludedIndex的所有日志(日志压缩)
            logModule.compressLogBySnapshot(installSnapshotRpcParam);

            // Reset state machine using snapshot contents (and load snapshot’s cluster configuration)
            // follower的状态机重新安装快照
            RaftSnapshot raftSnapshot = this.snapshotModule.readSnapshot();
            kvReplicationStateMachine.installSnapshot(raftSnapshot.getSnapshotData());
        }

        logger.info("installSnapshot end! serverId={}",this.serverId);

        return new InstallSnapshotRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm());
    }
<br>    /**
     * discard any existing or partial snapshot with a smaller index
     * 快照整体安装完毕，清理掉index小于等于快照中lastIncludedIndex的所有日志
     * */
    public void compressLogBySnapshot(InstallSnapshotRpcParam installSnapshotRpcParam){
        this.lastCommittedIndex = installSnapshotRpcParam.getLastIncludedIndex();
        if(this.lastIndex &lt; this.lastCommittedIndex){
            this.lastIndex = this.lastCommittedIndex;
        }

        try {
            buildNewLogFileRemoveCommittedLog();
        } catch (IOException e) {
            throw new MyRaftException("compressLogBySnapshot error",e);
        }
    }
<br><br>和lab2中一样，通过启动一个raft集群并触发几个case可以验证MyRaft日志压缩功能的正确性。<br>
<br>启动5个节点，关闭快照压缩功能，正常的进行写入操作。<br>
=&gt; 状态机/日志文件的数据是否符合预期
<br>启动5个节点，开启快照压缩功能，正常的进行写入操作，当日志文件超过阈值触发日志压缩后。<br>
=&gt; 状态机/日志文件/快照文件的数据是否符合预期
<br>启动5个节点，开启快照压缩功能，正常的进行写入操作，主动关闭一个节点。再进行一些写入，令leader生成最新的快照，然后让宕机的节点回到集群，看leader是否通过快照安装来完成快照/日志的同步。<br>
=&gt; 状态机/日志文件/快照文件的数据是否符合预期
<br><br>
<br>作为手写raft系列博客的第三篇，也是目前计划中的最后一篇博客(集群动态变更功能暂不实现)。博客中介绍了Raft的日志压缩功能以及从源码层面上分析MyRaft实现日志压缩功能的细节。
<br>raft是一个相对复杂的算法，因此MyRaft在功能实现上为了追求实现的简单性，舍弃了很多性能方面的优化(比如一把全局大锁防并发，快照数据完整放到内存中处理等等)。<br>
同时分布式系统中处处有并发、机器也时刻可能宕机，需要考虑的细节繁多。所以MyRaft即使通过了我自己设计的一些单测和集成测试的case，但肯定还存在许多尚未被发现bug，还请多多指教。
<br>博客中展示的完整代码在我的github上：<a rel="noopener nofollow" class="external-link" href="https://github.com/1399852153/MyRaft" target="_blank">https://github.com/1399852153/MyRaft</a> (release/lab3_log_compaction分支)，希望能帮助到对raft算法感兴趣的小伙伴。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/raft实现-实现日志压缩.html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft实现/Raft实现 - 实现日志压缩.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:46:25 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172559.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172559.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Raft实现 - 实现日志复制]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:raft" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#raft</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:raft" class="tag" target="_blank" rel="noopener nofollow">#raft</a><br><br>在上一篇博客中MyRaft实现了leader选举，为接下来实现日志复制功能打下了基础: <a data-tooltip-position="top" aria-label="https://www.cnblogs.com/xiaoxiongcanguan/p/17569697.html" rel="noopener nofollow" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17569697.html" target="_blank">手写raft(一) 实现leader选举</a><br>日志复制是raft最核心也是最复杂的功能，大体上来说一次正常的raft日志复制大致可以简化为以下几步完成：<br>
<br>客户端向raft集群发送一次操作请求(比如kv数据库状态机的写命令(set k1 v1))，如果接受到请求的节点是leader则受理该请求；<br>
如果不是leader则转发给自己认为的leader或者返回leader的地址让客户端向leader重新发送请求(如果过程中小概率发生了leader变更，则循环往复直到leader受理)
<br>leader在接受到请求后，生成对应的raft日志(logEntry)并追加写入到leader节点本地的日志文件中(整个过程需要加锁，防止多个请求并发而日志乱序)。
<br>leader本地日志追加写入完成后，向集群中的所有follower节点广播该日志(并行的rpc appendEntries)，follower接到该rpc请求后也将日志追加写入到自己的本地日志文件中，并返回成功。<br>
当超过半数的follower返回了成功时，leader认为该日志已经成功的复制到了集群中，可以提交到状态机中执行。<br>
如果成功的数量少于半数则认为该客户端请求失败，不提交到状态机中，并将本地写入的日志删除掉，让客户端去重试。
<br>leader提交日志给状态机后，会修改自己的lastApplied字段(最大已提交日志索引编号)，随后通过心跳等rpc交互令follower也提交本地对应索引的日志到状态机中
<br>至此，一次完整的日志复制过程完成，一切正常的情况下整个集群中所有节点的本地日志中都包含了对应请求的日志，每个节点的状态机也执行了对应日志包含的状态机指令。
<br><br><img alt="Pasted image 20240725172412.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172412.png"><br>
<br>上面关于raft日志复制功能的介绍看起来不算复杂，在一切正常的情况下系统似乎能很好的完成任务。但raft是一个分布式系统，分布式系统中会出现各种麻烦的异常情况，在上述任务的每一步、任一瞬间都可能出现网络故障、机器宕机(leader/follower都可能处理到一半就宕机)等问题，<br>
而如何在异常情况下依然能让系统正常完成任务就使得raft日志复制的逻辑变得复杂起来了。
<br>raft论文中对一些异常情况进行了简要介绍，并通过一些示例来证明算法的正确性。但具体落地到代码实现中还有更多的细节需要考虑，这也是为什么raft论文中反复强调算法易理解的重要性。
<br>因为算法容易理解，实现者就能对算法建立起直观的理解，处理异常case时就能更好的理解应该如何做以及为什么这样做是正确的。在下文中将会结合MyRaft的实现源码来详细分析raft是如何处理异常情况的。
<br><br><br>为了实现日志复制功能，lab2版本的MyRaft比起lab1额外新增了3个模块，分别是日志模块LogModule、状态机模块SimpleReplicationStateMachine和客户端模块RaftClient。<br><br>
<br>MyRaft的日志模块用于维护raft日志相关的逻辑，出于减少依赖的原因，MyRaft没有去依赖rocksDb等基于本地磁盘的数据库而是直接操作最原始的日志文件(RandomAccessFile)来实现日志存储功能。
<br>简单起见，LogModule直接通过一把全局的读写锁来防止并发问题。
<br>/**
 * raft日志条目
 * */
public class LogEntry implements Serializable {

    /**
     * 发布日志时的leader的任期编号
     * */
    private int logTerm;

    /**
     * 日志的索引编号
     * */
    private long logIndex;

    /**
     * 具体作用在状态机上的指令
     * */
    private Command command;
}
<br>LogEntry在磁盘文件中存储的示意图<br>
<img alt="Pasted image 20240725172431.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172431.png"><br><a data-tooltip-position="top" aria-label="https://github.com/1399852153/MyRaft/blob/release/lab2_log_replication/raft/src/main/java/myraft/module/LogModule.java" rel="noopener nofollow" class="external-link" href="https://github.com/1399852153/MyRaft/blob/release/lab2_log_replication/raft/src/main/java/myraft/module/LogModule.java" target="_blank">logModule类源码</a><br><br>
<br>状态机模块是一个K/V数据模型，本质上就是内存中维护了一个HashMap。状态机的读写操作就是对这个HashMap的读写操作，没有额外的逻辑。
<br>同时为了方便观察状态机中的数据状态，每次进行写操作时都整体刷新这个HashMap中的数据到对应的本地文件中(简单起见暂不考虑同步刷盘的性能问题)。
<br>MyRaft是一个极简的K/V数据库，其只支持最基本的get/set命令，因此作用在状态机中的所有指令天然都是幂等的，是可重复执行的。
<br>/**
 * 指令(取决于实现)
 * */
public interface Command extends Serializable {
}

/**
 * 写操作，把一个key设置为value
 * */
public class SetCommand implements Command {

   private String key;
   private String value;
}

public class GetCommand implements Command{

   private String key;
}
<br><br>
<br>Raft客户端模块就是一个rpc的客户端(不依赖注册中心，基于静态服务配置的点对点rpc)，请求时使用负载均衡随机的获得一个raft服务节点访问。
<br>public class RaftClient {

    private final Registry registry;
    private RaftService raftServiceProxy;
    private List&lt;ServiceInfo&gt; serviceInfoList;
    private final LoadBalance loadBalance = new SimpleRoundRobinBalance();

    public RaftClient(Registry registry) {
        this.registry = registry;
    }

    public void init(){
        ConsumerBootstrap consumerBootstrap = new ConsumerBootstrap()
            .registry(registry);

        // 注册消费者
        Consumer&lt;RaftService&gt; consumer = consumerBootstrap.registerConsumer(RaftService.class,new FastFailInvoker());
        this.raftServiceProxy = consumer.getProxy();
    }

    public void setRaftNodeConfigList(List&lt;RaftNodeConfig&gt; raftNodeConfigList) {
        this.serviceInfoList = raftNodeConfigList.stream().map(item-&gt;{
            ServiceInfo serviceInfo = new ServiceInfo();
            serviceInfo.setServiceName(RaftService.class.getName());
            serviceInfo.setUrlAddress(new URLAddress(item.getIp(),item.getPort()));
            return serviceInfo;
        }).collect(Collectors.toList());
    }

    public String doRequestRetry(Command command, int retryTime){
        RuntimeException ex = new RuntimeException();
        for(int i=0; i&lt;retryTime; i++){
            try {
                return doRequest(command);
            }catch (Exception e){
                ex = new RuntimeException(e);
                System.out.println("doRequestRetry error, retryTime=" + i);
            }
        }

        // n次重试后还是没成功
        throw ex;
    }

    public String doRequest(Command command){
        // 先让负载均衡选择请求任意节点
        ServiceInfo serviceInfo = loadBalance.select(this.serviceInfoList);
        ClientRequestResult clientRequestResult = doRequest(serviceInfo.getUrlAddress(),command);

        if(clientRequestResult.getLeaderAddress() == null){
            if(!clientRequestResult.isSuccess()){
                throw new MyRpcException("doRequest error!");
            }
            // 访问到了leader，得到结果
            return clientRequestResult.getValue();
        }else{
            // leaderAddress不为空，说明访问到了follower，得到follower给出的leader地址
            URLAddress urlAddress = clientRequestResult.getLeaderAddress();
            // 指定leader的地址去发起请求
            ClientRequestResult result = doRequest(urlAddress,command);

            return result.getValue();
        }
    }

    private ClientRequestResult doRequest(URLAddress urlAddress, Command command){
        // 相当于是点对点的rpc，用这种方式比较奇怪，但可以不依赖zookeeper这样的注册中心
        ConsumerRpcContextHolder.getConsumerRpcContext().setTargetProviderAddress(urlAddress);
        ClientRequestParam clientRequestParam = new ClientRequestParam(command);
        ClientRequestResult clientRequestResult = this.raftServiceProxy.clientRequest(clientRequestParam);
        ConsumerRpcContextHolder.removeConsumerRpcContext();

        return clientRequestResult;
    }
}
<br><br><br>
<br>raft是一个强一致的读写模型，只有leader才能对外进行服务。因此raft服务节点收到来自客户端的请求时，需要判断一下自己是否是leader，如果不是leader就返回自己认为的leader地址给客户端，让客户端重试。<br>
raft是一个分布式模型，在出现网络分区等情况下，原来是leader的节点(term更小的老leader)可能并不是目前真正的leader，而这个情况下接到客户端请求的老leader就会错误的处理客户端的请求，因而需要额外的机制来保证raft强一致的读写特性。
<br>线性强一致的写：raft的leader节点在处理客户端请求时会加写锁(线性一致)。提交指令到状态机中执行前，会预先写入一份本地日志，并将本地日志广播到集群中的所有follower节点上。如果自己已经不再是合法的leader，则本地日志的广播是无法在超过半数的节点上执行成功的。<br>
反过来说，只要大多数的节点都成功完成了日志复制的rpc请求(appendEntries)，则该写操作就是强一致下的写，因此可以将命令安全的提交到状态机中并向客户端返回成功。
<br>线性强一致的读：强一致的读就不能让老leader处理读请求，因为很可能老leader相比实际上合法的新leader缺失了一些最新的写操作，而导致返回过时的数据(破坏了强一致读的语义)。因此对于读指令，业界提出了几种常见的确保强一致读的方案。
<br><br>MyRaft考虑到实现的简单性，选择了方案2来实现强一致的读。<br>    public ClientRequestResult clientRequest(ClientRequestParam clientRequestParam) {
        // 不是leader
        if(this.serverStatusEnum != ServerStatusEnum.LEADER){
            if(this.currentLeader == null){
                // 自己不是leader，也不知道谁是leader直接报错
                throw new MyRaftException("current node not leader，and leader is null! serverId=" + this.serverId);
            }

            RaftNodeConfig leaderConfig = this.raftConfig.getRaftNodeConfigList()
                .stream().filter(item-&gt; Objects.equals(item.getServerId(), this.currentLeader)).findAny().get();

            // 把自己认为的leader告诉客户端(也可以改为直接转发请求)
            ClientRequestResult clientRequestResult = new ClientRequestResult();
            clientRequestResult.setLeaderAddress(new URLAddress(leaderConfig.getIp(),leaderConfig.getPort()));

            logger.info("not leader response known leader, result={}",clientRequestResult);
            return clientRequestResult;
        }

        // 是leader，处理读请求(线性一致读)
        if(clientRequestParam.getCommand() instanceof GetCommand){
            // 线性强一致的读，需要先进行一次心跳广播，判断当前自己是否还是leader
            boolean stillBeLeader = HeartbeatBroadcastTask.doHeartbeatBroadcast(this);
            if(stillBeLeader){
                // 还是leader，可以响应客户端
                logger.info("do client read op, still be leader");

                // Read-only operations can be handled without writing anything into the log.
                GetCommand getCommand = (GetCommand) clientRequestParam.getCommand();

                // 直接从状态机中读取就行
                String value = this.kvReplicationStateMachine.get(getCommand.getKey());

                ClientRequestResult clientRequestResult = new ClientRequestResult();
                clientRequestResult.setSuccess(true);
                clientRequestResult.setValue(value);

                logger.info("response getCommand, result={}",clientRequestResult);

                return clientRequestResult;
            }else{
                logger.info("do client read op, not still be leader");

                // 广播后发现自己不再是leader了，报错，让客户端重新自己找leader (客户端和当前节点同时误判，小概率发生)
                throw new MyRaftException("do client read op, but not still be leader!" + this.serverId);
            }
        }

        // 自己是leader，需要处理客户端的写请求

        // 构造新的日志条目
        LogEntry newLogEntry = new LogEntry();
        newLogEntry.setLogTerm(this.raftServerMetaDataPersistentModule.getCurrentTerm());
        // 新日志的索引号为当前最大索引编号+1
        newLogEntry.setLogIndex(this.logModule.getLastIndex() + 1);
        newLogEntry.setCommand(clientRequestParam.getCommand());

        logger.info("handle setCommand, do writeLocalLog entry={}",newLogEntry);

        // 预写入日志
        logModule.writeLocalLog(Collections.singletonList(newLogEntry));

        logger.info("handle setCommand, do writeLocalLog success!");

        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = logModule.replicationLogEntry(newLogEntry);

        logger.info("do replicationLogEntry, result={}",appendEntriesRpcResultList);

        // successNum需要加上自己的1票
        long successNum = appendEntriesRpcResultList.stream().filter(AppendEntriesRpcResult::isSuccess).count() + 1;
        if(successNum &gt;= this.raftConfig.getMajorityNum()){
            // If command received from client: append entry to local log, respond after entry applied to state machine (§5.3)

            // 成功复制到多数节点

            // 设置最新的已提交索引编号
            logModule.setLastCommittedIndex(newLogEntry.getLogIndex());
            // 作用到状态机上
            this.kvReplicationStateMachine.apply((SetCommand) newLogEntry.getCommand());
            // 思考一下：lastApplied为什么不需要持久化？ 状态机指令的应用和更新lastApplied非原子性会产生什么问题？
            logModule.setLastApplied(newLogEntry.getLogIndex());

            // 返回成功
            ClientRequestResult clientRequestResult = new ClientRequestResult();
            clientRequestResult.setSuccess(true);

            return clientRequestResult;
        }else{
            // 没有成功复制到多数,返回失败
            ClientRequestResult clientRequestResult = new ClientRequestResult();
            clientRequestResult.setSuccess(false);

            // 删掉之前预写入的日志条目
            // 思考一下如果删除完成之前，宕机了有问题吗？ 个人感觉是ok的
            logModule.deleteLocalLog(newLogEntry.getLogIndex());

            return clientRequestResult;
        }
    }
<br><br>下面详细介绍leader是如何向集群广播raftLog的。<br>
<br>raft的leader维护了两个非持久化的数据(Volatile state on leaders)，即在当前leader视角下follower节点同步raftLog的进度。<br>
一个数据是nextIndex，代表leader认为的follower应该接收的下一条log的索引值，leader初始化时乐观的估计设置其为leader当前最后一条日志索引值加1(代表着乐观估计follower和leader的日志进度是完全一致的)。<br>
一个数据是matchIndex，代表leader实际确认的follower已接受到的最后一条raft日志的索引值，leader初始化时悲观的将其初始化为0。<br>
由于follower是一个集合，所以论文中通过nextIndex[],matchIndex[]来描述，而在MyRaft中都用Map结构来维护。
<br>leader基于每个follower对应的nextIndex查找出所要发送的日志集合，并行的向所有follower发送appendEntries的rpc请求。<br>
当leader与follower进行rpc交互时，可能follower的日志同步进度并不像leader认为的那样乐观，很可能其实际所拥有的日志索引远小于leader最后一条日志的索引(follower侧的逻辑在下一节分析)。<br>
因此follower在这种情况下会返回失败，此时leader会将对应follower的nextIndex往回退(自减1)，循环往复的交互直到leader发送和follower所需的日志相匹配的那条日志(最坏情况下follower一条日志都没有，leader从第一条日志开始同步)
<br>当follower响应成功后，leader将会更新对应follower的nextIndex和matchIndex的值。当超过半数的follower都响应了对应index日志的appendEntries后，leader认为当前日志已经成功的复制到集群中多数的节点中了，则可以安全的将日志提交到状态机中了。
<br>/**
     * 向集群广播，令follower复制新的日志条目
     * */
    public List&lt;AppendEntriesRpcResult&gt; replicationLogEntry(LogEntry lastEntry) {
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();

        List&lt;Future&lt;AppendEntriesRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        for(RaftService node : otherNodeInCluster){
            // 并行发送rpc，要求follower复制日志
            Future&lt;AppendEntriesRpcResult&gt; future = this.rpcThreadPool.submit(()-&gt;{
                logger.info("replicationLogEntry start!");

                long nextIndex = this.currentServer.getNextIndexMap().get(node);

                AppendEntriesRpcResult finallyResult = null;

                // If last log index ≥ nextIndex for a follower: send AppendEntries RPC with log entries starting at nextIndex
                while(lastEntry.getLogIndex() &gt;= nextIndex){
                    AppendEntriesRpcParam appendEntriesRpcParam = new AppendEntriesRpcParam();
                    appendEntriesRpcParam.setLeaderId(currentServer.getServerId());
                    appendEntriesRpcParam.setTerm(currentServer.getCurrentTerm());
                    appendEntriesRpcParam.setLeaderCommit(this.lastCommittedIndex);

                    int appendLogEntryBatchNum = this.currentServer.getRaftConfig().getAppendLogEntryBatchNum();

                    // 要发送的日志最大index值
                    // (追进度的时候，就是nextIndex开始批量发送appendLogEntryBatchNum-1条(左闭右闭区间)；如果进度差不多那就是以lastEntry.index为界限全部发送出去)
                    long logIndexEnd = Math.min(nextIndex+(appendLogEntryBatchNum-1), lastEntry.getLogIndex());
                    // 读取出[nextIndex-1,logIndexEnd]的日志(左闭右闭区间),-1往前一位是为了读取出preLog的信息
                    List&lt;LocalLogEntry&gt; localLogEntryList = this.readLocalLog(nextIndex-1,logIndexEnd);

                    logger.info("replicationLogEntry doing! nextIndex={},logIndexEnd={},LocalLogEntryList={}",
                        nextIndex,logIndexEnd,JsonUtil.obj2Str(localLogEntryList));

                    List&lt;LogEntry&gt; logEntryList = localLogEntryList.stream()
                        .map(LogEntry::toLogEntry)
                        .collect(Collectors.toList());

                    // 索引区间大小
                    long indexRange = (logIndexEnd - nextIndex + 1);
                    if(logEntryList.size() == indexRange+1){
                        // 一般情况能查出区间内的所有日志

                        logger.info("find log size match!");
                        // preLog
                        LogEntry preLogEntry = logEntryList.get(0);
                        // 实际需要传输的log
                        List&lt;LogEntry&gt; needAppendLogList = logEntryList.subList(1,logEntryList.size());
                        appendEntriesRpcParam.setEntries(needAppendLogList);
                        appendEntriesRpcParam.setPrevLogIndex(preLogEntry.getLogIndex());
                        appendEntriesRpcParam.setPrevLogTerm(preLogEntry.getLogTerm());
                    }else if(logEntryList.size() &gt; 0 &amp;&amp; logEntryList.size() &lt;= indexRange){
                        logger.info("find log size not match!");
                        // 日志长度小于索引区间值，说明已经查到最前面的日志 (比如appendLogEntryBatchNum=5，但一共只有3条日志全查出来了)
                        appendEntriesRpcParam.setEntries(logEntryList);

                        // 约定好第一条记录的prev的index和term都是-1
                        appendEntriesRpcParam.setPrevLogIndex(-1);
                        appendEntriesRpcParam.setPrevLogTerm(-1);
                    } else{
                        // 正常情况是先持久化然后再广播同步日志，所以size肯定会大于0，也不应该超过索引区间值
                        // 走到这里不符合预期，日志模块有bug
                        throw new MyRaftException("replicationLogEntry logEntryList size error!" +
                            " nextIndex=" + nextIndex + " logEntryList.size=" + logEntryList.size());
                    }

                    logger.info("leader do appendEntries start, node={}, appendEntriesRpcParam={}",node,appendEntriesRpcParam);
                    AppendEntriesRpcResult appendEntriesRpcResult = node.appendEntries(appendEntriesRpcParam);
                    logger.info("leader do appendEntries end, node={}, appendEntriesRpcResult={}",node,appendEntriesRpcResult);

                    finallyResult = appendEntriesRpcResult;
                    // 收到更高任期的处理
                    boolean beFollower = currentServer.processCommunicationHigherTerm(appendEntriesRpcResult.getTerm());
                    if(beFollower){
                        return appendEntriesRpcResult;
                    }

                    if(appendEntriesRpcResult.isSuccess()){
                        logger.info("appendEntriesRpcResult is success, node={}",node);

                        // If successful: update nextIndex and matchIndex for follower (§5.3)

                        // 同步成功了，nextIndex递增一位
                        this.currentServer.getNextIndexMap().put(node,nextIndex+1);
                        this.currentServer.getMatchIndexMap().put(node,nextIndex);

                        nextIndex++;
                    }else{
                        // 因为日志对不上导致一致性检查没通过，同步没成功，nextIndex往后退一位

                        logger.info("appendEntriesRpcResult is false, node={}",node);

                        // If AppendEntries fails because of log inconsistency: decrement nextIndex and retry (§5.3)
                        nextIndex--;
                        this.currentServer.getNextIndexMap().put(node,nextIndex);
                    }
                }

                if(finallyResult == null){
                    // 说明有bug
                    throw new MyRaftException("replicationLogEntry finallyResult is null!");
                }

                logger.info("finallyResult={},node={}",node,finallyResult);

                return finallyResult;
            });

            futureList.add(future);
        }

        // 获得结果
        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = CommonUtil.concurrentGetRpcFutureResult(
                "do appendEntries", futureList,
                this.rpcThreadPool,2, TimeUnit.SECONDS);

        logger.info("leader replicationLogEntry appendEntriesRpcResultList={}",appendEntriesRpcResultList);

        return appendEntriesRpcResultList;
    }
<br><br>
<br>相比lab1，lab2版本的appendEntries除了之前已有的针对leader任期相关的校验和处理逻辑外，还新增了日志复制相关的逻辑。
<br>raftLog是顺序保存的，为了日志复制的安全性，raft的follower节点也必须和leader保持一致，将日志按照index索引值以从小到大的顺序存在本地的raft日志文件中。<br>
因此只有在已有第0-第N条日志的情况下，follower才能够安全的将第N+1条日志追加到本地日志文件中。为此raft作者设计了一系列的校验规则来保证这一点。
<br>首先，leader在发起appendEntries命令follower复制第N条(logIndex=N)日志时，会将第前一条日志(N-1)的term值(prevLogTerm)和index值(prevLogIndex)作为参数一并传递，follower会对这两个值进行校验以决定是否能安全的复制日志。<br>
follower会查询出对应索引为prevLogIndex的日志，如果没查出来说明进度没跟上leader那自然要返回复制失败，让leader重试把logIndex更小、更前面的日志发过来。<br>
如果查出来了对应的日志则还需要进一步对比请求参数中的prevLogTerm和follower本地对应日志的term值是否一致，如果一致则校验通过；如果不一致则说明follower保存了一个和leader不一致且未最终提交的日志，也要返回失败，让leader把更前面的日志发过来(具体原理后面会分析)。
<br>当follower这边对于prevLogTerm和prevLogIndex的校验都通过了后，说明leader此时已经找到了follower恰好需要的日志(日志同步的进度匹配上了)，则follower需要将发送过来的日志写入本地日志文件中。<br>
如果leader发过来的日志里对应index的本地日志不存在则直接追加到follower本地日志文件中即可；如果之前已存在则覆盖掉原来的日志即可。
<br>当一开始follower没有追上leader的日志进度时(比如follower宕机了一段时间再回到集群)，follower会一直返回同步失败，leader则会一直向前找直到找到follower所恰好需要的那条日志。在这之后的日志同步就会十分顺利了，直到follower和leader的日志完全一致。
<br>考虑到follower可能和leader的日志进度相差过大，一次回退一个索引值的匹配策略效率并不高。论文中提到一种可行的优化是让follower直接把自己的最后一个日志，或者倒数第N个日志的信息(term和index)返回给leader，leader就能够很快的从正确的日志位点开始同步。<br>
但这样会增加程序的复杂度并降低正常情况下进度一致时日志同步的效率，论文的作者认为出现这一问题的概率很低因此该优化是不必要的，简单起见MyRaft也没有做相关的优化。
<br>follower在成功将日志落盘后，根据请求参数中的leaderCommit值和自己本地的lastCommittedIndex(最大已提交日志索引号)来判断是否应该将本地的raftLog提交到状态机中执行。<br>
如果参数leaderCommit大于本地的lastCommittedIndex，说明leader已经把一些日志提交到状态机中了，而follower还没有。那么follower需要跟上leader的进度(不但日志进度要匹配，状态机进度也要匹配)。<br>
需要提交到状态机中日志最大的index值为leaderCommit和当前最新一条日志的index最小值(index of last new entry)，如果leaderCommit是最小值，说明follower已经赶上了leader，leader提交的日志follower本地都有。<br>
而如果index of last new entry是更小的，说明当前follower还没有追上leader提交到状态机的进度，把当前已有的所有本地日志都提交到状态机中即可(pushStatemachineApply方法)。
<br><br>raft论文在第5.3节重点解释了一下原因。<br>
<br>举个例子，一个5节点的集群，节点编号分别是abcde。在任期1中a是leader，尝试向集群复制一个日志(index=1，term=1，set k1=v1)，但是复制时失败了，只有节点b成功的复制了该日志，而cde都没有持有日志。
<br>恰好这时leader a宕机了，触发新选举后节点c成为了新的leader(任期term=2)。c接受到客户端请求，也尝试向集群复制一个日志(index=1，term=2, set k1=v2)，这个时候节点b有一个index=1，这个时候节点b必须让新的日志覆盖掉老的日志才能和leader保持一致。
<br>c的这次日志复制成功了，bcde四个节点都持有了该日志，则raft集群便可以将这个日志提交到状态机中了，最后集群的所有状态机中k1的值将会是v2，而不是之前复制失败而未提交的v1。<br>
<img alt="Pasted image 20240725172453.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172453.png">
<br>    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        if(appendEntriesRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("doAppendEntries term &lt; currentTerm");
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
        }

        if(appendEntriesRpcParam.getTerm() &gt;= this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // appendEntries请求中任期值如果大于自己，说明已经有一个更新的leader了，自己转为follower，并且以对方更大的任期为准
            this.serverStatusEnum = ServerStatusEnum.FOLLOWER;
            this.currentLeader = appendEntriesRpcParam.getLeaderId();
            this.raftServerMetaDataPersistentModule.setCurrentTerm(appendEntriesRpcParam.getTerm());
        }

        if(appendEntriesRpcParam.getEntries() == null || appendEntriesRpcParam.getEntries().isEmpty()){
            // 来自leader的心跳处理，清理掉之前选举的votedFor
            this.cleanVotedFor();
            // entries为空，说明是心跳请求，刷新一下最近收到心跳的时间
            raftLeaderElectionModule.refreshLastHeartbeatTime();

            long currentLastCommittedIndex = logModule.getLastCommittedIndex();
            logger.debug("doAppendEntries heartbeat leaderCommit={},currentLastCommittedIndex={}",
                appendEntriesRpcParam.getLeaderCommit(),currentLastCommittedIndex);

            if(appendEntriesRpcParam.getLeaderCommit() &gt; currentLastCommittedIndex) {
                // 心跳处理里，如果leader当前已提交的日志进度超过了当前节点的进度，令当前节点状态机也跟上
                // 如果leaderCommit &gt;= logModule.getLastIndex(),说明当前节点的日志进度不足，但可以把目前已有的日志都提交给状态机去执行
                // 如果leaderCommit &lt; logModule.getLastIndex(),说明当前节点进度比较快，有一些日志是leader已复制但还没提交的，把leader已提交的那一部分作用到状态机就行
                long minNeedCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), logModule.getLastIndex());
                pushStatemachineApply(minNeedCommittedIndex);
            }

            // 心跳请求，直接返回
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),true);
        }

        // logEntries不为空，是真实的日志复制rpc

        logger.info("do real log append! appendEntriesRpcParam={}",appendEntriesRpcParam);
        // AppendEntry可靠性校验，如果prevLogIndex和prevLogTerm不匹配，则需要返回false，让leader发更早的日志过来
        {
            LogEntry localPrevLogEntry = logModule.readLocalLog(appendEntriesRpcParam.getPrevLogIndex());
            if(localPrevLogEntry == null){
                // 当前节点日志条目为空，说明完全没有日志(默认任期为-1，这个是约定)
                localPrevLogEntry = LogEntry.getEmptyLogEntry();
            }

            if (localPrevLogEntry.getLogTerm() != appendEntriesRpcParam.getPrevLogTerm()) {
                //  Reply false if log doesn’t contain an entry at prevLogIndex
                //  whose term matches prevLogTerm (§5.3)
                //  本地日志和参数中的PrevLogIndex和PrevLogTerm对不上(对应日志不存在，或者任期对不上)
                logger.info("doAppendEntries localPrevLogEntry not match, localLogEntry={}",localPrevLogEntry);

                return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
            }
        }

        // 走到这里说明找到了最新的一条匹配的记录
        logger.info("doAppendEntries localEntry is match");

        List&lt;LogEntry&gt; newLogEntryList = appendEntriesRpcParam.getEntries();

        // 1. Append any new entries not already in the log
        // 2. If an existing entry conflicts with a new one (same index but different terms),
        //    delete the existing entry and all that follow it (§5.3)
        // 新日志的复制操作（直接整个覆盖掉prevLogIndex之后的所有日志,以leader发过来的日志为准）
        logModule.writeLocalLog(newLogEntryList, appendEntriesRpcParam.getPrevLogIndex());

        // If leaderCommit &gt; commitIndex, set commitIndex = min(leaderCommit, index of last new entry)
        if(appendEntriesRpcParam.getLeaderCommit() &gt; logModule.getLastCommittedIndex()){
            // 如果leaderCommit更大，说明当前节点的同步进度慢于leader，以新的entry里的index为准(更高的index还没有在本地保存(因为上面的appendEntry有效性检查))
            // 如果index of last new entry更大，说明当前节点的同步进度是和leader相匹配的，commitIndex以leader最新提交的为准

            LogEntry lastNewEntry = newLogEntryList.get(newLogEntryList.size()-1);
            long lastCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), lastNewEntry.getLogIndex());
            pushStatemachineApply(lastCommittedIndex);
        }

        // 返回成功
        return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(), true);
    }

    private void pushStatemachineApply(long lastCommittedIndex){
        long lastApplied = logModule.getLastApplied();

        // If commitIndex &gt; lastApplied: increment lastApplied, apply log[lastApplied] to state machine (§5.3)
        if(lastApplied &lt; lastCommittedIndex){
            // 作用在状态机上的日志编号低于集群中已提交的日志编号，需要把这些已提交的日志都作用到状态机上去
            logger.info("pushStatemachineApply.apply, lastApplied={},lastCommittedIndex={}",lastApplied,lastCommittedIndex);

            // 全读取出来(读取出来是按照index从小到大排好序的)
            List&lt;LocalLogEntry&gt; logEntryList = logModule.readLocalLog(lastApplied+1,lastCommittedIndex);

            logger.info("pushStatemachineApply.apply, logEntryList={}",logEntryList);

            List&lt;SetCommand&gt; setCommandList = logEntryList.stream()
                .filter(item-&gt;item.getCommand() instanceof SetCommand)
                .map(item-&gt;(SetCommand)item.getCommand())
                .collect(Collectors.toList());

            // 按照顺序依次作用到状态机中
            this.kvReplicationStateMachine.batchApply(setCommandList);
        }

        this.logModule.setLastCommittedIndex(lastCommittedIndex);
        this.logModule.setLastApplied(lastCommittedIndex);
    }
<br><br>
<br>前面的例子里提到在老leader宕机触发选举后，新的leader是可能把一些不一致的日志给覆盖清除掉以保证日志一致性。<br>
当leader广播日志并在半数以上follower成功复制后，并提交raftLog到状态机中后如果leader突然宕机了，raft是如何保证新的leader不会清理掉已提交到状态机中的日志的呢？
<br>raft的论文在5.4节安全性一节中提到了这一点，raft作者通过在leader选举过程中follower投票的环节中添加对双方日志的校验来保证已提交到状态机的日志绝对不会被覆盖。<br>
raft论文中保证安全性的核心思路共两点：一是candidate必须和超过半数的follower进行通信并得到选票；二是candidate的日志至少要和follower一样新(即follower有的日志candidate必须本地也有，反之则不需要成立)。<br>
基于这两点后能以此推导：已提交的日志一定在集群中超过半数的节点中存在 + 新当选的leader所包含的日志一定比集群中超过半数的节点更全面(至少一样全面) =&gt; 新的leader一定包含所有已提交的日志(只有包含所有已提交日志的节点才能被选为leader)
<br>那raft是如何在选举投票时令follower和candidate进行日志完整程度比对的呢？raft论文的5.4节中也提到了，具体规则如下：<br>
Raft通过比较两个节点中日志中最后一个条目的索引和任期来决定谁是最新的。

<br>如果两个日志中最后的条目有着不同的任期，则任期较后的日志是更新的。
<br>如果两个日志中最后的条目有着相同的任期，则较长的(注：索引值更大的)那个日志是更新的。


<br>candidate的requestVote的请求中会带上candidate自己最后一条日志的任期(lastLogTerm)和索引值(lastLogIndex),而处理请求的follower也需要查询出自己本地的最后一条日志出来，并基于上述规则比较到底是哪边的日志更新，更全面。<br>
只有当candidate的日志完整程度大于(更新)或等于(一样新)follower本地的日志时，follower才能将选票给到candidate。
<br>有了在选举逻辑中关于日志的完整性的校验，raft的日志复制功能就算基本完成了。而为什么这样的设计能保证日志复制的安全性，不会造成节点间数据的不一致，在raft的论文中有提到，在这里就不再赘述了。<br>
raft论文中给出的关于日志复制正确性的结论并不是那么显然(因为有不少异常的case需要琢磨)，希望读者能通过仔细推敲论文并自己动手实现raft来加深理解。
<br>在上一小节的例子中，假设任期1中的a成功的复制了日志，并且在b、c节点上复制成功，而d、e上没有复制成功。那么如果a在提交日志到状态机后宕机，则只有b、c才可能被选举为leader，因为b、c会拒绝来自d、e的requestVote(b、c的日志比d、e的新)而令其无法获得半数以上的选票，而反过来d、e则会同意投票给b、c。<br> /**
     * 处理投票请求
     * 注意：synchronized修饰防止不同candidate并发的投票申请处理，以FIFO的方式处理
     * */
    public synchronized RequestVoteRpcResult requestVoteProcess(RequestVoteRpcParam requestVoteRpcParam){
        if(this.currentServer.getCurrentTerm() &gt; requestVoteRpcParam.getTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 发起投票的candidate任期小于当前服务器任期，拒绝投票给它
            logger.info("reject requestVoteProcess! term &lt; currentTerm, currentServerId={}",currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 发起投票的节点任期高于当前节点，无条件投票给它(任期高的说了算)
        if(this.currentServer.getCurrentTerm() &lt; requestVoteRpcParam.getTerm()){
            // 刷新元数据
            this.currentServer.refreshRaftServerMetaData(
                new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
            // 任期没它高，自己转为follower
            this.currentServer.setServerStatusEnum(ServerStatusEnum.FOLLOWER);
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
        }

        // term任期值相同，需要避免同一任期内投票给不同的节点而脑裂
        if(this.currentServer.getVotedFor() != null &amp;&amp; !this.currentServer.getVotedFor().equals(requestVoteRpcParam.getCandidateId())){
            // If votedFor is null or candidateId（取反的卫语句）
            // 当前服务器已经把票投给了别人,拒绝投票给发起投票的candidate
            logger.info("reject requestVoteProcess! votedFor={},currentServerId={}",
                currentServer.getVotedFor(),currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 考虑日志条目索引以及任期值是否满足条件的情况（第5.4节中提到的安全性）
        // 保证leader必须拥有所有已提交的日志，即发起投票的candidate日志一定要比投票给它的节点更新
        LogEntry lastLogEntry = currentServer.getLogModule().getLastLogEntry();
        logger.info("requestVoteProcess lastLogEntry={}",lastLogEntry);
        if(lastLogEntry.getLogTerm() &gt; requestVoteRpcParam.getLastLogTerm()){
            // If the logs have last entries with different terms, then the log with the later term is more up-to-date.
            // 当前节点的last日志任期比发起投票的candidate更高(比candidate更新)，不投票给它
            logger.info("lastLogEntry.term &gt; candidate.lastLogTerm! voteGranted=false");
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }else if(lastLogEntry.getLogTerm() == requestVoteRpcParam.getLastLogTerm() &amp;&amp;
            lastLogEntry.getLogIndex() &gt; requestVoteRpcParam.getLastLogIndex()){
            // If the logs end with the same term, then whichever log is longer is more up-to-date.
            // 当前节点的last日志和发起投票的candidate任期一样，但是index比candidate的高(比candidate更新)，不投票给它

            logger.info("lastLogEntry.term == candidate.lastLogTerm &amp;&amp; " +
                "lastLogEntry.index &gt; candidate.lastLogIndex! voteGranted=false");
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }else{
            // candidate的日志至少与当前节点一样新(或者更新)，通过检查，可以投票给它
            logger.info("candidate log at least as new as the current node, valid passed!");
        }

        // 投票校验通过,刷新元数据
        this.currentServer.refreshRaftServerMetaData(
            new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
        this.currentServer.processCommunicationHigherTerm(requestVoteRpcParam.getTerm());
        return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
    }
<br><br>在raft日志复制的过程中的任意瞬间，集群中的每个节点都可能出现宕机、网络超时等异常情况。下面分析在出现这些异常时，raft是如何保证集群正常工作的。<br><br>client请求报错，client重试直到raft服务集群选举出新的leader后恢复工作。<br><br>client请求报错，重试直到选举出新leader(下面的异常情况client也是一样的处理)。<br>
raft集群会进行选举选出新leader。宕机的老leader在回到集群后对应index的本地日志将会被新leader给覆盖掉。<br><br>当leader广播将日志复制到少数节点中宕机，则可能存在两种情况。<br>
<br>成功落盘的少数节点在新一轮选举中当选leader，则宕机的老leader回到集群后已经落盘的对应raftLog会被保留下来。
<br>未成功落盘的节点当选了新leader，则宕机的老leader已经落盘的对应raftLog将会被覆盖清除掉。<br>
这两种情况都是正确的，因为未提交到状态机中的日志无论是被覆盖清除还是最终被提交，都是合理的。
<br><br>如果对应log成功复制到了多数节点中，则按照上面所分析的raft选举安全性，只有拥有最新raftLog的那多数的节点才有机会当选为新leader。<br>
因此宕机的老leader重新回到集群后，落盘的raftLog将会被保留下来。<br><br>整个集群的处理和3.4一样，宕机的老leader重新回到集群后，落盘的raftLog将会被保留下来。唯一的区别在于宕机leader节点的状态机将会重复执行同一条raftLog。<br>
解决这一问题的方法主要有两种：<br>
<br>要求状态机的具体实现能够容忍raftLog重复的执行，或者设计对相同log幂等的防护(MyRaft的方案，状态机数据不持久化并且只有纯set操作，无自增/自减等非幂等操作)。
<br>raft协议的实现中将lastApplied属性持久化，通过持久化lastApplied的方式来避免宕机恢复后重复执行日志。
<br><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/382888510/answer/2478166051" rel="noopener nofollow" class="external-link" href="https://www.zhihu.com/question/382888510/answer/2478166051" target="_blank">为什么 Raft 的 ApplyIndex 和 CommitIndex 不需要持久化？</a><br><br><br>MyRaft实现了一个非常基础的命令行交互式客户端(RpcCmdInteractiveClient)用于测试MyRaft这一kv数据库的读写功能。<br>/**
 * 命令行交互的客户端
 *
 * 只支持以下命令
 * 1. get [key]
 * 2. set [key] [value]
 * 3. quit
 * */
public class RpcCmdInteractiveClient {

    public static void main(String[] args) {
        // 客户端的超时时间必须大于raft内部rpc的超时时间，否则在节点故障时rpc会一直超时
        DefaultFuture.DEFAULT_TIME_OUT = 3000L;

        RaftClient raftClient = new RaftClient(RaftClusterGlobalConfig.registry);
        raftClient.init();
        raftClient.setRaftNodeConfigList(RaftClusterGlobalConfig.raftNodeConfigList);

        Scanner scan = new Scanner(System.in);

        System.out.println("RpcCmdInteractiveClient start, please input command:");

        while(scan.hasNext()) {
            String input = scan.nextLine();
            if(input.length() == 0){
                continue;
            }

            if (Objects.equals(input, "quit")) {
                scan.close();
                System.out.println("RpcCmdInteractiveClient quit success!");
                return;
            }

            if (input.startsWith("get")) {
                processGetCmd(raftClient,input);
            }else if(input.startsWith("set")){
                processSetCmd(raftClient,input);
            }else{
                System.out.println("un support cmd, please retry！");
            }
        }
    }

    private static void processGetCmd(RaftClient raftClient, String input){
        try {
            String[] cmdItem = input.split(" ");
            if (cmdItem.length != 2) {
                System.out.println("get cmd error, please retry！");
                return;
            }

            String key = cmdItem[1];
            String result = raftClient.doRequestRetry(new GetCommand(key),2);
            System.out.println("processGet result=" + result);
        }catch (Exception e){
            System.out.println("processGet error!");
            e.printStackTrace();
        }
    }

    private static void processSetCmd(RaftClient raftClient, String input){
        try {
            String[] cmdItem = input.split(" ");
            if (cmdItem.length != 3) {
                System.out.println("set cmd error, please retry！");
                return;
            }

            String key = cmdItem[1];
            String value = cmdItem[2];
            String result = raftClient.doRequestRetry(new SetCommand(key, value),2);
            System.out.println("processSet success=" + result);
        }catch (Exception e){
            System.out.println("processSetCmd error!");
            e.printStackTrace();
        }
    }
}
<br><br><img alt="Pasted image 20240725172512.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172512.png"><br><br>在github源码中的test目录下中有RpcClientNode(1-5)类,全部以main方法启动后便可形成一个5节点的raft集群。通过RpcCmdInteractiveClient便可以通过以下几个case简单验证MyRaft关于日志复制的基本功能。<br>
<br>启动所有节点，进行一系列的读写操作。检查每个节点中日志/状态机中的数据是否符合预期
<br>将任意一个节点关闭，删除掉状态机对应的文件(相当于清空了kv状态机里的数据)，重新启动后leader的心跳会触发全量日志再一次作用到状态机中。检查状态机的数据是否符合预期
<br>将任意一个节点关闭，继续进行一系列的读写操作。然后将节点重启恢复，在进行新的写操作后，leader会将宕机时丢失的那部分日志同步到该节点，并且日志是否成功的提交到状态机中执行。检查日志/状态机的数据是否符合预期
<br><br>
<br>作为手写raft系列博客的第二篇，在博客的第1节简单介绍了raft的日志复制功能，第2节详细分析了MyRaft关于日志复制功能的实现源码，第3节通过分析日志复制过程中异常情况的处理来证明raft日志复制功能的正确性。
<br>raft的日志复制功能是raft算法中最复杂的一部分，除了正常执行逻辑以外还包含了大量异常情况的处理。在博客中我结合MyRaft的源码尽可能的将自己理解的各种细节分享出来，希望能帮到对raft实现细节、正确性证明等相关内容感兴趣的读者。
<br>博客中展示的完整代码在我的github上：<a rel="noopener nofollow" class="external-link" href="https://github.com/1399852153/MyRaft" target="_blank">https://github.com/1399852153/MyRaft</a> (release/lab2_log_replication分支)，内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/raft实现-实现日志复制.html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft实现/Raft实现 - 实现日志复制.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:46:27 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172412.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172412.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[自己动手实现rpc框架(一) 实现点对点的rpc通信]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rpc" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rpc</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:rpc" class="tag" target="_blank" rel="noopener nofollow">#rpc</a> <a href="https://muqiuhan.github.io/wiki?query=tag:java" class="tag" target="_blank" rel="noopener nofollow">#java</a><br><br>RPC是远过程调用(Remote Procedure Call)的缩写形式，其区别于一个程序内部基本的过程调用(或者叫函数/方法调用)。<br>随着应用程序变得越来越复杂，在单个机器上中仅通过一个进程来运行整个应用程序的方式已经难以满足现实中日益增长的需求。<br>
开发者对应用程序进行模块化的拆分，以分布式部署的方式来降低程序整体的复杂度和提升性能方面的可拓展性(分而治之的思想)。<br>拆分后部署在不同机器上的各个模块无法像之前那样通过内存寻址的方式来互相访问，而是需要通过网络来进行通信。<br>
RPC最主要的功能就是在提供不同模块服务间的网络通信能力的同时，又尽可能的不丢失本地调用时语义的简洁性。rpc可以认为是分布式系统中类似人体经络一样的基础设施，因此有必要对其工作原理有一定的了解。<br><br>要学习rpc的原理，理论上最好的办法就是去看流行的开源框架源码。但dubbo这样成熟的rpc框架由于已经迭代了很多年，为了满足多样的需求而有着复杂的架构和庞大的代码量。对于普通初学者来说往往很难从层层抽象封装中把握住关于rpc框架最核心的内容。<br>MyRpc是我最近在学习MIT6.824分布式系统公开课时，使用java并基于netty实现的一个简易rpc框架，实现的过程中许多地方都参考了dubbo以及一些demo级别的rpc框架。<br>
MyRpc是demo级别的框架，理解起来会轻松不少。在对基础的rpc实现原理有一定了解后，能对后续研究dubbo等开源rpc框架带来很大的帮助。<br>目前MyRpc实现了以下功能<br>
<br>网络通信(netty做客户端、服务端网络交互，服务端使用一个线程池处理业务逻辑)
<br>实现消息的序列化（实现序列化方式的抽象，支持json、hessian、jdk序列化等）
<br>客户端代理生成(目前只实现了jdk动态代理)
<br>服务注册 + 注册中心集成(实现注册中心的抽象，但目前只支持用zookeeper做注册中心)
<br>集群负载均衡策略(实现负载均衡策略的抽象，支持roundRobin轮训，随机等)
<br>使用时间轮，支持设置消费者调用超时时间
<br>限于篇幅，以上功能会拆分为两篇博客分别介绍。其中前3个功能实现了基本的点对点通信的rpc功能，将在本篇博客中结合源码详细分析。<br><img alt="Pasted image 20240725171915.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171915.png"><br><br><br>MyRpc是以netty为基础的，下面展示一个最基础的netty客户端/服务端交互的demo。<br>netty服务端：<br>/**
 * 最原始的netty服务端demo
 * */
public class PureNettyServer {

    public static void main(String[] args) throws InterruptedException {
        ServerBootstrap bootstrap = new ServerBootstrap();
        EventLoopGroup bossGroup = new NioEventLoopGroup(1, new DefaultThreadFactory("NettyServerBoss", true));
        EventLoopGroup workerGroup = new NioEventLoopGroup(8,new DefaultThreadFactory("NettyServerWorker", true));

        bootstrap.group(bossGroup, workerGroup)
            .channel(NioServerSocketChannel.class)
            .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 实际调用业务方法的处理器
                        .addLast("serverHandler", new SimpleChannelInboundHandler&lt;ByteBuf&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf requestByteBuf) {
                                String requestStr = requestByteBuf.toString(CharsetUtil.UTF_8);
                                System.out.println("PureNettyServer read request=" + JsonUtil.json2Obj(requestStr, User.class));

                                // 服务端响应echo
                                ByteBuf byteBuf = Unpooled.copiedBuffer("echo:" + requestStr,CharsetUtil.UTF_8);
                                channelHandlerContext.writeAndFlush(byteBuf);
                            }
                        })
                    ;
                }
            });

        ChannelFuture channelFuture = bootstrap.bind("127.0.0.1", 8888).sync();

        System.out.println("netty server started!");
        // 一直阻塞在这里
        channelFuture.channel().closeFuture().sync();
    }
}
<br>netty客户端：<br>/**
 * 最原始的netty客户端demo
 * */
public class PureNettyClient {

    public static void main(String[] args) throws InterruptedException {
        Bootstrap bootstrap = new Bootstrap();
        EventLoopGroup eventLoopGroup = new NioEventLoopGroup(8,
            new DefaultThreadFactory("NettyClientWorker", true));

        bootstrap.group(eventLoopGroup)
            .channel(NioSocketChannel.class)
            .handler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        .addLast("clientHandler", new SimpleChannelInboundHandler&lt;ByteBuf&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf responseByteBuf) {
                                String responseStr = responseByteBuf.toString(CharsetUtil.UTF_8);
                                System.out.println("PureNettyClient received response=" + responseStr);
                            }
                        })
                    ;
                }
            });

        ChannelFuture channelFuture = bootstrap.connect("127.0.0.1", 8888).sync();
        Channel channel = channelFuture.sync().channel();

        // 发送一个user对象的json串
        User user = new User("Tom",10);
        ByteBuf requestByteBuf = Unpooled.copiedBuffer(JsonUtil.obj2Str(user), CharsetUtil.UTF_8);
        channel.writeAndFlush(requestByteBuf);

        System.out.println("netty client send request success!");
        channelFuture.channel().closeFuture().sync();
    }
}
<br>
<br>demo示例中，netty的服务端启动后绑定在本机127.0.0.1的8888端口上，等待来自客户端的连接。
<br>netty客户端向服务端发起连接请求，在成功建立连接后向服务端发送了一个User对象字符串对应的字节数组。
<br>服务端在接受到这一字节数组后反序列化为User对象并打印在控制台，随后echo响应了一个字符串。客户端在接受到响应后，将echo字符串打印在了控制台上
<br><br>上面展示了一个最基础的netty网络通信的demo，似乎一个点对点的传输功能已经得到了良好的实现。<br>
但作为一个rpc框架，还需要解决tcp传输层基于字节流的消息黏包/拆包问题。<br><br>操作系统实现的传输层tcp协议中，向上层的应用保证尽最大可能的(best effort delivery)、可靠的传输字节流，但并不关心实际传输的数据包是否总是符合应用层的要求。<br>
<br>黏包问题： 假设应用层发送的一次请求数据量比较小(比如0.1kb)，tcp层可能不会在接到应用请求后立即进行传输，而是会稍微等待一小会。<br>
这样如果应用层在短时间内需要传输多次0.1kb的请求，就可以攒在一起批量传输，传输效率会高很多。<br>
但这带来的问题就是接收端一次接受到的数据包内应用程序逻辑上的多次请求黏连在了一起，需要通过一些方法来将其拆分还原为一个个独立的信息给应用层。
<br>拆包问题： 假设应用层发送的一次请求数据量比较大(比如100Mb)，而tcp层的数据包容量的最大值是有限的，所以应用层较大的一次请求数据会被拆分为多个包分开发送。<br>
这就导致接收端接受到的某个数据包其实并不是完整的应用层请求数据，没法直接交给应用程序去使用，<br>
而必须等待后续对应请求的所有数据包都接受完成后，才能组装成完整的请求对象再交给应用层处理。
<br>可以看到，上述的黏包/拆包问题并不能看做是tcp的问题，而是应用层最终需求与tcp传输层功能不匹配导致的问题。<br>
tcp出于传输效率的考虑无法解决这个问题，所以黏包拆包问题最终只能在更上面的应用层自己来处理。
<br>一个数据包中可能同时存在黏包问题和拆包问题(如下图所示)<br>
<img alt="Pasted image 20240725171936.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171936.png"><br><br>解决黏包/拆包问题最核心的思路是，如何知道一个应用层完整请求的边界。<br>
对于黏包问题，基于边界可以独立的拆分出每一个请求；对于拆包问题，如果发现收到的数据包末尾没有边界，则继续等待新的数据包，直到发现边界后再一并上交给应用程序。<br>主流的解决黏包拆包的应用层协议设计方案有三种：<br><br>对于流行的rpc框架，一般都是选用性能与兼容性皆有的方案3：即自己设计一个固定大小的、包含了请求体长度字段的请求头。MyRpc参考dubbo，也设计了一个固定16字节大小的请求头(里面有几个字段暂时没用上)。<br>请求头: MessageHeader<br>/**
 * 共16字节的请求头
 * */
public class MessageHeader implements Serializable {

    public static final int MESSAGE_HEADER_LENGTH = 16;
    public static final int MESSAGE_SERIALIZE_TYPE_LENGTH = 5;
    public static final short MAGIC = (short)0x2233;

    // ================================ 消息头 =================================
    /**
     * 魔数(占2字节)
     * */
    private short magicNumber = MAGIC;

    /**
     * 消息标识(0代表请求事件；1代表响应事件， 占1位)
     * @see MessageFlagEnums
     * */
    private Boolean messageFlag;

    /**
     * 是否是双向请求(0代表oneWay请求；1代表twoWay请求）
     * （双向代表客户端会等待服务端的响应，单向则请求发送完成后即向上层返回成功)
     * */
    private Boolean twoWayFlag;

    /**
     * 是否是心跳消息(0代表正常消息；1代表心跳消息， 占1位)
     * */
    private Boolean eventFlag;

    /**
     * 消息体序列化类型(占5位，即所支持的序列化类型不得超过2的5次方，32种)
     * @see MessageSerializeType
     * */
    private Boolean[] serializeType;

    /**
     * 响应状态(占1字节)
     * */
    private byte responseStatus;

    /**
     * 消息的唯一id（占8字节）
     * */
    private long messageId;

    /**
     * 业务数据长度（占4字节）
     * */
    private int bizDataLength;
}
<br>完整的消息对象: MessageProtocol<br>public class MessageProtocol&lt;T&gt; implements Serializable {
    /**
     * 请求头
     * */
    private MessageHeader messageHeader;

    /**
     * 请求体(实际的业务消息对象)
     * */
    private T bizDataBody;
}
<br><img alt="Pasted image 20240725172037.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172037.png"><br><br>/**
 * rpc请求对象
 * */
public class RpcRequest implements Serializable {

    private static final AtomicLong INVOKE_ID = new AtomicLong(0);

    /**
     * 消息的唯一id（占8字节）
     * */
    private final long messageId;

    /**
     * 接口名
     * */
    private String interfaceName;

    /**
     * 方法名
     * */
    private String methodName;

    /**
     * 参数类型数组(每个参数一项)
     * */
    private Class&lt;?&gt;[] parameterClasses;

    /**
     * 实际参数对象数组(每个参数一项)
     * */
    private Object[] params;

    public RpcRequest() {
        // 每个请求对象生成时都自动生成单机全局唯一的自增id
        this.messageId = INVOKE_ID.getAndIncrement();
    }
}
<br>/**
 * rpc响应对象
 * */
public class RpcResponse implements Serializable {

    /**
     * 消息的唯一id（占8字节）
     * */
    private long messageId;

    /**
     * 返回值
     */
    private Object returnValue;

    /**
     * 异常值
     */
    private Exception exceptionValue;
}
<br><br>在上一节的netty demo中的消息处理器中，一共做了两件事情；一是将原始数据包的字节流转化成了应用程序所需的String对象；二是拿到String对象后进行响应的业务处理(比如打印在控制台上)。<br>
而netty框架允许配置多个消息处理器组成链条，按约定的顺序处理出站/入站的消息；因此从模块化的出发，应该将编码/解码的逻辑和实际业务的处理拆分成多个处理器。<br>在自定义的消息编码器、解码器中进行应用层请求/响应数据的序列化/反序列化，同时处理上述的黏包/拆包问题。<br>编解码工具类<br>public class MessageCodecUtil {

    /**
     * 报文协议编码
     * */
    public static &lt;T&gt; void messageEncode(MessageProtocol&lt;T&gt; messageProtocol, ByteBuf byteBuf) {
        MessageHeader messageHeader = messageProtocol.getMessageHeader();
        // 写入魔数
        byteBuf.writeShort(MessageHeader.MAGIC);

        // 写入消息标识
        byteBuf.writeBoolean(messageHeader.getMessageFlag());
        // 写入单/双向标识
        byteBuf.writeBoolean(messageHeader.getTwoWayFlag());
        // 写入消息事件标识
        byteBuf.writeBoolean(messageHeader.getEventFlag());
        // 写入序列化类型
        for(boolean b : messageHeader.getSerializeType()){
            byteBuf.writeBoolean(b);
        }
        // 写入响应状态
        byteBuf.writeByte(messageHeader.getResponseStatus());
        // 写入消息uuid
        byteBuf.writeLong(messageHeader.getMessageId());

        // 序列化消息体
        MyRpcSerializer myRpcSerializer = MyRpcSerializerManager.getSerializer(messageHeader.getSerializeType());
        byte[] bizMessageBytes = myRpcSerializer.serialize(messageProtocol.getBizDataBody());
        // 获得并写入消息正文长度
        byteBuf.writeInt(bizMessageBytes.length);
        // 写入消息正文内容
        byteBuf.writeBytes(bizMessageBytes);
    }

    /**
     * 报文协议header头解码
     * */
    public static MessageHeader messageHeaderDecode(ByteBuf byteBuf){
        MessageHeader messageHeader = new MessageHeader();
        // 读取魔数
        messageHeader.setMagicNumber(byteBuf.readShort());
        // 读取消息标识
        messageHeader.setMessageFlag(byteBuf.readBoolean());
        // 读取单/双向标识
        messageHeader.setTwoWayFlag(byteBuf.readBoolean());
        // 读取消息事件标识
        messageHeader.setEventFlag(byteBuf.readBoolean());

        // 读取序列化类型
        Boolean[] serializeTypeBytes = new Boolean[MessageHeader.MESSAGE_SERIALIZE_TYPE_LENGTH];
        for(int i=0; i&lt;MessageHeader.MESSAGE_SERIALIZE_TYPE_LENGTH; i++){
            serializeTypeBytes[i] = byteBuf.readBoolean();
        }
        messageHeader.setSerializeType(serializeTypeBytes);

        // 读取响应状态
        messageHeader.setResponseStatus(byteBuf.readByte());
        // 读取消息uuid
        messageHeader.setMessageId(byteBuf.readLong());

        // 读取消息正文长度
        int bizDataLength = byteBuf.readInt();
        messageHeader.setBizDataLength(bizDataLength);

        return messageHeader;
    }

    /**
     * 报文协议正文body解码
     * */
    public static &lt;T&gt; T messageBizDataDecode(MessageHeader messageHeader, ByteBuf byteBuf, Class&lt;T&gt; messageBizDataType){
        // 读取消息正文
        byte[] bizDataBytes = new byte[messageHeader.getBizDataLength()];
        byteBuf.readBytes(bizDataBytes);

        // 反序列化消息体
        MyRpcSerializer myRpcSerializer = MyRpcSerializerManager.getSerializer(messageHeader.getSerializeType());
        return (T) myRpcSerializer.deserialize(bizDataBytes,messageBizDataType);
    }
}
<br>自定义编码器: NettyEncoder<br>public class NettyEncoder&lt;T&gt; extends MessageToByteEncoder&lt;MessageProtocol&lt;T&gt;&gt; {

    @Override
    protected void encode(ChannelHandlerContext channelHandlerContext, MessageProtocol&lt;T&gt; messageProtocol, ByteBuf byteBuf) {
        // 继承自MessageToByteEncoder中，只需要将编码后的数据写入参数中指定的byteBuf中即可
        // MessageToByteEncoder源码逻辑中会自己去将byteBuf写入channel的
        MessageCodecUtil.messageEncode(messageProtocol,byteBuf);
    }
}
<br>自定义解码器: NettyDecoder<br>/**
 * netty 解码器
 */
public class NettyDecoder extends ByteToMessageDecoder {

    private static final Logger logger = LoggerFactory.getLogger(NettyDecoder.class);

    @Override
    protected void decode(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf, List&lt;Object&gt; list){
        do{
            try {
                // 保存读取前的读指针
                int beforeReadIndex = byteBuf.readerIndex();
                MessageDecodeResult messageDecodeResult = decodeHeader(byteBuf);

                if (messageDecodeResult.isNeedMoreData()) {
                    // 出现拆包没有读取到一个完整的rpc请求，还原byteBuf读指针，等待下一次读事件
                    byteBuf.readerIndex(beforeReadIndex);
                    break;
                } else {
                    // 正常解析完一个完整的message，交给后面的handler处理
                    list.add(messageDecodeResult.getMessageProtocol());
                }
            }catch (Exception e){
                // 比如decodeHeader里json序列化失败了等等.直接跳过这个数据包不还原了
                logger.error("NettyDecoder error!",e);
            }

            // 循环，直到整个ByteBuf读取完
        }while(byteBuf.isReadable());
    }
    
    private MessageDecodeResult decodeHeader(ByteBuf byteBuf){
        int readable = byteBuf.readableBytes();
        if(readable &lt; MessageHeader.MESSAGE_HEADER_LENGTH){
            // 无法读取到一个完整的header，说明出现了拆包，等待更多的数据
            return MessageDecodeResult.needMoreData();
        }

        // 读取header头
        MessageHeader messageHeader = MessageCodecUtil.messageHeaderDecode(byteBuf);

        int bizDataLength = messageHeader.getBizDataLength();
        if(byteBuf.readableBytes() &lt; bizDataLength){
            // 无法读取到一个完整的正文内容，说明出现了拆包，等待更多的数据
            return MessageDecodeResult.needMoreData();
        }

        // 基于消息类型标识，解析rpc正文对象
        boolean messageFlag = messageHeader.getMessageFlag();
        if(messageFlag == MessageFlagEnums.REQUEST.getCode()){
            RpcRequest rpcRequest = MessageCodecUtil.messageBizDataDecode(messageHeader,byteBuf,RpcRequest.class);
            MessageProtocol&lt;RpcRequest&gt; messageProtocol = new MessageProtocol&lt;&gt;(messageHeader,rpcRequest);
            // 正确的解析完一个rpc请求消息
            return MessageDecodeResult.decodeSuccess(messageProtocol);
        }else{
            RpcResponse rpcResponse = MessageCodecUtil.messageBizDataDecode(messageHeader,byteBuf,RpcResponse.class);
            MessageProtocol&lt;RpcResponse&gt; messageProtocol = new MessageProtocol&lt;&gt;(messageHeader,rpcResponse);
            // 正确的解析完一个rpc响应消息
            return MessageDecodeResult.decodeSuccess(messageProtocol);
        }
    }
}
<br><br>demo的服务示例:<br>public class User implements Serializable {

    private String name;
    private Integer age;
}
<br>public interface UserService {

    User getUserFriend(User user, String message);
}
<br>public class UserServiceImpl implements UserService {
    @Override
    public User getUserFriend(User user, String message) {
        System.out.println("execute getUserFriend, user=" + user + ",message=" + message);

        // demo返回一个不同的user对象回去
        return new User(user.getName() + ".friend", user.getAge() + 1);
    }
}
<br>netty服务端：<br>public class RpcServer {

    private static final Map&lt;String,Object&gt; interfaceImplMap = new HashMap&lt;&gt;();

    static{
        /**
         * 简单一点配置死实现
         * */
        interfaceImplMap.put(UserService.class.getName(), new UserServiceImpl());
    }

    public static void main(String[] args) throws InterruptedException {
        ServerBootstrap bootstrap = new ServerBootstrap();
        EventLoopGroup bossGroup = new NioEventLoopGroup(1, new DefaultThreadFactory("NettyServerBoss", true));
        EventLoopGroup workerGroup = new NioEventLoopGroup(8,new DefaultThreadFactory("NettyServerWorker", true));

        bootstrap.group(bossGroup, workerGroup)
            .channel(NioServerSocketChannel.class)
            .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 编码、解码处理器
                        .addLast("encoder", new NettyEncoder&lt;&gt;())
                        .addLast("decoder", new NettyDecoder())
                        // 实际调用业务方法的处理器
                        .addLast("serverHandler", new SimpleChannelInboundHandler&lt;MessageProtocol&lt;RpcRequest&gt;&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext ctx, MessageProtocol&lt;RpcRequest&gt; msg) {
                                // 找到本地的方法进行调用，并获得返回值(demo，简单起见直接同步调用)
                                MessageProtocol&lt;RpcResponse&gt; result = handlerRpcRequest(msg);

                                // 将返回值响应给客户端
                                ctx.writeAndFlush(result);
                            }
                        });
                }
            });

        ChannelFuture channelFuture = bootstrap.bind("127.0.0.1", 8888).sync();

        System.out.println("netty server started!");
        // 一直阻塞在这里
        channelFuture.channel().closeFuture().sync();
    }

    private static MessageProtocol&lt;RpcResponse&gt; handlerRpcRequest(MessageProtocol&lt;RpcRequest&gt; rpcRequestMessageProtocol){
        long requestMessageId = rpcRequestMessageProtocol.getMessageHeader().getMessageId();

        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageId(requestMessageId);
        messageHeader.setMessageFlag(MessageFlagEnums.RESPONSE.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(false);
        messageHeader.setSerializeType(rpcRequestMessageProtocol.getMessageHeader().getSerializeType());

        RpcResponse rpcResponse = new RpcResponse();
        rpcResponse.setMessageId(requestMessageId);

        try {
            // 反射调用具体的实现方法
            Object result = invokeTargetService(rpcRequestMessageProtocol.getBizDataBody());

            // 设置返回值
            rpcResponse.setReturnValue(result);
        }catch (Exception e){
            // 调用具体实现类时，出现异常，设置异常的值
            rpcResponse.setExceptionValue(e);
        }

        return new MessageProtocol&lt;&gt;(messageHeader,rpcResponse);
    }

    private static Object invokeTargetService(RpcRequest rpcRequest) throws Exception {
        String interfaceName = rpcRequest.getInterfaceName();
        Object serviceImpl = interfaceImplMap.get(interfaceName);

        // 按照请求里的方法名和参数列表找到对应的方法
        final Method method = serviceImpl.getClass().getMethod(rpcRequest.getMethodName(), rpcRequest.getParameterClasses());

        // 传递参数，反射调用该方法并返回结果
        return method.invoke(serviceImpl, rpcRequest.getParams());
    }
}
<br>netty客户端：<br>public class RpcClientNoProxy {

    public static void main(String[] args) throws InterruptedException {
        Bootstrap bootstrap = new Bootstrap();
        EventLoopGroup eventLoopGroup = new NioEventLoopGroup(8,
            new DefaultThreadFactory("NettyClientWorker", true));

        bootstrap.group(eventLoopGroup)
            .channel(NioSocketChannel.class)
            .handler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 编码、解码处理器
                        .addLast("encoder", new NettyEncoder&lt;&gt;())
                        .addLast("decoder", new NettyDecoder())
                        .addLast("clientHandler", new SimpleChannelInboundHandler&lt;MessageProtocol&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext channelHandlerContext, MessageProtocol messageProtocol) {
                                System.out.println("PureNettyClient received messageProtocol=" + messageProtocol);
                            }
                        })
                    ;
                }
            });

        ChannelFuture channelFuture = bootstrap.connect("127.0.0.1", 8888).sync();
        Channel channel = channelFuture.sync().channel();

        // 构造消息对象
        MessageProtocol&lt;RpcRequest&gt; messageProtocol = buildMessage();
        // 发送消息
        channel.writeAndFlush(messageProtocol);

        System.out.println("RpcClientNoProxy send request success!");
        channelFuture.channel().closeFuture().sync();
    }

    private static MessageProtocol&lt;RpcRequest&gt; buildMessage(){
        // 构造请求
        RpcRequest rpcRequest = new RpcRequest();
        rpcRequest.setInterfaceName("myrpc.demo.common.service.UserService");
        rpcRequest.setMethodName("getUserFriend");
        rpcRequest.setParameterClasses(new Class[]{User.class,String.class});

        User user = new User("Jerry",10);
        String message = "hello hello!";
        rpcRequest.setParams(new Object[]{user,message});

        // 构造协议头
        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageFlag(MessageFlagEnums.REQUEST.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(true);
        messageHeader.setSerializeType(MessageSerializeType.JSON.getCode());
        messageHeader.setMessageId(rpcRequest.getMessageId());

        return new MessageProtocol&lt;&gt;(messageHeader,rpcRequest);
    }
}
<br><img alt="Pasted image 20240725172059.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172059.png"><br><br>截止目前，我们已经实现了一个点对点rpc客户端/服务端交互的功能，但是客户端这边的逻辑依然比较复杂(buildMessage方法)。<br>
前面提到，rpc中很重要的功能就是保持本地调用时语义的简洁性，即客户端实际使用时是希望直接用以下这种方式来进行调用，而不是去繁琐的处理底层的网络交互逻辑。<br>    User user = new User("Jerry",10);
    String message = "hello hello!";
    // 发起rpc调用并获得返回值
    User userFriend = userService.getUserFriend(user,message);
    System.out.println("userService.getUserFriend result=" + userFriend);
<br>rpc框架需要屏蔽掉构造底层消息发送/接受，序列化/反序列化相关的复杂性，而这时候就需要引入代理模式(动态代理)了。<br>
在MyRpc的底层，我们将客户端需要调用的一个服务(比如UserService)抽象为Consumer对象，服务端的一个具体服务实现抽象为Provider对象。<br>
其中包含了对应的服务的类以及对应的服务地址，客户端这边使用jdk的动态代理生成代理对象，将复杂的、需要屏蔽的消息处理/网络交互等逻辑都封装在这个代理对象中。<br>public class Consumer&lt;T&gt; {

    private Class&lt;?&gt; interfaceClass;
    private T proxy;

    private Bootstrap bootstrap;
    private URLAddress urlAddress;

    public Consumer(Class&lt;?&gt; interfaceClass, Bootstrap bootstrap, URLAddress urlAddress) {
        this.interfaceClass = interfaceClass;
        this.bootstrap = bootstrap;
        this.urlAddress = urlAddress;

        ClientDynamicProxy clientDynamicProxy = new ClientDynamicProxy(bootstrap,urlAddress);

        this.proxy = (T) Proxy.newProxyInstance(
            clientDynamicProxy.getClass().getClassLoader(),
            new Class[]{interfaceClass},
            clientDynamicProxy);
    }

    public T getProxy() {
        return proxy;
    }

    public Class&lt;?&gt; getInterfaceClass() {
        return interfaceClass;
    }
}
<br>public class ConsumerBootstrap {

    private final Map&lt;Class&lt;?&gt;,Consumer&lt;?&gt;&gt; consumerMap = new HashMap&lt;&gt;();
    private final Bootstrap bootstrap;
    private final URLAddress urlAddress;

    public ConsumerBootstrap(Bootstrap bootstrap, URLAddress urlAddress) {
        this.bootstrap = bootstrap;
        this.urlAddress = urlAddress;
    }

    public &lt;T&gt; Consumer&lt;T&gt; registerConsumer(Class&lt;T&gt; clazz){
        if(!consumerMap.containsKey(clazz)){
            Consumer&lt;T&gt; consumer = new Consumer&lt;&gt;(clazz,this.bootstrap,this.urlAddress);
            consumerMap.put(clazz,consumer);
            return consumer;
        }

        throw new MyRpcException("duplicate consumer! clazz=" + clazz);
    }
}
<br>public class Provider&lt;T&gt; {

    private Class&lt;?&gt; interfaceClass;
    private T ref;
    private URLAddress urlAddress;
}
<br><br>/**
 * 客户端动态代理
 * */
public class ClientDynamicProxy implements InvocationHandler {

    private static final Logger logger = LoggerFactory.getLogger(ClientDynamicProxy.class);

    private final Bootstrap bootstrap;
    private final URLAddress urlAddress;

    public ClientDynamicProxy(Bootstrap bootstrap, URLAddress urlAddress) {
        this.bootstrap = bootstrap;
        this.urlAddress = urlAddress;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        Tuple&lt;Object,Boolean&gt; localMethodResult = processLocalMethod(proxy,method,args);
        if(localMethodResult.getRight()){
            // right为true,代表是本地方法，返回toString等对象自带方法的执行结果，不发起rpc调用
            return localMethodResult.getLeft();
        }

        logger.debug("ClientDynamicProxy before: methodName=" + method.getName());

        // 构造请求和协议头
        RpcRequest rpcRequest = new RpcRequest();
        rpcRequest.setInterfaceName(method.getDeclaringClass().getName());
        rpcRequest.setMethodName(method.getName());
        rpcRequest.setParameterClasses(method.getParameterTypes());
        rpcRequest.setParams(args);

        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageFlag(MessageFlagEnums.REQUEST.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(true);
        messageHeader.setSerializeType(GlobalConfig.messageSerializeType.getCode());
        messageHeader.setResponseStatus((byte)'a');
        messageHeader.setMessageId(rpcRequest.getMessageId());

        logger.debug("ClientDynamicProxy rpcRequest={}", JsonUtil.obj2Str(rpcRequest));

        ChannelFuture channelFuture = bootstrap.connect(urlAddress.getHost(),urlAddress.getPort()).sync();
        Channel channel = channelFuture.sync().channel();
        // 通过Promise，将netty的异步转为同步,参考dubbo DefaultFuture
        DefaultFuture&lt;RpcResponse&gt; defaultFuture = DefaultFutureManager.createNewFuture(channel,rpcRequest);

        channel.writeAndFlush(new MessageProtocol&lt;&gt;(messageHeader,rpcRequest));
        logger.debug("ClientDynamicProxy writeAndFlush success, wait result");

        // 调用方阻塞在这里
        RpcResponse rpcResponse = defaultFuture.get();

        logger.debug("ClientDynamicProxy defaultFuture.get() rpcResponse={}",rpcResponse);

        return processRpcResponse(rpcResponse);
    }

    /**
     * 处理本地方法
     * @return tuple.right 标识是否是本地方法， true是
     * */
    private Tuple&lt;Object,Boolean&gt; processLocalMethod(Object proxy, Method method, Object[] args) throws Exception {
        // 处理toString等对象自带方法，不发起rpc调用
        if (method.getDeclaringClass() == Object.class) {
            return new Tuple&lt;&gt;(method.invoke(proxy, args),true);
        }
        String methodName = method.getName();
        Class&lt;?&gt;[] parameterTypes = method.getParameterTypes();
        if (parameterTypes.length == 0) {
            if ("toString".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.toString(),true);
            } else if ("hashCode".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.hashCode(),true);
            }
        } else if (parameterTypes.length == 1 &amp;&amp; "equals".equals(methodName)) {
            return new Tuple&lt;&gt;(proxy.equals(args[0]),true);
        }

        // 返回null标识非本地方法，需要进行rpc调用
        return new Tuple&lt;&gt;(null,false);
    }

    private Object processRpcResponse(RpcResponse rpcResponse){
        if(rpcResponse.getExceptionValue() == null){
            // 没有异常，return正常的返回值
            return rpcResponse.getReturnValue();
        }else{
            // 有异常，往外抛出去
            throw new MyRpcRemotingException(rpcResponse.getExceptionValue());
        }
    }
}
<br><br>/**
 * 客户端 rpc响应处理器
 */
public class NettyRpcResponseHandler extends SimpleChannelInboundHandler&lt;MessageProtocol&lt;RpcResponse&gt;&gt; {

    private static final Logger logger = LoggerFactory.getLogger(NettyRpcResponseHandler.class);

    @Override
    protected void channelRead0(ChannelHandlerContext channelHandlerContext, MessageProtocol&lt;RpcResponse&gt; rpcResponseMessageProtocol) throws Exception {
        logger.debug("NettyRpcResponseHandler channelRead0={}",JsonUtil.obj2Str(rpcResponseMessageProtocol));

        // 触发客户端的future，令其同步阻塞的线程得到结果
        DefaultFutureManager.received(rpcResponseMessageProtocol.getBizDataBody());
    }
}
<br>public class DefaultFutureManager {

    private static Logger logger = LoggerFactory.getLogger(DefaultFutureManager.class);

    public static final Map&lt;Long,DefaultFuture&gt; DEFAULT_FUTURE_CACHE = new ConcurrentHashMap&lt;&gt;();

    public static void received(RpcResponse rpcResponse){
        Long messageId = rpcResponse.getMessageId();

        logger.debug("received rpcResponse={},DEFAULT_FUTURE_CACHE={}",rpcResponse,DEFAULT_FUTURE_CACHE);
        DefaultFuture defaultFuture = DEFAULT_FUTURE_CACHE.remove(messageId);

        if(defaultFuture != null){
            logger.debug("remove defaultFuture success");
            if(rpcResponse.getExceptionValue() != null){
                // 异常处理
                defaultFuture.completeExceptionally(rpcResponse.getExceptionValue());
            }else{
                // 正常返回
                defaultFuture.complete(rpcResponse);
            }
        }else{
            logger.debug("remove defaultFuture fail");
        }
    }

    public static DefaultFuture createNewFuture(Channel channel, RpcRequest rpcRequest){
        DefaultFuture defaultFuture = new DefaultFuture(channel,rpcRequest);

        return defaultFuture;
    }
}
<br><br>public class RpcClientProxy {

    public static void main(String[] args) throws InterruptedException {
        Bootstrap bootstrap = new Bootstrap();
        EventLoopGroup eventLoopGroup = new NioEventLoopGroup(8, new DefaultThreadFactory("NettyClientWorker", true));

        bootstrap.group(eventLoopGroup)
            .channel(NioSocketChannel.class)
            .handler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 编码、解码处理器
                        .addLast("encoder", new NettyEncoder&lt;&gt;())
                        .addLast("decoder", new NettyDecoder())

                        // 响应处理器
                        .addLast("clientHandler", new NettyRpcResponseHandler())
                    ;
                }
            });

        ConsumerBootstrap consumerBootstrap = new ConsumerBootstrap(bootstrap, new URLAddress("127.0.0.1", 8888));
        Consumer&lt;UserService&gt; userServiceConsumer = consumerBootstrap.registerConsumer(UserService.class);

        // 获得UserService的代理对象
        UserService userService = userServiceConsumer.getProxy();

        User user = new User("Jerry", 10);
        String message = "hello hello!";
        // 发起rpc调用并获得返回值
        User userFriend = userService.getUserFriend(user, message);
        System.out.println("userService.getUserFriend result=" + userFriend);
    }
}
<br>可以看到，引入了代理模式后的使用方式就变得简单很多了。<br>
到这一步，我们已经实现了一个点对点的rpc通信的能力，并且如博客开头中所提到的，没有丧失本地调用语义的简洁性。<br><br>
<br>这篇博客是我关于Mit6.824分布式系统公开课lab的第一篇博客，按照计划会将实现简易版rpc和raft k/v数据库的心得以博客的形式分享出来，希望能帮助到对分布式系统相关技术的小伙伴。
<br>打个广告：对于英语不好(没法直接啃生肉)但又对国外著名的计算机公开课(涉及操作系统、数据库、分布式系统、编译原理、计算机网络、算法等等)感兴趣的小伙伴，可以咨询simviso购买中英翻译质量很高的公开课视频(比如Mit6.824，b站上开放了一部分免费的视频：<a rel="noopener nofollow" class="external-link" href="https://www.bilibili.com/video/BV1x7411M7Sf" target="_blank">https://www.bilibili.com/video/BV1x7411M7Sf</a>)。
<br>博客中展示的完整代码在我的github上：<a rel="noopener nofollow" class="external-link" href="https://github.com/1399852153/MyRpc" target="_blank">https://github.com/1399852153/MyRpc</a> (release/lab1分支)，内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/自己动手实现rpc框架(一)-实现点对点的rpc通信.html</link><guid isPermaLink="false">Computer Science/Distributed System/RPC框架的实现/自己动手实现rpc框架(一) 实现点对点的rpc通信.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:47:19 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171915.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171915.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[自己动手实现rpc框架(二) 实现集群间rpc通信]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rpc" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rpc</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:rpc" class="tag" target="_blank" rel="noopener nofollow">#rpc</a> <a href="https://muqiuhan.github.io/wiki?query=tag:java" class="tag" target="_blank" rel="noopener nofollow">#java</a><br><br>上一篇博客中MyRpc框架实现了基本的点对点rpc通信功能。而在这篇博客中我们需要实现MyRpc的集群间rpc通信功能。<br>
<br><a data-href="自己动手实现rpc框架(一) 实现点对点的rpc通信" href="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/自己动手实现rpc框架(一)-实现点对点的rpc通信.html" class="internal-link" target="_self" rel="noopener nofollow">自己动手实现rpc框架(一) 实现点对点的rpc通信</a>
<br>上篇博客的点对点rpc通信实现中，客户端和服务端的ip地址和端口都是固定配置死的。而通常为了提升服务总负载，客户端和服务端都是以集群的方式部署的(水平拓展)，客户端和服务端的节点都不止1个。<br>集群条件下出现了很多新的问题需要解决：<br>
<br>对于某一特定服务，客户端该如何知道当前环境下哪些机器能提供这一服务?
<br>服务端集群中的某些节点如果发生了变化(比如老节点下线或宕机)，客户端该如何及时的感知到，而不会调用到已经停止服务的节点上？
<br>存在多个服务端时，客户端应该向哪一个服务端节点发起请求？怎样才能使得每个服务端的负载尽量均衡，而不会让某些服务端饥饿或者压力过大。
<br><br>
<br>针对第一个问题，最先想到的自然是直接在每个客户端都配置一个固定的服务端节点列表，但这一方案无法很好的解决服务端节点动态变化的问题。<br>
如果一个服务端节点下线了，就需要人工的去修改每个客户端那里维护的服务端节点列表的话，在集群节点数量较多、服务端节点上下线频繁的场景下是不可接受的。
<br>解决这一问题的思路是服务端节点信息的中心化，将服务端节点的信息都集中维护在一个地方。<br>
服务端在启动成功后将自己的信息注册在上面(服务注册)，而客户端也能实时的查询出最新的服务端列表(服务发现)。<br>
这个统一维护服务端节点信息的地方被叫做注册中心，一般是以独立服务的形式与rpc的服务端/客户端机器部署在同一环境内。
<br>由于节点信息的中心化，所以注册中心需要具备高可用能力(集群部署来提供容错能力)，避免单点故障而导致整个rpc集群的不可用。<br>
同时在服务端节点因为一些原因不可用时能实时的感知并移除掉对应节点，同时通知监听变更客户端(解决第二个关于provider信息实时性的问题)。<br>
因此zookeeper、eureka、nacos、etcd等等具备上述能力的中间件都被广泛的用作rpc框架的注册中心。
<br><br>MyRpc目前支持使用zookeeper作为注册中心。<br>
zookeeper作为一个高性能的分布式协调器，存储的数据以ZNode节点树的形式存在。ZNode节点有两种属性，有序/无序，持久/临时。<br>
<br>rpc框架中一般设置一个持久的根路径节点用于与zk上存储其它的业务数据作区分(例如/my_rpc)。
<br>在根节点下有着代表着某一特定服务的子节点，其也是持久节点。服务子节点的路径名是标识接口的唯一名称(比如包名+类名：myrpc.demo.common.service.UserService)
<br>而服务节点下则可以存储各种关于provider、consumer等等相关的元数据。<br>
MyRpc中为了简单起见，服务节点的子节点直接就是对应特定provider注册的临时节点。临时节点中数据保存了provider的ip/port等必要的信息。<br>
由于是临时节点，在provider因为各种故障而不可用而导致与zookeeper的连接断开，zookeeper会在等待一小会后将该临时节点删除，并通知监听该服务的客户端以刷新客户端的对应配置。
<br><img alt="Pasted image 20240725172146.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172146.png"><br>注册中心接口<br>/**
 * 注册中心的抽象
 * */
public interface Registry {

    /**
     * 服务注册
     * */
    void doRegistry(ServiceInfo serviceInfo);

    /**
     * 服务发现
     * */
    List&lt;ServiceInfo&gt; discovery(String serviceName);
}
<br>zookeeper注册中心实现(原始的zk客户端)<br>/**
 * 简易的zk注册中心(原始的zk客户端很多地方都需要用户去处理异常，但为了更简单的展示zk注册中心的使用，基本上没有处理这些异常情况)
 * */
public class ZookeeperRegistry implements Registry{

    private static final Logger logger = LoggerFactory.getLogger(ZookeeperRegistry.class);

    private final ZooKeeper zooKeeper;

    private final ConcurrentHashMap&lt;String,List&lt;ServiceInfo&gt;&gt; serviceInfoCacheMap = new ConcurrentHashMap&lt;&gt;();

    public ZookeeperRegistry(String zkServerAddress) {
        try {
            this.zooKeeper = new ZooKeeper(zkServerAddress,2000, event -&gt; {});

            // 确保root节点是一定存在的
            createPersistentNode(MyRpcRegistryConstants.BASE_PATH);
        } catch (Exception e) {
            throw new MyRpcException("init zkClient error",e);
        }
    }

    @Override
    public void doRegistry(ServiceInfo serviceInfo) {
        // 先创建永久的服务名节点
        createServiceNameNode(serviceInfo.getServiceName());
        // 再创建临时的providerInfo节点
        createProviderInfoNode(serviceInfo);
    }

    @Override
    public List&lt;ServiceInfo&gt; discovery(String serviceName) {
        return serviceInfoCacheMap.computeIfAbsent(serviceName,(key)-&gt; findProviderInfoList(serviceName));
    }

    private String getServiceNameNodePath(String serviceName){
        return MyRpcRegistryConstants.BASE_PATH + "/" + serviceName;
    }

    // ================================ zk工具方法 ==================================
    private void createServiceNameNode(String serviceName){
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceName);

            // 服务名节点是永久节点
            createPersistentNode(serviceNameNodePath);
            logger.info("createServiceNameNode success! serviceNameNodePath={}",serviceNameNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createServiceNameNode error",e);
        }
    }

    private void createProviderInfoNode(ServiceInfo serviceInfo){
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceInfo.getServiceName());
            // 子节点用一个uuid做path防重复
            String providerInfoNodePath = serviceNameNodePath + "/" + UUID.randomUUID();

            String providerInfoJsonStr = JsonUtil.obj2Str(serviceInfo);
            // providerInfo节点是临时节点(如果节点宕机了，zk的连接断开一段时间后，临时节点会被自动删除)
            zooKeeper.create(providerInfoNodePath, providerInfoJsonStr.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
            logger.info("createProviderInfoNode success! path={}",providerInfoNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createProviderInfoNode error",e);
        }
    }

    private void createPersistentNode(String path){
        try {
            if (zooKeeper.exists(path, false) == null) {
                // 服务名节点是永久节点
                zooKeeper.create(path, "".getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            }
        }catch (Exception e){
            throw new MyRpcException("createPersistentNode error",e);
        }
    }

    private List&lt;ServiceInfo&gt; findProviderInfoList(String serviceName){
        String serviceNameNodePath = getServiceNameNodePath(serviceName);

        List&lt;ServiceInfo&gt; serviceInfoList = new ArrayList&lt;&gt;();
        try {
            List&lt;String&gt; providerInfoPathList = zooKeeper.getChildren(serviceNameNodePath, new ZookeeperListener(serviceNameNodePath));
            for(String providerInfoPath : providerInfoPathList){
                try{
                    String fullProviderInfoPath = serviceNameNodePath + "/" + providerInfoPath;
                    byte[] data = zooKeeper.getData(fullProviderInfoPath,false,null);
                    String jsonStr = new String(data,StandardCharsets.UTF_8);
                    ServiceInfo serviceInfo = JsonUtil.json2Obj(jsonStr,ServiceInfo.class);

                    serviceInfoList.add(serviceInfo);
                }catch (Exception e){
                    logger.error("findProviderInfoList getData error",e);
                }
            }

            logger.info("findProviderInfoList={}",JsonUtil.obj2Str(serviceInfoList));
            return serviceInfoList;
        } catch (Exception e) {
            throw new MyRpcException("findProviderInfoList error",e);
        }
    }

    private class ZookeeperListener implements Watcher{
        private final String path;
        private final String serviceName;

        public ZookeeperListener(String serviceName) {
            this.path = getServiceNameNodePath(serviceName);
            this.serviceName = serviceName;
        }

        @Override
        public void process(WatchedEvent event) {
            logger.info("ZookeeperListener process! path={}",path);

            try {
                // 刷新缓存
                List&lt;ServiceInfo&gt; serviceInfoList = findProviderInfoList(path);
                serviceInfoCacheMap.put(serviceName,serviceInfoList);
            } catch (Exception e) {
                logger.error("ZookeeperListener getChildren error! path={}",path,e);
            }
        }
    }
}
<br>zookeeper注册中心实现(curator客户端，通过自动重试等操作解决了原生客户端的一些坑)<br>public class ZkCuratorRegistry implements Registry {
    private static final Logger logger = LoggerFactory.getLogger(ZkCuratorRegistry.class);

    private CuratorFramework curatorZkClient;

    private final ConcurrentHashMap&lt;String, List&lt;ServiceInfo&gt;&gt; serviceInfoCacheMap = new ConcurrentHashMap&lt;&gt;();
    private static ConcurrentHashMap&lt;String, PathChildrenCache&gt; nodeCacheMap = new ConcurrentHashMap&lt;&gt;();


    public ZkCuratorRegistry(String zkServerAddress) {
        try {
            this.curatorZkClient = CuratorFrameworkFactory.newClient(zkServerAddress, new ExponentialBackoffRetry(3000, 1));
            this.curatorZkClient.start();
        } catch (Exception e) {
            throw new MyRpcException("init zkClient error", e);
        }
    }

    @Override
    public void doRegistry(ServiceInfo serviceInfo) {
        // 先创建永久的服务名节点
        createServiceNameNode(serviceInfo.getServiceName());
        // 再创建临时的providerInfo节点
        createProviderInfoNode(serviceInfo);
    }

    @Override
    public List&lt;ServiceInfo&gt; discovery(String serviceName) {
        return this.serviceInfoCacheMap.computeIfAbsent(serviceName,(key)-&gt;{
            List&lt;ServiceInfo&gt; serviceInfoList = findProviderInfoList(serviceName);

            // 创建对子节点的监听
            String serviceNodePath = getServiceNameNodePath(serviceName);
            PathChildrenCache pathChildrenCache = new PathChildrenCache(curatorZkClient, serviceNodePath, true);
            try {
                pathChildrenCache.start();
                nodeCacheMap.put(serviceName,pathChildrenCache);
                pathChildrenCache.getListenable().addListener(new ZkCuratorListener(serviceName));
            } catch (Exception e) {
                throw new MyRpcException("PathChildrenCache start error!",e);
            }

            return serviceInfoList;
        });
    }

    private void createServiceNameNode(String serviceName) {
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceName);

            // 服务名节点是永久节点
            if (curatorZkClient.checkExists().forPath(serviceNameNodePath) == null) {
                curatorZkClient.create()
                    .creatingParentsIfNeeded()
                    .withMode(CreateMode.PERSISTENT)
                    .forPath(serviceNameNodePath);
            }

            logger.info("createServiceNameNode success! serviceNameNodePath={}", serviceNameNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createServiceNameNode error", e);
        }
    }

    private void createProviderInfoNode(ServiceInfo serviceInfo) {
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceInfo.getServiceName());
            // 子节点用一个uuid做path防重复
            String providerInfoNodePath = serviceNameNodePath + "/" + UUID.randomUUID();

            String providerInfoJsonStr = JsonUtil.obj2Str(serviceInfo);

            // providerInfo节点是临时节点(如果节点宕机了，zk的连接断开一段时间后，临时节点会被自动删除)
            curatorZkClient.create()
                .withMode(CreateMode.EPHEMERAL)
                .forPath(providerInfoNodePath, providerInfoJsonStr.getBytes(StandardCharsets.UTF_8));
            logger.info("createProviderInfoNode success! path={}", providerInfoNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createProviderInfoNode error", e);
        }
    }

    private String getServiceNameNodePath(String serviceName) {
        return MyRpcRegistryConstants.BASE_PATH + "/" + serviceName;
    }

    private List&lt;ServiceInfo&gt; findProviderInfoList(String serviceName) {
        String serviceNameNodePath = getServiceNameNodePath(serviceName);

        try {
            List&lt;String&gt; providerInfoPathList = curatorZkClient.getChildren().forPath(serviceNameNodePath);
            List&lt;ServiceInfo&gt; serviceInfoList = new ArrayList&lt;&gt;();

            for(String providerInfoPath : providerInfoPathList){
                try{
                    String fullProviderInfoPath = serviceNameNodePath + "/" + providerInfoPath;
                    byte[] data = curatorZkClient.getData().forPath(fullProviderInfoPath);
                    String jsonStr = new String(data,StandardCharsets.UTF_8);
                    ServiceInfo serviceInfo = JsonUtil.json2Obj(jsonStr,ServiceInfo.class);

                    serviceInfoList.add(serviceInfo);
                }catch (Exception e){
                    logger.error("findProviderInfoList getData error",e);
                }
            }

            logger.info("findProviderInfoList={}",JsonUtil.obj2Str(serviceInfoList));
            return serviceInfoList;
        } catch (Exception e) {
            throw new MyRpcException("findProviderInfoList error",e);
        }
    }

    private class ZkCuratorListener implements PathChildrenCacheListener {
        private final String serviceName;

        public ZkCuratorListener(String serviceName) {
            this.serviceName = serviceName;
        }

        @Override
        public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) {
            logger.info("ZookeeperListener process! serviceName={}",serviceName);

            try {
                // 刷新缓存
                List&lt;ServiceInfo&gt; serviceInfoList = findProviderInfoList(serviceName);
                serviceInfoCacheMap.put(serviceName,serviceInfoList);
            } catch (Exception e) {
                logger.error("ZookeeperListener getChildren error! serviceName={}",serviceName,e);
            }
        }
    }
}
<br><br>现在客户端已经能通过服务发现得到实时的provider集合了，那么客户端发起请求时应该如何决定向哪个provider发起请求以实现provider侧的负载均衡呢？<br>
<br>常见的负载均衡算法有很多，MyRpc抽象出了负载均衡算法的接口，并实现了最简单的两种负载均衡算法(无权重的纯随机 + roundRobin)。
<br>在实际的环境中，每个provider可能机器的配置、网络延迟、运行时的动态负载、请求处理的延迟等都各有不同，优秀的负载均衡算法能够通过预先的配置和采集运行时的各项指标来计算出最优的请求顺序。<br>
MyRpc实现的负载均衡算法在这里只起到一个抛砖引玉的参考作用。
<br>负载均衡接口<br>/**
 * 负载均衡选择器
 * */
public interface LoadBalance {

    ServiceInfo select(List&lt;ServiceInfo&gt; serviceInfoList);
}
<br>随机负载均衡<br>/**
 * 无权重，纯随机的负载均衡选择器
 * */
public class RandomLoadBalance implements LoadBalance{
    @Override
    public ServiceInfo select(List&lt;ServiceInfo&gt; serviceInfoList) {
        int selectedIndex = ThreadLocalRandom.current().nextInt(serviceInfoList.size());
        return serviceInfoList.get(selectedIndex);
    }
}
<br>/**
 * 无权重的轮训负载均衡（后续增加带权重的轮训）
 * */
public class SimpleRoundRobinBalance implements LoadBalance{

    private final AtomicInteger count = new AtomicInteger();

    @Override
    public ServiceInfo select(List&lt;ServiceInfo&gt; serviceInfoList) {
        if(serviceInfoList.isEmpty()){
            throw new MyRpcException("serviceInfoList is empty!");
        }

        // 考虑一下溢出，取绝对值
        int selectedIndex = Math.abs(count.getAndIncrement());
        return serviceInfoList.get(selectedIndex % serviceInfoList.size());
    }
}
<br><br>由于需要通过网络发起rpc调用，比起本地调用很容易因为网络波动、远端机器故障等原因而导致调用失败。<br>
客户端有时希望能通过重试等方式屏蔽掉可能出现的偶发错误，尽可能的保证rpc请求的成功率，最好rpc框架能解决这个问题。但另一方面，能够安全重试的基础是下游服务能够做到幂等，否则重复的请求会带来意想不到的后果，而不幂等的下游服务只能至多调用一次。<br>
因此rpc框架需要能允许用户不同的服务可以有不同的集群服务调用方式，这样幂等的服务可以配置成可自动重试N次的failover调用、只能调用1次的fast-fail调用或者广播调用等等方式。<br>MyRpc的Invoker接口用于抽象上述的不同集群调用方式，并简单的实现了failover和fast-fail等多种调用方式（参考dubbo）。<br>public interface InvokerCallable {
    RpcResponse invoke(NettyClient nettyClient);
}
<br>/**
 * 不同的集群调用方式
 * */
public interface Invoker {

    RpcResponse invoke(InvokerCallable callable, String serviceName,
                                      Registry registry, LoadBalance loadBalance);
}
<br>/**
 * 快速失败，无论成功与否调用1次就返回
 * */
public class FastFailInvoker implements Invoker {

  private static final Logger logger = LoggerFactory.getLogger(FastFailInvoker.class);

  @Override
  public RpcResponse invoke(InvokerCallable callable, String serviceName,
                            Registry registry, LoadBalance loadBalance) {
    List&lt;ServiceInfo&gt; serviceInfoList = registry.discovery(serviceName);
    logger.debug("serviceInfoList.size={},serviceInfoList={}",serviceInfoList.size(), JsonUtil.obj2Str(serviceInfoList));
    NettyClient nettyClient = InvokerUtil.getTargetClient(serviceInfoList,loadBalance);
    logger.info("ClientDynamicProxy getTargetClient={}", nettyClient);

    // fast-fail，简单的调用一次就行，有错误就直接向上抛
    return callable.invoke(nettyClient);
  }
}
<br>/**
 * 故障转移调用(如果调用出现了错误，则重试指定次数)
 * 1 如果重试过程中成功了，则快读返回
 * 2 如果重试了指定次数后还是没成功，则抛出异常
 * */
public class FailoverInvoker implements Invoker {

    private static final Logger logger = LoggerFactory.getLogger(FailoverInvoker.class);

    private final int defaultRetryCount = 2;
    private final int retryCount;

    public FailoverInvoker() {
        this.retryCount = defaultRetryCount;
    }

    public FailoverInvoker(int retryCount) {
        this.retryCount = Math.max(retryCount,1);
    }

    @Override
    public RpcResponse invoke(InvokerCallable callable, String serviceName, Registry registry, LoadBalance loadBalance) {
        MyRpcException myRpcException = null;

        for(int i=0; i&lt;retryCount; i++){
            List&lt;ServiceInfo&gt; serviceInfoList = registry.discovery(serviceName);
            logger.debug("serviceInfoList.size={},serviceInfoList={}",serviceInfoList.size(), JsonUtil.obj2Str(serviceInfoList));
            NettyClient nettyClient = InvokerUtil.getTargetClient(serviceInfoList,loadBalance);
            logger.info("ClientDynamicProxy getTargetClient={}", nettyClient);

            try {
                RpcResponse rpcResponse = callable.invoke(nettyClient);
                if(myRpcException != null){
                    // 虽然最终重试成功了，但是之前请求失败过
                    logger.warn("FailRetryInvoker finally success, but there have been failed providers");
                }
                return rpcResponse;
            }catch (Exception e){
                myRpcException = new MyRpcException(e);

                logger.warn("FailRetryInvoker callable.invoke error",e);
            }
        }

        // 走到这里说明经过了retryCount次重试依然不成功，myRpcException一定不为null
        throw myRpcException;
    }
}
<br><br>/**
 * 客户端动态代理
 * */
public class ClientDynamicProxy implements InvocationHandler {

    private static final Logger logger = LoggerFactory.getLogger(ClientDynamicProxy.class);

    private final Registry registry;
    private final LoadBalance loadBalance;
    private final Invoker invoker;

    public ClientDynamicProxy(Registry registry, LoadBalance loadBalance, Invoker invoker) {
        this.registry = registry;
        this.loadBalance = loadBalance;
        this.invoker = invoker;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        Tuple&lt;Object,Boolean&gt; localMethodResult = processLocalMethod(proxy,method,args);
        if(localMethodResult.getRight()){
            // right为true,代表是本地方法，返回toString等对象自带方法的执行结果，不发起rpc调用
            return localMethodResult.getLeft();
        }

        logger.debug("ClientDynamicProxy before: methodName=" + method.getName());

        String serviceName = method.getDeclaringClass().getName();

        // 构造请求和协议头
        RpcRequest rpcRequest = new RpcRequest();
        rpcRequest.setInterfaceName(method.getDeclaringClass().getName());
        rpcRequest.setMethodName(method.getName());
        rpcRequest.setParameterClasses(method.getParameterTypes());
        rpcRequest.setParams(args);

        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageFlag(MessageFlagEnums.REQUEST.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(true);
        messageHeader.setSerializeType(GlobalConfig.messageSerializeType.getCode());
        messageHeader.setResponseStatus((byte)'a');
        messageHeader.setMessageId(rpcRequest.getMessageId());

        logger.debug("ClientDynamicProxy rpcRequest={}", JsonUtil.obj2Str(rpcRequest));

        RpcResponse rpcResponse = this.invoker.invoke((nettyClient)-&gt;{
            Channel channel = nettyClient.getChannel();
            // 将netty的异步转为同步,参考dubbo DefaultFuture
            DefaultFuture&lt;RpcResponse&gt; newDefaultFuture = DefaultFutureManager.createNewFuture(channel,rpcRequest);

            try {
                nettyClient.send(new MessageProtocol&lt;&gt;(messageHeader,rpcRequest));

                // 调用方阻塞在这里
                return newDefaultFuture.get();
            } catch (Exception e) {
                throw new MyRpcException("InvokerCallable error!",e);
            }
        },serviceName,registry,loadBalance);

        logger.debug("ClientDynamicProxy defaultFuture.get() rpcResponse={}",rpcResponse);

        return processRpcResponse(rpcResponse);
    }

    /**
     * 处理本地方法
     * @return tuple.right 标识是否是本地方法， true是
     * */
    private Tuple&lt;Object,Boolean&gt; processLocalMethod(Object proxy, Method method, Object[] args) throws Exception {
        // 处理toString等对象自带方法，不发起rpc调用
        if (method.getDeclaringClass() == Object.class) {
            return new Tuple&lt;&gt;(method.invoke(proxy, args),true);
        }
        String methodName = method.getName();
        Class&lt;?&gt;[] parameterTypes = method.getParameterTypes();
        if (parameterTypes.length == 0) {
            if ("toString".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.toString(),true);
            } else if ("hashCode".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.hashCode(),true);
            }
        } else if (parameterTypes.length == 1 &amp;&amp; "equals".equals(methodName)) {
            return new Tuple&lt;&gt;(proxy.equals(args[0]),true);
        }

        // 返回null标识非本地方法，需要进行rpc调用
        return new Tuple&lt;&gt;(null,false);
    }

    private Object processRpcResponse(RpcResponse rpcResponse){
        if(rpcResponse.getExceptionValue() == null){
            // 没有异常，return正常的返回值
            return rpcResponse.getReturnValue();
        }else{
            // 有异常，往外抛出去
            throw new MyRpcRemotingException(rpcResponse.getExceptionValue());
        }
    }
}
<br><br>客户端发起请求后，可能由于网络原因，可能由于服务端负载过大等原因而迟迟无法收到回复。<br>
出于性能或者自身业务的考虑，客户端不能无限制的等待下去，因此rpc框架需要能允许客户端设置请求的超时时间。在一定的时间内如果无法收到响应则需要抛出超时异常，令调用者及时的感知到问题。<br>在客户端侧DefaultFuture.get方法，指定超时时间是可以做到这一点的。<br>
但其依赖底层操作系统的定时任务机制，虽然超时时间的精度很高(nanos级别)，但在高并发场景下性能不如时间轮。<br>
具体原理可以参考我之前的博客：<a data-tooltip-position="top" aria-label="https://www.cnblogs.com/xiaoxiongcanguan/p/17128575.html" rel="noopener nofollow" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17128575.html" target="_blank">时间轮TimeWheel工作原理解析</a><br>MyRpc参考dubbo，引入时间轮来实现客户端设置请求超时时间的功能。<br>public class DefaultFutureManager {

    private static final Logger logger = LoggerFactory.getLogger(DefaultFutureManager.class);

    public static final Map&lt;Long,DefaultFuture&gt; DEFAULT_FUTURE_CACHE = new ConcurrentHashMap&lt;&gt;();
    public static final HashedWheelTimer TIMER = new HashedWheelTimer();

    public static void received(RpcResponse rpcResponse){
        Long messageId = rpcResponse.getMessageId();

        logger.debug("received rpcResponse={},DEFAULT_FUTURE_CACHE={}",rpcResponse,DEFAULT_FUTURE_CACHE);
        DefaultFuture defaultFuture = DEFAULT_FUTURE_CACHE.remove(messageId);

        if(defaultFuture != null){
            logger.debug("remove defaultFuture success");
            if(rpcResponse.getExceptionValue() != null){
                // 异常处理
                defaultFuture.completeExceptionally(rpcResponse.getExceptionValue());
            }else{
                // 正常返回
                defaultFuture.complete(rpcResponse);
            }
        }else{
            // 可能超时了，接到响应前已经remove掉了这个future(超时和实际接到请求都会调用received方法)
            logger.debug("remove defaultFuture fail");
        }
    }

    public static DefaultFuture createNewFuture(Channel channel, RpcRequest rpcRequest){
        DefaultFuture defaultFuture = new DefaultFuture(channel,rpcRequest);
        // 增加超时处理的逻辑
        newTimeoutCheck(defaultFuture);

        return defaultFuture;
    }

    public static DefaultFuture getFuture(long messageId){
        return DEFAULT_FUTURE_CACHE.get(messageId);
    }

    /**
     * 增加请求超时的检查任务
     * */
    public static void newTimeoutCheck(DefaultFuture defaultFuture){
        TimeoutCheckTask timeoutCheckTask = new TimeoutCheckTask(defaultFuture.getMessageId());
        TIMER.newTimeout(timeoutCheckTask, defaultFuture.getTimeout(), TimeUnit.MILLISECONDS);
    }
}
<br>public class TimeoutCheckTask implements TimerTask {

    private final long messageId;

    public TimeoutCheckTask(long messageId) {
        this.messageId = messageId;
    }

    @Override
    public void run(Timeout timeout) {
        DefaultFuture defaultFuture = DefaultFutureManager.getFuture(this.messageId);
        if(defaultFuture == null || defaultFuture.isDone()){
            // 请求已经在超时前返回，处理过了,直接返回即可
            return;
        }

        // 构造超时的响应
        RpcResponse rpcResponse = new RpcResponse();
        rpcResponse.setMessageId(this.messageId);
        rpcResponse.setExceptionValue(new MyRpcTimeoutException(
            "request timeout：" + defaultFuture.getTimeout() + " channel=" + defaultFuture.getChannel()));

        DefaultFutureManager.received(rpcResponse);
    }
}
<br><br>
<br>经过两个迭代，目前MyRpc已经是一个麻雀虽小五脏俱全的rpc框架了。<br>
虽然无论在功能上还是在各种细节的处理上都还有很多需要优化的地方，但作为一个demo级别的框架，其没有过多的抽象封装，更有利于rpc框架的初学者去理解。
<br>做为Mit6.824课程学习的一部分，rpc的实现到此就暂时告一段落。后续我会继续分享实现简易raft kv数据库的学习心得。
<br>博客中展示的完整代码在我的github上：<a rel="noopener nofollow" class="external-link" href="https://github.com/1399852153/MyRpc" target="_blank">https://github.com/1399852153/MyRpc</a> (release/lab2分支)，内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/自己动手实现rpc框架(二)-实现集群间rpc通信.html</link><guid isPermaLink="false">Computer Science/Distributed System/RPC框架的实现/自己动手实现rpc框架(二) 实现集群间rpc通信.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:47:20 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172146.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172146.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[分布式任务调度框架]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:java" class="tag" target="_blank" rel="noopener nofollow">#java</a><br>前段时间，公司要改造现有的单节点调度为分布式任务调度，然后就研究了目前市面上主流的开源分布式任务调度框架，用起来就一个感觉：麻烦！特别是之前在一个类里写了好多个调度任务，改造起来更加麻烦。我这人又比较懒，总感觉用了别人写好的工具还要改一大堆，心里就有点不舒服。于是我就想自己写一个框架，毕竟自己觉得分布式任务调度在所有分布式系统中是最简单的，因为一般公司任务调度本身不可能同时调度海量的任务，很大的并发，改造成分布式主要还是为了分散任务到多个节点，以便同一时间处理更多的任务。后面有一天，我在公司前台取快递，看到这样一个现象:我们好几个同事(包括我)在前台那从头到尾看快递是不是自己的，是自己的就取走，不是就忽略，然后我就受到了启发。这个场景类比到分布式调度系统中，我们可以认为是快递公司或者快递员已经把每个快递按照我们名字电话分好了快递，我们只需要取走自己的就行了。但是从另外一个角度看，也可以理解成我们每个人都是从头到尾看了所有快递，然后按照某种约定的规则，如果是自己的快递就拿走，不是自己的就忽略继续看下一个。如果把快递想象成任务，一堆人去拿一堆快递也可以很顺利的拿到各自的快递，那么一堆节点自己去取任务是不是也可以很好的处理各自的任务呢?<br>传统的分布式任务调度都有一个调度中心，这个调度中心也都要部署称多节点的集群，避免单点故障，然后还有一堆执行器，执行器负责执行调度中心分发的任务。按照上面的启发，我的思路是放弃中心式的调度中心直接由各个执行器节点去公共的地方按照约定的规则去取任务，然后执行。设计示意图如下<br><img alt="Pasted image 20240507114701.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/pasted-image-20240507114701.png">有人可能怀疑那任务db库不是有单点问题吗，我想反问下，难道其他的分布式任务调度框架没有这个问题吗？针对数据库单点我们可以单独类似业务库那样考虑高可用方案，这里不是这篇文章的讨论重点。很明显我们重点放在执行节点那里到底怎么保证高可用，单个任务不会被多个节点同时执行，单个节点执行到一半突然失联了，这个任务怎么办等复杂的问题。后续我们使用未经修饰的代码的方式一一解决这个问题（未经修饰主要是没有优化结构流水账式的代码风格，主要是很多人包括我自己看别人源码时总是感觉晕头转向的，仿佛置身迷宫般，看起来特别费劲，可能是我自己境界未到吧）<br>既然省略了集中式的调度，那么既然叫任务调度很明显必须要有调度的过程，不然多个节点去抢一个任务怎么避免冲突呢?我这里解决方式是：首先先明确一个任务的几种状态:待执行，执行中，有异常，已完成。每个节点起一个线程一直去查很快就要开始执行的待执行任务，然后遍历这些任务，使用乐观锁的方式先更新这个任务的版本号（版本号+1）和状态（变成执行中），如果更新成功就放入节点自己的延时队列中等待执行。由于每个节点的线程都是去数据库查待执行的任务，很明显变成执行中的任务下次就不会被其他节点再查询到了，至于对于那些在本节点更新状态之前就查到的待执行任务也会经过乐观锁尝试后更新失败从而跳过这个任务，这样就可以避免一个任务同时被多个节点重复执行。关键代码如下:<br>package com.rdpaas.task.scheduler;

import com.rdpaas.task.common.*;
import com.rdpaas.task.config.EasyJobConfig;
import com.rdpaas.task.repository.NodeRepository;
import com.rdpaas.task.repository.TaskRepository;
import com.rdpaas.task.strategy.Strategy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import javax.annotation.PostConstruct;

import java.util.Date;
import java.util.List;
import java.util.concurrent.*;

/**
 * 任务调度器
 * @author rongdi
 * @date 2019-03-13 21:15
 */
@Component
public class TaskExecutor {

    private static final Logger logger = LoggerFactory.getLogger(TaskExecutor.class);

    @Autowired
    private TaskRepository taskRepository;

    @Autowired
    private NodeRepository nodeRepository;

    @Autowired
    private EasyJobConfig config;
    
    /**
     * 创建任务到期延时队列
      */
	    private DelayQueue&lt;DelayItem&lt;Task&gt;&gt; taskQueue = new DelayQueue&lt;&gt;();

    /**
     * 可以明确知道最多只会运行2个线程，直接使用系统自带工具就可以了
     */
    private ExecutorService bossPool = Executors.newFixedThreadPool(2);

    /**
     * 声明工作线程池
     */
    private ThreadPoolExecutor workerPool;


    @PostConstruct
    public void init() {
/**
         * 自定义线程池，初始线程数量corePoolSize，线程池等待队列大小queueSize，当初始线程都有任务，并且等待队列满后
         * 线程数量会自动扩充最大线程数maxSize，当新扩充的线程空闲60s后自动回收.自定义线程池是因为Executors那几个线程工具
         * 各有各的弊端，不适合生产使用
         */
        workerPool = new ThreadPoolExecutor(config.getCorePoolSize(), config.getMaxPoolSize(), 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(config.getQueueSize()));
        /**
         * 执行待处理任务加载线程
         */
        bossPool.execute(new Loader());
        /**
         * 执行任务调度线程
         */
        bossPool.execute(new Boss());
    
    }

    class Loader implements Runnable {

        @Override
        public void run() {
            for(;;) {
                try { 
            /**
                     * 查找还有指定时间(单位秒)开始的主任务列表
                     */
                    List&lt;Task&gt; tasks = taskRepository.listPeddingTasks(config.getFetchDuration());
                    if(tasks == null || tasks.isEmpty()) {
                        continue;
                    }
                    for(Task task:tasks) {
                        
                        task.setStatus(TaskStatus.DOING);
                        task.setNodeId(config.getNodeId());
                        /**
                         * 使用乐观锁尝试更新状态，如果更新成功，其他节点就不会更新成功。如果在查询待执行任务列表
                         * 和当前这段时间有节点已经更新了这个任务，version必然和查出来时候的version不一样了,这里更新
                         * 必然会返回0了
                         */
                        int n = taskRepository.updateWithVersion(task);
                        Date nextStartTime = task.getNextStartTime();
                        if(n == 0 || nextStartTime == null) {
                            continue;
                        }
                        /**
                         * 封装成延时对象放入延时队列
                         */
                        task = taskRepository.get(task.getId());
                        DelayItem&lt;Task&gt; delayItem = new DelayItem&lt;Task&gt;(nextStartTime.getTime() - new Date().getTime(), task);
                        taskQueue.offer(delayItem);
                        
                    }
                    Thread.sleep(config.getFetchPeriod());
                } catch(Exception e) {
                    logger.error("fetch task list failed,cause by:{}", e);
                }
            }
        }
        
    }
    
    class Boss implements Runnable {
        @Override
        public void run() {
            for (;;) {
                try {
                     /**
                     * 时间到了就可以从延时队列拿出任务对象,然后交给worker线程池去执行
                     */
                    DelayItem&lt;Task&gt; item = taskQueue.take();
                    if(item != null &amp;&amp; item.getItem() != null) {
                        Task task = item.getItem();
                        workerPool.execute(new Worker(task));
                    }
                     
                } catch (Exception e) {
                    logger.error("fetch task failed,cause by:{}", e);
                }
            }
        }

    }

    class Worker implements Runnable {

        private Task task;

        public Worker(Task task) {
            this.task = task;
        }

        @Override
        public void run() {
            logger.info("Begin to execute task:{}",task.getId());
            TaskDetail detail = null;
            try {
                //开始任务
                detail = taskRepository.start(task);
                if(detail == null) return;
                //执行任务
                task.getInvokor().invoke();
                //完成任务
                finish(task,detail);
                logger.info("finished execute task:{}",task.getId());
            } catch (Exception e) {
                logger.error("execute task:{} error,cause by:{}",task.getId(), e);
                try {
                    taskRepository.fail(task,detail,e.getCause().getMessage());
                } catch(Exception e1) {
                    logger.error("fail task:{} error,cause by:{}",task.getId(), e);
                }
            }
        }

    }

    /**
     * 完成子任务，如果父任务失败了，子任务不会执行
     * @param task
     * @param detail
     * @throws Exception
     */
    private void finish(Task task,TaskDetail detail) throws Exception {

        //查看是否有子类任务
        List&lt;Task&gt; childTasks = taskRepository.getChilds(task.getId());
        if(childTasks == null || childTasks.isEmpty()) {
            //当没有子任务时完成父任务
            taskRepository.finish(task,detail);
            return;
        } else {
            for (Task childTask : childTasks) {
                //开始任务
                TaskDetail childDetail = null;
                try {
                    //将子任务状态改成执行中
                    childTask.setStatus(TaskStatus.DOING);
                    childTask.setNodeId(config.getNodeId());
                    //开始子任务
                    childDetail = taskRepository.startChild(childTask,detail);
                    //使用乐观锁更新下状态，不然这里可能和恢复线程产生并发问题
                    int n = taskRepository.updateWithVersion(childTask);
                    if (n &gt; 0) {
                        //再从数据库取一下，避免上面update修改后version不同步
                        childTask = taskRepository.get(childTask.getId());
                        //执行子任务
                        childTask.getInvokor().invoke();
                        //完成子任务
                        finish(childTask, childDetail);
                    }
                } catch (Exception e) {
                    logger.error("execute child task error,cause by:{}", e);
                    try {
                        taskRepository.fail(childTask, childDetail, e.getCause().getMessage());
                    } catch (Exception e1) {
                        logger.error("fail child task error,cause by:{}", e);
                    }
                }
            }
            /**
             * 当有子任务时完成子任务后再完成父任务
             */
            taskRepository.finish(task,detail);

        }

    }

}
<br>如上所述，可以保证一个任务同一个时间只会被一个节点调度执行。这时候如果部署多个节点，正常应该可以很顺利的将任务库中的任务都执行到，就像一堆人去前台取快递一样，可以很顺利的拿走所有快递。毕竟对于每个快递不是自己的就是其他人的，自己的快递也不会是其他人的。但是这里的调度和取快递有一点不一样，取快递的每个人都知道怎么去区分到底哪个快递是自己的。这里的调度完全没这个概念，完全是哪个节点运气好使用乐观锁更新了这个任务状态就是哪个节点的。总的来说区别就是需要一个约定的规则，快递是不是自己的，直接看快递上的名字和手机号码就知道了。任务到底该不该自己执行我们也可以出一个这种规则，明确哪些任务那些应该是哪些节点可以执行，从而避免无谓的锁竞争。这里可以借鉴负载均衡的那些策略，目前我想实现如下规则:<br>
<br>id_hash : 按照任务自增id的对节点个数取余，余数值和当前节点的实时序号匹配，可以匹配就可以拿走执行，否则请自觉忽略掉这个任务。
<br>least_count：最少执行任务的节点优先去取任务。
<br>weight：按照节点权重去取任务。
<br>default： 默认先到先得，没有其它规则。
<br>根据上面规则也可以说是任务的负载均衡策略可以知道除了默认规则，其余规则都需要知道全局的节点信息，比如节点执行次数，节点序号，节点权重等，所以我们需要给节点添加一个心跳，隔一个心跳周期上报一下自己的信息到数据库，心跳核心代码如下:<br>  /**
    * 创建节点心跳延时队列
    */

    private DelayQueue&lt;DelayItem&lt;Node&gt;&gt; heartBeatQueue = new DelayQueue&lt;&gt;();
 　　/**
     * 可以明确知道最多只会运行2个线程，直接使用系统自带工具
     */
    private ExecutorService bossPool = Executors.newFixedThreadPool(2);
　　 
　　 @PostConstruct
    public void init() {
        /**
         * 如果恢复线程开关是开着，并且心跳开关也是开着
         */
        if(config.isRecoverEnable() &amp;&amp; config.isHeartBeatEnable()) {
            /**
             * 初始化一个节点到心跳队列，延时为0，用来注册节点
             */
            heartBeatQueue.offer(new DelayItem&lt;&gt;(0,new Node(config.getNodeId())));
            /**
             * 执行心跳线程
             */
            bossPool.execute(new HeartBeat());
            /**
             * 执行异常恢复线程
             */
            bossPool.execute(new Recover());
        }
    }
 　　class HeartBeat implements Runnable {
        @Override
        public void run() {
            for(;;) {
                try {
                    /**
                     * 时间到了就可以从延时队列拿出节点对象，然后更新时间和序号，
                     * 最后再新建一个超时时间为心跳时间的节点对象放入延时队列，形成循环的心跳
                     */
                    DelayItem&lt;Node&gt; item = heartBeatQueue.take();
                    if(item != null &amp;&amp; item.getItem() != null) {
                        Node node = item.getItem();
                        handHeartBeat(node);
                    }
                    heartBeatQueue.offer(new DelayItem&lt;&gt;(config.getHeartBeatSeconds() * 1000,new Node(config.getNodeId())));
                } catch (Exception e) {
                    logger.error("task heart beat error,cause by:{} ",e);
                }
            }
        }
    }

    /**
     * 处理节点心跳
     * @param node
     */
    private void handHeartBeat(Node node) {
        if(node == null) {
            return;
        }
        /**
         * 先看看数据库是否存在这个节点
         * 如果不存在：先查找下一个序号，然后设置到node对象中，最后插入
         * 如果存在：直接根据nodeId更新当前节点的序号和时间
         */
        Node currNode= nodeRepository.getByNodeId(node.getNodeId());
        if(currNode == null) {
            node.setRownum(nodeRepository.getNextRownum());
            nodeRepository.insert(node);
        } else  {
            nodeRepository.updateHeartBeat(node.getNodeId());
        }

    }
<br>数据库有了节点信息后，我们就可以实现各种花式的取任务的策略了，代码如下:<br>/**
 * 抽象的策略接口
 * @author rongdi
 * @date 2019-03-16 12:36
 */
public interface Strategy {

    /**
     * 默认策略
     */
    String DEFAULT = "default";
    
    /**
     * 按任务ID hash取余再和自己节点序号匹配
     */
    String ID_HASH = "id_hash";
    
    /**
     * 最少执行次数
     */
    String LEAST_COUNT = "least_count";
    
    /**
     * 按节点权重
     */
    String WEIGHT = "weight";
    
    
    public static Strategy choose(String key) {
        switch(key) {
            case ID_HASH:
                return new IdHashStrategy();
            case LEAST_COUNT:
                return new LeastCountStrategy();
            case WEIGHT:
                return new WeightStrategy();
            default:
                return new DefaultStrategy();
        }
    }
    
    public boolean accept(List&lt;Node&gt; nodes,Task task,Long myNodeId);
    
}
<br>/**
 * 按照任务ID hash方式针对有效节点个数取余，然后余数+1后和各个节点的顺序号匹配，
 * 这种方式效果其实等同于轮询，因为任务id是自增的
 * @author rongdi
 * @date 2019-03-16
 */
public class IdHashStrategy implements Strategy {

    /**
     * 这里的nodes集合必然不会为空，外面调度那判断了，而且是按照nodeId的升序排列的
     */
    @Override
    public boolean accept(List&lt;Node&gt; nodes, Task task, Long myNodeId) {
        int size = nodes.size();
        long taskId = task.getId();
        /**
         * 找到自己的节点
         */
        Node myNode = nodes.stream().filter(node -&gt; node.getNodeId() == myNodeId).findFirst().get();
        return myNode == null ? false : (taskId % size) + 1 == myNode.getRownum();
    }

}
<br>/**
 * 最少处理任务次数策略，也就是每次任务来了，看看自己是不是处理任务次数最少的，是就可以消费这个任务
 * @author rongdi
 * @date 2019-03-16 21:56
 */
public class LeastCountStrategy implements Strategy {

    @Override
    public boolean accept(List&lt;Node&gt; nodes, Task task, Long myNodeId) {

        /**
         * 获取次数最少的那个节点,这里可以类比成先按counts升序排列然后取第一个元素
         * 然后是自己就返回true
         */
        Optional&lt;Node&gt; min = nodes.stream().min((o1, o2) -&gt; o1.getCounts().compareTo(o2.getCounts()));
        
        return min.isPresent()? min.get().getNodeId() == myNodeId : false;
    }
    
}
<br>/**
 * 按权重的分配策略,方案如下，假如
 * 节点序号   1     ,2     ,3       ,4
 * 节点权重   2     ,3     ,3       ,2
 * 则取余后 0,1 | 2,3,4 | 5,6,7 | 8,9
 * 序号1可以消费按照权重的和取余后小于2的
 * 序号2可以消费按照权重的和取余后大于等于2小于2+3的
 * 序号3可以消费按照权重的和取余后大于等于2+3小于2+3+3的
 * 序号3可以消费按照权重的和取余后大于等于2+3+3小于2+3+3+2的
 * 总结：本节点可以消费的按照权重的和取余后大于等于前面节点的权重和小于包括自己的权重和的这个范围
 * 不知道有没有大神有更好的算法思路
 * @author rongdi
 * @date 2019-03-16 23:16
 */
public class WeightStrategy implements Strategy {

    @Override
    public boolean accept(List&lt;Node&gt; nodes, Task task, Long myNodeId) {
        Node myNode = nodes.stream().filter(node -&gt; node.getNodeId() == myNodeId).findFirst().get();
        if(myNode == null) {
            return false;
        }
        /**
         * 计算本节点序号前面的节点的权重和
         */
        int preWeightSum = nodes.stream().filter(node -&gt; node.getRownum() &lt; myNode.getRownum()).collect(Collectors.summingInt(Node::getWeight));
        /**
         * 计算全部权重的和
         */
        int weightSum = nodes.stream().collect(Collectors.summingInt(Node::getWeight));
        /**
         * 计算对权重和取余的余数
         */
        int remainder = (int)(task.getId() % weightSum);
        return remainder &gt;= preWeightSum &amp;&amp; remainder &lt; preWeightSum + myNode.getWeight();
    }
    
}
<br>然后我们再改造下调度类:<br>/**
     * 获取任务的策略
     */
    private Strategy strategy;


    @PostConstruct
    public void init() {
        /**
         * 根据配置选择一个节点获取任务的策略
         */
        strategy = Strategy.choose(config.getNodeStrategy());
        /**
         * 自定义线程池，初始线程数量corePoolSize，线程池等待队列大小queueSize，当初始线程都有任务，并且等待队列满后
         * 线程数量会自动扩充最大线程数maxSize，当新扩充的线程空闲60s后自动回收.自定义线程池是因为Executors那几个线程工具
         * 各有各的弊端，不适合生产使用
         */
        workerPool = new ThreadPoolExecutor(config.getCorePoolSize(), config.getMaxPoolSize(), 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(config.getQueueSize()));
        /**
         * 执行待处理任务加载线程
         */
        bossPool.execute(new Loader());
        /**
         * 执行任务调度线程
         */
        bossPool.execute(new Boss());
    
    }

    class Loader implements Runnable {

        @Override
        public void run() {
            for(;;) {
                try { 
                    /**
                     * 先获取可用的节点列表
                     */
                    List&lt;Node&gt; nodes = nodeRepository.getEnableNodes(config.getHeartBeatSeconds() * 2);
                    if(nodes == null || nodes.isEmpty()) {
                        continue;
                    }
                    /**
                     * 查找还有指定时间(单位秒)开始的主任务列表
                     */
                    List&lt;Task&gt; tasks = taskRepository.listPeddingTasks(config.getFetchDuration());
                    if(tasks == null || tasks.isEmpty()) {
                        continue;
                    }
                    for(Task task:tasks) {
                        
                        boolean accept = strategy.accept(nodes, task, config.getNodeId());
                        /**
                         * 不该自己拿就不要抢
                         */
                        if(!accept) {
                            continue;
                        }
                        task.setStatus(TaskStatus.DOING);
                        task.setNodeId(config.getNodeId());
                        /**
                         * 使用乐观锁尝试更新状态，如果更新成功，其他节点就不会更新成功。如果在查询待执行任务列表
                         * 和当前这段时间有节点已经更新了这个任务，version必然和查出来时候的version不一样了,这里更新
                         * 必然会返回0了
                         */
                        int n = taskRepository.updateWithVersion(task);
                        Date nextStartTime = task.getNextStartTime();
                        if(n == 0 || nextStartTime == null) {
                            continue;
                        }
                        /**
                         * 封装成延时对象放入延时队列
                         */
                        task = taskRepository.get(task.getId());
                        DelayItem&lt;Task&gt; delayItem = new DelayItem&lt;Task&gt;(nextStartTime.getTime() - new Date().getTime(), task);
                        taskQueue.offer(delayItem);
                        
                    }
                    Thread.sleep(config.getFetchPeriod());
                } catch(Exception e) {
                    logger.error("fetch task list failed,cause by:{}", e);
                }
            }
        }
        
    }
<br>如上可以通过各种花式的负载策略来平衡各个节点获取的任务，同时也可以显著降低各个节点对同一个任务的竞争。但是还有个问题，假如某个节点拿到了任务更新成了执行中，执行到一半，没执行完也没发生异常，突然这个节点由于各种原因挂了，那么这时候这个任务永远没有机会再执行了。这就是传说中的占着茅坑不拉屎。解决这个问题可以用最终一致系统常见的方法，异常恢复线程。在这种场景下只需要检测一下指定心跳超时时间（比如默认3个心跳周期）下没有更新心跳时间的节点所属的未完成任务，将这些任务状态重新恢复成待执行，并且下次执行时间改成当前就可以了。核心代码如下:<br>class Recover implements Runnable {
        @Override
        public void run() {
            for (;;) {
                try {
                    /**
                     * 查找需要恢复的任务,这里界定需要恢复的任务是任务还没完成，并且所属执行节点超过3个
                     * 心跳周期没有更新心跳时间。由于这些任务由于当时执行节点没有来得及执行完就挂了，所以
                     * 只需要把状态再改回待执行，并且下次执行时间改成当前时间，让任务再次被调度一次
                     */
                    List&lt;Task&gt; tasks = taskRepository.listRecoverTasks(config.getHeartBeatSeconds() * 3);
                    if(tasks == null || tasks.isEmpty()) {
                        return;
                    }
                   /**
                    * 先获取可用的节点列表
                    */
                   List&lt;Node&gt; nodes = nodeRepository.getEnableNodes(config.getHeartBeatSeconds() * 2);
                   if(nodes == null || nodes.isEmpty()) {
                       return;
                   }
                   long maxNodeId = nodes.get(nodes.size() - 1).getNodeId();
                    for (Task task : tasks) {
                        /**
                         * 每个节点有一个恢复线程，为了避免不必要的竞争,从可用节点找到一个最靠近任务所属节点的节点
                         */
                        long currNodeId = chooseNodeId(nodes,maxNodeId,task.getNodeId());
                        long myNodeId = config.getNodeId();
                        /**
                         * 如果不该当前节点处理直接跳过
                         */
                        if(currNodeId != myNodeId) {
                            continue;
                        }
                        /**
                         * 直接将任务状态改成待执行，并且节点改成当前节点
                         */
                        task.setStatus(TaskStatus.PENDING);
                        task.setNextStartTime(new Date());
                        task.setNodeId(config.getNodeId());
                        taskRepository.updateWithVersion(task);
                    }
                    Thread.sleep(config.getRecoverSeconds() * 1000);
                } catch (Exception e) {
                    logger.error("Get next task failed,cause by:{}", e);
                }
            }
        }

    }
　　/**
     * 选择下一个节点
     * @param nodes
     * @param maxNodeId
     * @param nodeId
     * @return
     */
    private long chooseNodeId(List&lt;Node&gt; nodes,long maxNodeId,long nodeId) {
        if(nodes.size() == 0 || nodeId &gt;= maxNodeId) {
            return nodes.get(0).getNodeId();
        }
        return nodes.stream().filter(node -&gt; node.getNodeId() &gt; nodeId).findFirst().get().getNodeId();
    }
<br>如上为了避免每个节点的异常恢复线程对同一个任务做无谓的竞争，每个异常任务只能被任务所属节点ID的下一个正常节点去恢复。这样处理后就能确保就算出现了上面那种任务没执行完节点挂了的情况，一段时间后也可以自动恢复。总的来说上面那些不考虑优化应该可以做为一个还不错的任务调度框架了。如果你们以为这样就完了，我只能说抱歉了，还有，哈哈！前面提到我是嫌弃其它任务调度用起来麻烦，特别是习惯用spring的注解写调度的，那些很可能一个类里写了n个带有@Scheduled注解的调度方法，这样改造起来更加麻烦，我是希望做到如下方式就可以直接整合到分布式任务调度里：<br>/**
 * 测试调度功能
 * @author rongdi
 * @date 2019-03-17 16:54
 */
@Component
public class SchedulerTest {

    @Scheduled(cron = "0/10 * * * * ?")
    public void test1() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间1:"+sdf.format(new Date()));
    }
    
    @Scheduled(cron = "0/20 * * * * ?",parent = "test1")
    public void test2() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间2:"+sdf.format(new Date()));
    }

    @Scheduled(cron = "0/10 * * * * ?",parent = "test2")
    public void test3() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间3:"+sdf.format(new Date()));
    }

    @Scheduled(cron = "0/10 * * * * ?",parent = "test3")
    public void test4() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间4:"+sdf.format(new Date()));
    }

}
<br>为了达到上述目标，我们还需要在spring启动后加载自定义的注解（名称和spring的一样），代码如下:<br>/**
 * spring容器启动完后，加载自定义注解
 * @author rongdi
 * @date 2019-03-15 21:07
 */
@Component
public class ContextRefreshedListener implements ApplicationListener&lt;ContextRefreshedEvent&gt; {

    @Autowired
    private TaskExecutor taskExecutor;

    /**
     * 用来保存方法名/任务名和任务插入后数据库的ID的映射,用来处理子任务新增用
     */
    private Map&lt;String,Long&gt; taskIdMap = new HashMap&lt;&gt;();

    @Override
    public void onApplicationEvent(ContextRefreshedEvent event) {
        /**
         * 判断根容器为Spring容器，防止出现调用两次的情况（mvc加载也会触发一次）
          */
        if(event.getApplicationContext().getParent()==null){
            /**
             * 判断调度开关是否打开
             * 如果打开了：加载调度注解并将调度添加到调度管理中
             */
            ApplicationContext context = event.getApplicationContext();
            Map&lt;String,Object&gt; beans = context.getBeansWithAnnotation(org.springframework.scheduling.annotation.EnableScheduling.class);
            if(beans == null) {
                return;
            }
            /**
             * 用来存放被调度注解修饰的方法名和Method的映射
             */
            Map&lt;String,Method&gt; methodMap = new HashMap&lt;&gt;();
            /**
             * 查找所有直接或者间接被Component注解修饰的类，因为不管Service，Controller等都包含了Component，也就是
             * 只要是被纳入了spring容器管理的类必然直接或者间接的被Component修饰
             */
            Map&lt;String,Object&gt; allBeans = context.getBeansWithAnnotation(org.springframework.stereotype.Component.class);
            Set&lt;Map.Entry&lt;String,Object&gt;&gt; entrys = allBeans.entrySet();
            /**
             * 遍历bean和里面的method找到被Scheduled注解修饰的方法,然后将任务放入任务调度里
             */
            for(Map.Entry entry:entrys){
                Object obj = entry.getValue();
                Class clazz = obj.getClass();
                Method[] methods = clazz.getMethods();
                for(Method m:methods) {
                    if(m.isAnnotationPresent(Scheduled.class)) {
                        methodMap.put(clazz.getName() + Delimiters.DOT + m.getName(),m);
                    }
                }
            }
            /**
             * 处理Sheduled注解
             */
            handleSheduledAnn(methodMap);
            /**
             * 由于taskIdMap只是启动spring完成后使用一次，这里可以直接清空
             */
            taskIdMap.clear();
        }
    }

    /**
     * 循环处理方法map中的所有Method
     * @param methodMap
     */
    private void handleSheduledAnn(Map&lt;String,Method&gt; methodMap) {
        if(methodMap == null || methodMap.isEmpty()) {
            return;
        }
        Set&lt;Map.Entry&lt;String,Method&gt;&gt; entrys = methodMap.entrySet();
        /**
         * 遍历bean和里面的method找到被Scheduled注解修饰的方法,然后将任务放入任务调度里
         */
        for(Map.Entry&lt;String,Method&gt; entry:entrys){
            Method m = entry.getValue();
            try {
                handleSheduledAnn(methodMap,m);
            } catch (Exception e) {
                e.printStackTrace();
                continue;
            }
        }
    }

    /**
     * 递归添加父子任务
     * @param methodMap
     * @param m
     * @throws Exception
     */
    private void handleSheduledAnn(Map&lt;String,Method&gt; methodMap,Method m) throws Exception {
        Class&lt;?&gt; clazz = m.getDeclaringClass();
        String name = m.getName();
        Scheduled sAnn = m.getAnnotation(Scheduled.class);
        String cron = sAnn.cron();
        String parent = sAnn.parent();
        /**
         * 如果parent为空，说明该方法代表的任务是根任务，则添加到任务调度器中，并且保存在全局map中
         * 如果parent不为空，则表示是子任务，子任务需要知道父任务的id
         * 先根据parent里面代表的方法全名或者方法名（父任务方法和子任务方法在同一个类直接可以用方法名，
         * 不然要带上类的全名）从taskIdMap获取父任务ID
         * 如果找不到父任务ID，先根据父方法全名在methodMap找到父任务的method对象，调用本方法递归下去
         * 如果找到父任务ID，则添加子任务
         */
        if(StringUtils.isEmpty(parent)) {
            if(!taskIdMap.containsKey(clazz.getName() + Delimiters.DOT + name)) {
                Long taskId = taskExecutor.addTask(name, cron, new Invocation(clazz, name, new Class[]{}, new Object[]{}));
                taskIdMap.put(clazz.getName() + Delimiters.DOT + name, taskId);
            }
        } else {
            String parentMethodName = parent.lastIndexOf(Delimiters.DOT) == -1 ? clazz.getName() + Delimiters.DOT + parent : parent;
            Long parentTaskId = taskIdMap.get(parentMethodName);
            if(parentTaskId == null) {
                Method parentMethod = methodMap.get(parentMethodName);
                handleSheduledAnn(methodMap,parentMethod);
                /**
                 * 递归回来一定要更新一下这个父任务ID
                 */
                parentTaskId = taskIdMap.get(parentMethodName);
            }
            if(parentTaskId != null &amp;&amp; !taskIdMap.containsKey(clazz.getName() + Delimiters.DOT + name)) {
                Long taskId = taskExecutor.addChildTask(parentTaskId, name, cron, new Invocation(clazz, name, new Class[]{}, new Object[]{}));
                taskIdMap.put(clazz.getName() + Delimiters.DOT + name, taskId);
            }

        }


    }
}
<br>上述代码就完成了spring初始化完成后加载了自己的自定义任务调度的注解，并且也受spring的调度开关@EnableScheduling的控制，实现无缝整合到spring或者springboot中去，达到了我这种的懒人的要求。<br>好了其实写这个框架差不多就用了5天业余时间，估计会有一些隐藏的坑，不过明显的坑我自己都解决了，开源出来的目的既是为了抛砖引玉，也为了广大屌丝程序员提供一种新的思路，希望对大家有所帮助，同时也希望大家多帮忙找找bug，一起来完善这个东西,大神们请忽略。文笔不好，主要是好久没写作文了，请大家多多担待。详细的流水账式的源码加上长篇大论式的汉语注释尽情查看:<a rel="noopener nofollow" class="external-link" href="https://github.com/rongdi/easy-job" target="_blank">https://github.com/rongdi/easy-job</a>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/分布式任务调度框架.html</link><guid isPermaLink="false">Computer Science/Distributed System/分布式任务调度框架/分布式任务调度框架.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:47:20 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/pasted-image-20240507114701.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/pasted-image-20240507114701.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Active Standby 双机热备]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a><br><img title="image.png" alt="image.png" src="https://ucc.alicdn.com/pic/developer-ecology/b0652d61627e4e29b4e373dd1b5073ae.png?x-oss-process=image/resize,s_500,m_lfit" referrerpolicy="no-referrer"><br>
​<br>
淘宝按照单元切分的异地多活架构<br>
​<br>
注意看图中的数据同步箭头。以交易单元为例，属于交易单元的业务数据，将与中心单元进行双向同步；不属于交易单元的业务数据，单向从中心单元同步。中心单元承担了最复杂的业务场景，业务单元承担了相对单一的场景。对于业务单元，可以进行弹性伸缩和容灾；对于中心单元，扩展能力较差，稳定性要求更高。可以遇见，大部分的故障都会出现在中心单元。<br>
​<br>
按照业务进行单元切分，已经需要对代码和架构进行彻底的改造了（可能这也是为什么阿里要先从双活再切到多活，历时3年）。比如，业务拆分，依赖拆分，网状改星状，分布式事务，缓存失效等。除了对于编码的要求很高以外，对测试和运维也有非常大的挑战。如此复杂的情况，如何进行自动化覆盖，如何进行演练，如何改造流水线。这种级别的灾备，不是一般公司敢做的，投入产出也不成正比。不过还是可以把这种场景当作我们的“假想敌”，去思考我们自己的业务，未来会怎么发展，需要做到什么级别的灾备。相对而言，饿了么的多活方案可能更适合大多数的企业。<br>
​<br>
本文只是通过画图的方式进行了简单的描述，其实异地多活是需要很多很强大的基础能力的。比如，数据传输，数据校验，数据操作层（简化客户端控制写和同步的过程）等。<br>
​<br><br>​<br>
文末，留几个问题大家可以思考一下：<br>
​<br>
<br>&nbsp; 假设你在做饿了么的开发，服务按照异地多活方式部署，sharding key根据省市区进行分片。假设买家在多个城市交汇的地方，比如，十字路口的四个位置分别是4个城市，那么如何处理才能让他拉到比较正常的数据？
<br>&nbsp; 你们现在的业务模块中，哪些业务是可以做多活的，哪些无法做多活？
<br>&nbsp; 所有的业务都要做多活吗？还是只需要核心业务做多活？<br>
​<br>
相关实践学习<br>
​<br>
SLB负载均衡实践<br>
​<br>
本场景通过使用阿里云负载均衡 SLB 以及对负载均衡 SLB 后端服务器 ECS 的权重进行修改，快速解决服务器响应速度慢的问题<br>
​<br>
负载均衡入门与产品使用指南<br>
​<br>
负载均衡（Server Load Balancer）是对多台云服务器进行流量分发的负载均衡服务，可以通过流量分发扩展应用系统对外的服务能力，通过消除单点故障提升应用系统的可用性。 本课程主要介绍负载均衡的相关技术以及阿里云负载均衡产品的使用方法。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/active-standby-双机热备.html</link><guid isPermaLink="false">Computer Science/Distributed System/Active Standby 双机热备.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:48:27 GMT</pubDate><enclosure url="https://ucc.alicdn.com/pic/developer-ecology/b0652d61627e4e29b4e373dd1b5073ae.png?x-oss-process=image/resize,s_500,m_lfit" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://ucc.alicdn.com/pic/developer-ecology/b0652d61627e4e29b4e373dd1b5073ae.png?x-oss-process=image/resize,s_500,m_lfit"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Building Scalable Distributed Systems - Distributed System Architecture Blueprint - A Whirlwind Tour]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:scala" class="tag" target="_blank" rel="noopener nofollow">#scala</a> <br>In this article, we’ll introduce some of the fundamental approaches to scaling a software system. The type of systems this series of articles is oriented towards are the Internet-facing systems we all utilize every day. I’ll let you name your favorite. These systems accept requests from users through Web and mobile interfaces, store and retrieve data based on user requests or events (e.g. a GPS-based system), and have some intelligent features such as providing recommendations or providing notifications based on previous user interactions.<br>We’ll start with a simple system design and show how it can be scaled. In the process, several concepts will be introduced that we’ll cover in much more detail later in this series. Hence this article gives a broad overview of these concepts and how they aid in scalability — truly a whirlwind tour! If you’ve missed Part 1 in this series, <a data-tooltip-position="top" aria-label="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" rel="noopener nofollow" class="external-link" href="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" target="_blank">here it is</a>!<br><br>Virtually all massive-scale systems start off small and grow due to their success. It’s common, and sensible, to start with a development framework such as Ruby on Rails or Django or equivalent that promotes rapid development to get a system quickly up and running. A typical, very simple software architecture for ‘starter’ systems that closely resembles what you get with rapid development frameworks is shown in Figure 1. This comprises a client tier, application service tier, and a database tier. If you use Rails or equivalent, you also get a framework that hardwires a Model-View-Controller (MVC) pattern for Web application processing and an Object-Relational Mapper (ORM) that generates SQL queries.<br>With this architecture, users submit requests to the application from their mobile app or Web browser across the Internet. The magic of Internet networking delivers these requests to the application service which is running on a machine hosted in some corporate or commercial cloud data center. Communications uses a standard network protocol, typically HTTP.<br>The application service runs code that supports an application programming interface (API) that clients use to format data and send HTTP requests to. Upon receipt of a request, the service executes the code associated with the requested API. In the process, it may read from or write to a database, depending on the semantics of the API. When the request is complete, the server sends the results to the client to display in their app or browser.<br><img alt="Basic system architecture" src="https://miro.medium.com/v2/resize:fit:584/1*MTjAf75AZDXNDLO7iWgsxQ.png" referrerpolicy="no-referrer"><br>Figure 1 Basic Multi-Tier Distributed Systems Architecture<br>Many systems conceptually look exactly like this. The application service code exploits an execution environment that enables multiple requests from multiple users to be processed simultaneously. There’s a myriad of these application server technologies — JEE and Spring for Java, <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Flask_(web_framework)" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Flask_(web_framework)" target="_blank">Flask for Python</a> – that are widely used in this scenario.<br>This approach leads to what is generally known as a monolithic architecture. Single, monolithic services grow in complexity as the application becomes more feature-rich. This eventually makes it hard to modify and test rapidly, and their execution footprint can become extremely heavyweight as all the API implementations run in the same application service.<br>Still, as long as request loads stay relatively low, this application architecture can suffice. The service has the capacity to process requests with consistently low latency. If request loads keep growing, this means eventually latencies will grow as the server has insufficient CPU/memory capacity for the concurrent request volume and hence requests will take longer to process. In these circumstances, our single server is overloaded and has become a bottleneck.<br>In this case, the first strategy for scaling is usually to ‘scale up’ the application service hardware. For example, if your application is running on AWS, you might upgrade your server from a modest t3.xlarge instance with 4 (virtual) CPUs and 16GBs of memory to a t3.2xlarge instance <a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">which doubles the number of vCPUs and memory available for the application</a>.<br>Scale up is simple. It gets many real-world applications a long way to supporting larger workloads. It obviously just costs more money for hardware, but that’s scaling for you.<br>It’s inevitable however that for many applications the load will grow to a level that will swamp a single server node, no matter how many CPUs and how much memory you have. That’s when you need a new strategy — namely scaling out, or horizontal scaling, that we touched on <a data-tooltip-position="top" aria-label="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" rel="noopener nofollow" class="external-link" href="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" target="_blank">in the first article in this series</a>.<br><br>Scaling out relies on the ability to replicate a service in the architecture and run multiple copies on multiple server nodes. Requests from clients are distributed across the replicas so that in theory, if we have N replicas, each server node processes {#requests/N} requests. This simple strategy increases an application’s capacity and hence scalability.<br>To successfully scale out an application, we need two fundamental elements in our design. As illustrated in Figure 2, these are:<br>A Load balancer: All user requests are sent to a load balancer, which chooses a service replica to process the request. Various strategies exist for choosing a service, all with the core aim of keeping each processing resource equally busy. The load balancer also relays the responses from the service back to the client. Most load balancers belong to a class of Internet components known as <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Reverse_proxy" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Reverse_proxy" target="_blank">reverse proxies</a>, which control access to server resources for client requests. As an intermediary, reverse proxies add an extra network hop for a request, and hence need to be extremely low latency to minimize the overheads they introduce. There are many off-the-shelf load balancing solutions as well as cloud-provider specific ones, and we’ll cover the general characteristics of these in much more detail in a later article.<br>Stateless services: For load balancing to be effective and share requests evenly, the load balancer must be free to send consecutive requests from the same client to different service instances for processing. This means the API implementations in the services must retain no knowledge, or state, associated with an individual client’s session. When a user accesses an application, a user session is created by the service, and a unique session identified is managed internally to identify the sequence of user interactions and track session state. A classic example of session state is a shopping cart. To use a load balancer effectively, the data representing the current contents of a user’s cart must be stored somewhere — typically a data store — such that any service replica can access this state when it receives a request as part of a user session. In Figure 2 this is labeled as a Session Store.<br><img alt="Scaling out the service tier with a load balancer" src="https://miro.medium.com/v2/resize:fit:802/1*ZoDFOdsaG-hVdRegLsoR4w.png" referrerpolicy="no-referrer"><br>Figure 2 Scale out Architecture<br>Scale out is attractive as, in theory, you can keep adding new (virtual) hardware and services to handle increased user request loads and keep latencies consistent and low. As soon as you see latencies rising, you deploy another service instance. This requires no code changes and hence is relatively cheap — you just pay for the hardware you deploy.<br>Scale out has another highly attractive feature. If one of the services fails, the requests it is processing will be lost. But as the failed service manages no session state, these requests can be simply reissued by the client and sent to another service instance for processing. This means the application is resilient to failures in the application service software and hardware, thus enhancing the application’s availability. Availability is a key feature of distributed systems and one we will discuss in-depth in a later article too.<br>Unfortunately, as with any engineering solution, simple scaling out has limits. As you add new service instances, the request processing capacity grows, potentially infinitely. At some stage, however, reality will bite and the capability of your single database to provide low latency query responses will diminish. Slow queries will mean longer response times for clients. If requests keep arriving faster than they are being processed, some system components will fail as resources will be exhausted and clients will see exceptions and request timeouts. Essentially your database has become a bottleneck that you must engineer away in order to scale your application further.<br><br>Scaling up by increasing the number of CPUs, memory and disks in a database server can go a long way to increasing a system’s capacity. For example, at the time of writing Google Cloud Platform can provision a SQL database on a db-n1-highmem-96 node, which has 96 vCPUs, 624GB of memory, 30TBs of disk and can support 4000 connections. This will cost somewhere between $6K and $16K per year, which sounds a good deal to me. Scaling up is a very common database scalability strategy.<br>Large databases need constant care and attention from highly skilled database administrators to keep them tuned and running fast. There’s a lot of wizardry in this job — e.g. query tuning, disk partitioning, indexing, on-node caching — and hence database administrators are valuable people that you want to be very nice to. They can make your application services highly responsive indeed.<br>In conjunction with scale up, a highly effective approach is to query the database as infrequently as possible in your services. This can be achieved by employing distributed caching in the service tier. A cache stores recently retrieved and commonly accessed database results in memory so they can be quickly retrieved without placing a burden on the database. For data that is frequently read and changes rarely, your processing logic must be modified to first check a distributed cache, such as a <a data-tooltip-position="top" aria-label="https://redis.io/" rel="noopener nofollow" class="external-link" href="https://redis.io/" target="_blank">Redis</a> or <a data-tooltip-position="top" aria-label="https://memcached.org/" rel="noopener nofollow" class="external-link" href="https://memcached.org/" target="_blank">memcached</a> store. These cache technologies are essentially distributed Key-Value stores with very simple APIs. This scheme is illustrated in Figure 3. Note that the Session Store from Figure 2 has disappeared. This is because we can use a general-purpose distributed cache to store session identifiers along with application data.<br>Accessing the cache requires a remote call from your service, but if the data you need is in the cache, on a fast network this is far less expensive than querying the database instance. Introducing a caching layer also requires your processing logic to be modified to check for cached data. If what you want is not in the cache — a cache miss — your code must still query the database and load the results into the cache as well as return the results to the caller. You also need to decide when to remove or invalidate cached results — this depends on your application’s tolerance to serving stale results to clients and the volume of data you cache.<br><img alt="Introducing a distributed cache" src="https://miro.medium.com/v2/resize:fit:762/1*7KFWeMMfq814m5pEnp21_Q.png" referrerpolicy="no-referrer"><br>Figure 3 Introducing Distributed Caching<br>A well-designed caching scheme can be absolutely invaluable in scaling a system. Caching works great for data that rarely changes and is accessed frequently, such as inventory, event and contact data. If you can handle a large percentage, like 80% or more, of read requests from your cache, then you effectively buy extra capacity at your databases as they are not involved in handling requests.<br>Still, many systems need to rapidly access many terabytes and larger data stores, which make a single database effectively prohibitive. In these systems, a distributed database is needed.<br><br>There are more distributed database technologies around in 2020 than you probably want to imagine. It’s a complex area, and one we’ll cover extensively in another article. In very general terms, there are two major categories:<br>
<br>Distributed SQL stores from major vendors such as Oracle and IBM. These enable organizations to scale out their SQL database relatively seamlessly by storing the data across multiple disks that are queried by multiple database engine replicas. These multiple engines logically appear to the application as a single database, hence minimizing code changes.
<br>Distributed so-called NoSQL stores from a whole array of vendors. These products use a variety of data models and query languages. They distribute data across multiple nodes that run the database engine, each with their own locally attached storage. Again, the location of the data is transparent to the application and typically controlled by the design of the data model through hashing functions on database keys. Leading products in this category are Cassandra, MongoDB and Neo4j.<br>
<img alt="Figure 4 Scaling the Data Tier using a Distributed Database" src="https://miro.medium.com/v2/resize:fit:922/1*i34jGfA1SXllJoszNSz6Jw.png" referrerpolicy="no-referrer">
<br>Figure 4 Scaling the Data Tier using a Distributed Database<br>Figure 4 shows how our architecture incorporates a distributed database. As the data volumes grow, a distributed database has features to enable the number of storage nodes to be increased. As nodes are added (or removed), the data managed across all nodes is rebalanced to attempt to ensure the processing and storage capacity of each node is equally utilized.<br>Distributed databases also promote availability. They support replicating each data storage node so if one node fails or cannot be accessed due to network problems, another copy of the data is available. The models utilized for replication and the trade-offs these require (spoiler — consistency) are fodder for another article.<br>If you are utilizing a major cloud provider, there are also two deployment choices for your data tier. You can deploy your own virtual resources and build, configure, and administer your own distributed database servers. Alternatively, you can utilize cloud-hosted databases such as DynamoDB. The latter category simplifies the administrative effort associated with managing, monitoring, and scaling the database, as many of these tasks essentially become the responsibility of the cloud provider you choose. As usual, the no free lunch principle applies.<br><br>Any realistic system that we need to scale will have many different services that interact to process a request. For example, accessing a Web page on the Amazon.com site can require in excess of <a data-tooltip-position="top" aria-label="https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html" rel="noopener nofollow" class="external-link" href="https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html" target="_blank">100 different services being called</a> before an aggregate response is returned to the user.<br>The beauty of the stateless, load-balanced, cached architecture we are elaborating in this article is that we can extend the core design principles and build a multi-tiered, multi-service application. In fulfilling a request, a service can call one or more dependent services, which in turn are replicated and load-balanced. A simple example is shown in Figure 5. There are many nuances in how the services interact, and <a data-tooltip-position="top" aria-label="https://medium.com/@i.gorton/six-rules-of-thumb-for-scaling-software-architectures-a831960414f9" rel="noopener nofollow" class="external-link" href="https://medium.com/@i.gorton/six-rules-of-thumb-for-scaling-software-architectures-a831960414f9" target="_blank">how applications ensure rapid responses from dependent services</a>. Again, we’ll cover these in detail in later articles.<br><img alt="Figure 5 Scaling the Processing Capacity across multiple tiers" src="https://miro.medium.com/v2/resize:fit:970/1*5QvIxJtIO5B_bxbyPtmqNg.png" referrerpolicy="no-referrer"><br>Figure 5 Scaling the Processing Capacity across multiple tiers<br>This design also promotes having different, load-balanced services at each tier in the architecture. For example, Figure 6 illustrates two replicated Internet-facing services that both utilized a core service providing database access. Each service is load balanced and employs caching to provide high performance and availability. This design is often used, for example, to provide a service for Web clients and a service for mobile clients, each of which can be scaled independently based on the load they experience. Its commonly called the <a data-tooltip-position="top" aria-label="https://samnewman.io/patterns/architectural/bff/" rel="noopener nofollow" class="external-link" href="https://samnewman.io/patterns/architectural/bff/" target="_blank">Backend For Frontend (BFF) pattern</a>.<br><img alt="Figure 6 Scalable Architecture with Multiple Services" src="https://miro.medium.com/v2/resize:fit:1114/1*wGBhGR8D9ZuWutOeYiMJvQ.png" referrerpolicy="no-referrer"><br>Figure 6 Scalable Architecture with Multiple Services<br>In addition, by breaking the application into multiple independent services, we can scale each based on the service demand. If for example, we see an increasing volume of requests from mobile users and decreasing volumes from Web users, we can provision different numbers of instances for each service to satisfy demand. This is a major advantage of refactoring monolithic applications into multiple independent services, which can be separately built, tested, deployed, and scaled.<br><br>Most client application requests expect a response. A user might want to see all auction items for a given product category or see the real estate that is available for sale in a given location. In these examples, the client sends a request and waits until a response is received. The time interval between sending the request and receiving the result is the latency of the request. We can decrease latencies by using caching and precalculated responses, but many requests will still result in database access.<br>A similar scenario exists for requests that update data in an application. If a user updates their delivery address immediately prior to placing an order, the new delivery address must be persisted so that the user can confirm the address before they hit the ‘purchase’ button. This is known as ‘read your own writes’. The latency, in this case, includes the time for the database write, which is confirmed by the response the user receives.<br>Some update requests however can be successfully responded to without fully persisting the data in a database. For example, the skiers and snowboarders amongst you will be familiar with lift ticket scanning systems that check you have a valid pass to ride the lifts that day. They also record which lifts you take, the time you get on, and so on. Nerdy skier/snowboarders can then use the resort’s mobile app to see how many lifts they ride in a day.<br>As a person waits to get on a lift, a scanner device validates the pass using an RFID chip reader. The information about the rider, lift, and time is then sent over the Internet to a data capture service operated by the ski resort. The lift rider doesn’t have to wait for this update to occur, as the latency could slow down the lift loading process. There’s also no expectation from the lift rider that they can instantly use their app to ensure this data has been captured. They just get on the lift, talk smack with their friends, and plan their next run.<br>Service implementations can exploit this type of scenario to reduce latencies and improve responsiveness. The data about the event is sent to the service, which acknowledges receipt and concurrently stores the data in a remote queue for subsequent writing to the database. Writing a message to a queue is much faster than writing to a database, and this enables the request to be successfully acknowledged much more quickly. Another backend service is deployed to read messages from the queue and write the data to the database. When the user checks their lift rides — maybe 3 hours or 3 days later — the data has been persisted successfully.<br>The basic architecture to implement this approach is illustrated in Figure 7.<br><img alt="Figure 7 Increasing Responsiveness with Queueing" src="https://miro.medium.com/v2/resize:fit:1054/1*Te6RjrHDEQ3scq_kVbpwEQ.png" referrerpolicy="no-referrer"><br>Figure 7 Increasing Responsiveness with Queuing<br>Whenever the results of a write operation are not immediately needed, an application can use this approach to improve responsiveness and hence scalability. Many queueing technologies exist that applications can utilize, and we’ll discuss how these operate in later articles. These queueing platforms all provide asynchronous communications. A producer service writes to the queue, which acts as temporary storage, while another consumer service removes messages from the queue and makes the necessary updates to, in our example, a database that stores skier lift ride details.<br>The key is that the data eventually gets persisted. Eventually typically means a few seconds at most, but use cases that employ this design should be resilient to longer delays without impacting the user experience.<br><br>This chapter has provided a whirlwind tour of the major approaches we can utilize to scale out an Internet-facing system as a collection of communicating services and distributed databases. Much detail has been brushed over, and as you all no doubt know, in software systems the devil is in the detail.<br>Another area this chapter has skirted around is the subject of software architecture. We’ve used the term services for distributed components in an architecture that implement application business logic and database access. These services are independently deployed processes that communicate using remote communications mechanisms such as HTTP. In architectural terms, these services are most closely mirrored by those in the Service Oriented Architecture (SOA) pattern, an established architectural approach for building distributed systems. A more modern evolution of this approach revolves around microservices. These tend to be more cohesive, encapsulated services that promote continuous development and deployment.<br>If you’d like a much more in-depth discussion of these, and software architecture issues in general, then Mark Richards’ and Neal Ford’s book is an excellent place to start.<br>Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering Approach 1st Edition, O’Reilly Media, 2020<br>Finally, there’s a class of big data software architectures that address some of the issues that rise to the fore with very large data collections. One of the most prominent is data reprocessing. This occurs when raw data that has already been stored and analyzed needs to be re-analyzed due to code changes. This reprocessing may occur due to software fixes, or the introduction of new algorithms that can derive more insights from the original raw data. There’s a good discussion of the Lambda and Kappa architectures, both of which are prominent in this space, in this article.<br>Jay Krepps, <a data-tooltip-position="top" aria-label="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" rel="noopener nofollow" class="external-link" href="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" target="_blank">Questioning the Lambda Architecture</a>,]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/building-scalable-distributed-systems-distributed-system-architecture-blueprint-a-whirlwind-tour.html</link><guid isPermaLink="false">Computer Science/Distributed System/Building Scalable Distributed Systems - Distributed System Architecture Blueprint - A Whirlwind Tour.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:48:25 GMT</pubDate><enclosure url="https://miro.medium.com/v2/resize:fit:584/1*MTjAf75AZDXNDLO7iWgsxQ.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:584/1*MTjAf75AZDXNDLO7iWgsxQ.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Building Scalable Distributed Systems - Introduction to Scalable Systems]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:scala" class="tag" target="_blank" rel="noopener nofollow">#scala</a><br>The last 20 years have seen unprecedented growth in the size, complexity, and capacity of software systems. This rate of growth is hardly likely to slow down in the next 20 years — what these future systems will look like is close to unimaginable right now. The one thing we can guarantee is that more and more software systems will need to be built with constant growth — more requests, more data, more analysis — as a primary design driver.<br>Scalable is the term used in software engineering to describe software systems that can accommodate growth. In this first part of the series, we will explore what precisely is meant by the ability to scale — known, not surprisingly, as scalability. We’ll also describe a few examples that put hard numbers on the capabilities and characteristics of contemporary applications and give a brief history of the origins of the massive systems we routinely build today. Finally, we will describe two general principles for achieving scalability that will recur in various forms throughout the rest of this series of articles and examine the indelible link between scalability and cost.<br><br>Intuitively, scalability is a pretty straightforward concept. If we ask Wikipedia for a definition, it tells us “scalability is the property of a system to handle a growing amount of work by adding resources to the system”. We all know how we scale a highway system — we add more traffic lanes so it can handle a greater number of vehicles. Some of my favorite people know how to scale beer production — they add more capacity in terms of the number and size of brewing vessels, the number of staff to perform and manage the brewing process, and the number of kegs they can fill with tasty fresh brews. Think of any physical system — a transit system, an airport, elevators in a building — and how we increase capacity is pretty obvious.<br>Unlike physical systems, software is somewhat amorphous. It is not something you can point at, see, touch, feel, and get a sense of how it behaves internally from external observation. It’s a digital artifact. At its core, the stream of 1’s and 0’s that make up executable code and data are hard for anyone to tell apart. So, what does scalability mean in terms of a software system?<br>Put very simply, and without getting into definition wars, scalability defines a software system’s capability to handle growth in some dimension of its operations. Examples of operational dimensions are:<br>
<br>the number of simultaneous user or external (e.g. sensor) requests a system can process
<br>the amount of data a system can effectively process and manage
<br>the value that can be derived from the data a system stores
<br>For example, imagine a major supermarket chain is rapidly opening new stores and increasing the number of self-checkout kiosks in every store. This requires the core supermarket software systems to:<br>
<br>Handle increased volume from item sale scanning without decreased response time. Instantaneous responses to item scans are necessary to keep customers happy.
<br>Process and store the greater data volumes generated from increased sales. This data is needed for inventory management, accounting, planning and likely many other functions.
<br>Derive ‘real-time’ (e.g. hourly) sales data summaries from each store, region and country and compare to historical trends. This trend data can help highlight unusual events in regions (e.g. unexpected weather conditions, large crowds at events, etc.) and help the stores affected quickly respond.
<br>Evolve the stock ordering prediction subsystem to be able to correctly anticipate sales (and hence the need for stock reordering) as the number of stores and customers grow
<br>These dimensions are effectively the scalability requirements of a system. If over a year, the supermarket chain opens 100 new stores and grows sales by 400 times (some of the new stores are big), then the software system needs to scale to provide the necessary processing capacity to enable the supermarket to operate efficiently. If the systems don’t scale, we could lose sales as customers are unhappy. We might hold stock that will not be sold quickly, increasing costs. We might miss opportunities to increase sales by responding to local circumstances with special offerings. All these reduce customer satisfaction and profits. None are good for business.<br>Successfully scaling is therefore crucial for our imaginary supermarket’s business growth, and is in fact the lifeblood of many modern internet applications. But for most business and Government systems, scalability is not a primary quality requirement in the early stages of development and deployment. New features to enhance usability and utility become the drivers of our development cycles. As long as performance is adequate under normal loads, we keep adding user-facing features to enhance the system’s business value.<br>Still, it’s not uncommon for systems to evolve into a state where enhanced performance and scalability become a matter of urgency or even survival. Attractive features and high utility breed success, which brings more requests to handle and more data to manage. This often heralds a tipping point, where design decisions that made sense under light loads are now suddenly technical debt. External trigger events often cause these tipping points — look in the March/April 2020 media at the many reports of Government Unemployment and supermarket online ordering sites crashing under demand caused by the coronavirus pandemic.<br>Increasing a systems’ capacity in some dimension by increasing resources is commonly called scaling up or scaling out — we’ll explore the difference between these later in this series. In addition, unlike physical systems, it is often equally important to be able to scale down the capacity of a system to reduce costs. The canonical example of this is Netflix, which has a predictable regional diurnal load that it needs to process. Simply, a lot more people are watching Netflix in any geographical region at 9 pm than are at 5 am. This enables Netflix to reduce its processing resources during times of lower load. This saves the cost of running the processing nodes that are used in the Amazon cloud, as well as societally worthy things such as reducing data center power consumption. Compare this to a highway. At night when few cars are on the road, we don’t retract lanes (except for repairs). The full road capacity is available for the few drivers to go as fast as they like.<br>There’s a lot more to consider about scalability in software systems, but let’s come back to these issues after examining the scale of some contemporary software systems circa 2020.<br><br>Looking ahead in this technology game is always fraught with danger. In 2008 I wrote [1]:<br>“While petabyte datasets and gigabit data streams are today’s frontiers for data-intensive applications, no doubt 10 years from now we’ll fondly reminisce about problems of this scale and be worrying about the difficulties that looming exascale applications are posing.”<br>Reasonable sentiments, it is true, but exascale? That’s almost commonplace in today’s world. Google reported multiple exabytes of <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=eNliOm9NtCM" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=eNliOm9NtCM" target="_blank">Gmail in 2014</a>, and by now, do all Google services manage a yottabyte or more? I don’t know. I’m not even sure I know what a yottabyte is! Google won’t tell us about their storage, but I wouldn’t bet against it. Similarly, how much data do Amazon store in the various AWS data stores for their clients. And how many requests does, say, DynamoDB process per second collectively, for all client applications supported. Think about these things for too long and your head will explode.<br>A great source of information that sometimes gives insights into contemporary operational scales are the major Internet company’s technical blogs. There are also Web sites analyzing Internet traffic that are highly illustrative of traffic volumes. Let’s take a couple of ‘point in time’ examples to illustrate a few things we do know today. Bear in mind these will look almost quaint in a year or four.<br>
<br>Facebook’s engineering blog describes <a data-tooltip-position="top" aria-label="https://engineering.fb.com/data-infrastructure/scribe/" rel="noopener nofollow" class="external-link" href="https://engineering.fb.com/data-infrastructure/scribe/" target="_blank">Scribe</a>, their solution for collecting, aggregating, and delivering petabytes of log data per hour, with low latency and high throughput. Facebook’s computing infrastructure comprises millions of machines, each of which generates log files that capture important events relating to system and application health. Processing these log files, for example, from a Web server, can give development teams insights into their application’s behavior and performance, and support fault finding. Scribe is a custom buffered queuing solution that can transport logs from servers at a rate of several terabytes per second and deliver them to downstream analysis and data warehousing systems. That, my friends, is a lot of data!
<br>You can see live Internet traffic for numerous services at <a data-tooltip-position="top" aria-label="http://www.internetlivestats.com" rel="noopener nofollow" class="external-link" href="http://www.internetlivestats.com" target="_blank">www.internetlivestats.com</a>. Dig around and you’ll find statistics like Google handles around 3.5 billion search requests a day, Instagram uploads about 65 million photos per day, and there is something like 1.7 billion web sites. It is a fun site with lots of information to amaze you. Note the data is not really ‘live’, just estimates based on statistical analyses of multiple data sources.
<br>In 2016 Google published a paper describing the <a data-tooltip-position="top" aria-label="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" rel="noopener nofollow" class="external-link" href="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" target="_blank">characteristics of their code base</a>. Amongst the many startling facts reported is: “The repository contains 86TBs of data, including approximately two billion lines of code in nine million unique source files.” Remember, this was 2016.
<br>Still, real, concrete data on the scale of the services provided by major Internet sites remain shrouded in commercial-in-confidence secrecy. Luckily, we can get some deep insights into the request and data volumes handled at Internet scale through the annual usage report from one tech company. You can browse their incredibly detailed <a data-tooltip-position="top" aria-label="https://www.pornhub.com/insights/2019-year-in-review" rel="noopener nofollow" class="external-link" href="https://www.pornhub.com/insights/2019-year-in-review" target="_blank">usage statistics here from 2019</a>. It’s a fascinating glimpse into the capabilities of massive scale systems. Beware though, this is Pornhub.com. The report is not for the squeamish. Here’s one PG-13 illustrative data point — they had 42 billion visits in 2019! I’ll let interested readers browse the data in the report to their heart's content. Some of the statistics will definitely make your eyes bulge!<br><br>I am sure many readers will have trouble believing there was civilized life without Internet search, YouTube, and social media. By coincidence, the day I type this sentence is the 15 year anniversary of the first video <a data-tooltip-position="top" aria-label="https://kogo.iheart.com/content/2020-04-23-youtube-celebrates-15th-anniversary-by-featuring-first-video-ever-posted/" rel="noopener nofollow" class="external-link" href="https://kogo.iheart.com/content/2020-04-23-youtube-celebrates-15th-anniversary-by-featuring-first-video-ever-posted/" target="_blank">being uploaded to YouTube</a>. Only 15 years. Yep, it is hard for even me to believe. There’s been a lot of wine under the bridge since then. I can’t remember how we survived!<br>So, let’s take a brief look back in time at how we arrived at the scale of today’s systems. This is from a personal perspective — one which started at college in 1981 when my class of 60 had access to a shared lab of 8 state-of-the-art so-called <a data-tooltip-position="top" aria-label="http://pcmuseum.tripod.com/comphis4.html" rel="noopener nofollow" class="external-link" href="http://pcmuseum.tripod.com/comphis4.html" target="_blank"><em></em></a>microcomputers. By today’s standards, micro they were not.<br><br>An age dominated by mainframe and minicomputers. These were basically timeshared multiuser systems where users interacted with the machines via ‘dumb’ terminals. Personal computers emerged in the early 1980s and developed throughout the decade to become useful business and (relatively) powerful development machines. They were rarely networked, however, especially early in the decade. The <a data-tooltip-position="top" aria-label="https://www.internetsociety.org/internet/history-internet/brief-history-internet/" rel="noopener nofollow" class="external-link" href="https://www.internetsociety.org/internet/history-internet/brief-history-internet/" target="_blank">first limited incarnation of the Internet emerged during this time</a>. By the end of the 1980s, development labs, universities, and increasingly businesses had email and access to exotic internet-based resources such as <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Usenet" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Usenet" target="_blank">Usenet discussion forums</a> — think of a relatively primitive and incredibly polite Reddit.<br><br>Personal computers and networking technology, both LANs and WANS, continued to improve dramatically through this period. This created an environment ripe for the creation of the World Wide Web (WWW) as we know it today. The catalyst was the HTTP/HTML technology that had been <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web" target="_blank">pioneered at CERN by Tim Berners-Lee</a> during the 1980s. In 1993 CERN made the WWW technology available on a royalty-free basis. And the rest is history — a platform for information sharing and money-making had been created. By 1995, the number of web sites was tiny, but the seeds of the future were planted with companies like Yahoo! in 1994 and Amazon and eBay in 1995<br><br>During this period, the number of web sites grew from <a data-tooltip-position="top" aria-label="https://www.nngroup.com/articles/100-million-websites/" rel="noopener nofollow" class="external-link" href="https://www.nngroup.com/articles/100-million-websites/" target="_blank">around 10,000 to 10 million</a>, a truly explosive growth period. Networking bandwidth and access also grew rapidly, with initially dial-up modems for home users (yep, dial-up) and then early broadband technologies becoming available.<br>This surge in users with Internet access heralded a profound change in how we had to think about building systems. Take for example a retail bank. Before providing online services, it was possible to accurately predict the loads the bank’s business systems would experience. You knew how many people worked in the bank and used the internal systems, how many terminals/PCs were connected to the bank’s networks, how many ATMs you had to support, and the number and nature of connections to other financial institutions. Armed with this knowledge, we could build systems that support say a maximum of 3000 concurrent users, safe in the knowledge that this number could not be exceeded. Growth would also be relatively slow, and probably most of the time (eg outside business hours) the load would be a lot less than the peak. This made our software design decisions and hardware provisioning a lot easier.<br>Now imagine our retail bank decides to let all customers have Internet banking access. And the bank has 5 million customers. What is our maximum load now? How will the load be dispersed during a business day? When are the peak periods? What happens if we run a limited-time promotion to try and sign up new customers? Suddenly our relatively simple and contained business systems environment is disrupted by the higher average and peak loads and unpredictability you see from Internet-based user populations.<br>During this period, companies like Amazon, eBay, Google, Yahoo! and the like were pioneering many of the design principles and early versions of advanced technologies for highly scalable systems. They had to, as their request loads and data volumes were growing exponentially.<br><br>The late 1990s and early 2000’s saw massive investments in, and technological innovations from so-called ‘dot com’ companies, all looking to provide innovative and valuable online businesses. Spending was huge, and not all investments were well-targeted. This led to a little event called the ‘<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Dot-com_bubble" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Dot-com_bubble" target="_blank">dot com crash</a>’ during 2000/2001. By 2002 the technology landscape was littered with failed investments — anyone remember Pets.Com? Nope. Me neither. About 50% of dot com’s disappeared during this period. Of those that survived, albeit, with much lower valuations, many have become the staples we all know and use today.<br>The number of Web sites grew from around 10 to 80 million during this period, and new service and business models emerged. In 2005, YouTube was launched. 2006 saw Facebook become available to the public. In the same year, Amazon Web Services, which had low key beginnings in 2004, relaunched with its S3 and EC2 services. The modern era of Internet-scale computing and cloud-hosted systems was born.<br><br>We now live in a world with nearly 2 billion web sites, of which about 20% are active. There are something like <a data-tooltip-position="top" aria-label="https://www.internetworldstats.com/stats.htm" rel="noopener nofollow" class="external-link" href="https://www.internetworldstats.com/stats.htm" target="_blank">4 billion Internet users</a>. Huge data centers operated by public cloud operators like AWS, GCP and Azure, along with a myriad of private data centers, for example <a data-tooltip-position="top" aria-label="https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-behind-twitter-scale.html" rel="noopener nofollow" class="external-link" href="https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-behind-twitter-scale.html" target="_blank">Twitter’s operational infrastructure</a>, are scattered around the planet. Clouds host millions of applications, with engineers provisioning and operating their computational and data storage systems using sophisticated cloud management portals. Powerful, feature-rich cloud services make it possible for us to build, deploy, and scale our systems literally with a few clicks of a mouse. All you must do is pay your cloud provider bill at the end of the month.<br>This is the world that this series of articles targets. A world where our applications need to exploit the key principles for building scalable systems and leverage highly scalable infrastructure platforms. Bear in mind, in modern applications, most of the code executed is not written by your organization. It is part of the containers, databases, messaging systems, and other components that you compose into your application through API calls and build directives. This makes the selection and use of these components at least as important as the design and development of your own business logic. They are architectural decisions that are not easy to change.<br><br>As we have already discussed, the basic aim of scaling a system is to increase its capacity in some application-specific dimension. A common dimension is increasing the number of requests that a system can process in a given time period. This is known as the system’s throughput. Let’s use an analogy to explore two basic principles we have available to us for scaling our systems and increasing throughput.<br>In 1932, one of the world’s great icons, <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Sydney_Harbour_Bridge" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Sydney_Harbour_Bridge" target="_blank">the Sydney Harbor Bridge</a>, was opened. Now it is a fairly safe assumption that traffic volumes in 2020 are somewhat higher than in 1932. If you have driven over the bridge at peak hour in the last 30 years, then you know that its capacity is exceeded considerably every day. So how do we increase throughput on physical infrastructures such as bridges?<br>This issue became very prominent in Sydney in the 1980s, when it was realized that the capacity of the harbor crossing had to be increased. The solution was the rather less iconic <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Sydney_Harbour_Tunnel" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Sydney_Harbour_Tunnel" target="_blank">Sydney Harbor</a> tunnel, which essentially follows the same route underneath the harbor. This provides 4 more lanes of traffic and hence added roughly 1/3rd more capacity to harbor crossings.<br>In not too far away Auckland, their <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge" target="_blank">harbor bridge</a> also had a capacity problem as it was built in 1959 with only 4 lanes. In essence, they adopted the same solution as Sydney, namely to increase capacity. But rather than build a tunnel, they ingeniously doubled the number of lanes by expanding the bridge with the hilariously named ‘<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge#'Nippon_clip-ons'" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge#'Nippon_clip-ons'" target="_blank">Nippon Clipons’</a>, which widened the bridge on each side. Ask a Kiwi to say ‘Nippon Clipons’ and you will understand why this is funny.<br>These examples illustrate the first strategy we have in software systems to increase capacity. We basically replicate the software processing resources to provide more capacity to handle requests and thus increase throughput, as shown in Figure 1. These replicated processing resources are analogous to the laneways on bridges, providing a mostly independent processing pathway for a stream of arriving requests. Luckily, in cloud-based software systems, replication can be achieved at the click of a mouse, and we can effectively replicate our processing resources thousands of times. We have it a lot easier than bridge builders in that respect.<br><img alt="With one server we can process 100 requests per second. If we deploy 3 servers, we can process 300 requests per second." src="https://miro.medium.com/v2/resize:fit:1062/1*N1__-Yxk_qEcuFqzY8gyZg.png" referrerpolicy="no-referrer"><br>Figure 1 Increasing Capacity through Replication<br>The second strategy for scalability can also be illustrated with our bridge example. In Sydney, some observant people realized that in the mornings a lot more vehicles cross the bridge from north to south, and in the afternoon we see the reverse pattern. A smart solution was therefore devised — allocate more of the lanes to the high demand direction in the morning, and sometime in the afternoon, switch this around. This effectively increased the capacity of the bridge without allocating any new resources — we optimized the resources we already had available.<br>We can follow this same approach in software to scale our systems. If we can somehow optimize our processing, by maybe using more efficient algorithms, adding extra indexes in our databases to speed up queries, or even rewriting our server in a faster programming language, we can increase our capacity without increasing our resources. The canonical example of this is Facebook’s creation of (the now discontinued) <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/HipHop_for_PHP" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/HipHop_for_PHP" target="_blank">HipHop for PHP</a>, which increased the speed of Facebook’s web page generation by up to 6 times by compiling PHP code to C++. This enabled the site to process many more requests with the same resources.<br>We’ll revisit these two design principles — namely replication and optimization — many times in the remainder of this series. You will see that there are many complex implications of adopting these principles that arise from the fact that we are building distributed systems. Distributed systems have properties that make designing scalable systems ‘interesting’, where interesting in this context has both positive and negative connotations.<br><br>Let’s take a trivial hypothetical example to examine the relationship between scalability and costs. Assume we have a Web-based (e.g. web server and database) system that can service a load of 100 concurrent requests with a mean response time of 1 second. We get a business requirement to scale up this system to handle 1000 concurrent requests with the same response time. Without making any changes, a simple load test of this system reveals the performance shown in Figure 2 (left). As the request load increases, we see the mean response time steadily grow to 10 seconds with the projected load. Clearly this is not scalable and cannot satisfy our requirements in its current deployment configuration.<br><img alt="On the left, response times grow rapidly. One the right, response times stay flat as load increases." src="https://miro.medium.com/v2/resize:fit:1400/1*JKhgncDUTSwPe0lw5iIZGA.png" referrerpolicy="no-referrer"><br>Figure 2 Scaling an application. (Left) — non-scalable performance. (Right) — scalable performance<br>Clearly some engineering effort is needed in order to achieve the required performance. Figure 2 (right) shows the system’s performance after it has been modified. It now provides the specified response time with 1000 concurrent requests. Hence, we have successfully scaled the system. Party time!<br>A major question looms, however. Namely, how much effort and resources were required to achieve this performance? Perhaps it was simply a case of scaling up by running the Web server on a more powerful (virtual) machine. Performing such reprovisioning on a cloud might take 30 minutes at most. Slightly more complex would be reconfiguring the system to scale out and run multiple instances of the Web server to increase capacity. Again, this should be a simple, low-cost configuration change for the application, with no code changes needed. These would be excellent outcomes.<br>However, scaling a system isn’t always so easy. The reasons for this are many and varied, but here are some possibilities:<br>
<br>the database becomes less responsive with 1000 requests per second, requiring an upgrade to a new machine
<br>the Web server generates a lot of content dynamically and this reduces response time under load. A possible solution is to alter the code to more efficiently generate the content, thus reducing processing time per request.
<br>the request load creates hot spots in the database when many requests try to access and update the same records simultaneously. This requires a schema redesign and subsequent reloading of the database, as well as code changes to the data access layer.
<br>the Web server framework that was selected emphasized ease of development over scalability. The model it enforces means that the code simply cannot be scaled to meet the request load requirements, and a complete rewrite is required. Another framework? Another programming language even?
<br>There’s a myriad of other potential causes, but hopefully, these illustrate the increasing effort that might be required as we move from possibility (1) to possibility (4).<br>Now let’s assume option (1), upgrading the database server, requires 15 hours of effort and a thousand dollars extra cloud costs per month for a more powerful server. This is not prohibitively expensive. And let’s assume option (4), a rewrite of the Web application layer, requires 10,000 hours of development due to implementing in a new language (e.g. Java instead of Ruby). Options (2) and (3) fall somewhere in between options (1) and (4). The cost of 10,000 hours of development is seriously significant. Even worse, while the development is underway, the application may be losing market share and hence money due to its inability to satisfy client requests loads. These kinds of situations can cause systems and businesses to fail.<br>This simple scenario illustrates how resource and effort costs are inextricably tied to scalability. If a system is not designed intrinsically to scale, then the downstream costs and resources of increasing its capacity to meet requirements may be massive. For some applications, such as <a data-tooltip-position="top" aria-label="https://www.bloomberg.com/news/articles/2014-09-24/obamacare-website-costs-exceed-2-billion-study-finds" rel="noopener nofollow" class="external-link" href="https://www.bloomberg.com/news/articles/2014-09-24/obamacare-website-costs-exceed-2-billion-study-finds" target="_blank">Healthcare.gov</a>, these (more than $2 billion) costs are borne and the system is modified to eventually meet business needs. For others, such as Oregon’s health care exchange, an inability to scale rapidly at low cost can be an expensive ($303million) death knell.<br>We would never expect someone would attempt to scale up the capacity of a family home to become a 50-floor office building. The home doesn’t have the architecture, materials, and foundations for this to be even a remote possibility without being completely demolished and rebuilt. Similarly, we shouldn’t expect software systems that do not employ scalable architectures, mechanisms and technologies to be quickly evolved to meet greater capacity needs. The foundations of scale need to be built in from the beginning, with the recognition that the components will evolve over time. By employing design and development principles that promote scalability, we can more rapidly and cheaply scale systems to meet rapidly growing demands.<br>Software systems that can be scaled exponentially while costs grow linearly are known as hyperscale systems, defined as:<br>“Hyperscale systems exhibit exponential growth in computational and storage capabilities while exhibiting linear growth rates in the costs of resources required to build, operate, support, and evolve the required software and hardware resources.”<br>You can read more about hyperscale systems <a data-tooltip-position="top" aria-label="https://www.researchgate.net/publication/318049054_Chapter_2_Hyperscalability_-_The_Changing_Face_of_Software_Architecture" rel="noopener nofollow" class="external-link" href="https://www.researchgate.net/publication/318049054_Chapter_2_Hyperscalability_-_The_Changing_Face_of_Software_Architecture" target="_blank">in this article</a>[3].<br><br>The ability to scale an application quickly and cost-effectively should be a defining quality of the software architecture of contemporary Internet-facing applications. We have two basic ways to achieve scalability, namely increasing system capacity, typically through replication, and performance optimization of system components. The rest of this series of articles will delve deeply into how these two basic principles manifest themselves in constructing scalable distributed systems. Get ready for a wild ride.<br><br>
<br>
Ian Gorton, Paul Greenfield, Alex Szalay, and Roy Williams. 2008. Data-Intensive Computing in the 21st Century. Computer 41, 4 (April 2008), 30–32.

<br>
Rachel Potvin and Josh Levenberg. 2016. Why Google stores billions of lines of code in a single repository. Commun. ACM 59, 7 (July 2016), 78–87.

<br>
Ian Gorton (2017). Chapter 2. Hyperscalability — The Changing Face of Software Architecture. 10.1016/B978–0–12–805467–3.00002–8.

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/building-scalable-distributed-systems-introduction-to-scalable-systems.html</link><guid isPermaLink="false">Computer Science/Distributed System/Building Scalable Distributed Systems - Introduction to Scalable Systems.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:48:25 GMT</pubDate><enclosure url="https://miro.medium.com/v2/resize:fit:1062/1*N1__-Yxk_qEcuFqzY8gyZg.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:1062/1*N1__-Yxk_qEcuFqzY8gyZg.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[对分布式系统的错误认知]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a><br>20 多年前，Peter Deutsch 和 James Gosling 定义了分布式计算的 8 个谬误。这些是许多开发人员对分布式系统做出的错误假设。从长远来看，这些假设通常被证明是错误的。<br><img src="https://pic3.zhimg.com/v2-fb25b92f31e4317bc84860bdc2145896_b.jpg" referrerpolicy="no-referrer"><br>让我们来看看每个谬误，讨论问题和潜在的解决方案。<br><br><br>分布式系统本质上是由多个通过网络连接和通信的节点、系统或服务组成。网络中的任何变化或故障都可能中断通信影响系统的功能。<br>这些故障可能由多种原因引起，比如海底电缆的断裂、交换机故障、断电等。此外，许多用于保障网络可靠性的硬件设备也依赖于某种形式的软件。如果这些软件中存在细微的bug，也可能导致网络问题。这些都说明了一个事实：网络并不可靠。<br>举个简单的例子，当你在与朋友视频通话时，突然网络中断了，这正是网络不可靠性的表现。可能是路由器出现了故障、网络正在维护，或者其他外部因素导致了中断。<br>因此，在开发API时，必须考虑网络的不可靠性，并设计容错机制，如重试、幂等操作、自动重新连接等，以确保在网络出现问题时，系统依然能够稳定运行。<br><img src="https://pica.zhimg.com/v2-906f7a2f4974266d75797102d98dbce2_b.jpg" referrerpolicy="no-referrer"><br><br>在分布式系统中，数据中心或服务器可能分布在全球各地。虽然这种地理上的分布增强了系统的弹性和可靠性，但同时也增加了机器之间通信和数据传输的时间。此外，由于网络本身的不可靠性，数据包可能会出现延迟，从而进一步延长客户端请求完成的时间。延迟为零的概念是不正确的。<br>当你在中国访问一个位于美国的网站时，数据需要经过多个路由器和服务器传输，这个过程会产生延迟。这就是延迟不是零的表现。<br><img src="https://pic3.zhimg.com/v2-f86374253f639b98bd13dc7ddbba9d24_b.jpg" referrerpolicy="no-referrer"><br><br><img src="https://pic4.zhimg.com/v2-4213fda24569aff3df9095720e6c860d_b.jpg" referrerpolicy="no-referrer"><br>带宽是指单位时间内可以通过网络传输的数据量。尽管现代大规模应用程序可能让人觉得带宽似乎是无限的，但实际上，当数据需求超过可用带宽时（例如，在线视频流或在线游戏），网络速度可能会下降，出现限速的情况。<br>此外，当多个服务共享相同的基础设施资源（如消息代理）时，为了防止某个高流量的生产者占用过多资源，影响其他服务的性能，每个生产者的带宽通常会受到限制。<br>如果你有过晚上刷视频的经历，你会发现看视频偶尔会卡顿，这是因为晚上很多人都在使用网络，带宽被占满了。<br>这一切都说明带宽是有限的 。<br><br>在设计接受用户输入的网页表单时，有一个基本原则：永远不要信任用户输入。<br>同样，关于连接开放Wi-Fi网络的建议是：尽量避免连接开放的Wi-Fi。此外，在设计不同服务之间通过网络进行通信时，必须验证请求的来源。这是因为互联网充满了恶意用户，他们会嗅探网络流量以解码通信，或尝试攻击防火墙以访问私有网络、服务或数据库。<br>例如你在咖啡店连接公共Wi-Fi，黑客可能会嗅探你的网络流量，获取你的私人信息，这就是网络不安全的例子。<br>这一切表明网络本质上并不安全。因此您在设计系统时要牢记安全性，您需要在网络，基础架构和应用程序级别进行不同的安全检查。<br><img src="https://picx.zhimg.com/v2-6ac79ddc9ff249cc218c1be6558169f7_b.jpg" referrerpolicy="no-referrer"><br><br>网络拓扑结构并不是一成不变的。它可能因为意外的故障而改变，比如当你的应用服务器故障需要更换时；也可能是有意改变的，比如增加或删除服务器实例，以应对不同的工作负载。如今，随着云计算和容器技术的发展，这种变化变得更加频繁和明显。<br><img src="https://pic3.zhimg.com/v2-e20cd8840b9ec464a91131e0f2fb4044_b.jpg" referrerpolicy="no-referrer"><br>因此，在设计系统时，必须考虑到系统的弹性和扩展能力，能够根据工作负载的变化灵活地添加或删除服务器，使系统适应网络环境的变化。<br><br>在小型系统中，可能确实只有一个管理员来负责运维工作。<br>但在现代计算中，由于云计算工具的普及和软件应用程序的复杂性不断增加，一个人几乎不可能处理从设计到开发再到部署的所有事情。大多数互联网应用程序由多个微服务组成，每个微服务都有不同的团队负责，他们各自管理开发、部署工具和周期。因此，让一个人负责所有事情是不现实的。<br>例如，在一家大型互联网公司，邮件服务、社交平台、视频服务等各个服务由不同的团队开发和维护，每个团队都有专门的管理人员。<br><img src="https://pic1.zhimg.com/v2-ad3bd27dfa91312c911504b9a1f78ed0_b.jpg" referrerpolicy="no-referrer"><br><br><img src="https://pic4.zhimg.com/v2-1b48055c0dcb98caa777135ad03511bf_b.jpg" referrerpolicy="no-referrer"><br>在分布式系统中，为了实现不同实体之间的通信，数据必须通过网络传输。这就像现实中运输货物一样，通过网络传输数据也需要物理硬件、网络带宽、软件和电力等资源。虽然这些传输成本单次看起来可能很小，但随着时间的推移，但最终它们构成了运营在线大规模应用程序的总体费用的很大一部分。<br>当你使用云存储服务上传大量数据时，会发现需要支付带宽费用，这就是数据传输成本的体现。<br><br>在互联网上运行的不同服务或节点，通常在硬件规格和操作参数方面各不相同。例如，每个连接到互联网的设备都有不同的硬件规格，你的手机和公司的服务器性能和配置完全不同，<br>如果在设计系统时假设所有设备都具有相同的配置，会导致性能或兼容性问题。例如，如果开发一个应用时假设所有设备都是高性能服务器，那么在手机上运行时，可能会非常缓慢。这一原则同样适用于跨多个地理位置部署的大规模分布式系统，其中的物理机器往往具有不同的配置。<br><img src="https://pic4.zhimg.com/v2-b15322b01a4cb9e31606d7862f3a95ab_b.jpg" referrerpolicy="no-referrer"><br><a class="internal-link" data-href="//zhuanlan.zhihu.com/p/713553493" href="https://muqiuhan.github.io/wiki/zhuanlan.zhihu.com/p/713553493" target="_self" rel="noopener nofollow">发布于 2024-08-09 08:13</a>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/对分布式系统的错误认知.html</link><guid isPermaLink="false">Computer Science/Distributed System/对分布式系统的错误认知.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:48:27 GMT</pubDate><enclosure url="https://pic3.zhimg.com/v2-fb25b92f31e4317bc84860bdc2145896_b.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://pic3.zhimg.com/v2-fb25b92f31e4317bc84860bdc2145896_b.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[我为什么认为 Actor 不合适 CPU 密集任务]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:distributed" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#distributed</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:actor" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#actor</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:distributed" class="tag" target="_blank" rel="noopener nofollow">#distributed</a> <a href="https://muqiuhan.github.io/wiki?query=tag:actor" class="tag" target="_blank" rel="noopener nofollow">#actor</a><br>从面向对象来说，面向对象中，将世间万物看作一个个对象，从相同对象中总结（抽象）出一个个类，比如人类，猫类等等，但是面向对象理论在落实的编程语言中时，没有考虑到世界线问题，即世界的时间流转问题，举个例子：<br>课堂上，老师在讲台上讲课（ teacher 对象的 speak 方法正在执行中），下边的学生在说话（student 对象的 speak 也在执行）。从现实生活中看，这个是自然而然的，但是，在计算机世界， 讲课和说话操作都需要在thread 中才能执行。所以，object 和 thread 就有匹配问题了，理论上，你应该为每一个 object 都匹配一个 thread, 毕竟现实中就是这样，老师和学生都有各自的 thread,可以同时执行操作。但是计算机世界中的问题就是不可能为每一个 object 起一个 thread。所以面向对象编程语言在计算机中实现时，线程问题做的就会比较奇怪了。<br>而 Actor  model 实际上是屏蔽了 thread 概念的（不接触thread,也不需要理解thread概念）。 Actor  model 中假设所有的 Actor 之间的运行是并行的，就和现实世界一样的。老师 Actor 和学生的 Actor 的运行底层是不相关的，相关的是老师在 speak（相当于群发消息），学生也在 speak（单发给同桌或者给后座的人），老师听到有人在说话（接收一个消息），然后老师拿粉笔打了说话的学生（单发一个粉笔头给那个学生）。<br>由此可见， Actor 是比面向对象更接近现实的模型。<br>从上一个问题来看， Actor 模型比oo更接近现实世界，但是，落实在计算机中时，就涉及到， Actor 怎么执行的问题。因为计算机中没有那么多的thread, 所以 Actor 在实现时，必然是通过少量的 thread 来执行大量的 Actor 实体。 即在一个 thread 中分时执行多个 Actor 。当然，这是开发 Actor 模型的人需要关注的问题，用 Actor 模型的人可以不关注。<br>所以，这里就引出了这个问题：如果在 Actor 中执行 CPU 密集任务会怎么样？<br>结论就是，CPU 密集任务把 Actor 的底层执行机制给拖垮了，thread 在执行一个 Actor 时，因耗时太长，并占用大量 CPU 资源，导致其他 Actor 无法执行。 Actor 的执行机制出现断档， Actor 世界出现 stop the world。STW 问题对于 Actor 本身不会有啥影响，反正世界都停了，无非就是多等一会儿。但是对于我们这个上帝来说，这个 Actor 世界已经不能流畅运行了。<br>所以，不管是 go, 还是 akka,或者 vert.x，都会提醒你，不要在 Actor 或类 Actor 实例中执行 block 太长的任务。<br>所以，怎么解决这个问题呢？<br>
答案是：使用异步。<br>
异步操作就可以把一个 block 变成不 block 的任务，从而在 Actor 世界中实现目标。<br>最后，再考虑一下用 Actor 模型来执行 CPU 密集任务的问题。<br>如果在 Actor 中执行一个 CPU 密集任务，首先会导致其他 Actor 的执行被推迟，其次，系统本身的 Actor （比如 log, monitor 等）也会推迟。如果能接受这个影响，那么用 Actor 来执行 CPU 密集任务也没有问题。<br>另一种方案是，把CPU密集 Actor 放到单独的 executor 中，与其他 Actor 分离，充分利用多线程特性]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/我为什么认为-actor-不合适-cpu-密集任务.html</link><guid isPermaLink="false">Computer Science/Distributed System/我为什么认为 Actor 不合适 CPU 密集任务.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:48:25 GMT</pubDate></item><item><title><![CDATA[Linux 内存管理]]></title><description><![CDATA[ 
 <br><br>以 Intel Core i7 处理器为例，64 位虚拟地址的格式为：全局页目录项（9位）+ 上层页目录项（9位）+ 中间页目录项（9位）+&nbsp;页表项（9位）+&nbsp;页内偏移（12位）。共 48 位组成的虚拟内存地址。<br>
全局页目录项就类比我们日常生活中收获地址里的省，上层页目录项就类比市，中间层页目录项类比区县，页表项类比街道小区，页内偏移类比我们所在的楼栋和几层几号。
<br>32 位虚拟地址的格式为：页目录项（10位）+ 页表项（10位） + 页内偏移（12位）。共 32 位组成的虚拟内存地址。<br>
进程虚拟内存空间中的每一个字节都有与其对应的虚拟内存地址，一个虚拟内存地址表示进程虚拟内存空间中的一个特定的字节。
<br><br>
<br><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247486732&amp;idx=1&amp;sn=435d5e834e9751036c96384f6965b328&amp;chksm=ce77cb4bf900425d33d2adfa632a4684cf7a63beece166c1ffedc4fdacb807c9413e8c73f298&amp;token=1468822011&amp;lang=zh_CN&amp;scene=21#wechat_redirect" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247486732&amp;idx=1&amp;sn=435d5e834e9751036c96384f6965b328&amp;chksm=ce77cb4bf900425d33d2adfa632a4684cf7a63beece166c1ffedc4fdacb807c9413e8c73f298&amp;token=1468822011&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank">一步一图带你深入理解 Linux 虚拟内存管理</a>
<br><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247486879&amp;idx=1&amp;sn=0bcc59a306d59e5199a11d1ca5313743&amp;chksm=ce77cbd8f90042ce06f5086b1c976d1d2daa57bc5b768bac15f10ee3dc85874bbeddcd649d88&amp;scene=21#wechat_redirect" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247486879&amp;idx=1&amp;sn=0bcc59a306d59e5199a11d1ca5313743&amp;chksm=ce77cbd8f90042ce06f5086b1c976d1d2daa57bc5b768bac15f10ee3dc85874bbeddcd649d88&amp;scene=21#wechat_redirect" target="_blank">一步一图带你深入理解 Linux 物理内存管理</a>
<br><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247487111&amp;idx=1&amp;sn=e57371f9c3e6910f4f4721aa0787e537&amp;chksm=ce77c8c0f90041d67b2d344d413a2573f3662a1a64a802b41d4618982fcbff1617d9a5da9f7b&amp;token=1720271116&amp;lang=zh_CN#rd" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247487111&amp;idx=1&amp;sn=e57371f9c3e6910f4f4721aa0787e537&amp;chksm=ce77c8c0f90041d67b2d344d413a2573f3662a1a64a802b41d4618982fcbff1617d9a5da9f7b&amp;token=1720271116&amp;lang=zh_CN#rd" target="_blank">深入理解 Linux 物理内存分配全链路实现</a>
<br><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247487228&amp;idx=1&amp;sn=85e44fa5b090b29ab23ca6abf98da221&amp;chksm=ce77c8bbf90041ad4958a3871a880a3f6d812e282530ea047d9eaf76d9f03aafb4b987a64ae9&amp;cur_album_id=2559805446807928833&amp;scene=189#wechat_redirect" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU3Mjc3Ng==&amp;mid=2247487228&amp;idx=1&amp;sn=85e44fa5b090b29ab23ca6abf98da221&amp;chksm=ce77c8bbf90041ad4958a3871a880a3f6d812e282530ea047d9eaf76d9f03aafb4b987a64ae9&amp;cur_album_id=2559805446807928833&amp;scene=189#wechat_redirect" target="_blank">深度剖析 Linux 伙伴系统的设计与实现</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux-内存管理/linux-内存管理.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/Linux 内存管理/Linux 内存管理.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 12 Feb 2025 05:40:57 GMT</pubDate></item><item><title><![CDATA[Linux中的硬连接和软连接]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:linux" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#linux</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:linux" class="tag" target="_blank" rel="noopener nofollow">#linux</a> <a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a><br>Linux链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。默认情况下，ln命令产生硬链接。<br><br>硬连接指通过索引节点来进行连接。在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)。在Linux中，多个文件名指向同一索引节点是存在的。一般这种连接就是硬连接。硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，文件真正删除的条件是与之相关的所有硬连接文件均被删除。<br><br>另外一种连接称之为符号连接（Symbolic Link），也叫软连接。软链接文件有类似于Windows的快捷方式。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含的有另一文件的位置信息。<br>2. 通过实验加深理解<br>[root@Linux]$ touch f1 #创建一个测试文件f1
[root@Linux]$ ln f1 f2 #创建f1的一个硬连接文件f2
[root@Linux]$ ln -s f1 f3 #创建f1的一个符号连接文件f3
[root@Linux]$ ls -li # -i参数显示文件的inode节点信息
total 0
9797648 -rw-r--r-- 2 root root 0 Apr 21 08:11 f1
9797648 -rw-r--r-- 2 root root 0 Apr 21 08:11 f2
9797649 lrwxrwxrwx 1 root root 2 Apr 21 08:11 f3 -&gt; f1
<br>从上面的结果中可以看出，硬连接文件f2与原文件f1的inode节点相同，均为9797648，然而符号连接文件的inode节点不同。<br>[root@Linux]$ echo "I am f1 file" &gt;&gt;f1
[root@Linux]$ cat f1
I am f1 file
[root@Linux]$ cat f2
I am f1 file
[root@Linux]$ cat f3
I am f1 file
[root@Linux]$ rm -f f1
[root@Linux]$ cat f2
I am f1 file
[root@Linux]$ cat f3
cat: f3: No such file or directory
<br>通过上面的测试可以看出：当删除原始文件f1后，硬连接f2不受影响，但是符号连接f1文件无效。<br>3. 总结<br>
<br>删除符号连接f3,对f1,f2无影响；
<br>删除硬连接f2，对f1,f3也无影响；
<br>删除原文件f1，对硬连接f2没有影响，导致符号连接f3失效；
<br>同时删除原文件f1,硬连接f2，整个文件会真正的被删除。
<br><br><br>在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号inode 。<br>
<br>软连接，其实就是新建立一个文件，这个文件就是专门用来指向别的文件的（那就和windows 下的快捷方式的那个文件有很接近的意味）。软链接产生的是一个新的文件，但这个文件的作用就是专门指向某个文件的，删了这个软连接文件，那就等于不需要这个连接，和原来的存在的实体原文件没有任何关系，但删除原来的文件，则相应的软连接不可用（cat那个软链接文件，则提示“没有该文件或目录“）
<br>硬连接是不会建立inode的，他只是在文件原来的inode link count域再增加1而已，也因此硬链接是不可以跨越文件系统的。相反是软连接会重新建立一个inode，当然inode的结构跟其他的不一样，他只是一个指明源文件的字符串信息。一旦删除源文件，那么软连接将变得毫无意义。而硬链接删除的时候，系统调用会检查inode link count的数值，如果他大于等于1，那么inode不会被回收。因此文件的内容不会被删除。
<br>硬链接实际上是为文件建一个别名，链接文件和原文件实际上是同一个文件。可以通过ls -i来查看一下，这两个文件的inode号是同一个，说明它们是同一个文件；而软链接建立的是一个指向，即链接文件内的内容是指向原文件的指针，它们是两个文件。
<br>软链接可以跨文件系统，硬链接不可以；
<br>软链接可以对一个不存在的文件名(filename)进行链接（当然此时如果你vi这个软链接文件，linux会自动新建一个文件名为filename的文件）,硬链接不可以（其文件必须存在，inode必须存在）；
<br>软链接可以对目录进行连接，硬链接不可以。
<br>两种链接都可以通过命令 ln 来创建。ln 默认创建的是硬链接。
<br>使用 -s 开关可以创建软链接。
<br><br><br>链接简单说实际上是一种文件共享的方式，是 POSIX 中的概念，主流文件系统都支持链接文件。<br><br>你可以将链接简单地理解为 Windows 中常见的快捷方式（或是 OS X 中的替身），Linux 中常用它来解决一些库版本的问题，通常也会将一些目录层次较深的文件链接到一个更易访问的目录中。在这些用途上，我们通常会使用到软链接（也称符号链接）。<br><br>下面我们进入正题，来探讨一下软硬两种链接到底有什么区别？<br>首先，从使用的角度讲，两者没有任何区别，都与正常的文件访问方式一样，支持读写，如果是可执行文件的话也可以直接执行。<br>那区别在哪呢？在底层的原理上。<br>为了解释清楚，我们首先在自己的一个工作目录下创建一个文件，然后对这个文件进行链接的创建：<br>$ touch myfile &amp;&amp; echo "This is a plain text file." &gt; myfile
$ cat myfile
 
This is a plain text file.
<br>现在我们创建了一个普通地不能再普通的文件了。然后我们对它创建一个硬链接，并查看一下当前目录：<br>$ ln myfile hard
$ ls -li
 
25869085 -rw-r--r-- 2 unixzii staff 27 7 8 17:39 hard
25869085 -rw-r--r-- 2 unixzii staff 27 7 8 17:39 myfile
<br>在 ls 结果的最左边一列，是文件的 inode 值，你可以简单把它想成 C 语言中的指针。它指向了物理硬盘的一个区块，事实上文件系统会维护一个引用计数，只要有文件指向这个区块，它就不会从硬盘上消失。<br>你也看到了，这两个文件就如同一个文件一样，inode 值相同，都指向同一个区块。<br>然后我们修改一下刚才创建的 hard 链接文件：<br>$ echo "New line" &gt;&gt; hard
$ cat myfile
 
This is a plain text file.
New line
<br>可以看到，这两个文件果真就是一个文件。<br>
下面我们看看软链接（也就是符号链接）和它有什么区别。<br>$ ln -s myfile soft
$ ls -li
 
25869085 -rw-r--r-- 2 unixzii staff 36 7 8 17:45 hard
25869085 -rw-r--r-- 2 unixzii staff 36 7 8 17:45 myfile
25869216 lrwxr-xr-x 1 unixzii staff 6 7 8 17:47 soft -&gt; myfile
<br>诶，你会发现，这个软链接的 inode 竟然不一样啊，并且它的文件属性上也有一个 l 的 flag，这就说明它与之前我们创建的两个文件根本不是一个类型。<br>下面我们试着删除 myfile 文件，然后分别输出软硬链接的文件内容：<br>$ rm myfile
$ cat hard
 
This is a plain text file.
New line
<br>$ cat soft
 
cat: soft: No such file or directory
<br>之前的硬链接没有丝毫地影响，因为它 inode 所指向的区块由于有一个硬链接在指向它，所以这个区块仍然有效，并且可以访问到。<br>
然而软链接的 inode 所指向的内容实际上是保存了一个绝对路径，当用户访问这个文件时，系统会自动将其替换成其所指的文件路径，然而这个文件已经被删除了，所以自然就会显示无法找到该文件了。<br>为验证这一猜想，我们再向这个软链接写点东西：<br>$ echo "Something" &gt;&gt; soft
$ ls
 
hard myfile soft
<br>可以看到，刚才删除的 myfile 文件竟然又出现了！这就说明，当我们写入访问软链接时，系统自动将其路径替换为其所代表的绝对路径，并直接访问那个路径了。<br>
<br>硬链接： 与普通文件没什么不同，inode 都指向同一个文件在硬盘中的区块
<br>软链接： 保存了其代表的文件的绝对路径，是另外一种文件，在硬盘上有独立的区块，访问时替换自身路径。<br>
<img alt="Linux中的硬链接和软链接.png" src="https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬链接和软链接.png">
<br><br><br>当把原文件myfile删除之后，通过链接是无法访问，通过硬链接还是可以访问的  <br>
<br>软连接其实保存的是原文件的路径，访问软连接就是通过绝对路径进行访问的。访问不到原文件说明--文件消失了或者文件移动到其他目录下面了  
<br>把原myfile文件删除之后，但是通过硬链接还是可以访问的
<br>在这个场景下，文件并没有移动或者不移动之说。文件就是一块物理存储区域的内容。只有当指向存储区的指针全部消失，存储区才会被回收（内容也就没了）。所谓的文件名，只是指向存储区的一个flag。软链接是这个flag的路径，先找到这个flag，再访问存储区。硬链接是新建了一个不同文件名但指向同一个存储区的flag。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬连接和软连接.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/Linux中的硬连接和软连接/Linux中的硬连接和软连接.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:49:23 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬链接和软链接.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬链接和软链接.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Inter-process communication in Linux -- Shared storage]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:linux" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#linux</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:linux" class="tag" target="_blank" rel="noopener nofollow">#linux</a> <a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a><br>This is the first article in a series&nbsp;about <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Inter-process_communication" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Inter-process_communication" target="_blank">interprocess communication</a> (IPC) in Linux. The series uses code examples in C to clarify the following IPC mechanisms:<br>
<br>Shared files
<br>Shared memory (with semaphores)
<br>Pipes (named and unnamed)
<br>Message queues
<br>Sockets
<br>Signals
<br>This article reviews some core concepts before moving on to the first two of these mechanisms: shared files and shared memory.<br><br>A process is a program in execution, and each process has its own address space, which comprises the memory locations that the process is allowed to access. A process has one or more threads of execution, which are sequences of executable instructions: a single-threaded process has just one thread, whereas a multi-threaded process has more than one thread. Threads within a process share various resources, in particular, address space. Accordingly, threads within a process can communicate straightforwardly through shared memory, although some modern languages (e.g., Go) encourage a more disciplined approach such as the use of thread-safe channels. Of interest here is that different processes, by default, do not share memory.<br>There are various ways to launch processes that then communicate, and two ways dominate in the examples that follow:<br>
<br>A terminal is used to start one process, and perhaps a different terminal is used to start another.
<br>The system function fork is called within one process (the parent) to spawn another process (the child).
<br>The first examples take the terminal approach. The <a data-tooltip-position="top" aria-label="http://condor.depaul.edu/mkalin" rel="noopener nofollow" class="external-link" href="http://condor.depaul.edu/mkalin" target="_blank">code examples</a> are available in a ZIP file on my website.<br><br>Programmers are all too familiar with file access, including the many pitfalls (non-existent files, bad file permissions, and so on) that beset the use of files in programs. Nonetheless, shared files may be the most basic IPC mechanism. Consider the relatively simple case in which one process (producer) creates and writes to a file, and another process (consumer) reads from this same file:<br>         writes  +-----------+  reads
producer--------&gt;| disk file |&lt;-------consumer
                 +-----------+
<br>The obvious challenge in using this IPC mechanism is that a race condition might arise: the producer and the consumer might access the file at exactly the same time, thereby making the outcome indeterminate. To avoid a race condition, the file must be locked in a way that prevents a conflict between a write operation and any another operation, whether a read or a write. The locking API in the standard system library can be summarized as follows:<br>
<br>A producer should gain an exclusive lock on the file before writing to the file. An exclusive lock can be held by one process at most, which rules out a race condition because no other process can access the file until the lock is released.
<br>A consumer should gain at least a shared lock on the file before reading from the file. Multiple readers can hold a shared lock at the same time, but no writer can access a file when even a single reader holds a shared lock.
<br>A shared lock promotes efficiency. If one process is just reading a file and not changing its contents, there is no reason to prevent other processes from doing the same. Writing, however, clearly demands exclusive access to a file.<br>The standard I/O library includes a utility function named fcntl that can be used to inspect and manipulate both exclusive and shared locks on a file. The function works through a file descriptor, a non-negative integer value that, within a process, identifies a file. (Different file descriptors in different processes may identify the same physical file.) For file locking, Linux provides the library function flock, which is a thin wrapper around fcntl. The first example uses the fcntl function to expose API details.<br><br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;string.h&gt;

#define FileName "data.dat"
#define DataString "Now is the winter of our discontent\nMade glorious summer by this sun of York\n"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1); /* EXIT_FAILURE */
}

int main() {
  struct flock lock;
  lock.l_type = F_WRLCK;    /* read/write (exclusive versus shared) lock */
  lock.l_whence = SEEK_SET; /* base for seek offsets */
  lock.l_start = 0;         /* 1st byte in file */
  lock.l_len = 0;           /* 0 here means 'until EOF' */
  lock.l_pid = getpid();    /* process id */

  int fd; /* file descriptor to identify a file within a process */
  if ((fd = open(FileName, O_RDWR | O_CREAT, 0666)) &lt; 0)  /* -1 signals an error */
    report_and_exit("open failed...");

  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0) /** F_SETLK doesn't block, F_SETLKW does **/
    report_and_exit("fcntl failed to get lock...");
  else {
    write(fd, DataString, strlen(DataString)); /* populate data file */
    fprintf(stderr, "Process %d has written to data file...\n", lock.l_pid);
  }

  /* Now release the lock explicitly. */
  lock.l_type = F_UNLCK;
  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)
    report_and_exit("explicit unlocking failed...");

  close(fd); /* close the file: would unlock if needed */
  return 0;  /* terminating the process would unlock as well */
}
<br>The main steps in the producer program above can be summarized as follows:<br>
<br>The program declares a variable of type struct flock, which represents a lock, and initializes the structure's five fields. The first initialization:  
lock.l_type = F_WRLCK; /* exclusive lock */

  makes the lock an exclusive (read-write) rather than a shared (read-only) lock. If the producer gains the lock, then no other process will be able to write or read the file until the producer releases the lock, either explicitly with the appropriate call to fcntl or implicitly by closing the file. (When the process terminates, any opened files would be closed automatically, thereby releasing the lock.)<br>

<br>The program then initializes the remaining fields. The chief effect is that the entire file is to be locked. However, the locking API allows only designated bytes to be locked. For example, if the file contains multiple text records, then a single record (or even part of a record) could be locked and the rest left unlocked.
<br>The first call to fcntl:  
if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)

  tries to lock the file exclusively, checking whether the call succeeded. In general, the fcntl function returns -1 (hence, less than zero) to indicate failure. The second argument F_SETLK means that the call to fcntl does not block: the function returns immediately, either granting the lock or indicating failure. If the flag F_SETLKW (the W at the end is for wait) were used instead, the call to fcntl would block until gaining the lock was possible. In the calls to fcntl, the first argument fd is the file descriptor, the second argument specifies the action to be taken (in this case,&nbsp;F_SETLK for setting the lock), and the third argument is the address of the lock structure (in this case,&nbsp;&amp;lock).<br>

<br>If the producer gains the lock, the program writes two text records to the file.
<br>After writing to the file, the producer changes the lock structure's l_type field to the unlock value:  
lock.l_type = F_UNLCK;

  and calls fcntl to perform the unlocking operation. The program finishes up by closing the file and exiting.<br>

<br><br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;

#define FileName "data.dat"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1); /* EXIT_FAILURE */
}

int main() {
  struct flock lock;
  lock.l_type = F_WRLCK;    /* read/write (exclusive) lock */
  lock.l_whence = SEEK_SET; /* base for seek offsets */
  lock.l_start = 0;         /* 1st byte in file */
  lock.l_len = 0;           /* 0 here means 'until EOF' */
  lock.l_pid = getpid();    /* process id */

  int fd; /* file descriptor to identify a file within a process */
  if ((fd = open(FileName, O_RDONLY)) &lt; 0)  /* -1 signals an error */
    report_and_exit("open to read failed...");

  /* If the file is write-locked, we can't continue. */
  fcntl(fd, F_GETLK, &amp;lock); /* sets lock.l_type to F_UNLCK if no write lock */
  if (lock.l_type != F_UNLCK)
    report_and_exit("file is still write locked...");

  lock.l_type = F_RDLCK; /* prevents any writing during the reading */
  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)
    report_and_exit("can't get a read-only lock...");

  /* Read the bytes (they happen to be ASCII codes) one at a time. */
  int c; /* buffer for read bytes */
  while (read(fd, &amp;c, 1) &gt; 0)    /* 0 signals EOF */
    write(STDOUT_FILENO, &amp;c, 1); /* write one byte to the standard output */

  /* Release the lock explicitly. */
  lock.l_type = F_UNLCK;
  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)
    report_and_exit("explicit unlocking failed...");

  close(fd);
  return 0;
}
<br>The consumer program is more complicated than necessary to highlight features of the locking API. In particular, the consumer program first checks whether the file is exclusively locked and only then tries to gain a shared lock. The relevant code is:<br>lock.l_type = F_WRLCK;
...
fcntl(fd, F_GETLK, &amp;lock); /* sets lock.l_type to F_UNLCK if no write lock */
if (lock.l_type != F_UNLCK)
  report_and_exit("file is still write locked...");
<br>The F_GETLK operation specified in the fcntl call checks for a lock, in this case, an exclusive lock given as F_WRLCK in the first statement above. If the specified lock does not exist, then the fcntl call automatically changes the lock type field to F_UNLCK to indicate this fact. If the file is exclusively locked, the consumer terminates. (A more robust version of the program might have the consumer sleep a bit and try again several times.)<br>If the file is not currently locked, then the consumer tries to gain a shared (read-only) lock (F_RDLCK). To shorten the program, the F_GETLK call to fcntl could be dropped because the F_RDLCK call would fail if a read-write lock already were held by some other process. Recall that a read-only lock does prevent any other process from writing to the file, but allows other processes to read from the file. In short, a shared lock can be held by multiple processes. After gaining a shared lock, the consumer program reads the bytes one at a time from the file, prints the bytes to the standard output, releases the lock, closes the file, and terminates.<br>Here is the output from the two programs launched from the same terminal with % as the command line prompt:<br>% ./producer
Process 29255 has written to data file...

% ./consumer
Now is the winter of our discontent
Made glorious summer by this sun of York
<br>In this first code example, the data shared through IPC is text: two lines from Shakespeare's play Richard III. Yet, the shared file's contents could be voluminous, arbitrary bytes (e.g., a digitized movie), which makes file sharing an impressively flexible IPC mechanism. The downside is that file access is relatively slow, whether the access involves reading or writing. As always, programming comes with tradeoffs. The next example has the upside of IPC through shared memory, rather than shared files, with a corresponding boost in performance.<br><br>Linux systems provide two separate APIs for shared memory: the legacy System V API and the more recent POSIX one. These APIs should never be mixed in a single application, however. A downside of the POSIX approach is that features are still in development and dependent upon the installed kernel version, which impacts code portability. For example, the POSIX API, by default, implements shared memory as a memory-mapped file: for a shared memory segment, the system maintains a backing file with corresponding contents. Shared memory under POSIX can be configured without a backing file, but this may impact portability. My example uses the POSIX API with a backing file, which combines the benefits of memory access (speed) and file storage (persistence).<br>The shared-memory example has two programs, named memwriter and memreader, and uses a semaphore to coordinate their access to the shared memory. Whenever shared memory comes into the picture with a writer, whether in multi-processing or multi-threading, so does the risk of a memory-based race condition; hence, the semaphore is used to coordinate (synchronize) access to the shared memory.<br>The memwriter program should be started first in its own terminal. The memreader program then can be started (within a dozen seconds) in its own terminal. The output from the memreader is:<br>This is the way the world ends...
<br>Each source file has documentation at the top explaining the link flags to be included during compilation.<br>Let's start with a review of how semaphores work as a synchronization mechanism. A general semaphore also is called a counting semaphore, as it has a value (typically initialized to zero) that can be incremented. Consider a shop that rents bicycles, with a hundred of them in stock, with a program that clerks use to do the rentals. Every time a bike is rented, the semaphore is incremented by one; when a bike is returned, the semaphore is decremented by one. Rentals can continue until the value hits 100 but then must halt until at least one bike is returned, thereby decrementing the semaphore to 99.<br>A binary semaphore is a special case requiring only two values: 0 and 1. In this situation, a semaphore acts as a mutex: a mutual exclusion construct. The shared-memory example uses a semaphore as a mutex. When the semaphore's value is 0, the memwriter alone can access the shared memory. After writing, this process increments the semaphore's value, thereby allowing the memreader to read the shared memory.<br><br>/** Compilation: gcc -o memwriter memwriter.c -lrt -lpthread **/
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/mman.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;semaphore.h&gt;
#include &lt;string.h&gt;
#include "shmem.h"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1);
}

int main() {
  int fd = shm_open(BackingFile,      /* name from smem.h */
                    O_RDWR | O_CREAT, /* read/write, create if needed */
                    AccessPerms);     /* access permissions (0644) */
  if (fd &lt; 0) report_and_exit("Can't open shared mem segment...");

  ftruncate(fd, ByteSize); /* get the bytes */

  caddr_t memptr = mmap(NULL,       /* let system pick where to put segment */
                        ByteSize,   /* how many bytes */
                        PROT_READ | PROT_WRITE, /* access protections */
                        MAP_SHARED, /* mapping visible to other processes */
                        fd,         /* file descriptor */
                        0);         /* offset: start at 1st byte */
  if ((caddr_t) -1  == memptr) report_and_exit("Can't get segment...");

  fprintf(stderr, "shared mem address: %p [0..%d]\n", memptr, ByteSize - 1);
  fprintf(stderr, "backing file:       /dev/shm%s\n", BackingFile );

  /* semaphore code to lock the shared mem */
  sem_t* semptr = sem_open(SemaphoreName, /* name */
                           O_CREAT,       /* create the semaphore */
                           AccessPerms,   /* protection perms */
                           0);            /* initial value */
  if (semptr == (void*) -1) report_and_exit("sem_open");

  strcpy(memptr, MemContents); /* copy some ASCII bytes to the segment */

  /* increment the semaphore so that memreader can read */
  if (sem_post(semptr) &lt; 0) report_and_exit("sem_post");

  sleep(12); /* give reader a chance */

  /* clean up */
  munmap(memptr, ByteSize); /* unmap the storage */
  close(fd);
  sem_close(semptr);
  shm_unlink(BackingFile); /* unlink from the backing file */
  return 0;
}
<br>Here's an overview of how the memwriter and memreader programs communicate through shared memory:<br>
<br>The memwriter program, shown above, calls the shm_open function to get a file descriptor for the backing file that the system coordinates with the shared memory. At this point, no memory has been allocated. The subsequent call to the misleadingly named function ftruncate:  
ftruncate(fd, ByteSize); /* get the bytes */

  allocates ByteSize bytes, in this case, a modest 512 bytes. The memwriter and memreader programs access the shared memory only, not the backing file. The system is responsible for synchronizing the shared memory and the backing file.<br>

<br>The memwriter then calls the mmap function:  
caddr_t memptr = mmap(NULL,       /* let system pick where to put segment */
                      ByteSize,   /* how many bytes */
                      PROT_READ | PROT_WRITE, /* access protections */
                      MAP_SHARED, /* mapping visible to other processes */
                      fd,         /* file descriptor */
                      0);         /* offset: start at 1st byte */

  to get a pointer to the shared memory. (The memreader makes a similar call.) The pointer type caddr_t starts with a c for calloc, a system function that initializes dynamically allocated storage to zeroes. The memwriter uses the memptr for the later write operation, using the library strcpy (string copy) function.<br>

<br>At this point, the memwriter is ready for writing, but it first creates a semaphore to ensure exclusive access to the shared memory. A race condition would occur if the memwriter were writing while the memreader was reading. If the call to sem_open succeeds:  
sem_t* semptr = sem_open(SemaphoreName, /* name */
                         O_CREAT,       /* create the semaphore */
                         AccessPerms,   /* protection perms */
                         0);            /* initial value */

  then the writing can proceed. The SemaphoreName (any unique non-empty name will do) identifies the semaphore in both the memwriter and the memreader. The initial value of zero gives the semaphore's creator, in this case,&nbsp;the memwriter, the right to proceed, in this case, to the write operation.<br>

<br>After writing, the memwriter increments the semaphore value to 1:  
if (sem_post(semptr) &lt; 0) ..

  with a call to the sem_post function. Incrementing the semaphore releases the mutex lock and enables the memreader to perform its read operation. For good measure, the memwriter also unmaps the shared memory from the memwriter address space:
munmap(memptr, ByteSize); /* unmap the storage *

  This bars the memwriter from further access to the shared memory.<br>

<br><br>/** Compilation: gcc -o memreader memreader.c -lrt -lpthread **/
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/mman.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;semaphore.h&gt;
#include &lt;string.h&gt;
#include "shmem.h"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1);
}

int main() {
  int fd = shm_open(BackingFile, O_RDWR, AccessPerms);  /* empty to begin */
  if (fd &lt; 0) report_and_exit("Can't get file descriptor...");

  /* get a pointer to memory */
  caddr_t memptr = mmap(NULL,       /* let system pick where to put segment */
                        ByteSize,   /* how many bytes */
                        PROT_READ | PROT_WRITE, /* access protections */
                        MAP_SHARED, /* mapping visible to other processes */
                        fd,         /* file descriptor */
                        0);         /* offset: start at 1st byte */
  if ((caddr_t) -1 == memptr) report_and_exit("Can't access segment...");

  /* create a semaphore for mutual exclusion */
  sem_t* semptr = sem_open(SemaphoreName, /* name */
                           O_CREAT,       /* create the semaphore */
                           AccessPerms,   /* protection perms */
                           0);            /* initial value */
  if (semptr == (void*) -1) report_and_exit("sem_open");

  /* use semaphore as a mutex (lock) by waiting for writer to increment it */
  if (!sem_wait(semptr)) { /* wait until semaphore != 0 */
    int i;
    for (i = 0; i &lt; strlen(MemContents); i++)
      write(STDOUT_FILENO, memptr + i, 1); /* one byte at a time */
    sem_post(semptr);
  }

  /* cleanup */
  munmap(memptr, ByteSize);
  close(fd);
  sem_close(semptr);
  unlink(BackingFile);
  return 0;
}
<br>In both the memwriter and memreader programs, the shared-memory functions of main interest are shm_open and mmap: on success, the first call returns a file descriptor for the backing file, which the second call then uses to get a pointer to the shared memory segment. The calls to shm_open are similar in the two programs except that the memwriter program creates the shared memory, whereas the memreader only accesses this already created memory:<br>int fd = shm_open(BackingFile, O_RDWR | O_CREAT, AccessPerms); /* memwriter */
int fd = shm_open(BackingFile, O_RDWR, AccessPerms);           /* memreader */
<br>With a file descriptor in hand, the calls to mmap are the same:<br>caddr_t memptr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
<br>The first argument to mmap is NULL, which means that the system determines where to allocate the memory in virtual address space. It's possible (but tricky) to specify an address instead. The MAP_SHARED flag indicates that the allocated memory is shareable among processes, and the last argument (in this case, zero) means that the offset for the shared memory should be the first byte. The size argument specifies the number of bytes to be allocated (in this case, 512), and the protection argument indicates that the shared memory can be written and read.<br>When the memwriter program executes successfully, the system creates and maintains the backing file; on my system, the file is /dev/shm/shMemEx, with shMemEx as my name (given in the header file shmem.h) for the shared storage. In the current version of the memwriter and memreader programs, the statement:<br>shm_unlink(BackingFile); /* removes backing file */
<br>removes the backing file. If the unlink statement is omitted, then the backing file persists after the program terminates.<br>The memreader, like the memwriter, accesses the semaphore through its name in a call to sem_open. But the memreader then goes into a wait state until the memwriter increments the semaphore, whose initial value is 0:<br>if (!sem_wait(semptr)) { /* wait until semaphore != 0 */
<br>Once the wait is over, the memreader reads the ASCII bytes from the shared memory, cleans up, and terminates.<br>The shared-memory API includes operations explicitly to synchronize the shared memory segment and the backing file. These operations have been omitted from the example to reduce clutter and&nbsp;keep the&nbsp;focus on the memory-sharing and semaphore code.<br>The memwriter and memreader programs are likely to execute without inducing a race condition even if the semaphore code is removed: the memwriter creates the shared memory segment and writes immediately to it; the memreader cannot even access the shared memory until this has been created. However, best practice requires that shared-memory access is synchronized whenever a write operation is in the mix, and the semaphore API is important enough to be highlighted in a code example.<br><br>The shared-file and shared-memory examples show how processes can communicate through shared storage, files in one case and memory segments in the other. The APIs for both approaches are relatively straightforward. Do these approaches have a common downside? Modern applications often deal with streaming data, indeed, with massively large streams of data. Neither the shared-file nor the shared-memory approaches are well suited for massive data streams. Channels of one type or another are better suited. Part 2 thus introduces channels and message queues, again with code examples in C.<br>[<a data-tooltip-position="top" aria-label="https://opensource.com/downloads/guide-inter-process-communication-linux" rel="noopener nofollow" class="external-link" href="https://opensource.com/downloads/guide-inter-process-communication-linux" target="_blank">Download</a> the complete guide to inter-process communication in Linux]<br><br>More Linux resources<br>
<br><a data-tooltip-position="top" aria-label="https://developers.redhat.com/cheat-sheets/linux-commands-cheat-sheet/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://developers.redhat.com/cheat-sheets/linux-commands-cheat-sheet/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Linux commands cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://developers.redhat.com/cheat-sheets/advanced-linux-commands/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://developers.redhat.com/cheat-sheets/advanced-linux-commands/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Advanced Linux commands cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://www.redhat.com/en/services/training/rh024-red-hat-linux-technical-overview?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://www.redhat.com/en/services/training/rh024-red-hat-linux-technical-overview?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Free online course: RHEL Technical Overview</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/downloads/cheat-sheet-networking?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://opensource.com/downloads/cheat-sheet-networking?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Linux networking cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/downloads/cheat-sheet-selinux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://opensource.com/downloads/cheat-sheet-selinux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">SELinux cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/downloads/linux-common-commands-cheat-sheet?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://opensource.com/downloads/linux-common-commands-cheat-sheet?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Linux common commands cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/resources/what-are-linux-containers?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://opensource.com/resources/what-are-linux-containers?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">What are Linux containers?</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/tags/linux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener nofollow" class="external-link" href="https://opensource.com/tags/linux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Our latest Linux articles</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/inter-process-communication-in-linux-shared-storage.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/Inter-process communication in Linux -- Shared storage.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:49:23 GMT</pubDate></item><item><title><![CDATA[Linux OOM 的配置]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:linux" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#linux</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a> <a href="https://muqiuhan.github.io/wiki?query=tag:linux" class="tag" target="_blank" rel="noopener nofollow">#linux</a> <br>OOM（Out-of-Memory）机制是内核的一部分，用于处理内存消耗过度的情况。OOM机制的责任是选择一个或多个高内存消耗的进程，并终止它们以释放内存。<br>在Linux中，进程的OOM Score决定了在出现内存不足的情况下，进程是最可能被终止的。OOM Score越高的进程被终止的可能性也就越大。<br>不应该直接修改一个进程的OOM Score。通常可以通过以下方式来影响OOM的行为：<br>调整系统级别的内存限制（例如sysctl调整vm.overcommit_memory限制）
调整内存使用（例如在应用程序代码中管理内存，或使用可调整的进程或容器限制）
调整OOM Score的全局设置（例如修改/proc/sys/vm/oom_score_adj值，该值影响所有进程）
<br>可以使用 sysctl 来调整Linux系统中的 vm.overcommit_memory 设置。这个设置控制Linux内核如何处理内存过度分配的情况。<br><br>以下是在 Linux 中使用 sysctl 修改 vm.overcommit_memory 设置的步骤：<br>
<br>确认当前的vm.overcommit_memory设置。在终端中运行以下命令：sysctl vm.overcommit_memory
<br>vm.overcommit_memory 有以下三个值可供选择：<br>
<br>
0: 表示内核将检查是否有足够的可用内存来满足进程的需要，如果没有足够的内存，内核将杀死进程并释放内存。

<br>
1：表示内核将允许进程分配所有它们请求的虚拟内存，并且不会检查物理内存是否足够。当物理内存不足时，进程可能会因无可用内存而崩溃。

<br>
2：表示内核将允许进程分配所有它们请求的虚拟内存，但会检查交换空间和物理内存是否足够。如果两者都不足，内核将杀死进程并释放内存。

<br>如果想将值设置为2，可以运行:  sudo sysctl vm.overcommit_memory=2<br>如果希望永久保存修改的值，可以将它们添加到 /etc/sysctl.conf 文件中。在文件的末尾添加以下行：vm.overcommit_memory = &lt;value&gt;<br>注意事项：请注意操作系统版本和内核版本是否支持您设置的值。另外，请谨慎地进行内存设置的更改，以免出现不可预料的行为。<br>调整内存使用<br>
<br>
在应用程序代码中管理内存：应用程序可以使用一些技术来管理内存，例如手动释放内存、使用内存池和内存重用等。可以帮助应用程序避免过度使用内存，从而减少出现 OOM 问题的概率。

<br>
使用可调整的进程或容器限制：可以使用可调整的进程或容器限制来限制进程或容器可以使用的内存量。这些限制可以根据需要进行调整，并可用于避免过度使用内存和 OOM 问题。

<br>
升级硬件：如果出现频繁的 OOM 问题，则可以考虑升级系统硬件，例如增加内存容量。这将提高系统的内存容量，并减少 OOM 问题的发生。

<br>调整 OOM Score 的全局设置<br>OOM Score 是 Linux 系统中用来确定进程优先级的指标，可以通过更改该值来调整系统中进程的优先级:<br>vm.overcommit_memory = 2
vm.overcommit_ratio = 80
vm.oom_kill_allocating_task = 0
<br>
<br>vm.overcommit_memory：表示系统内存充足性检查机制。0表示表示系统将会检查并根据需要减少内存使用，1表示系统将允许分配超出系统内存总量的内存（即虚拟内存），2表示所有的内存请求都分配，而不管交换空间是否够用。
<br>vm.overcommit_ratio：表示当开启 vm.overcommit_memory 时，向应用层分配物理内存与虚拟内存的比例。默认值为50%，所以将该值调整为80%可以给应用层更多的物理内存。
<br>vm.oom_kill_allocating_task：表示当系统出现 OOM（out of memory）时，是否杀掉正在分配内存的任务以释放内存。将该值设置为0可以让系统直接杀掉已存在的进程，而不是正在分配内存的进程。
<br>保存并退出文件，重启系统（或者执行以下命令使设置生效）：<br>sudo sysctl -p
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux-oom-的配置.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/Linux OOM 的配置.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 13 Oct 2024 12:18:28 GMT</pubDate></item><item><title><![CDATA[Linux 网络调优]]></title><description><![CDATA[ 
 <br>妈的，卡死了这网。<br><br><br>MTU是最大传输单元，指的是网络接口一次能传输的最大数据包大小，单位是字节，过大可能导致分片或丢包，过小则会增加协议开销，降低吞吐量。<br>关于优化网络性能，需要考虑不同场景下的最佳MTU值。比如，以太网默认是1500，但如果是VPN或者隧道，可能需要调整到更小，比如1400，以适应额外的头部开销。如果使用巨型帧（Jumbo Frames），MTU可以设置为9000，但这需要整个网络路径的支持。<br><br>可以通过 ping 命令测试不分片的最大 MTU：<br># 注：-s 指定数据部分大小，总包大小 = -s + 28（IP头20 + ICMP头8）
ping -M do -s 1472 192.168.1.1
<br>查看默认值:<br>ip link show 设备 | grep mtu
<br>调整方式:<br>sudo ifconfig interface mtu 1500
<br><br>可以通过 /proc/sys/net/core/ 下的文件加大 tcp 缓冲区的大小，除此之外，还可以根据自己的网络环境使用不同的拥塞控制算法：<br>CUBIC是Linux的默认算法，基于丢包反馈调整窗口，适合高带宽延迟积的网络。而BBR则关注带宽和RTT，避免依赖丢包，可能在高丢包环境下表现更好。Vegas是通过测量RTT变化来判断拥塞，可能在低延迟要求的场景有用。Westwood可能针对无线网络优化，因为无线网络容易有随机丢包。<br>传统的有线网络用CUBIC或Reno可能更合适，而移动网络可能适合Westwood或Vegas。BBR适合需要高吞吐且延迟敏感的环境，比如视频流。另外，像DCTCP适用于数据中心内部，因为处理的是短暂流量和低延迟需求。<br>简单来说：<br><br>详细来说：<br>
<br>CUBIC

<br>原理：基于丢包反馈的算法，通过三次函数调整拥塞窗口（Cubic 函数），逐步探测带宽上限。
<br>适用场景：高带宽延迟积（BDP）网络（如现代互联网），Linux 默认算法。
<br>优点：

<br>在高带宽长距离网络中表现稳定。
<br>兼容性强，适合混合网络环境。


<br>缺点：

<br>依赖丢包作为拥塞信号，可能在高丢包或动态网络中效率下降。




<br>Reno

<br>原理：经典 AIMD（加性增/乘性减）算法，通过慢启动、拥塞避免、快速重传和快速恢复管理窗口。
<br>适用场景：传统有线网络，低带宽或低延迟环境。
<br>优点：

<br>简单可靠，广泛兼容。


<br>缺点：

<br>对高带宽延迟积网络效率低。
<br>无法快速适应突发流量。




<br>BBR (Bottleneck Bandwidth and RTT)

<br>原理：基于带宽和 RTT 测量动态调整窗口，主动探测瓶颈带宽和最小延迟。
<br>适用场景：高吞吐、低延迟需求（如视频流、CDN），高丢包或波动网络（如卫星链路）。
<br>优点：

<br>避免依赖丢包信号，减少排队延迟。
<br>在高丢包网络中仍能保持高吞吐。


<br>缺点：

<br>需要（≥4.9） 以上的内核版本才支持。
<br>可能与其他算法共存时不公平占用带宽。




<br>Vegas

<br>原理：通过测量 RTT 变化预测拥塞，提前调整窗口以避免丢包。
<br>适用场景：低延迟敏感场景（如实时通信），低竞争网络。
<br>优点：

<br>减少丢包和排队延迟。
<br>更平滑的吞吐。


<br>缺点：

<br>在网络中存在非 Vegas 流时性能下降。
<br>对 RTT 测量误差敏感。




<br>Westwood/Westwood+

<br>原理：基于带宽估计调整窗口，优化无线网络中的随机丢包问题。
<br>适用场景：无线网络（Wi-Fi、移动网络），高随机丢包环境。
<br>优点：

<br>区分拥塞丢包和无线误码丢包，减少不必要降窗。


<br>缺点：

<br>对带宽估计依赖较高，复杂网络下可能不稳定。




<br>DCTCP (Data Center TCP)

<br>原理：利用 ECN（显式拥塞通知）标记，快速响应拥塞，保持低队列深度。
<br>适用场景：数据中心内部网络（短 RTT、高吞吐、低延迟需求）。
<br>优点：

<br>极低延迟和队列积压。
<br>适合突发流量。


<br>缺点：

<br>需要网络设备支持 ECN。
<br>不适用于广域网。




<br>HyStart (Hybrid Slow Start)

<br>原理：改进的慢启动算法，结合延迟和丢包信号避免过早进入拥塞避免阶段。
<br>适用场景：高带宽网络中避免激进慢启动导致的丢包。
<br>优点：

<br>减少慢启动阶段的过度膨胀。


<br>缺点：

<br>通常作为其他算法（如 CUBIC）的补充模块使用。




<br>BIC (Binary Increase Congestion Control)

<br>原理：通过二分搜索快速收敛到公平带宽分配，CUBIC 的前身。
<br>适用场景：早期高带宽网络。
<br>优点：

<br>在高 BDP 网络中比 Reno 更快收敛。


<br>缺点：

<br>已被 CUBIC 取代，算法较为陈旧。




<br>查看可用算法：<br>
sysctl net.ipv4.tcp_available_congestion_control<br>查看当前算法：<br>
sysctl net.ipv4.tcp_congestion_control<br>切换算法（如切换到 BBR）：<br>
sysctl -w net.ipv4.tcp_congestion_control=bbr<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://developer.aliyun.com/article/1593928" target="_blank">https://developer.aliyun.com/article/1593928</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux-网络调优.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/Linux 网络调优.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 31 Jan 2025 09:15:46 GMT</pubDate></item><item><title><![CDATA[systemctl freeze]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:linux" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#linux</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:systemctl" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#systemctl</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a> <a href="https://muqiuhan.github.io/wiki?query=tag:linux" class="tag" target="_blank" rel="noopener nofollow">#linux</a> <a href="https://muqiuhan.github.io/wiki?query=tag:systemctl" class="tag" target="_blank" rel="noopener nofollow">#systemctl</a><br>
systemctl freeze 会使用 cgroup 的 freeze 功能，让进程立刻冻结，但是保留进程运行时状态和上下文，调试完可以用 thaw 恢复，此外，systemctl whoami 可以立刻获取进程所属的单元（最近的那个单元，不是最远的层级）。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/systemctl-freeze.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/systemctl freeze.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:07 GMT</pubDate></item><item><title><![CDATA[udevd Operation and Configuration]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:linux" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#linux</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:linux" class="tag" target="_blank" rel="noopener nofollow">#linux</a> <a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a><br>The udevd daemon operates as follows:<br>
<br>The kernel sends udevd a notification event, called a uevent, through an internal network link.
<br>udevd loads all of the attributes in the uevent.
<br>udevd parses its rules, filters and updates the uevent based on<br>
those rules, and takes actions or sets more attributes accordingly. An incoming uevent that udevd receives from the kernel might look like this:
<br>ACTION=change
DEVNAME=sde
DEVPATH=/devices/pci0000:00/0000:00:1a.0/usb1/1-1/1-1.2/1-1.2:1.0/host4/
target4:0:0/4:0:0:3/block/sde
DEVTYPE=disk
DISK_MEDIA_CHANGE=1
MAJOR=8
MINOR=64
SEQNUM=2752
SUBSYSTEM=block
UDEV_LOG=3
<br>This particular event is a change to a device. After receiving the uevent, udevd knows the name of the device, the sysfs device path, and a number of other attributes associated with the properties; it is now ready to start processing rules.<br>
The rules files are in the /lib/udev/rules.d and /etc/udev/rules.d directories. The rules in /lib are the defaults, and the rules in /etc are overrides. A full explanation of the rules would be tedious, and you can learn much more from the udev(7) manual page, but here is some basic information about how udevd reads them:<br>
<br>udevd reads rules from start to finish of a rules file.
<br>After reading a rule and possibly executing its action, udevd<br>
continues reading the current rules file for more applicable rules.
<br>There are directives (such as GOTO) to skip over parts of rules files<br>
if necessary. These are usually placed at the top of a rules file to skip over the entire file if it’s irrelevant to a particular device that udevd is configuring.
<br>Let’s look at the symbolic links from the /dev/sda example. Those links were defined by rules in /lib/udev/rules.d/60-persistent-storage.rules. Inside, you’ll see the following lines:<br># ATA
KERNEL=="sd*[!0-9]|sr*", ENV{ID_SERIAL}!="?*", SUBSYSTEMS=="scsi",
ATTRS{vendor}=="ATA", IMPORT{program}="ata_id --export $devnode"
# ATAPI devices (SPC-3 or later)
KERNEL=="sd*[!0-9]|sr*", ENV{ID_SERIAL}!="?*", SUBSYSTEMS=="scsi",
ATTRS{type}=="5",ATTRS{scsi_level}=="[6-9]*", IMPORT{program}="ata_id --
export $devnode"
<br>These rules match ATA disks and optical media presented through the kernel’s SCSI subsystem. You can see that there are a few rules to catch different ways the devices may be represented, but the idea is that udevd will try to match a device starting with sd or sr but without a number (with the KERNEL =="sd*[!0-9]|sr*" expression), as well as a subsystem (SUBSYSTEMS=="scsi"), and, finally, some other attributes, depending on the type of device. If all of those conditional expressions are true in either of the rules, udevd moves to the next and final expression:<br>IMPORT{program}="ata_id --export $tempnode"
<br>This is not a conditional. Instead, it’s a directive to import variables from the /lib/udev/ata_id command. If you have such a disk, try it yourself on the command line. It will look like this:<br># /lib/udev/ata_id --export /dev/sda
ID_ATA=1
ID_TYPE=disk
ID_BUS=ata
ID_MODEL=WDC_WD3200AAJS-22L7A0
ID_MODEL_ENC=WDC\x20WD3200AAJS22L7A0\x20\x20\x20\x20\x20\x20\x20\x20\x20
\x20
\x20\x20\x20\x20\x20\x20\x20\x20\x20
ID_REVISION=01.03E10
ID_SERIAL=WDC_WD3200AAJS-22L7A0_WD-WMAV2FU80671
--snip--
<br>The import now sets the environment so that all of the variable names in this output are set to the values shown. For example, any rule that follows will now recognize ENV{ID_TYPE} as disk. In the two rules we’ve seen so far, of particular note is ID_SERIAL. In each rule, this conditional appears second:<br>ENV{ID_SERIAL}!="?*"
<br>This expression evaluates to true if ID_SERIAL is not set. Therefore, if ID_SERIAL is set, the conditional is false, the entire current rule does not apply, and udevd moves to the next rule.<br>
Why is this here? The purpose of these two rules is to run ata_id to find the serial number of the disk device and then add these attributes to the current working copy of the uevent. You’ll find this general pattern in many udev rules. With ENV{ID_SERIAL} set, udevd can now evaluate this rule later on in the rules file, which looks for any attached SCSI disks:<br>KERNEL=="sd*|sr*|cciss*", ENV{DEVTYPE}=="disk", ENV{ID_SERIAL}=="?
*",SYMLINK+="disk/by-id/$env{ID_BUS}-$env{ID_SERIAL}"
<br>You can see that this rule requires ENV{ID_SERIAL} to be set, and it has one directive:<br>SYMLINK+="disk/by-id/$env{ID_BUS}-$env{ID_SERIAL}"
<br>This directive tells udevd to add a symbolic link for the incoming device. So now you know where the device symbolic links came from! You may be wondering how to tell a conditional expression from a directive.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/udevd-operation-and-configuration.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/udevd Operation and Configuration.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:49:23 GMT</pubDate></item><item><title><![CDATA[Understanding Big and Little Endian Byte Order]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:arch" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#arch</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a> <a href="https://muqiuhan.github.io/wiki?query=tag:arch" class="tag" target="_blank" rel="noopener nofollow">#arch</a><br>Problems with byte order are frustrating, and I want to spare you the grief I experienced. Here's the key:<br>
<br>Problem: Computers speak different languages, like people. Some write data "left-to-right" and others "right-to-left".

<br>A machine can read its own data just fine - problems happen when one computer stores data and a different type tries to read it.


<br>Solutions

<br>Agree to a common format (i.e., all network traffic follows a single format), or
<br>Always include a header that describes the format of the data. If the header appears backwards, it means data was stored in the other format and needs to be converted.


<br><br>The most important concept is to recognize the difference between a number and the data that represents it.<br>A number is an abstract concept, such as a count of something. You have ten fingers. The idea of "ten" doesn't change, no matter what representation you use: ten, 10, diez (Spanish), ju (Japanese), 1010 (binary), X (Roman numeral)... these representations all point to the same concept of "ten".<br>Contrast this with data. Data is a physical concept, a raw sequence of bits and bytes stored on a computer. Data has no inherent meaning and must be interpreted by whoever is reading it.<br>Data is like human writing, which is simply marks on paper. There is no inherent meaning in these marks. If we see a line and a circle (like this: |O) we may interpret it to mean "ten".<br>But we assumed the marks referred to a number. They could have been the letters "IO", a moon of Jupiter. Or perhaps the Greek goddess. Or maybe an abbreviation for Input/Output. Or someone's initials. Or the number 2 in binary ("10"). The list of possibilities goes on.<br>The point is that a single piece of data (|O) can be interpreted in many ways, and the meaning is unclear until someone clarifies the intent of the author.<br>Computers face the same problem. They store data, not abstract concepts, and do so using a sequence of 1's and 0's. Later, they read back the 1's and 0's and try to recreate the abstract concept from the raw data. Depending on the assumptions made, the 1's and 0's can mean very different things.<br>Why does this problem happen? Well, there's no rule that all computers must use the same language, just like there's no rule all humans need to. Each type of computer is internally consistent (it can read back its own data), but there are no guarantees about how another type of computer will interpret the data it created.<br>Basic concepts<br>
<br>Data (bits and bytes, or marks on paper) is meaningless; it must be interpreted to create an abstract concept, like a number.
<br>Like humans, computers have different ways to store the same abstract concept. (i.e., we have many ways to say "ten": ten, 10, diez, etc.)
<br><br>Thankfully, most computers agree on a few basic data formats (this was not always the case). This gives us a common starting point which makes our lives a bit easier:<br>
<br>A bit has two values (on or off, 1 or 0)
<br>A byte is a sequence of 8 bits

<br>The "leftmost" bit in a byte is the biggest. So, the binary sequence 00001001 is the decimal number 9. 00001001 = (23 + 20 = 8 + 1 = 9).
<br>Bits are numbered from right-to-left. Bit 0 is the rightmost and the smallest; bit 7 is leftmost and largest.


<br>We can use these basic agreements as a building block to exchange data. If we store and read data one byte at a time, it will work on any computer. The concept of a byte is the same on all machines, and the idea of which byte is first, second, third (Byte 0, Byte 1, Byte 2...) is the same on all machines.<br>If computers agree on the order of every byte, what's the problem?<br>Well, this is fine for single-byte data, like ASCII text. However, a lot of data needs to be stored using multiple bytes, like integers or floating-point numbers. And there is no agreement on how these sequences should be stored.<br><br>Consider a sequence of 4 bytes, named W X Y and Z - I avoided naming them A B C D because they are hex digits, which would be confusing. So, each byte has a value and is made up of 8 bits.<br> Byte Name:    W       X       Y       Z<br>
Location:     0       1       2       3<br>
Value (hex):  0x12    0x34    0x56    0x78<br>For example, W is an entire byte, 0x12 in hex or 00010010 in binary. If W were to be interpreted as a number, it would be "18" in decimal (by the way, there's nothing saying we have to interpret it as a number - it could be an ASCII character or something else entirely).<br>With me so far? We have 4 bytes, W X Y and Z, each with a different value.<br><br>Pointers are a key part of programming, especially the C programming language. A pointer is a number that references a memory location. It is up to us (the programmer) to interpret the data at that location.<br>In C, when you cast a pointer to certain type (such as a char  or int ), it tells the computer how to interpret the data at that location. For example, let's declare<br>void p = 0; // p is a pointer to an unknown data type<br>
// p is a NULL pointer -- do not dereference<br>
char c;     // c is a pointer to a char, usually a single byte<br>Note that we can't get the data from p because we don't know its type. p could be pointing at a single number, a letter, the start of a string, your horoscope, an image -- we just don't know how many bytes to read, or how to interpret what's there.<br>Now, suppose we write<br>c = (char *)p;<br>Ah -- now this statement tells the computer to point to the same place as p, and interpret the data as a single character (char is typically a single byte, use uint8_t if not true on your machine). In this case, c would point to memory location 0, or byte W. If we printed c, we'd get the value in W, which is hex 0x12 (remember that W is a whole byte).<br>This example does not depend on the type of computer we have -- again, all computers agree on what a single byte is (in the past this was not the case).<br>The example is helpful, even though it is the same on all computers -- if we have a pointer to a single byte (char *, a single byte), we can walk through memory, reading off a byte at a time. We can examine any memory location and the endian-ness of a computer won't matter -- every computer will give back the same information.<br><br>Problems happen when computers try to read multiple bytes. Some data types contain multiple bytes, like long integers or floating-point numbers. A single byte has only 256 values, so can store 0 - 255.<br>Now problems start - when you read multi-byte data, where does the biggest byte appear?<br>
<br>Big endian machine: Stores data big-end first. When looking at multiple bytes, the first byte (lowest address) is the biggest.
<br>Little endian machine: Stores data little-end first. When looking at multiple bytes, the first byte is smallest.
<br>The naming makes sense, eh? Big-endian thinks the big-end is first. (By the way, the big-endian / little-endian naming comes from Gulliver's Travels, where the Lilliputans argue over whether to break eggs on the little-end or big-end. Sometimes computer debates are just as meaningful :-))<br>Again, endian-ness does not matter if you have a single byte. If you have one byte, it's the only data you read so there's only one way to interpret it (again, because computers agree on what a byte is).<br>Now suppose we have our 4 bytes (W X Y Z) stored the same way on a big-and little-endian machine. That is, memory location 0 is W on both machines, memory location 1 is X, etc.<br>We can create this arrangement by remembering that bytes are machine-independent. We can walk memory, one byte at a time, and set the values we need. This will work on any machine:<br>c = 0;     // point to location 0 (won't work on a real machine!)<br>
c = 0x12; // Set W's value<br>
c = 1;     // point to location 1<br>
c = 0x34; // Set X's value<br>
...        // repeat for Y and Z; details left to reader<br>This code will work on any machine, and we have both set up with bytes W, X, Y and Z in locations 0, 1, 2 and 3.<br><br>Now let's do an example with multi-byte data (finally!). Quick review: a "short int" is a 2-byte (16-bit) number, which can range from 0 - 65535 (if unsigned). Let's use it in an example:<br>short s; // pointer to a short int (2 bytes)<br>
s = 0;    // point to location 0; s is the value<br>So, s is a pointer to a short, and is now looking at byte location 0 (which has W). What happens when we read the value at s?<br>
<br>Big endian machine: I think a short is two bytes, so I'll read them off: location s is address 0 (W, or 0x12) and location s + 1 is address 1 (X, or 0x34). Since the first byte is biggest (I'm big-endian!), the number must be 256  byte 0 + byte 1, or 256W + X, or 0x1234. I multiplied the first byte by 256 (2^8) because I needed to shift it over 8 bits.<br>

<br>Little endian machine: I don't know what Mr. Big Endian is smoking. Yeah, I agree a short is 2 bytes, and I'll read them off just like him: location s is 0x12, and location s + 1 is 0x34. But in my world, the first byte is the littlest! The value of the short is byte 0 + 256  byte 1, or 256X + W, or 0x3412.<br>

<br>Keep in mind that both machines start from location s and read memory going upwards. There is no confusion about what location 0 and location 1 mean. There is no confusion that a short is 2 bytes.<br>But do you see the problem? The big-endian machine thinks s = 0x1234 and the little-endian machine thinks s = 0x3412. The same exact data gives two different numbers. Probably not a good thing.<br><br>Let's do another example with 4-byte integer for "fun":<br>int i; // pointer to an int (4 bytes on 32-bit machine)<br>
i = 0;  // points to location zero, so i is the value there<br>Again we ask: what is the value at i?<br>
<br>Big endian machine: An int is 4 bytes, and the first is the largest. I read 4 bytes (W X Y Z) and W is the largest. The number is 0x12345678.
<br>Little endian machine: Sure, an int is 4 bytes, but the first is smallest. I also read W X Y Z, but W belongs way in the back -- it's the littlest. The number is 0x78563412.
<br>Same data, different results - not a good thing. Here's an interactive example using the numbers above, feel free to plug in your own:<br>
<img alt="Pasted image 20241220110455.png" src="https://muqiuhan.github.io/wiki/computer-science/operating-system/understanding-big-and-little-endian-byte-order/attachments/pasted-image-20241220110455.png"><br><br>Issues with byte order are sometimes called the NUXI problem: UNIX stored on a big-endian machine can show up as NUXI on a little-endian one.<br>Suppose we want to store 4 bytes (U, N, I and X) as two shorts: UN and IX. Each letter is a entire byte, like our WXYZ example above. To store the two shorts we would write:<br>short s; // pointer to set shorts<br>
s = 0;    // point to location 0<br>
s = UN;  // store first short: U  256 + N (fictional code)<br>
s = 2;    // point to next location<br>
s = IX;  // store second short: I * 256 + X<br>This code is not specific to a machine. If we store "UN" on a machine and ask to read it back, it had better be "UN"! I don't care about endian issues, if we store a value on one machine and read it back on the same machine, it must be the same value.<br>However, if we look at memory one byte at a time (using our char * trick), the order could vary. On a big endian machine we see:<br>Byte:     U N I X<br>
Location: 0 1 2 3<br>Which make sense. U is the biggest byte in "UN" and is stored first. The same goes for IX: I is the biggest, and stored first.<br>On a little-endian machine we would see:<br>Byte:     N U X I<br>
Location: 0 1 2 3<br>And this makes sense also. "N" is the littlest byte in "UN" and is stored first. Again, even though the bytes are stored "backwards" in memory, the little-endian machine knows it is little endian, and interprets them correctly when reading the values back. Also, note that we can specify hex numbers such as x = 0x1234 on any machine. Even a little-endian machine knows what you mean when you write 0x1234, and won't force you to swap the values yourself (you specify the hex number to write, and it figures out the details and swaps the bytes in memory, under the covers. Tricky.).<br>This scenario is called the "NUXI" problem because byte sequence UNIX is interpreted as NUXI on the other type of machine. Again, this is only a problem if you exchange data -- each machine is internally consistent.<br><br>Computers are connected - gone are the days when a machine only had to worry about reading its own data. Big and little-endian machines need to talk and get along. How do they do this?<br><br>The easiest approach is to agree to a common format for sending data over the network. The standard network order is actually big-endian, but some people get uppity that little-endian didn't win... we'll just call it "network order".<br>To convert data to network order, machines call a function hton (host-to-network). On a big-endian machine this won't actually do anything, but we won't talk about that here (the little-endians might get mad).<br>But it is important to use hton before sending data, even if you are big-endian. Your program may be so popular it is compiled on different machines, and you want your code to be portable (don't you?).<br>Similarly, there is a function ntoh (network to host) used to read data off the network. You need this to make sure you are correctly interpreting the network data into the host's format. You need to know the type of data you are receiving to decode it properly, and the conversion functions are:<br> htons() - "Host to Network Short"<br>
htonl() - "Host to Network Long"<br>
ntohs() - "Network to Host Short"<br>
ntohl() - "Network to Host Long"<br>Remember that a single byte is a single byte, and order does not matter.<br>These functions are critical when doing low-level networking, such as verifying the checksums in IP packets. If you don't understand endian issues correctly your life will be painful - take my word on this one. Use the translation functions, and know why they are needed.<br><br>The other approach is to include a magic number, such as 0xFEFF, before every piece of data. If you read the magic number and it is 0xFEFF, it means the data is in the same format as your machine, and all is well.<br>If you read the magic number and it is 0xFFFE (it is backwards), it means the data was written in a format different from your own. You'll have to translate it.<br>A few points to note. First, the number isn't really magic, but programmers often use the term to describe the choice of an arbitrary number (the BOM could have been any sequence of different bytes). It's called a byte-order mark because it indicates the byte order the data was stored in.<br>Second, the BOM adds overhead to all data that is transmitted. Even if you are only sending 2 bytes of data, you need to include a 2-byte BOM. Ouch!<br>Unicode uses a BOM when storing multi-byte data (some Unicode character encodings can have 2, 3 or even 4-bytes per character). XML avoids this mess by storing data in UTF-8 by default, which stores Unicode information one byte at a time. And why is this cool?<br>(Repeated for the 56th time) "Because endian issues don't matter for single bytes".<br>Right you are.<br>Again, other problems can arise with BOM. What if you forget to include the BOM? Do you assume the data was sent in the same format as your own? Do you read the data and see if it looks "backwards" (whatever that means) and try to translate it? What if regular data includes the BOM by coincidence? These situations are not fun.<br><br>Ah, what a philosophical question.<br>Each byte-order system has its advantages. Little-endian machines let you read the lowest-byte first, without reading the others. You can check whether a number is odd or even (last bit is 0) very easily, which is cool if you're into that kind of thing. Big-endian systems store data in memory the same way we humans think about data (left-to-right), which makes low-level debugging easier.<br>But why didn't everyone just agree to one system? Why do certain computers have to try and be different?<br>Let me answer a question with a question: Why doesn't everyone speak the same language? Why are some languages written left-to-right, and others right-to-left?<br>Sometimes communication systems develop independently, and later need to interact.<br><br>Endian issues are an example of the general encoding problem - data needs to represent an abstract concept, and later the concept needs to be created from the data. This topic deserves its own article (or series), but you should have a better understanding of endian issues. More information:<br>
<br><a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Endianness" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Endianness" target="_blank">Wikipedia entry</a>
<br><a data-tooltip-position="top" aria-label="http://www.rdrop.com/~cary/html/endian_faq.html" rel="noopener nofollow" class="external-link" href="http://www.rdrop.com/~cary/html/endian_faq.html" target="_blank">Endian Faq</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/understanding-big-and-little-endian-byte-order/understanding-big-and-little-endian-byte-order.html</link><guid isPermaLink="false">Computer Science/Operating System/Understanding Big and Little Endian Byte Order/Understanding Big and Little Endian Byte Order.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:44:22 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/operating-system/understanding-big-and-little-endian-byte-order/attachments/pasted-image-20241220110455.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/operating-system/understanding-big-and-little-endian-byte-order/attachments/pasted-image-20241220110455.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[FreeBSD 允许非 Root 用户使用 80 端口]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:freebsd" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#freebsd</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:freebsd" class="tag" target="_blank" rel="noopener nofollow">#freebsd</a> <a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a><br>In experimenting with FreeNAS jails I wanted to allow a web service to use port 80. Normally 80 is a high order port reserved for root-level processes for security reasons. Since this is a FreeBSD jail and not a full on system I’m not worried about this.<br>The command to do so is fairly simple (thanks to <a data-tooltip-position="top" aria-label="http://hyber.org/privbind.yaws" rel="noopener nofollow" class="external-link" href="http://hyber.org/privbind.yaws" target="_blank">this page</a> for information)<br>sysctl net.inet.ip.portrange.reservedhigh=0<br>The above command is not permanent; to make it so add it to /etc/sysctl.conf:<br>echo "net.inet.ip.portrange.reservedhigh=0" &gt;&gt; /etc/sysctl.conf]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/freebsd-允许非-root-用户使用-80-端口.html</link><guid isPermaLink="false">Computer Science/Operating System/FreeBSD 允许非 Root 用户使用 80 端口.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:49:23 GMT</pubDate></item><item><title><![CDATA[多线程的一点小想法]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:multi-thread" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#multi-thread</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:multi-thread" class="tag" target="_blank" rel="noopener nofollow">#multi-thread</a> <a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a> <br>标准的做法是：如果计算机有 32 个核心，那么开一个 FIFO 队列，把要计算的数据扔到这个队列里，然后开 36 个线程，每个线程去队列中取数据，算完一个再去队列中取一下个数据....直到队列中没数据。<br>CPU 密集型任务比 CPU 核心数 (如果 CPU 有超线程技术，可以按算上超线程的核心数) 略多几个是最快的，过多的线程反而会更慢，没有空闲的核心，过多的线程只能等待，并没有并行运算效果，反而线程不停的切来切去，浪费性能，通常每次切换过程需要耗费大约 1000 个 CPU 时钟周期。<br>略多几个的原因是，CPU 在真正运算开始前，需要等待数据被装入寄存器，而数据从内存装入寄存器，也需要一些时间，略多几个，可以让 CPU 在等待数据就绪时，切换到数据已经就绪的线程上开始运算。 (多发射的乱序执行也是解决内存速度跟不上cpu的一种有效手段，但是并不是切换到另一个线程)<br>如果数据在内存中连续，数据量不大 (相对CPU高速缓存) ，那么装载数据到寄存器的开销就会比较小，就可以附加少一点的线程，比如 1~2 个就够了，反之，数据分散，缓存命中率低，就需要多几个线程，比如 6~8 个。<br>另外，还可以考虑使用 SIMD 加速计算，如果数据量再多，还可以考虑CUDA-GPU加速。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/多线程的一点小想法.html</link><guid isPermaLink="false">Computer Science/Operating System/多线程的一点小想法.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:10 GMT</pubDate></item><item><title><![CDATA[Google 的 monorepo 实践与 trunk based development]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:git" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#git</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:git" class="tag" target="_blank" rel="noopener nofollow">#git</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <br><br>早期的 Google 员工决定使用通过集中式源代码控制系统管理的共享代码库。这种方法已经为 Google 服务了超过 16 年，如今 Google 的绝大多数软件资产继续存储在单一的、共享的存储库中。与此同时，Google 软件开发人员的数量稳步增加，Google 代码库的规模呈指数级增长。因此，用于托管代码库的技术也有了显著的发展。<br><img alt="Pasted image 20250102153026.png" src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102153026.png"><br>Google使用自主开发的版本控制系统来托管一个大型 monorepo，该代码库对公司中的大多数软件开发人员可见并供其使用。这个集中式系统是 Google 许多开发人员工作流程的基础。除此之外还有“trunk-based development”策略以及构建工作流和保持 Google 代码库健康的支持系统，包括用于静态分析、代码清理和简化代码审查等软件。<br><br>Google代码库包含大约10亿个文件，在 Google 18 年的历史中大约有 3500 万次提交，该存储库包含 86TB 的数据，包括大约 900 万个源文件，20 亿行代码。<br>2014年，谷歌存储库中大约每周都涉及 25 万个文件约 1500 万行代码的更改。这个存储库中的内容被来自世界各国数十个分公司的 25,000 多名谷歌软件开发人员共享。在一个常规的工作日中，他们向代码库提交的更改高达 16,000 项，另外 24,000 项更改由自动化系统提交。每天，存储库处理数十亿个文件读取请求，在流量高峰期间，每秒查询率约为 800,000次，平均每个工作日约为每秒 500,000 次。这些流量大部分来自谷歌的分布式构建和测试系统。<br><img alt="Pasted image 20250102153707.png" src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102153707.png"><br><img alt="Pasted image 20250102153716.png" src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102153716.png"><br>2012年10月，Google 的中央存储库增加了对 Windows 和 Mac 用户的支持（在此之前仅支持Linux），每周提交图显示，直到2012年，提交率一直由人类用户主导，当时谷歌转向了自主的源码控制实现。<br><img alt="Pasted image 20250102153837.png" src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102153837.png"><br>管理这种规模的存储库及其工作活动对谷歌来说一直是一个持续的挑战。尽管进行了几年的实验，谷歌还是没能找到一个商业可用或开源的版本控制系统来支持这种规模的单个存储库。于是出现了一个为存储、版本控制这个代码库而构建的专有系统，代号为 Piper。<br><br>在阐述使用 monorepo 的优缺点之前，需要一些关于Google工具和工作流程的背景知识。<br><br>Piper存储一个大型存储库，并在标准的 Google 基础架构之上实现。Piper 分布在全球 10 个 Google 数据中心中，依靠 Paxos 算法来保证跨 replicas 的一致性。这种架构提供了高度的冗余（在 Paxos 算法中，"redundancy" 通常指的是系统中为了提高可靠性和容错能力而引入的冗余机制。），并有助于优化 Google 软件开发人员的使用体验。此外，缓存和异步操作对开发人员屏蔽了大部分网络延迟（这很重要，因为 Google 基于云的工具链的全部 feature 需要开发人员在线使用。）<br>在 Piper 发布之前的 10 多年里，谷歌一直依赖于一个托管在一台机器上的主要 Perforce实例，以及定制的缓存基础设施。持续增大的存储库是 Google 开发 Piper 的动机。<br>由于谷歌的源代码是公司最重要的资产之一，安全性也是 Piper 设计中的一个关键考虑因素。Piper 支持文件级权限控制。大多数存储库对所有 Piper 用户都是可见的；然而，重要的配置文件或包括关键业务算法在内的文件会得到更严格的控制。此外，对 Piper 中文件的读写访问都有记录。如果敏感数据意外地提交给 Piper，可以很轻松的删掉有问题的文件。日志允许管理员在删除有问题的文件之前确定是否有人访问了该文件。<br>在 Piper 工作流中，开发人员在更改之前先在存储库中创建文件的本地副本。这些文件存储在开发人员拥有的工作区中。Piper 工作区就相当于 Apache Subversion 中的工作副本、Git 中的本地克隆或 Perforce 中的客户端。Piper 存储库中的更新可以根据需要拉入工作区并与正在进行的工作合并。工作区的快照可以与其他开发人员共享以进行重新查看。工作区中的文件只有在完成谷歌代码审查过程后才会提交到中央存储库。<br><img alt="Pasted image 20250102155121.png" src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102155121.png"><br>所有对文件的写入都以快照的形式存储在 CitC 中，这样就可以根据需要重新覆盖以前的工作。快照可以被显式命名、恢复或标记以供审查。CitC 工作区可以在任何可以连接到基于云的存储系统的机器上使用，这使切换到另一台机器上无缝的继续工作变得方便。<br>开发人员还能够在 CitC 工作区中查看彼此的工作。在云中存储所有正在进行的工作是谷歌工作流过程的一个重要元素。<br>一些工作流利用 CitC 使软件开发人员能够更高效地使用大型代码库。例如，当发送更改以进行代码审查时，开发人员可以启用自动提交选项，这在代码作者和审查者处于不同时区时特别有用。当审查被标记为完成时，测试将运行；如果通过，代码将被提交到存储库，而无需进一步的人工干预。<br>Piper 也可以在没有 CitC 的情况下使用。开发人员可以将 Piper 工作区存储在他们的本地机器上。Piper 与 Git 的互操作性较为有限。如今，超过 80% 的Piper用户使用 CitC，由于 CitC 提供的许多优势，采用率持续增长。<br>Piper 和 CitC 使得在谷歌这种规模的代码量上使用单一的、整体的存储库成为可能。这些系统的设计和架构都受到谷歌采用的 Trunk-based development. 开发范式的严重影响。<br><br>Google的 "Trunk-Based Development"（TBD）策略是一种软件开发方法，其中开发者频繁地将代码变更集成到一个共享的主分支（称为trunk或mainline）中，而不是在长期存在的特性分支上工作，这种方法强调持续交付工作的软件，与敏捷方法论相辅相成。<br><br>
<br>主分支是代码库最可靠的来源，允许开发者定期集成变更以维护稳定性。
<br>开发者在一天中多次将代码变更集成到主分支，减少在单独分支上花费的时间，并允许早期发现和解决集成问题。
<br>专注于对代码进行小的、自包含的变更，这种方法将工作分解成更小的部分，有助于减少复杂性、最小化冲突，并加快集成和审查过程。
<br>使用特性开关可以将代码部署与新功能的发布分开，允许逐步推出新功能，进行A/B测试，必要时容易回滚。
<br>定期稳定主分支是维护其可发布状态的常见做法。团队分配专门时间来解决集成问题、解决冲突以及修复任何回归或缺陷。
<br>作为持续集成过程的一部分，定期进行自动化测试，如单元测试和集成测试。这种方法让团队能够快速识别和解决回归、兼容性和功能问题。
<br>TBD 鼓励持续集成，开发者频繁将变更合并到主线中。这种方法确保代码定期集成，有助于早期识别和解决集成问题，减少大型和复杂合并的风险，并使团队能够尽早发现问题。通过在共享主干上工作更有效地协作。多个团队成员可以同时在不同功能上工作，无需另开长时间线的特性分支。这种开发策略使团队能够将较小的、增量变更交付到生产环境中。<br>TBD 这种迭代方法促进了来自用户和利益相关者的快速反馈，使根据他们的反馈进行适应和整合变更变得更加容易。由于长时间线的特性分支可能导致代码冲突和复杂性显著增加。相比之下，TBD 通过最小化分支分离的时间来保持代码库的可维护性。<br><br>也称为功能标志，是 TBD 中用于分离代码部署和功能发布的工具。它们允许团队在不改变代码或只修改少量代码的情况下改变系统行为，从而控制功能的发布、权限策略、测试策略和控制突发事件等。<br>
<br>团队可以逐步推出新功能，进行 A/B 测试，并在必要时轻松回滚。例如，在开发一个新用户界面时，可以通过特性开关在生产环境中隐藏该界面，直到其实现完成。
<br>和分支抽象（Branch-by-Abstraction）技术结合使用，允许团队在不中断现有功能的情况下，逐步替换旧的实现。这种技术使得新旧代码可以并行存在，直到新代码完全准备好发布。
<br>特性开关可以解决多个开关之间的依赖问题，例如，可以控制一个开关为真，则另一个开关不能为真。此外，它们还可以在开关变化时触发副作用，如数据库更新或远程API 调用，这通过所谓的“开关伴侣”（toggle companions）实现。
<br><br>发布分支的管理是TBD中的另一个重要方面，特别是在需要隔离不兼容的开发或延迟发布功能时。<br>通常，实施TBD的团队会在发布前几天拉出一个发布分支，以保留线上环境的代码，同时不影响其他开发继续往主干分支上提交代码。这种方式允许团队在计划发布的间隙进行bug修复。<br>根据不兼容策略，发布分支不应该继续承载新功能开发。发布分支的目的是隔离不兼容的代码，而不是作为新功能开发的地方。<br>在某些情况下，发布分支在发布后会被删除。这是为了保持代码库的清洁和集中管理，避免旧的发布分支成为未来的负担。<br>对于高效的持续交付团队，如果他们在生产环境中发现了bug，他们可以选择向前推进策略，即直接在主干分支上修复bug，然后从主干分支发布到生产环境，这样可以忽略发布分支。<br><img alt="Pasted image 20250102162058.png" src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102162058.png"><br><br>钻石依赖问题（Diamond Dependency Problem）是一个在软件工程中常见的依赖管理问题，尤其在处理多层次依赖关系时。这个问题通常发生在以下情况：当有两个或更多的依赖项共享同一个公共依赖项，并且这些依赖项需要这个公共依赖项的不同版本时。<br>具体来说，钻石依赖问题可以通过一个简单的图形来解释：<br>      A
     / \
    B   C
     \ /
      D
<br>在这个图形中，组件A依赖于组件B和C，而B和C又都依赖于组件D。但是，如果B需要D的版本1，而C需要D的版本2，那么组件A就面临一个问题：它无法同时满足B和C的依赖要求。这就形成了一个所谓的“钻石形状”的依赖结构，因为从A向下看，B和C在D处相遇，形成了钻石的四个顶点。<br>在Google的实践中，由于所有代码都存储在同一个代码库中，钻石依赖问题基本可以避免。因为当一个团队需要增加功能时，所有团队都会接收到增加的新功能，保持所有人处于依赖项的 HEAD 版。在这种模型中，版本号的概念并不存在，因为所有团队都使用同一版本的代码。这种方法减少了版本冲突的可能性，并简化了依赖管理。<br>
<img alt="Pasted image 20250102162554.png" src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102162554.png"><br><br>
<br><a data-tooltip-position="top" aria-label="https://dl.acm.org/doi/pdf/10.1145/2854146" rel="noopener nofollow" class="external-link" href="https://dl.acm.org/doi/pdf/10.1145/2854146" target="_blank">Why Google Stores Billions of Lines of Code in a Single Repository</a>
<br><a data-tooltip-position="top" aria-label="https://cloud.tencent.com/developer/article/1505551" rel="noopener nofollow" class="external-link" href="https://cloud.tencent.com/developer/article/1505551" target="_blank">版本分支管理标准 - Trunk Based Development 主干开发模型</a>
<br><a data-tooltip-position="top" aria-label="https://geekdaxue.co/books/Software-Engineering-at-Google" rel="noopener nofollow" class="external-link" href="https://geekdaxue.co/books/Software-Engineering-at-Google" target="_blank">《Software Engineering at Google》中文版-谷歌的软件工程</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/GoogleCloudPlatform/cloud-opensource-java/diffs/1?base_sha=aa48adecbc4a7e63d9c6d9b27a7bf8809a30a438&amp;head_user=garrettjonesgoogle&amp;name=master&amp;pull_number=1253&amp;qualified_name=refs%2Fheads%2Fmaster&amp;sha1=aa48adecbc4a7e63d9c6d9b27a7bf8809a30a438&amp;sha2=980288e5355d90d61fe7d552bcdd468cc390c92a&amp;short_path=bfe1c71&amp;unchanged=expanded&amp;w=false" rel="noopener nofollow" class="external-link" href="https://github.com/GoogleCloudPlatform/cloud-opensource-java/diffs/1?base_sha=aa48adecbc4a7e63d9c6d9b27a7bf8809a30a438&amp;head_user=garrettjonesgoogle&amp;name=master&amp;pull_number=1253&amp;qualified_name=refs%2Fheads%2Fmaster&amp;sha1=aa48adecbc4a7e63d9c6d9b27a7bf8809a30a438&amp;sha2=980288e5355d90d61fe7d552bcdd468cc390c92a&amp;short_path=bfe1c71&amp;unchanged=expanded&amp;w=false" target="_blank">What is a diamond dependency conflict?</a>
<br><a data-tooltip-position="top" aria-label="https://koenvangilst.nl/blog/trunkbased-development" rel="noopener nofollow" class="external-link" href="https://koenvangilst.nl/blog/trunkbased-development" target="_blank">Why I Prefer Trunk-Based Development</a>
<br><a data-tooltip-position="top" aria-label="https://edgamat.com/2022/03/27/Using-Release-Feature-Toggles.html" rel="noopener nofollow" class="external-link" href="https://edgamat.com/2022/03/27/Using-Release-Feature-Toggles.html" target="_blank">Using Release Feature Toggles for Trunk-Based Development</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/google-的-monorepo-实践与-trunk-based-development.html</link><guid isPermaLink="false">Computer Science/Other/Google 的 monorepo 实践与 trunk based development/Google 的 monorepo 实践与 trunk based development.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:44:47 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102153026.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/other/google-的-monorepo-实践与-trunk-based-development/attachments/pasted-image-20250102153026.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Obsidian custom font to Android iOS devices]]></title><description><![CDATA[ 
 <br>Hi, I want to share how I added a custom font to Android/iOS devices. This technique employs Obsidian’s CSS snippet feature so you do not need to install or create a custom theme.<br>You need a WOFF2 file for your custom font. If you only have TTF file or something, convert them to WOFF2. There are plenty of tools online to do this.

Convert WOFF2 to base64-encoded string and embed it in fonts.css file (you can name this file as you want). You can do this on https://hellogreg.github.io/woff2base/ 1.3k. Copy the generated string to your CSS file.

Add your fonts.css under .obsidian/snippets/ folder.

Open Obsidian and go to Settings &gt; Appearance &gt; CSS snippets and enable your fonts.css.

Now Obsidian will recognize your custom font even if your phone does not have it. You can go to Settings &gt; Appearance &gt; Font and change the font to yours.
<br>If Obsidian does not recognize your font, check the font-family property in your fonts.css. font-familiy property and Settings &gt; Appearance &gt; Font setting must exactly match.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/other/obsidian-custom-font-to-android-ios-devices.html</link><guid isPermaLink="false">Computer Science/Other/Obsidian custom font to Android iOS devices.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:04 GMT</pubDate></item><item><title><![CDATA[Obsidian Vault 的 .obsidian 目录中的各文件作用]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:obsidian" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#obsidian</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:obsidian" class="tag" target="_blank" rel="noopener nofollow">#obsidian</a><br>
一个简单的例子是:<br>.obsidian
├── appearance.json
├── app.json
├── community-plugins.json
├── core-plugins.json
├── core-plugins-migration.json
├── plugins
│&nbsp;&nbsp; └── obsidian-git
│&nbsp;&nbsp;     ├── data.json
│&nbsp;&nbsp;     ├── main.js
│&nbsp;&nbsp;     ├── manifest.json
│&nbsp;&nbsp;     └── styles.css
└── workspace.json
<br>其中：<br>
<br>appearance.json 包含当前 vault 的外观设置，例如主题、字体大小和行高，例如:
<br>{  
 "interfaceFontFamily": "HarmonyOS Sans SC",  
 "textFontFamily": "HarmonyOS Sans SC",  
 "monospaceFontFamily": "FrankMono",  
 "accentColor": "",  
 "cssTheme": "Listive",  
 "nativeMenus": false  
}
<br>
<br>app.json 包含了编辑器的一些设置，例如 vim mode 和断行设置:
<br>{  
 "vimMode": true,  
 "strictLineBreaks": false,  
 "promptDelete": false  
}
<br>
<br>community-plugins.json：包含有关当前 vault 中安装的社区插件的数据，例如:
<br>[  
 "dataview",  
 "novel-word-count",  
 "remotely-save",  
 "obsidian-tasks-plugin",  
 "obsidian-style-settings",  
 "obsidian-advanced-slides",  
 "calendar",  
 "3d-graph-new",  
 "webpage-html-export"  
]
<br>
<br>core-plugins.json: 就是当前 valut 中的所有内置插件，例如:
<br> "file-explorer",  
 "global-search",  
 "switcher",  
 "graph",  
 "backlink",  
 "canvas",  
 "outgoing-link",  
 "tag-pane",  
 "page-preview",  
 "daily-notes",  
 "templates",  
 "note-composer",  
 "command-palette",  
 "editor-status",  
 "bookmarks",  
 "outline",  
 "word-count",  
 "file-recovery"  
]
<br>	- `core-plugins-migration.json`: 就是 `core-plugins.json` 中的内置插件的开关状态，例如:
<br>{  
 "file-explorer": true,  
 "global-search": true,  
 "switcher": true,  
 "graph": true,  
 "backlink": true,  
 "canvas": true,  
 "outgoing-link": true,  
 "tag-pane": true,  
 "properties": false,  
 "page-preview": true,  
 "daily-notes": true,  
 "templates": true,  
 "note-composer": true,  
 "command-palette": true,  
 "slash-command": false,  
 "editor-status": true,  
 "bookmarks": true,  
 "markdown-importer": false,  
 "zk-prefixer": false,  
 "random-note": false,  
 "outline": true,  
 "word-count": true,  
 "slides": false,  
 "audio-recorder": false,  
 "workspaces": false,  
 "file-recovery": true,  
 "publish": false,  
 "sync": false  
}
<br>
<br>workspace.json：包含有关当前 Vault 工作区的数据，包括打开的文件、窗口和布局的设置，例如：
<br>{  
 "main": {  
   "id": "7bc59326f5497d4a",  
   "type": "split",  
   "children": [  
     {  
       "id": "06fc674d29eb4c3a",  
       "type": "tabs",  
       "children": [  
         {  
           "id": "97d62f14e2fad2f0",  
           "type": "leaf",  
           "state": {  
             "type": "markdown",  
             "state": {  
               "file": "README.md",  
               "mode": "source",  
               "source": false  
             }  
           }  
         },  
         {  
           "id": "c94063b5e5f6f995",  
           "type": "leaf",  
           "state": {  
             "type": "markdown",  
             "state": {  
               "file": "README.md",  
               "mode": "source",  
               "source": false  
             }  
           }  
         },  
         {  
           "id": "143d5ea7a56d7e1d",  
           "type": "leaf",  
           "state": {  
             "type": "markdown",  
             "state": {  
               "file": "测试.md",  
               "mode": "source",  
               "source": false  
             }  
           }  
         }  
       ],  
       "currentTab": 2  
     }
			...
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/other/obsidian-vault-的-.obsidian-目录中的各文件作用.html</link><guid isPermaLink="false">Computer Science/Other/Obsidian Vault 的 .obsidian 目录中的各文件作用.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:07 GMT</pubDate></item><item><title><![CDATA[多线程的一点小想法]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:multi-thread" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#multi-thread</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:multi-thread" class="tag" target="_blank" rel="noopener nofollow">#multi-thread</a><br>标准的做法是：如果计算机有 32 个核心，那么开一个 FIFO 队列，把要计算的数据扔到这个队列里，然后开 36 个线程，每个线程去队列中取数据，算完一个再去队列中取一下个数据....直到队列中没数据。<br>CPU 密集型任务比 CPU 核心数 (如果 CPU 有超线程技术，可以按算上超线程的核心数) 略多几个是最快的，过多的线程反而会更慢，没有空闲的核心，过多的线程只能等待，并没有并行运算效果，反而线程不停的切来切去，浪费性能，通常每次切换过程需要耗费大约 1000 个 CPU 时钟周期。<br>略多几个的原因是，CPU 在真正运算开始前，需要等待数据被装入寄存器，而数据从内存装入寄存器，也需要一些时间，略多几个，可以让 CPU 在等待数据就绪时，切换到数据已经就绪的线程上开始运算。 (多发射的乱序执行也是解决内存速度跟不上cpu的一种有效手段，但是并不是切换到另一个线程)<br>如果数据在内存中连续，数据量不大 (相对CPU高速缓存) ，那么装载数据到寄存器的开销就会比较小，就可以附加少一点的线程，比如 1~2 个就够了，反之，数据分散，缓存命中率低，就需要多几个线程，比如 6~8 个。<br>另外，还可以考虑使用 SIMD 加速计算，如果数据量再多，还可以考虑CUDA-GPU加速。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/other/多线程的一点小想法.html</link><guid isPermaLink="false">Computer Science/Other/多线程的一点小想法.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 11:51:21 GMT</pubDate></item><item><title><![CDATA[寻找并删除 Git 记录中的大文件]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:git" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#git</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:git" class="tag" target="_blank" rel="noopener nofollow">#git</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a><br>
最近发现 <a data-tooltip-position="top" aria-label="https://github.com/harttle/harttle.github.io" rel="noopener nofollow" class="external-link" href="https://github.com/harttle/harttle.github.io" target="_blank">HarttleLand 的 Git 仓库</a> 已经达到了 142M，严重影响 Fork 和 Clone。 今晨 Harttle 从 Git 记录中定位了数百个大文件并将其删除，现在仓库恢复了 27M 的大小。 借此机会，本文来介绍查找和重写 Git 记录的命令：git rev-list, git filter-branch。 本文用于学习用途，生产环境请考虑使用 <a data-tooltip-position="top" aria-label="https://rtyley.github.io/bfg-repo-cleaner/" rel="noopener nofollow" class="external-link" href="https://rtyley.github.io/bfg-repo-cleaner/" target="_blank">bfg</a> 等效率工具（感谢 oott123 的评论）。<br>首先通过 rev-list 来找到仓库记录中的大文件：<br>git rev-list --objects --all | grep "$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk '{print$1}')"
<br>然后通过 filter-branch 来重写这些大文件涉及到的所有提交（重写历史记录）：<br>git filter-branch -f --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch your-file-name' --tag-name-filter cat -- --all
<br><br>
如果你熟知 Git 的存储方式，跳过此节。
<br>Git 仓库位于项目根目录的 .git 文件夹，其中保存了从仓库建立（git init）以来所有的代码增删。 每一个提交（Commit）相当于一个 Patch 应用在之前的项目上，借此一个项目可以回到任何一次提交时的文件状态。<br>于是在 Git 中删除一个文件时，Git 只是记录了该删除操作，该记录作为一个 Patch 存储在 .git 中。 删除前的文件仍然在 Git 仓库中保存着。直接删除文件并提交起不到给 Git 仓库瘦身的效果。<br>在 Git 仓库彻底删除一个文件只有一种办法：重写（Rewrite）涉及该文件的所有提交。 幸运的是借助 git filter-branch 便可以重写历史提交，当然这也是 Git 中最危险的操作。 可以说比 rm -rf * 危险一万倍。<br><br>我清楚地记得曾提交过名为 recent-badge.psd 的文件。这是一个很大的 PhotoShop 文件，我要把它删掉。 filter-branch 命令可以用来重写 Git 仓库中的提交， 利用 filter-branch 的 --index-filter 参数便能把它从所有 Git 提交中删除。<br>$ git filter-branch -f --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch assets/img/recent-badge.psd' --tag-name-filter cat -- --all
Rewrite 2771f50d45a0293668a30af77983d87886441640 (264/982)rm 'assets/img/recent-badge.psd'
Rewrite 1a98ecb3f39e1f200e31754714eec18bc92848ce (265/982)rm 'assets/img/recent-badge.psd'
Rewrite d4e61cfb1d88187b0561d283e663b81b738df2c7 (270/982)rm 'assets/img/recent-badge.psd'
Rewrite 4ba0df06b26cf86fd39c2cda6b012c521cbc4dc1 (271/982)rm 'assets/img/recent-badge.psd'
Rewrite 242ae98060c77863f5e826ba7e1ec47
<br>--index-filter 参数用来指定一条 Bash 命令，然后 Git 会检出（checkout）所有的提交， 执行该命令，然后重新提交。我们在提交前移除了 recent-badge.psd 文件， 这个文件便从 Git 的所有记录中完全消失了。<br>
--all 参数告诉 Git 我们需要重写所有分支（或引用）。
<br><br>删掉了 recent-badge.psd 后我仍不满足，我要找到所有的大文件，并把它删掉。 verify-pack 命令用来验证 Git 打包的归档文件，我们用它来找到那些大文件。 例如：<br>$ git verify-pack -v .git/objects/pack/*.idx
8fa15d279de33ce28a3289fd33951374084231e4 tree   135 137 144088922
a44a50b2ffb1f8283c8e64aafb8e7628249d7453 tree   33 43 144089059
b57d99f38fe22491e4a2d30c2b081ecb7bbb329c tree   99 97 144089102
2d4ffaffc11758d561ea1a6d57dd8ee17ee1d836 blob   644952 644959 144089199
8cf81ebfeec409f19e7a47a76517317f3bfa268d blob   695898 695871 144734158
...
<br>
-v（verbose）参数是打印详细信息。
<br>输出的第一列是文件 ID，第二列表示文件（blob）或目录（tree），第三列是文件大小。 现在得到了所有的文件 ID 及其大小，需要写一点 Bash 了！<br>先按照第三列排序，并取最大的 5 条，然后打印出每项的第一列（这一列是文件 ID）：<br>$ git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -10 | awk '{print$1}'
f846f156d16f74243b67e3dabec58a3128744352 
4a1546e732b2e2a352b7bf220c1a22ad859abf89 
f72d04efe6d0b41b067f9fbbc62455f28d3670d2 
49bdf300ddf57d1946bc9c6570d94a38ac9d6a50 
9c073d4177af5d2e43ada41f92efb18d9462a536
<br>现在变得到了最大的 5 个文件的 ID，而我需要文件名才能用 filter-branch 移除它。 我现在需要文件 ID 和文件名的映射关系。<br><br>rev-list 命令用来列出 Git 仓库中的提交，我们用它来列出所有提交中涉及的文件名及其 ID。 该命令可以指定只显示某个引用（或分支）的上下游的提交。例如：<br>git rev-list foo bar ^baz
<br>将会列出所有从 foo 和 bar 可到达，但从 baz 不可到达的提交。我们将会用到 rev-list 的两个参数：<br>
<br>--objects：列出该提交涉及的所有文件 ID。
<br>--all：所有分支的提交，相当于指定了位于 /refs 下的所有引用。
<br>我们看看这条命令的输出：<br>$ git rev-list --objects --all
c252878ac09a3979a80520b82a71dc2dae4529f9
7bc7d05c6097063f531580ba4c32921464a6c456 _drafts
dcce26ed53fbb869dc7d5b71742d2f9e523bfe42 _layouts
414186c794a0d58695abb75c548bdbfec1de2763 _layouts/default.html
1934eeffe3d242375510dff28cffa6de6b3de367 _layouts/post.html
5f14647875f2177a6d37b8bfbcdb4629af595b64 _posts
6cdbb293d453ced07e6a07e0aa6e580e6a5538f4 _posts/2013-10-12-2.md
...
<br>现在就得到了文件名（如 _posts/2013-10-12-2.md）和 ID（如 6cdbb293d453ced07e6a07e0aa6e580e6a5538f4 ）的映射关系。<br><br>前面我们通过 rev-list 得到了文件名-ID 的对应关系，通过 verify-pack 得到了最大的 5 个文件 ID。 用后者筛选前者便能得到最大的 5 个文件的文件名：<br>$ git rev-list --objects --all | grep "$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk '{print$1}')"
f846f156d16f74243b67e3dabec58a3128744352 assets/img/recent-badge.psd
4a1546e732b2e2a352b7bf220c1a22ad859abf89 assets/img/album/me/IMG_0276.JPG
f72d04efe6d0b41b067f9fbbc62455f28d3670d2 assets/img/album/me/IMG_0389.JPG
49bdf300ddf57d1946bc9c6570d94a38ac9d6a50 assets/img/album/me/IMG_0813.JPG
9c073d4177af5d2e43ada41f92efb18d9462a536 assets/img/album/me/IMG_0891.JPG
<br>先把上面输出存到 large-files.txt 中。还记得吗？--tree-filter 参数中我们需要给出一行的文件名列表。上述列表我们需要处理一下：<br>$ cat large-files.txt| awk '{print $2}' | tr '\n' ' '
assets/img/recent-badge.psd assets/img/album/me/IMG_0276.JPG assets/img/album/me/IMG_0389.JPG assets/img/album/me/IMG_0813.JPG assets/img/album/me/IMG_0891.JPG
<br>现在便得到了一行的文件列表。把它存到 large-files-inline.txt 中。<br><br>现在得到了要删除的大文件列表 large-files-inline.txt，把它传入到 --tree-filter 中即可：<br>git filter-branch -f --prune-empty --index-filter "git rm -rf --cached --ignore-unmatch `cat large-files-inline.txt`" --tag-name-filter cat -- --all
<br>
注意这里 --index-filter 的参数要用双引号，因为 cat large-files-inline.txt 还需要 Bash 的解析。
<br>至此已经干掉了那些大文件，来看看瘦身了多少吧！ 注意 filter-branch 之后 .git 目录下会有大量的备份。 需要克隆一份当前仓库来看效果：<br>git clone --no-hardlinks file:///Users/harttle/harttle.land /tmp/harttle.land
<br>仓库大小变为 25.76M 了！从原来的 142M！<br>
也可以进入项目目录通过 du -d 1 -h 查看磁盘占用的大小。
<br>当然到此为止我们更改的都是本地仓库，现在把这些改变 Push 到远程仓库中去！<br>git push origin --force --all
<br>
因为不是 fast forward，所以需要指定 --force 参数。
<br>这里的 --all 会将所有分支都推送到 origin 上。当然你也可以只推送 master 分支： git push origin master --force。但是！如果其它远程分支有那些大文件提交的话，仍然没有瘦身！<br><br>
<br><a rel="noopener nofollow" class="external-link" href="http://naleid.com/blog/2012/01/17/finding-and-purging-big-files-from-git-history" target="_blank">http://naleid.com/blog/2012/01/17/finding-and-purging-big-files-from-git-history</a>
<br><a rel="noopener nofollow" class="external-link" href="http://stackoverflow.com/questions/6403601/purging-file-from-git-repo-failed-unable-to-create-new-backup" target="_blank">http://stackoverflow.com/questions/6403601/purging-file-from-git-repo-failed-unable-to-create-new-backup</a>
<br><a rel="noopener nofollow" class="external-link" href="http://dalibornasevic.com/posts/2-permanently-remove-files-and-folders-from-git-repo" target="_blank">http://dalibornasevic.com/posts/2-permanently-remove-files-and-folders-from-git-repo</a>
<br><a data-tooltip-position="top" aria-label="https://git-scm.com/book/zh/v1/Git-%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86-Git-References" rel="noopener nofollow" class="external-link" href="https://git-scm.com/book/zh/v1/Git-%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86-Git-References" target="_blank">https://git-scm.com/book/zh/v1/Git-内部原理-Git-References</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/other/寻找并删除-git-记录中的大文件.html</link><guid isPermaLink="false">Computer Science/Other/寻找并删除 Git 记录中的大文件.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 03:02:59 GMT</pubDate></item><item><title><![CDATA[C++ 20 实现 string split]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:cpp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#cpp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:cpp" class="tag" target="_blank" rel="noopener nofollow">#cpp</a><br>C++20引入了范围库ranges，其中提供的两个范围适配器std::split、std::lazy_split可以使我们以一种更为优雅的形式实现split:<br>#include &lt;concept&gt;
#include &lt;ranges&gt;
#include &lt;algorithm&gt;
#include &lt;format&gt;
#include &lt;iostream&gt;

#define stdr  std::ranges
#define stdrv std::ranges::views

template&lt;template&lt;typename&gt; typename Container = std::vector, typename Arg = std::string_view&gt;
auto Split(std::string_view str, std::string_view delimiter)
{
	Container&lt;Arg&gt; myCont;
	auto temp = str 
		| stdrv::split(delimiter)
		| stdrv::transform([](auto&amp;&amp; r)
			{
				return Arg(std::addressof(*r.begin()), stdr::distance(r));
			});
	auto iter = std::inserter(myCont, myCont.end());
	stdr::for_each(temp, [&amp;](auto&amp;&amp; x) { iter = {x.begin(), x.end()}; });
	return myCont;
}
int main()
{
	std::string str = "Hello233C++20233and233New233Spilt";
	std::string delimiter = "233";
	auto&amp;&amp; strCont = Split&lt;std::list, std::string&gt;(str, delimiter);
	stdr::for_each(strCont, [](auto&amp;&amp; x) { std::cout &lt;&lt; std::format("{} ", x); });
}
//output: Hello C++20 and New Spilt
<br>C++20没有提供关键的 ranges::to&lt;container&gt;函数，导致demo中还需要额外封装并手写for_each来写入数据，等到C++23实装了该函数，split的实现会比现在简洁优雅的多，真正做到方便泛用、无需封装：<br>auto&amp;&amp; strCont = str
	| stdrv::lazy_split(delimiter)
	| stdr ::to&lt;std::vector&lt;std::string&gt;&gt;;
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/c++/c++-20-实现-string-split.html</link><guid isPermaLink="false">Computer Science/Programming Language/C++/C++ 20 实现 string split.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:39:54 GMT</pubDate></item><item><title><![CDATA[C++ vector 的 push_back 和 emplace_back]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:cpp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#cpp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:cpp" class="tag" target="_blank" rel="noopener nofollow">#cpp</a><br>/// Inserts a new element at the end of the vector, right after its current last element. This new element is constructed in place using args as the arguments for its constructor.
/// This effectively increases the container size by one, which causes an automatic reallocation of the allocated storage space if -and only if- the new vector size surpasses the current vector capacity.
/// The element is constructed in-place by calling allocator_traits::construct with args forwarded.
///A similar member function exists, push_back, which either copies or moves an existing object into the container.
template &lt;class... Args&gt;
void emplace_back (Args&amp;&amp;... args);
<br>push_back 会构造一个临时对象，这个临时对象会被拷贝或者移入到容器中，然而 emplace_back 会直接根据传入的参数在容器的适当位置进行构造而避免拷贝或者移动。<br>传统观点认为 push_back 会构造一个临时对象，这个临时对象会被移入到 v 中，然而 emplace_back 会直接根据传入的参数在适当位置进行构造而避免拷贝或者移动。从标准库代码的实现角度来说这是对的，但是对于提供了优化的编译器来讲，上面示例中最后两行表达式生成的代码其实没有区别。<br>真正的区别在于，emplace_back 更加强大，它可以调用任何类型的（只要存在）构造函数。而 push_back 会更加严谨，它只调用隐式构造函数。隐式构造函数被认为是安全的。如果能够通过对象 T 隐式构造对象 U，就认为 U 能够完整包含 T 的所有内容，这样将 T 传递给 U 通常是安全的。正确使用隐式构造的例子是用 std::uint32_t 对象构造 std::uint64_t 对象，错误使用隐式构造的例子是用 double 构造 std::uint8_t。<br>如果想要调用显示构造函数，那么就调用 emplace_back。如果只希望调用隐式构造函数，那么请使用更加安全的 push_back:<br>std::vector&lt;std::unique_ptr&lt;T&gt;&gt; v;
T a;
v.emplace_back(std::addressof(a)); // compiles
v.push_back(std::addressof(a)); // fails to compile
<br>std::unique_ptr&lt;T&gt; 包含了显示构造函数通过 T* 进行构造。因为 emplace_back 能够调用显示构造函数，所以传递一个裸指针并不会产生编译错误。然而，当 v 超出了作用域，std::unique_ptr&lt;T&gt; 的析构函数会尝试 delete 类型 T* 的指针，而类型 T* 的指针并不是通过 new 来分配的，因为它保存的是栈对象的地址，因此 delete 行为是未定义的。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/c++/c++-vector-的-push_back-和-emplace_back.html</link><guid isPermaLink="false">Computer Science/Programming Language/C++/C++ vector 的 push_back 和 emplace_back.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:38:05 GMT</pubDate></item><item><title><![CDATA[C++ 的 Trait]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:cpp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#cpp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:cpp" class="tag" target="_blank" rel="noopener nofollow">#cpp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br>
C++ 的 traits 技术，是一种约定俗称的技术方案，用来为同一类数据（包括自定义数据类型和内置数据类型）提供统一的类型名（traits），这样可以统一的操作函数，例如 advance(), swap(), encode()/decode() 等。
<br>例如，拥有义类型Foo, Bar，以及编译器自带类型 int, double, string，我们想要为这些不同的类型提供统一的编码函数 decode() 。<br>除了使用 trait 技术之外，函数重载和模板函数 + 内置字段也可以实现，前者每增加一种数据类型就需要重新实现一个函数，而同一类数据（int, unsinged int）可以使用同样的编码方法。我们想要的是针对同一种数据类型，只编写一个函数。后者对于系统自定义变量 int, double 而言，是无法在其内部定义 type 的。<br>traits 技术的关键在于，使用另外的模板类 type_traits 来保存不同数据类型的 type，这样就可以兼容自定义数据类型和内置数据类型:<br>// 定义数据 type 类
enum Type {
  TYPE_1,
  TYPE_2,
  TYPE_3
}
<br>对于自定义类型，在类内部定义 type，然后在 traits 类中定义同样的 type:<br>// 自定义数据类型
class Foo {
public:
  Type type = TYPE_1;
};
class Bar {
public:
  Type type = TYPE_2;
};
template&lt;typename T&gt;
struct type_traits {
  Type type = T::type;
}
<br>对于内置数据类型，使用模板类的特化为自定义类型生成独有的 type_traits:<br>// 内置数据类型
template&lt;typename int&gt;
struct type_traits {
  Type type = Type::TYPE_1;
}
template&lt;typename double&gt;
struct type_traits {
  Type type = Type::TYPE_3;
}
<br>这样就可以为不同数据类型生成统一的模板函数<br>// 统一的编码函数
template&lt;typename T&gt;
void decode&lt;const T&amp; data, char* buf) {
  if(type_traits&lt;T&gt;::type == Type::TYPE_1) {
    ...
  }
  else if(type_traits&lt;T&gt;::type == Type::TYPE_2) {
    ...
  }
}
<br>总结<br>
<br>traits 技术的关键在于使用第三方模板类 traits，利用模板特化的功能, 实现对自定义数据和编译器内置数据的统一
<br>这个例子使用了枚举变量来表示数据类型，而实际操作中通常使用不同的类来表示不同的类型，这样可以在编写模板函数时更好的优化。
<br>tratis 技术常见于标准库的实现中，但对日常开发中降低代码冗余也有很好的借鉴意义
<br>C++20 提供了Concept 的特性，使用Concept 可以使得实现类似的功能更加方便
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/c++/c++-的-trait.html</link><guid isPermaLink="false">Computer Science/Programming Language/C++/C++ 的 Trait.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:40:27 GMT</pubDate></item><item><title><![CDATA[My fading frustration with ClojureScript]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:clojure" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#clojure</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:clojure" class="tag" target="_blank" rel="noopener nofollow">#clojure</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a><br>I’ve talked about <a data-tooltip-position="top" aria-label="https://mauricio.szabo.link/blog/2018/04/05/my-frustration-with-clojurescript/" rel="noopener nofollow" class="external-link" href="https://mauricio.szabo.link/blog/2018/04/05/my-frustration-with-clojurescript/" target="_blank">at another post</a> on how ClojureScript frustrates me, mostly because I was doing some Node.JS work and Figwheel simply wasn’t working correctly. Now, it’s time to revisit these points:<br>A little update: I talked a little with Thomas Heller, Shadow-CLJS creator, and he pointed me some issues with this article, so I’ll update it acordingly<br><br>Figwheel and Lein are not the best tools to work with ClojureScript. Since I discovered shadow-cljs, things are working way better than before: I can reload ClojureScript code from any target, and I’m even experimenting with Hubot (and it works really fine too). The only thing I’m missing is my profiles.clj file, but I can live with that (and I can always use Shadow with Lein if I need profiles.clj).<br>Also, I’m working on a new package for Atom (and in the future, for another editors too) called <a data-tooltip-position="top" aria-label="http://github.com/mauricioszabo/atom-chlorine/" rel="noopener nofollow" class="external-link" href="http://github.com/mauricioszabo/atom-chlorine/" target="_blank">Chlorine</a>. One of the ideas is to offer better ClojureScript support (we have Autocomplete now!), using Socket REPL for solutions (even self-hosted REPLs like Lumo and Plank) and even wrap UNREPL protocol in Clojure. So far, is still in the very beginning but things are looking promising!<br><br>Forget Figwheel at all: Shadow-CLJS is probably the best tooling for ClojureScript ever. It auto-reloads ClojureScript code for the browser, for node.js, for node modules, and it integrates with almost everything that you want. It controls release optimizations, have sensible defaults, and even have post-compile hooks (so you can hook Clojure code to do something after some compilation phases). Also, it integrates with node-modules (no more maven-wrappers for JS libraries!) and have some warnings when you use some kind of ClojureScript code that would break :advanced compilation. And, let’s not forget that you can control the refresh reload phase, it adds a userful :include-macros in ns form (that will include all macros from the namespace being required), and controls exports in a sane manner. But first let’s begin with the feature that I found most useful: :before-load-async.  <br>When you’re developing ClojureScript code, one of the killer features is the abilty to reload your code after save, so at any moment you have the most recent version of your code running. Let’s see how it works in practice: imagine you’re developing a simple static website (no react, no vue.js, nothing – just a simple site that’ll calculate the sum of two input boxes). You already have the HTML file with two input boxes (IDs “n1” and “n2”) and a “results” div. You’ll want to add Shadow-CLJS in this project.<br>Simple: just npm install shadow-cljs, then npx shadow-cljs init, and edit the shadow-cljs.edn file. You’ll want to add a build targeting browser, so the file will look like this:<br><br>Most of the keys should be self-explanatory. The ones that are not are:<br>
<br>:asset-path mostly says that, relative to the root of your webserver, where assets will be put – for instance, suppose that your webserver will load static assets from the public directory. Then, your config will be :output-dir "public/js", so that the compiler will generate code in a folder that your webserver will be able to serve, and :asset-path "js", so the compiler will generate “requires” relative to “js” folder, not “public/js”
<br>:devtools registers a “hook function” that’ll be called when Shadow-CLJS have compiled the source paths, and it is ready to reload our code. This “reload function” will receive a parameter (let’s call it done) that, when called, it informs shadow-cljs that it’s time to use the new versions of our functions. So, let’s first look at the main function:
<br><br>You can start our compiler with npx shadow-cljs watch app, and yes, we’re using jQuery. Also, there’s a prn function that’ll be used to debug. If you include js/main.js as a script in your webpage, this code will work – but it’ll not reload. So, after these functions, we can add our reload function that’ll just call our main function after informing shadow that we did our cleanup:<br><br>If you change your code in any way – maybe change the math operation to multiplication – you’ll see that our page changes correctly. You’ll also see, in the console, that we’re printing our numbers twice per change. This is because jQuery’s .on method stacks callbacks: this means that we’re still listening to our old code. What we have to do is to clean our callbacks before reloading our code, and this is quite simple with Shadow:<br><br>UPDATE: As Thomas Heller told me, :before-load-async is used do signal my application that a reload is due, so I can control what to happen before the reload, and also to inform shadow that it can continue reloading our code. If we do any async stuff to reload code – let’s say, resolve a promise (something like (-&gt; clear-things (.then done)) – or if the reload uses some async code, this will not work. In this case, it’s better to register another callback, :after-load, and use it to re-activate our code, something like:<br><br><br>Now, this probably misses some points about why this is completely awesome. So, let’s imagine a different use-case: a reload function for an Atom editor’s package. In Atom, almost every side-effect function that you can call will return an Disposable object: one that you can dispose and nullify the effect (for instance, if you listen to changes in an editor, you can .dispose() those changes and un-listen to changes).<br>What you can do is to create a CompositeDisposable, and when you refresh, you .dispose everything, re-creates the CompositeDisposable, then re-adds your effects. This means that while you’re developing your plug-in, you can stop subscriptions, delete your commands, add new commands to editor, re-load subscriptions, all just saving your code. If this is not awesome, I don’t really know what it is. The code to do it is surprisingly simple:<br><br>For projects like Atom or other editors’ plug-ins (like VSCode, NeoVim, Oni) this give a temendous power. Also, this can be used while developing extensions for browsers, or projects when it’s tedious to unload everything, reload everything, just to see you have called a function with the arguments swapped…<br><br>With a simple npx shadow-cljs release app, Shadow will try to compile your code with :advanced optimizations. But sometimes, this won’t work: imagine that in our jQuery example, :advanced will rename jQuery to something else, so things will not work. There are two options you can use: inside your build id (in our case, :app) you can add the key :compiler-options {:infer-externs :auto}. This will throw an warning when you’re using code that Shadow-CLJS can’t infer. Also, you can use npx shadow-cljs release --debug app to compile with :advanced features, but will pretty-print files and will make meaningful names (so you can debug where things went wrong).<br>This is not exclusive of Shadow-CLJS, but the compiler just make easier to access these options. Now, what is exclusive from Shadow-CLJS is an extension to ns form (UPDATE: it is not. As Thomas told me, it uses a different implementation and have more checks, but it works in CLJS too):<br><br>This means no more :refer-macros or :require-macros. This magically finds the macros in that required namespace, and allows then to be used in the current ns. Simpler code, less things to remember, and cleaner. Also, I never had the compiler problems that I had with figwheel – that it compiles a version, then sometime later you clean things and it didn’t compile anymore. Or, that it compiles a version, and there’s a runtime exception with a compiler error.<br>Yet, for now, there are still problems with .cljc files that have macros, and some toolings that I was unable to make it work (cljs-complete being one of the most critical for me). Even with these limitations, developing with ClojureScript is now my primary choice for JS projects (when I can choose), even if there’s already JS code in production (because it integrates with existing code like a breeze too). It wasn’t before, and this was all thanks to a better tooling.<br>So yes, tools make all the difference!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/clojure/my-fading-frustration-with-clojurescript.html</link><guid isPermaLink="false">Computer Science/Programming Language/Clojure/My fading frustration with ClojureScript.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:43:07 GMT</pubDate></item><item><title><![CDATA[Building a self-contained game in CSharp under 8 kilobytes]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:csharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#csharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:game" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#game</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:csharp" class="tag" target="_blank" rel="noopener nofollow">#csharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:game" class="tag" target="_blank" rel="noopener nofollow">#game</a><br><br>Jan 03, 2020<br>28 minute read<br>
NOTE: This article captures a point in time in the past. While the general information is still correct, the CoreRT project got folded into <a data-tooltip-position="top" aria-label="https://learn.microsoft.com/dotnet/core/deploying/native-aot/" rel="noopener nofollow" class="external-link" href="https://learn.microsoft.com/dotnet/core/deploying/native-aot/" target="_blank">Native AOT publishing</a> in .NET 7 and is now a supported part of .NET. The information about sizes is no longer accurate (and much better), neither is the information about support for dynamic code (both interpreter and JIT are unsupported).
<br>As someone who grew up in the times of 1.44 MB floppy disks and 56 kbit modems, I’ve always liked small programs. I could fit many small programs on a floppy disk I carried with me. If a program couldn’t fit on my floppy disk, I started thinking about why - does it have a lot of graphics? Is there music? Can the program do many complex things? Or is it simply bloated?<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" target="_blank"></a><img alt="3.5&quot; floppies, photo by Brett Jordan on Unsplash" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" referrerpolicy="no-referrer"><br>3.5" floppies, photo by Brett Jordan on Unsplash<br>These days, disk space became so cheap (and huge flashdrives so ubiquitous) that people gave up on optimizing for size.<br>One place where size still matters is transfers: when transferring a program over a wire, megabytes equate to seconds. A fast 100 MBit connection can only push through 12 megabytes per second in the best case. If on the other end of the wire is a person waiting for a download to finish, the difference between five seconds and one second can have meaningful impact on their experience.<br>The person could be exposed to the transfer times either directly (user is downloading a program over network), or indirectly (a serverless service is getting deployed to respond to a web request).<br>People typically perceive anything faster than 0.1 seconds as instant, 3.0 seconds is about the limit for user’s flow to stay uninterrupted, and you would have a hard time to keep the user engaged after 10 seconds.<br>
While smaller is not essential anymore, it’s still better.
<br>This article came out as an experiment to find out just how small a useful self-contained C# executable can be. Can C# apps hit the sizes where users would consider the download times instant? Would it enable C# to be used in places where it isn’t used right now?<br><br>A self-contained application is an application that includes everything that’s necessary for it to run on a vanilla installation of the operating system.<br>The C# compiler belongs to a group of compilers targeting a virtual machine (Java and Kotlin being another notable members of the group): the output of the C# compiler is an executable that requires some sort of virtual machine (VM) to execute. One can’t just install a barebone operating system and expect to be able to run programs produced by the C# compiler on it.<br>At least on Windows, it used to be the case that one could rely on a machine-wide installation of the .NET Framework to run the outputs of the C# compiler. Nowadays there are many Windows SKUs that no longer carry the framework with it (IoT, Nano Server, ARM64,…). .NET Framework also doesn’t support the latest enhancements to the C# language. It’s kind of on its way out.<br>For a C# app to be self-contained, it needs to include the runtime and all the class libraries it uses. It’s a lot of stuff to fit into the 8 kB that we budget for!<br><br>We’re going to build a clone of the Snake game. Here’s the finished product:<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/02-snake.gif" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/02-snake.gif" target="_blank"></a><img alt="Snake game" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/02-snake.gif" referrerpolicy="no-referrer"><br>Snake game<br>If you’re not interested in the game mechanics, feel free to skip over to the interesting parts where we shrink the game from 65 megabytes to 8 kilobytes in 9 steps (scroll down to where you see graphs).<br>The game will run in text mode and we’ll use the box drawing characters to draw the snake. I’m sure Vulcan or DirectX would be a lot more fun, but we’ll get by with System.Console.<br><br>We’re going to build a no-allocation game - and by no-allocation I don’t mean the “don’t allocate in the game loop” that is common among C# game devs. I mean “the new keyword with reference types is forbidden in the entire codebase”. The reasons for that will become apparent at the final stretch of shrinking the game.<br>With such restriction in place, one might wonder if there’s any point in using C# after all: without the new keyword, we won’t be using the garbage collector, we can’t throw exceptions, etc. - a language like C would work just as well.<br>One reason to use C# is “because we can”. The other reason is testability and code sharing - while the game as a whole is no-allocation, it doesn’t mean that parts of it couldn’t be reused in a different project that doesn’t have such constrains. For example, parts of the game could be included from an xUnit project to get unit test coverage. If one chooses C to build the game, things have to stay constrained by what C can do even when the code is reused from elsewhere. But since C# provides a good mix of high level and low level constructs, we can follow the “high level by default, low level when necessary” philosophy.<br>To reach the 8 kB deployment size, the low level part will be necessary.<br><br>Let’s start with a struct that represents the frame buffer. Frame buffer is a component that holds the pixels (or in this case characters) to be drawn to the screen.<br>unsafe struct FrameBuffer
{
    public const int Width = 40;
    public const int Height = 20;
    public const int Area = Width * Height;

    fixed char _chars[Area];

    public void SetPixel(int x, int y, char character)
    {
        _chars[y * Width + x] = character;
    }

    public void Clear()
    {
        for (int i = 0; i &lt; Area; i++)
            _chars[i] = ' ';
    }

    public readonly void Render()
    {
        Console.SetCursorPosition(0, 0);

        const ConsoleColor snakeColor = ConsoleColor.Green;

        Console.ForegroundColor = snakeColor;

        for (int i = 1; i &lt;= Area; i++)
        {
            char c = _chars[i - 1];

            if (c == '*' || (c &gt;= 'A' &amp;&amp; c &lt;= 'Z') || (c &gt;= 'a' &amp;&amp; c &lt;= 'z'))
            {
                Console.ForegroundColor = c == '*' ? ConsoleColor.Red : ConsoleColor.White;
                Console.Write(c);
                Console.ForegroundColor = snakeColor;
            }
            else
                Console.Write(c);

            if (i % Width == 0)
            {
                Console.SetCursorPosition(0, i / Width - 1);
            }
        }
    }
}
<br>We provide methods to set individual pixels, clear the frame buffer, and to render the contents of the frame buffer into System.Console. The rendering step special cases a couple characters so that we get colorful output without having to keep track of color for each pixel of the frame buffer.<br>One interesting thing to call out is the fixed char _chars[Area] field: this is the C# syntax to declare a <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/unsafe-code-pointers/fixed-size-buffers" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/unsafe-code-pointers/fixed-size-buffers" target="_blank">fixed array</a>. A fixed array is an array whose individual elements are a part of the struct. You can think of it as a shortcut for a set of fields char _char_0, _char_1, _char_2, _char_3,... _char_Area that one can access as an array. The size of this array needs to be a compile time constant so that the size of the entire struct is fixed.<br>We can’t go overboard with the size of a fixed array because being a part of a struct, the array needs to live on the stack and stacks tend to be limited to a small number of bytes (1 MB per thread, typically). But 40  20  2 bytes (width  height  sizeof(char)) should be fine.<br>Next thing we need is a random number generator. The one that comes with .NET is a reference type (for good reasons!) and we forbid ourselves the new keyword - we can’t use it. A simple struct will do:<br>struct Random
{
    private uint _val;

    public Random(uint seed)
    {
        _val = seed;
    }

    public uint Next() =&gt; _val = (1103515245 * _val + 12345) % 2147483648;
}
<br>This random number generator is not great, but we don’t need anything sophisticated.<br>Now we only need something that wraps the snake logic. Time for a Snake struct:<br>struct Snake
{
    public const int MaxLength = 30;

    private int _length;

    // Body is a packed integer that packs the X coordinate, Y coordinate, and the character
    // for the snake's body.
    // Only primitive types can be used with C# `fixed`, hence this is an `int`.
    private unsafe fixed int _body[MaxLength];

    private Direction _direction;
    private Direction _oldDirection;

    public Direction Course
    {
        set
        {
            if (_oldDirection != _direction)
                _oldDirection = _direction;

            if (_direction - value != 2 &amp;&amp; value - _direction != 2)
                _direction = value;
        }
    }

    public unsafe Snake(byte x, byte y, Direction direction)
    {
        _body[0] = new Part(x, y, DirectionToChar(direction, direction)).Pack();
        _direction = direction;
        _oldDirection = direction;
        _length = 1;
    }

    public unsafe bool Update()
    {
        Part oldHead = Part.Unpack(_body[0]);
        Part newHead = new Part(
            (byte)(_direction switch
            {
                Direction.Left =&gt; oldHead.X == 0 ? FrameBuffer.Width - 1 : oldHead.X - 1,
                Direction.Right =&gt; (oldHead.X + 1) % FrameBuffer.Width,
                _ =&gt; oldHead.X,
            }),
            (byte)(_direction switch
            {
                Direction.Up =&gt; oldHead.Y == 0 ? FrameBuffer.Height - 1 : oldHead.Y - 1,
                Direction.Down =&gt; (oldHead.Y + 1) % FrameBuffer.Height,
                _ =&gt; oldHead.Y,
            }),
            DirectionToChar(_direction, _direction)
            );

        oldHead = new Part(oldHead.X, oldHead.Y, DirectionToChar(_oldDirection, _direction));

        bool result = true;

        for (int i = 0; i &lt; _length - 1; i++)
        {
            Part current = Part.Unpack(_body[i]);
            if (current.X == newHead.X &amp;&amp; current.Y == newHead.Y)
                result = false;
        }

        _body[0] = oldHead.Pack();

        for (int i = _length - 2; i &gt;= 0; i--)
        {
            _body[i + 1] = _body[i];
        }

        _body[0] = newHead.Pack();

        _oldDirection = _direction;

        return result;
    }

    public unsafe readonly void Draw(ref FrameBuffer fb)
    {
        for (int i = 0; i &lt; _length; i++)
        {
            Part p = Part.Unpack(_body[i]);
            fb.SetPixel(p.X, p.Y, p.Character);
        }
    }

    public bool Extend()
    {
        if (_length &lt; MaxLength)
        {
            _length += 1;
            return true;
        }
        return false;
    }

    public unsafe readonly bool HitTest(int x, int y)
    {
        for (int i = 0; i &lt; _length; i++)
        {
            Part current = Part.Unpack(_body[i]);
            if (current.X == x &amp;&amp; current.Y == y)
                return true;
        }

        return false;
    }

    private static char DirectionToChar(Direction oldDirection, Direction newDirection)
    {
        const string DirectionChangeToChar = "│┌?┐┘─┐??└│┘└?┌─";
        return DirectionChangeToChar[(int)oldDirection * 4 + (int)newDirection];
    }

    // Helper struct to pack and unpack the packed integer in _body.
    readonly struct Part
    {
        public readonly byte X, Y;
        public readonly char Character;

        public Part(byte x, byte y, char c)
        {
            X = x;
            Y = y;
            Character = c;
        }

        public int Pack() =&gt; X &lt;&lt; 24 | Y &lt;&lt; 16 | Character;
        public static Part Unpack(int packed) =&gt; new Part((byte)(packed &gt;&gt; 24), (byte)(packed &gt;&gt; 16), (char)packed);
    }

    public enum Direction
    {
        Up, Right, Down, Left
    }
}
<br>The state that a snake needs to track is:<br>
<br>the coordinates of each pixel that represents the snake’s body,
<br>the current length of the snake,
<br>the current direction of the snake,
<br>past direction of the snake (in case we need to draw the “bend” character instead of a straight line)
<br>The snake provides methods to Extend the length of snake by one (returns false if the snake is already at full length), to HitTest a pixel with the snake’s body, to Draw the snake into a FrameBuffer and to Update the snake’s position as a response to a game tick (returns false if the snake ate itself). There’s also a property to set the current Course of the snake.<br>We use the same fixed array trick that we used in the frame buffer to keep the snake no-allocation. It means the maximum length of the snake has to be a compile time constant.<br>The last thing we need is the game loop:<br>struct Game
{
    enum Result
    {
        Win, Loss
    }

    private Random _random;

    private Game(uint randomSeed)
    {
        _random = new Random(randomSeed);
    }

    private Result Run(ref FrameBuffer fb)
    {
        Snake s = new Snake(
            (byte)(_random.Next() % FrameBuffer.Width),
            (byte)(_random.Next() % FrameBuffer.Height),
            (Snake.Direction)(_random.Next() % 4));

        MakeFood(s, out byte foodX, out byte foodY);

        long gameTime = Environment.TickCount64;

        while (true)
        {
            fb.Clear();

            if (!s.Update())
            {
                s.Draw(ref fb);
                return Result.Loss;
            }

            s.Draw(ref fb);

            if (Console.KeyAvailable)
            {
                ConsoleKeyInfo ki = Console.ReadKey(intercept: true);
                switch (ki.Key)
                {
                    case ConsoleKey.UpArrow:
                        s.Course = Snake.Direction.Up; break;
                    case ConsoleKey.DownArrow:
                        s.Course = Snake.Direction.Down; break;
                    case ConsoleKey.LeftArrow:
                        s.Course = Snake.Direction.Left; break;
                    case ConsoleKey.RightArrow:
                        s.Course = Snake.Direction.Right; break;
                }
            }

            if (s.HitTest(foodX, foodY))
            {
                if (s.Extend())
                    MakeFood(s, out foodX, out foodY);
                else
                    return Result.Win;
            }

            fb.SetPixel(foodX, foodY, '*');

            fb.Render();

            gameTime += 100;

            long delay = gameTime - Environment.TickCount64;
            if (delay &gt;= 0)
                Thread.Sleep((int)delay);
            else
                gameTime = Environment.TickCount64;
        }
    }

    void MakeFood(in Snake snake, out byte foodX, out byte foodY)
    {
        do
        {
            foodX = (byte)(_random.Next() % FrameBuffer.Width);
            foodY = (byte)(_random.Next() % FrameBuffer.Height);
        }
        while (snake.HitTest(foodX, foodY));
    }

    static void Main()
    {
        Console.SetWindowSize(FrameBuffer.Width, FrameBuffer.Height);
        Console.SetBufferSize(FrameBuffer.Width, FrameBuffer.Height);
        Console.Title = "See Sharp Snake";
        Console.CursorVisible = false;

        FrameBuffer fb = new FrameBuffer();

        while (true)
        {
            Game g = new Game((uint)Environment.TickCount64);
            Result result = g.Run(ref fb);

            string message = result == Result.Win ? "You win" : "You lose";

            int position = (FrameBuffer.Width - message.Length) / 2;
            for (int i = 0; i &lt; message.Length; i++)
            {
                fb.SetPixel(position + i, FrameBuffer.Height / 2, message[i]);
            }

            fb.Render();

            Console.ReadKey(intercept: true);
        }
    }
}
<br>We use the random number generator to generate a random position and direction of the snake, we randomly place the food on the game surface, making sure it doesn’t overlap the snake, and start the game loop.<br>Within the game loop we ask the snake to update its position and check whether it ate itself. We then draw the snake, check the keyboard for input, hit-test the snake with the food, and render everything to the console.<br>That’s pretty much it. Let’s see where we are in terms of size.<br><br>I’ve placed the game in <a data-tooltip-position="top" aria-label="https://github.com/MichalStrehovsky/SeeSharpSnake" rel="noopener nofollow" class="external-link" href="https://github.com/MichalStrehovsky/SeeSharpSnake" target="_blank">a GitHub repo</a> so that you can follow along. The project file will produce the game in different configurations depending on the Mode property passed to publish. To produce the default configuration with CoreCLR, run:<br>$ dotnet publish -r win-x64 -c Release
<br>This will produce a single EXE file that has whopping 65 MB. The produced EXE includes the game, the .NET Runtime, and the base class libraries that are the standard part of .NET. You might say “still better than Electron” and call it good, but let’s see if we can do better.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/03-graph1.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/03-graph1.png" target="_blank"></a><img alt="Starting point" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/03-graph1.png" referrerpolicy="no-referrer"><br>Starting point<br><br>IL trimming shipped with .NET Core 3.0 - trimming removes unused code from your app by scanning the entire program and removing assemblies that are unreferenced. To use it with the project, pass a PublishTrimmed property to publish. Like so:<br>$ dotnet publish -r win-x64 -c Release /p:PublishTrimmed=true
<br>With this setting, the game shrinks to 25 MB. It’s a nice 60% reduction, but far from our 8 kB goal.<br>Trimming has more aggressive settings that are not publicly exposed and they could bring this down further, but in the end, we’re going to be limited by the size of the CoreCLR runtime itself - coreclr.dll - at 5.3 MB. We might have reached a dead end on the road to a 8 kB game.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/04-graph2.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/04-graph2.png" target="_blank"></a><img alt="After trimming" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/04-graph2.png" referrerpolicy="no-referrer"><br>After trimming<br><br><a data-tooltip-position="top" aria-label="https://www.mono-project.com/" rel="noopener nofollow" class="external-link" href="https://www.mono-project.com/" target="_blank">Mono</a> is another .NET runtime that for many is the synonym for Xamarin. To build a single executable with the C# snake, we can use the mkbundle tool that comes with Mono:<br>$ mkbundle SeeSharpSnake.dll --simple -o SeeSharpSnake.exe
<br>This will produce a 12.3 MB executable that depends on mono-2.0-sgen.dll that itself has 5.9 MB - so we’re looking at 18.2 MB in total. When trying to launch it, I hit “Error mapping file: mono_file_map_error failed”, but I’m going to trust that except for this bug, things would work with Mono and the result would be 18.2 MB.<br>Unlike CoreCLR, Mono also depends on the Visual C++ runtime redistributable library that is not available in a default Windows installation: to keep the goal of the app being self-contained, we need to carry this library with the app. This increases the footprint of the application by another megabyte or so.<br>We would likely be able to make things smaller by adding trimming to the mix, but we’re going to hit the same problem as with CoreCLR - the size of the runtime (mono-2.0-sgen.dll) is 5.9 MB (plus the size of the C++ runtime libraries on top of it), and represents the floor of where any possible IL-level optimization could bring us.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/05-graph3.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/05-graph3.png" target="_blank"></a><img alt="With Mono" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/05-graph3.png" referrerpolicy="no-referrer"><br>With Mono<br><br>It is clear that to get anywhere near the 8 kB goal, we need to take the runtime out of the app. The only .NET runtime where this is possible is <a data-tooltip-position="top" aria-label="https://github.com/dotnet/corert" rel="noopener nofollow" class="external-link" href="https://github.com/dotnet/corert" target="_blank">CoreRT</a>. While it’s common to call CoreRT a “runtime”, it’s closer to being a “runtime library”. It’s not a virtual machine like CoreCLR or Mono - the CoreRT’s runtime is just a set of functions that support ahead of time generated native code produced by CoreRT’s ahead of time compiler.<br>CoreRT comes with libraries that make CoreRT look like any other .NET runtime: there’s a library that adds GC, library that adds support for reflection, library that adds a JIT, library that adds an interpreter, etc. But all of those libraries are optional (and that includes the GC).<br>More on how CoreRT differs from CoreCLR and Mono is in <a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2019/05/01/fight-the-global-warming-compile-your-csharp-apps-ahead-of-time/" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2019/05/01/fight-the-global-warming-compile-your-csharp-apps-ahead-of-time/" target="_blank">this article</a>. When I was reading about the runtime of the <a data-tooltip-position="top" aria-label="https://theartofmachinery.com/2017/06/04/what_is_the_d_runtime.html" rel="noopener nofollow" class="external-link" href="https://theartofmachinery.com/2017/06/04/what_is_the_d_runtime.html" target="_blank">D language</a>, it reminded me of CoreRT a lot. The article is an interesting read too.<br>Let’s see where we’re with the default CoreRT configuration:<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT
<br>This comes down to 4.7 MB. It’s the smallest so far, but still not good enough.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/06-graph4.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/06-graph4.png" target="_blank"></a><img alt="With CoreRT" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/06-graph4.png" referrerpolicy="no-referrer"><br>With CoreRT<br>The CoreRT ahead of time compiler offers a <a data-tooltip-position="top" aria-label="http://aka.ms/OptimizeCoreRT" rel="noopener nofollow" class="external-link" href="http://aka.ms/OptimizeCoreRT" target="_blank">vast number</a> of settings that affect code generation. By default, the compiler tries to maximize the generated code speed and compatibility with other .NET runtimes at the expense of the size of the generated executable.<br>The compiler has a built-in trimmer that removes unused code. The “CoreRT-Moderate” setting that we define in the Snake project relaxes one of the restrictions on removing unused code that allows more removal. We also ask the compiler to trade program speed for some extra bytes. Most .NET programs would work just fine in this mode.<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-Moderate
<br>We’re now at 4.3 MB.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/07-graph5.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/07-graph5.png" target="_blank"></a><img alt="With stuff removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/07-graph5.png" referrerpolicy="no-referrer"><br>With stuff removed<br><br>I’ve grouped a couple more compilation options into a “high savings” mode. This mode is going to remove support for things that many apps would notice, but Snake (being the low level thing that it is) won’t.<br>We are going to remove:<br>
<br>Stack trace data for framework implementation details
<br>Exception messages in framework-thrown exceptions
<br>Support for non-English locales
<br>EventSource instrumentation
<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-High
<br>We’ve reached 3.0 MB. This is 5% of what we started with, but CoreRT has one more trick up its sleeve.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/08-graph6.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/08-graph6.png" target="_blank"></a><img alt="With more stuff removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/08-graph6.png" referrerpolicy="no-referrer"><br>With more stuff removed<br><br>Substantial part of the CoreRT runtime libraries is dedicated to the implementation of the .NET reflection surface area. Because CoreRT is an ahead-of-time-compiled runtime-library-based .NET implementation, it doesn’t need most of the data structures a typical VM-based runtime (like CoreCLR and Mono) needs. This data includes things like names of types, methods, signatures, base types, etc. CoreRT embeds this data because programs using .NET reflection need it, but not because it’s needed for the runtime to operate. I call this data “the reflection tax”, because that’s what it is for the runtime.<br>CoreRT supports a <a data-tooltip-position="top" aria-label="https://github.com/dotnet/corert/blob/master/Documentation/using-corert/reflection-free-mode.md" rel="noopener nofollow" class="external-link" href="https://github.com/dotnet/corert/blob/master/Documentation/using-corert/reflection-free-mode.md" target="_blank">reflection-free mode</a> that avoids this tax. You might feel that a lot of .NET code wouldn’t work without reflection and you might be right, but a surprising amount of things do work: Gui.cs, System.IO.Pipelines, or even a basic WinForms app. Snake will definitely work, so let’s turn this mode on:<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-ReflectionFree
<br>We’re now at 1.2 MB. The reflection tax is a pretty heavy tax!<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/09-graph7.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/09-graph7.png" target="_blank"></a><img alt="With more stuff removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/09-graph7.png" referrerpolicy="no-referrer"><br>With more stuff removed<br><br>Now we’ve reached the end of what’s possible with the .NET SDK and we need to get our hands dirty. What we’re going to do now is starting to be ridiculous and I wouldn’t expect anyone else to do this. We’re going to rely on the implementation details of the CoreRT compiler and runtime.<br>As we saw earlier, CoreRT is a set of runtime libraries coupled with an ahead of time compiler. What if we replace the runtime libraries with a minimal reimplementation? We’ve decided not to use the garbage collector and that makes this job much more feasible.<br>Let’s start with the easy things:<br>namespace System.Threading
{
    static class Thread
    {
        [DllImport("api-ms-win-core-synch-l1-2-0")]
        public static extern void Sleep(int delayMs);
    }
}

namespace System
{
    static class Environment
    {
        [DllImport("api-ms-win-core-sysinfo-l1-1-0")]
        private static extern long GetTickCount64();

        public static long TickCount64 =&gt; GetTickCount64();
    }
}
<br>There - we just reimplemented Thread.Sleep and Environment.TickCount64 (for Windows) while avoiding all dependencies on the existing runtime library.<br>Let’s do the same for the subset of System.Console that the game uses:<br>namespace System
{
    static class Console
    {
        private enum BOOL : int
        {
            FALSE = 0,
            TRUE = 1,
        }

        [DllImport("api-ms-win-core-processenvironment-l1-1-0")]
        private static unsafe extern IntPtr GetStdHandle(int c);

        private readonly static IntPtr s_outputHandle = GetStdHandle(-11);

        private readonly static IntPtr s_inputHandle = GetStdHandle(-10);

        [DllImport("api-ms-win-core-console-l2-1-0.dll", EntryPoint = "SetConsoleTitleW")]
        private static unsafe extern BOOL SetConsoleTitle(char* c);
        public static unsafe string Title
        {
            set
            {
                fixed (char* c = value)
                    SetConsoleTitle(c);
            }
        }

        [StructLayout(LayoutKind.Sequential)]
        struct CONSOLE_CURSOR_INFO
        {
            public uint Size;
            public BOOL Visible;
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleCursorInfo(IntPtr handle, CONSOLE_CURSOR_INFO* cursorInfo);

        public static unsafe bool CursorVisible
        {
            set
            {
                CONSOLE_CURSOR_INFO cursorInfo = new CONSOLE_CURSOR_INFO
                {
                    Size = 1,
                    Visible = value ? BOOL.TRUE : BOOL.FALSE
                };
                SetConsoleCursorInfo(s_outputHandle, &amp;cursorInfo);
            }
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleTextAttribute(IntPtr handle, ushort attribute);

        public static ConsoleColor ForegroundColor
        {
            set
            {
                SetConsoleTextAttribute(s_outputHandle, (ushort)value);
            }
        }

        [StructLayout(LayoutKind.Sequential)]
        private struct KEY_EVENT_RECORD
        {
            public BOOL KeyDown;
            public short RepeatCount;
            public short VirtualKeyCode;
            public short VirtualScanCode;
            public short UChar;
            public int ControlKeyState;
        }

        [StructLayout(LayoutKind.Sequential)]
        private struct INPUT_RECORD
        {
            public short EventType;
            public KEY_EVENT_RECORD KeyEvent;
        }

        [DllImport("api-ms-win-core-console-l1-2-0", EntryPoint = "PeekConsoleInputW", CharSet = CharSet.Unicode)]
        private static unsafe extern BOOL PeekConsoleInput(IntPtr hConsoleInput, INPUT_RECORD* lpBuffer, uint nLength, uint* lpNumberOfEventsRead);

        public static unsafe bool KeyAvailable
        {
            get
            {
                uint nRead;
                INPUT_RECORD buffer;
                while (true)
                {
                    PeekConsoleInput(s_inputHandle, &amp;buffer, 1, &amp;nRead);

                    if (nRead == 0)
                        return false;

                    if (buffer.EventType == 1 &amp;&amp; buffer.KeyEvent.KeyDown != BOOL.FALSE)
                        return true;

                    ReadConsoleInput(s_inputHandle, &amp;buffer, 1, &amp;nRead);
                }
            }
        }

        [DllImport("api-ms-win-core-console-l1-2-0", EntryPoint = "ReadConsoleInputW", CharSet = CharSet.Unicode)]
        private static unsafe extern BOOL ReadConsoleInput(IntPtr hConsoleInput, INPUT_RECORD* lpBuffer, uint nLength, uint* lpNumberOfEventsRead);

        public static unsafe ConsoleKeyInfo ReadKey(bool intercept)
        {
            uint nRead;
            INPUT_RECORD buffer;
            do
            {
                ReadConsoleInput(s_inputHandle, &amp;buffer, 1, &amp;nRead);
            }
            while (buffer.EventType != 1 || buffer.KeyEvent.KeyDown == BOOL.FALSE);

            return new ConsoleKeyInfo((char)buffer.KeyEvent.UChar, (ConsoleKey)buffer.KeyEvent.VirtualKeyCode, false, false, false);
        }

        struct SMALL_RECT
        {
            public short Left, Top, Right, Bottom;
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleWindowInfo(IntPtr handle, BOOL absolute, SMALL_RECT* consoleWindow);

        public static unsafe void SetWindowSize(int x, int y)
        {
            SMALL_RECT rect = new SMALL_RECT
            {
                Left = 0,
                Top = 0,
                Right = (short)(x - 1),
                Bottom = (short)(y - 1),
            };
            SetConsoleWindowInfo(s_outputHandle, BOOL.TRUE, &amp;rect);
        }

        [StructLayout(LayoutKind.Sequential)]
        struct COORD
        {
            public short X, Y;
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleScreenBufferSize(IntPtr handle, COORD size);

        public static void SetBufferSize(int x, int y)
        {
            SetConsoleScreenBufferSize(s_outputHandle, new COORD { X = (short)x, Y = (short)y });
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleCursorPosition(IntPtr handle, COORD position);

        public static void SetCursorPosition(int x, int y)
        {
            SetConsoleCursorPosition(s_outputHandle, new COORD { X = (short)x, Y = (short)y });
        }

        [DllImport("api-ms-win-core-console-l1-2-0", EntryPoint = "WriteConsoleW")]
        private static unsafe extern BOOL WriteConsole(IntPtr handle, void* buffer, int numChars, int* charsWritten, void* reserved);

        public static unsafe void Write(char c)
        {
            int dummy;
            WriteConsole(s_outputHandle, &amp;c, 1, &amp;dummy, null);
        }
    }
}
<br>Let’s rebuild the game with this replacement framework:<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-ReflectionFree /p:IncludePal=true
<br>Unsurprisingly, this didn’t save us much. The APIs we’re replacing are already relatively lightweight, and rewriting them only gains a couple kilobytes that are not worth mentioning. But this is an important stepping stone to the last step in our journey.<br><br>The remaining 1.2 MB of code and data in the Snake game is there to support things we don’t see, but are there - ready in case we need them. There’s the garbage collector, support for exception handling, the code to format and print stack traces to the console when an unhandled exception happens, and many other things that are “under the hood”.<br>The compiler could detect that none of this is needed and avoid generating them, but what we’re trying to do is so weird that it’s not worth adding compiler features to support it. The way to avoid it is to simply provide an alternative runtime library.<br>Let’s start with redefining a minimal version of the base types:<br>namespace System
{
    public class Object
    {
        // The layout of object is a contract with the compiler.
        public IntPtr m_pEEType;
    }
    public struct Void { }

    // The layout of primitive types is special cased because it would be recursive.
    // These really don't need any fields to work.
    public struct Boolean { }
    public struct Char { }
    public struct SByte { }
    public struct Byte { }
    public struct Int16 { }
    public struct UInt16 { }
    public struct Int32 { }
    public struct UInt32 { }
    public struct Int64 { }
    public struct UInt64 { }
    public struct IntPtr { }
    public struct UIntPtr { }
    public struct Single { }
    public struct Double { }

    public abstract class ValueType { }
    public abstract class Enum : ValueType { }

    public struct Nullable&lt;T&gt; where T : struct { }
    
    public sealed class String
    {
        // The layout of the string type is a contract with the compiler.
        public readonly int Length;
        public char _firstChar;

        public unsafe char this[int index]
        {
            [System.Runtime.CompilerServices.Intrinsic]
            get
            {
                return Internal.Runtime.CompilerServices.Unsafe.Add(ref _firstChar, index);
            }
        }
    }
    public abstract class Array { }
    public abstract class Delegate { }
    public abstract class MulticastDelegate : Delegate { }

    public struct RuntimeTypeHandle { }
    public struct RuntimeMethodHandle { }
    public struct RuntimeFieldHandle { }

    public class Attribute { }
}

namespace System.Runtime.CompilerServices
{
    internal sealed class IntrinsicAttribute : Attribute { }

    public class RuntimeHelpers
    {
        public static unsafe int OffsetToStringData =&gt; sizeof(IntPtr) + sizeof(int);
    }
}

namespace System.Runtime.InteropServices
{
    public enum CharSet
    {
        None = 1,
        Ansi = 2,
        Unicode = 3,
        Auto = 4,
    }

    public sealed class DllImportAttribute : Attribute
    {
        public string EntryPoint;
        public CharSet CharSet;
        public DllImportAttribute(string dllName) { }
    }

    public enum LayoutKind
    {
        Sequential = 0,
        Explicit = 2,
        Auto = 3,
    }

    public sealed class StructLayoutAttribute : Attribute
    {
        public StructLayoutAttribute(LayoutKind layoutKind) { }
    }
}
namespace Internal.Runtime.CompilerServices
{
    public static unsafe partial class Unsafe
    {
        // The body of this method is generated by the compiler.
        // It will do what Unsafe.Add is expected to do. It's just not possible to express it in C#.
        [System.Runtime.CompilerServices.Intrinsic]
        public static extern ref T Add&lt;T&gt;(ref T source, int elementOffset);
    }
}
<br>At this point let’s forgo the project file and dotnet CLI and launch the individual tools directly. We start by launching the C# compiler (CSC). I recommend launching these commands from the “x64 Native Tools Command Prompt for VS 2019” - it’s in your Start menu if you have Visual Studio installed. The right version of tools is on the PATH in that window.<br>The /noconfig, /nostdlib, and /runtimemetadataversion are the magic switches needed to compile something that defines System.Object. I chose the .ilexe file extension instead of .exe because .exe will be used for the finished product.<br>$ csc.exe /debug /O /noconfig /nostdlib /runtimemetadataversion:v4.0.30319 MiniBCL.cs Game\FrameBuffer.cs Game\Random.cs Game\Game.cs Game\Snake.cs Pal\Thread.Windows.cs Pal\Environment.Windows.cs Pal\Console.Windows.cs /out:zerosnake.ilexe /langversion:latest /unsafe
<br>This will successfully compile the IL bytecode version of the game with the C# compiler. We still need some sort of runtime to execute it.<br>Let’s try to feed this to the CoreRT ahead of time compiler to generate native code from the IL. If you followed the steps above, you’ll find ilc.exe, the CoreRT ahead of time compiler, in your NuGet package cache (somewhere like %USERPROFILE%.nuget\packages\runtime.win-x64.microsoft.dotnet.ilcompiler\1.0.0-alpha-27402–01\Tools).<br>$ ilc.exe zerosnake.ilexe -o zerosnake.obj --systemmodule zerosnake --Os -g
<br>This is going to crash with “Expected type ‘Internal.Runtime.CompilerHelpers.StartupCodeHelpers’ not found in module ‘zerosnake’”. Turns out that besides the obvious minimum that a managed developer would expect, there’s also a minimum that the CoreRT compiler needs to compile the input.<br>Let’s skip to the chase and add what’s needed:<br>namespace Internal.Runtime.CompilerHelpers
{
    // A class that the compiler looks for that has helpers to initialize the
    // process. The compiler can gracefully handle the helpers not being present,
    // but the class itself being absent is unhandled. Let's add an empty class.
    class StartupCodeHelpers
    {
    }
}

namespace System
{
    // A special type that the compiler uses to implement generic interfaces
    // (e.g. IEnumerable&lt;T&gt;) on arrays. Our arrays won't implement any generic interfaces.
    class Array&lt;T&gt; : Array { }
}

namespace System.Runtime.InteropServices
{
    // Custom attribute that marks a class as having special "Call" intrinsics.
    // The compiler has special logic handling types with this attribute.
    internal class McgIntrinsicsAttribute : Attribute { }
}

namespace System.Runtime.CompilerServices
{
    // A class responsible for running static constructors. The compiler will call into this
    // code to ensure static constructors run and that they only run once.
    [System.Runtime.InteropServices.McgIntrinsics]
    internal static class ClassConstructorRunner
    {
        private static unsafe IntPtr CheckStaticClassConstructionReturnNonGCStaticBase(ref StaticClassConstructionContext context, IntPtr nonGcStaticBase)
        {
            CheckStaticClassConstruction(ref context);
            return nonGcStaticBase;
        }

        private static unsafe void CheckStaticClassConstruction(ref StaticClassConstructionContext context)
        {
            // Very simplified class constructor runner. In real world, the class constructor runner
            // would need to be able to deal with potentially multiple threads racing to initialize
            // a single class, and would need to be able to deal with potential deadlocks
            // between class constructors.

            // If the class is already initialized, we're done.
            if (context.initialized == 1)
                return;

            // Mark the class as initialized.
            context.initialized = 1;

            // Run the class constructor.
            Call&lt;int&gt;(context.cctorMethodAddress);
        }

        // This is a special compiler intrinsic that calls the method pointed to by pfn.
        // The compiler generates code for this and we can just mark it `extern`.
        // Once C# gets proper function pointer support (planned for C# 9), this won't be needed.
        [System.Runtime.CompilerServices.Intrinsic]
        private static extern T Call&lt;T&gt;(System.IntPtr pfn);
    }

    // This data structure is a contract with the compiler. It holds the address of a static
    // constructor and a flag that specifies whether the constructor already executed.
    [System.Runtime.InteropServices.StructLayout(System.Runtime.InteropServices.LayoutKind.Sequential)]
    public struct StaticClassConstructionContext
    {
        // Pointer to the code for the static class constructor method. This is initialized by the
        // binder/runtime.
        public IntPtr cctorMethodAddress;

        // Initialization state of the class. This is initialized to 0. Every time managed code checks the
        // cctor state the runtime will call the classlibrary's CheckStaticClassConstruction with this context
        // structure unless initialized == 1. This check is specific to allow the classlibrary to store more
        // than a binary state for each cctor if it so desires.
        public int initialized;
    }
}
<br>Let’s rebuild the IL bytecode with this newly added code and re-rerun ILC.<br>$ csc.exe /debug /O /noconfig /nostdlib /runtimemetadataversion:v4.0.30319 MiniRuntime.cs MiniBCL.cs Game\FrameBuffer.cs Game\Random.cs Game\Game.cs Game\Snake.cs Pal\Thread.Windows.cs Pal\Environment.Windows.cs Pal\Console.Windows.cs /out:zerosnake.ilexe /langversion:latest /unsafe
$ ilc.exe zerosnake.ilexe -o zerosnake.obj --systemmodule zerosnake --Os -g
<br>Now we have zerosnake.obj - a standard object file that is no different from object files produced by other native compilers such as C or C++. The last step is linking it. We’ll use the link.exe tool that should be on the PATH of our “x64 Native Tools Command Prompt” (you might need to install the C/C++ development tools in Visual Studio).<br>link.exe /debug:full /subsystem:console zerosnake.obj /entry:__managed__Main
<br>The __managed__Main symbol name is a contract with the compiler - it’s the name of the managed entrypoint of the program that ILC created.<br>But it doesn’t work:<br>error LNK2001: unresolved external symbol RhpPInvoke
error LNK2001: unresolved external symbol SetConsoleTextAttribute
error LNK2001: unresolved external symbol WriteConsoleW
error LNK2001: unresolved external symbol GetStdHandle
...
fatal error LNK1120: 17 unresolved externals
<br>Some of these symbols look familiar - the linker doesn’t know where to look for the Windows APIs we call. Let’s add the import libraries for those:<br>$ link.exe /debug:full /subsystem:console zerosnake.obj /entry:__managed__Main kernel32.lib ucrt.lib
<br>This looks better - only 4 unresolved symbols:<br>error LNK2001: unresolved external symbol RhpPInvoke
error LNK2001: unresolved external symbol RhpPInvokeReturn
error LNK2001: unresolved external symbol RhpReversePInvoke2
error LNK2001: unresolved external symbol RhpReversePInvokeReturn2
fatal error LNK1120: 4 unresolved externals
<br>The remaining missing symbols are helpers that the compiler expects to find in the runtime library. The fact they’re missing is only discovered at the time of linking because these helpers are typically implemented in assembly and the compiler only refers to them by their symbolic name (as opposed to other compiler-required types and methods we provided above).<br>The helpers set up and tear down the stack frames when native code calls into managed code, and managed code calls into native code. This is necessary for the GC to operate. Since we don’t have a GC, let’s stub them out with a piece of C# and another magical attribute that the compiler understands.<br>namespace System.Runtime
{
    // Custom attribute that the compiler understands that instructs it
    // to export the method under the given symbolic name.
    internal sealed class RuntimeExportAttribute : Attribute
    {
        public RuntimeExportAttribute(string entry) { }
    }
}

namespace Internal.Runtime.CompilerHelpers
{
    class StartupCodeHelpers
    {
        // The containing type for these methods doesn't matter.
        // Let's park them in StarupCodeHelpers.
        
        [System.Runtime.RuntimeExport("RhpReversePInvoke2")]
        static void RhpReversePInvoke2(System.IntPtr frame) { }
        [System.Runtime.RuntimeExport("RhpReversePInvokeReturn2")]
        static void RhpReversePInvokeReturn2(System.IntPtr frame) { }
        [System.Runtime.RuntimeExport("RhpPInvoke")]
        static void RhpPinvoke(System.IntPtr frame) { }
        [System.Runtime.RuntimeExport("RhpPInvokeReturn")]
        static void RhpPinvokeReturn(System.IntPtr frame) { }
    }
}
<br>After rebuilding the C# source code with these modifications and re-running ILC, the linking will finally succeed.<br>We’re now at 27 kilobytes and the game still works!<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/10-graph8.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/10-graph8.png" target="_blank"></a><img alt="With runtime removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/10-graph8.png" referrerpolicy="no-referrer"><br>With runtime removed<br><br>The remaining kilobytes can be shaved off by using tricks native developers use to shrink their native apps.<br>We’re going to:<br>
<br>Disable incremental linking
<br>Strip relocation information
<br>Merge similar sections within the executable
<br>Set internal alignment within the executable to a small value
<br>$ link.exe /debug:full /subsystem:console zerosnake.obj /entry:__managed__Main kernel32.lib ucrt.lib /merge:.modules=.rdata /merge:.pdata=.rdata /incremental:no /DYNAMICBASE:NO /filealign:16 /align:16
<br>Success! 8176 bytes!<br>The game still works, and interestingly, it’s still fully debuggable - feel free to open the EXE in Visual Studio (File -&gt; Open Solution), open one of the C# files that are part of the game, set a breakpoint in it, hit F5 to launch the EXE, and see the breakpoint getting hit. You can disable optimizations in ILC to make the executable even more debuggable - just drop the –Os argument.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/11-graph9.png" rel="noopener nofollow" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/11-graph9.png" target="_blank"></a><img alt="After messing with linker" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/11-graph9.png" referrerpolicy="no-referrer"><br>After messing with linker<br><br>The executable still carries some data that is not essential - the ILC compiler just doesn’t expose command line options to disable their generation.<br>One of those data structures that gets generated but we don’t need is GC information for the individual methods. CoreRT has a precise garbage collector that requires each method to describe where references to GC heap are at each instruction of the method body. Since we don’t have a garbage collector in the Snake game, this data is unnecessary. Other runtimes (e.g. Mono) use a conservative garbage collector that doesn’t require this data (it simply assumes any piece of the stack and CPU registers could be a GC reference) - a conservative garbage collector trades GC performance for extra size savings. The precise garbage collector used in CoreRT can operate in conservative mode too, but it hasn’t been hooked up yet. It’s a potential future addition that we could then leverage to make things even smaller.<br>Maybe one day we can make a simplified version of our game fit into a 512 byte boot sector. Until then, happy hacking!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/csharp/building-a-self-contained-game-in-csharp-under-8-kilobytes.html</link><guid isPermaLink="false">Computer Science/Programming Language/CSharp/Building a self-contained game in CSharp under 8 kilobytes.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:43:07 GMT</pubDate><enclosure url="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Isomorphic .NET Support in Extism]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:csharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#csharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:dotnet" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#dotnet</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:csharp" class="tag" target="_blank" rel="noopener nofollow">#csharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:dotnet" class="tag" target="_blank" rel="noopener nofollow">#dotnet</a> <br>In the early days of computing, software was pretty much locked into specific hardware setups. Then came operating systems, providing a layer of abstraction over the hardware. However, that still didn’t quite cut it. Businesses wanted to write their apps once and have them work on all sorts of platforms. This demand gave birth to higher-level languages that brought along their own runtimes, making it possible for software to run seamlessly on different operating systems.<br>Today, many organizations manage a multitude of programming languages for distinct tasks. While several runtimes, such as .NET’s Common Language Runtime (“CLR”) and Oracle’s Java Virtual Machine (“JVM”), support multiple languages, they still operate within isolated ecosystems. Each ecosystem has its package managers and libraries, often leading to unnecessary code duplication and duplicated efforts.<br>WebAssembly (Wasm) to the rescue! Wasm allows developers to create applications and libraries in their language of choice, meaning you can build a library in one language and smoothly integrate it with others.<br>Microsoft has long recognized Wasm’s potential and has heavily invested in <a data-tooltip-position="top" aria-label="https://dotnet.microsoft.com/en-us/apps/aspnet/web-apps/blazor" rel="noopener nofollow" class="external-link" href="https://dotnet.microsoft.com/en-us/apps/aspnet/web-apps/blazor" target="_blank">Blazor</a>. Blazor runs .NET code in the browser. This way, C# and F# programmers can reuse their existing skills to write interactive apps for the web. In .NET 8, Microsoft is adding experimental support for WASI too. Which means .NET apps compiled to Wasm can now run on the server, computers, and mobile phones!<br>That’s where Extism steps in. Languages have varying levels of support for running and compiling to Wasm, and capabilities may change depending on what runtime is being used. Extism ensures that your plugins will run consistently across languages and platforms. We’ve <a data-tooltip-position="top" aria-label="https://dylibso.com/blog/why-extism/" rel="noopener nofollow" class="external-link" href="https://dylibso.com/blog/why-extism/" target="_blank">previously talked about the added value of Extism</a>, but in a nutshell, Extism offers a set of unified and user-friendly Plugin Development Kits (PDKs) and Software Development Kits (SDKs) that make it easy for you to develop and run plugins in your preferred programming language.<br><br>Extism makes it all the more convenient to build Wasm plugins. For example, consider this scenario: you have a messaging bot platform, and you want to empower your users to create their own bots. Typically, messaging platforms provide webhooks that users can use to write bots. But we want to invert the situation: we run the users code on our own infrastructure! Here’s a glimpse of how it could look when you use the <a data-tooltip-position="top" aria-label="https://github.com/extism/dotnet-pdk" rel="noopener nofollow" class="external-link" href="https://github.com/extism/dotnet-pdk" target="_blank">.NET Extism PDK</a>:<br>  static class Plugin
{
    [UnmanagedCallersOnly(EntryPoint = "bot_name")]
    public static void BotName()
    {
        Pdk.SetOutput("weather bot");
    }

    [UnmanagedCallersOnly(EntryPoint = "respond")]
    public static void Respond()
    {
        var message = Pdk.GetInputString();

        if (message.Contains("hi", StringComparison.OrdinalIgnoreCase))
        {
            Reply("Hello :-)");
        }
        else if (message.Contains("weather", StringComparison.OrdinalIgnoreCase))
        {
            // Get secrets and configuration from the Host
            if (!Pdk.TryGetConfig("weather-api-key", out var apiKey))
            {
                throw new Exception("Beep boop malfunction detected: API Key is not configured!");
            }

            var block = MemoryBlock.Find(Env.GetUserInfo());
            var json = block.ReadString();
            var userInfo = JsonSerializer.Deserialize&lt;UserInfo&gt;(json);

            // Call HTTP APIs
            var query = $"https://api.weatherapi.com/v1/current.json?key={apiKey}&amp;q={userInfo.City}&amp;aqi=no";
            var response = Pdk.SendRequest(new HttpRequest(query));
            var responseJson = response.Body.ReadString();
            var apiResponse = JsonSerializer.Deserialize&lt;ApiResponse&gt;(responseJson);

            Reply($"The current temparature in {userInfo.City} is {apiResponse.current.temp_c}°C");
        }
        else
        {
            Reply("""
                Hi, I am the weather bot. Commands:
                1. Hi
                2. How's the weather?
                """);
        }
    }

    static void Reply(string message)
    {
        var block = Pdk.Allocate(message);
        Env.SendMessage(block.Offset);
    }
}

// Import functions from the host
static class Env
{
    [DllImport("extism", EntryPoint = "send_message")]
    public static extern void SendMessage(ulong offset);

    [DllImport("extism", EntryPoint = "user_info")]
    public static extern ulong GetUserInfo();
}
<br>This small example demonstrates how we can easily import functions from the host, share data between the host and the plugin, and make HTTP requests. The host has full control over which HTTP hosts the plugin can make requests to and which files the plugin has access to.<br>We have also exported two functions for the host: bot_name provides the name of the bot and respond can process a message sent by the user. The .NET Extism PDK add the following capabilities on top of the .NET WASI SDK:<br>
<br>Support for DllImport for importing functions from the host.
<br>Support for UnmanagedCallersOnly for exporting C# and F# functions.
<br>A global exception handler that propagates the exceptions back to the host.
<br>Easily share data between the plugin and the host.<br>
Extism has PDKs for <a data-tooltip-position="top" aria-label="https://extism.org/docs/category/write-a-plug-in" rel="noopener nofollow" class="external-link" href="https://extism.org/docs/category/write-a-plug-in" target="_blank">many popular programming languages</a>, so your users can write plugins in their favorite programming language and it would still work the same way!
<br><br>Running plugins is equally easy thanks to our <a data-tooltip-position="top" aria-label="https://extism.org/docs/category/integrate-into-your-codebase" rel="noopener nofollow" class="external-link" href="https://extism.org/docs/category/integrate-into-your-codebase" target="_blank">Host SDKs</a>, here is how you’d call the plugin above using our <a data-tooltip-position="top" aria-label="https://github.com/extism/dotnet-sdk" rel="noopener nofollow" class="external-link" href="https://github.com/extism/dotnet-sdk" target="_blank">.NET SDK</a>:<br>var manifest = new Manifest(new PathWasmSource("Plugin.wasm"))
{
	// Provide configurations and secrets for the plugins
	Config = new Dictionary&lt;string, string&gt;
	{
		{ "weather-api-key", Environment.GetEnvironmentVariable("weather-api-key") }
	},

 	// Control which HTTP hosts the plugins can call
 	AllowedHosts = ["api.weatherapi.com"]
};

// Provide custom host functions
var functions = new[]
{
    HostFunction.FromMethod("send_message", IntPtr.Zero, (CurrentPlugin plugin, long offset) =&gt;
    {
        var message = plugin.ReadString(offset);
        Console.WriteLine($"bot says: {message}");
    }),

    HostFunction.FromMethod("user_info", IntPtr.Zero, (CurrentPlugin plugin) =&gt;
    {
        var json = JsonSerializer.Serialize(new UserInfo
        {
            FullName = "John Smith",
            City = "New York"
        });

        return plugin.WriteString(json);
    })
};

var plugin = new Plugin(manifest, functions, withWasi: true);

// Call functions exported by the plugin
var botName = plugin.Call("bot_name", "");
Console.WriteLine($"Bot Name: {botName}");

// Call respond with an empty input
plugin.Call("respond", "");

while (true)
{
    Console.Write("&gt; ");
    var message = Console.ReadLine();

    // Easily cancel plugin calls
    var cts = new CancellationTokenSource();
    cts.CancelAfter(TimeSpan.FromSeconds(1));

    plugin.Call("respond", message, cts.Token);
}
<br>And here is the result of running the host:<br>PS D:\x\dotnet\isomophic-dotnet-demo\Host&gt; dotnet run
Bot Name: weather bot
bot says: Hi, I am the weather bot. Commands:
1. Hi
2. How's the weather?
&gt; hi
bot says: Hello :-)
&gt; weather?
bot says: The current temparature in New York is 10.6°C
&gt;
<br>While this example is very simple, it serves as the foundation for a robust and highly extensible bot framework. Utilizing Wasm for this scenario offers a host of advantages:<br>
<br>Sandboxed Plugins: Wasm ensures that plugins operate within a secure sandbox. They can only access resources such as memory, files, and sockets if explicitly allowed by the Host.
<br>Language Flexibility: Your users can develop plugins in their preferred programming language.
<br>Resource Management: You can set precise limits on memory usage and execution timeouts, enhancing control over your bot framework.
<br>Less network traffic: Since you’re running the plugins on your own server, there is no need for webhooks which can improve the responsiveness of the bots.<br>
The complete code for this sample is <a data-tooltip-position="top" aria-label="https://github.com/dylibso/isomorphic-dotnet-demo" rel="noopener nofollow" class="external-link" href="https://github.com/dylibso/isomorphic-dotnet-demo" target="_blank">available on GitHub</a>.
<br>Finally, we want to thank the incredible .NET team for their invaluable support and guidance through the WASI SDK challenges. We’re excited about the future of .NET in Wasm.<br><a data-tooltip-position="top" aria-label="https://twitter.com/mhmd_azeez" rel="noopener nofollow" class="external-link" href="https://twitter.com/mhmd_azeez" target="_blank">Muhammad Azeez</a>, Senior Software Engineer]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/csharp/isomorphic-.net-support-in-extism.html</link><guid isPermaLink="false">Computer Science/Programming Language/CSharp/Isomorphic .NET Support in Extism.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:43:07 GMT</pubDate></item><item><title><![CDATA[FSM - Functional State Machines]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br><br>Let's refer to a type descriptions of a finite state machine (FSM) that we saw in <a data-tooltip-position="top" aria-label="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp#solution-3-functional-state-machine-like-approach" rel="noopener nofollow" class="external-link" href="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp#solution-3-functional-state-machine-like-approach" target="_blank">Functional, State Machine-like approach</a>:<br>module StateMachines =
    type FSM&lt;'Input, 'Output&gt; = FSM of 'Output
    //          \______ used  mostly for documentation - what input is possible
    type Evolve&lt;'Input, 'Output, 'FSM&gt; = 'Input -&gt; 'FSM -&gt; 'Output * 'FSM
    type Aggregate&lt;'Command, 'Event, 'State&gt; = FSM&lt;'Command, 'Event list * 'State&gt;
    //                           'Input   ____________/                \________ 'Output with implicit state
<br>It captures the story behind our intentions - Evolve&lt;'Input, 'Output, 'FSM&gt; function expresses what we can do with a finite state machine (FSM&lt;'Input, 'Output&gt;).<br>Imagine that we have a magical device that no one ever dreamed of - Func-o-meter.<br>Func-o-meter can measure and tell us how functional the given piece of code is.<br><img alt="Pasted image 20240912175130.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/attachments/pasted-image-20240912175130.png"><br>
A device for measuring functional levels of a given piece of code
<br>Regarding the symbols on the screen:<br>
<br>L means Low
<br>M means Medium
<br>H means High
<br>Sounds magical?<br>Good, because it is pure (pun intended) magic.<br>We just need to point at the code with those two strangely looking antennas and we can get the measurement result back.<br>The result you just saw, dear Reader, was me trying to use our Func-o-meter on <a data-tooltip-position="top" aria-label="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp#solution-1-object-oriented-approach" rel="noopener nofollow" class="external-link" href="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp#solution-1-object-oriented-approach" target="_blank">Object-Oriented approach</a> from previous tale.<br>Well, it's not that functional, isn't it?<br>Let's try to use Func-o-meter on our little snippet, presented just before.<br><img alt="Pasted image 20240912175307.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/attachments/pasted-image-20240912175307.png"><br>
Showing how much our FSM-like implementation is functional
<br>And here we are - it's pretty functional in comparison to OO one.<br>We are on the good path, aren't we?<br>Actually, it's really neat function (pun intended) of Func-o-meter - you are able to compare results of two consecutive measurements.<br>But wait, do you have any problem?<br><br>Building something just to build something is really interesting idea, but no.<br>We need to have a goal.<br>A problem to solve.<br>This time our "problem" is pretty straightforward - we are going to try to model door mechanism.<br>That mechanism can be either opened, or closed or locked.<br>As you probably can predict, dear Reader, only some actions are able to be applied to a door mechanism in a certain state.<br>We could conclude the rules in the following list:<br>
<br>door mechanism starts as Closed
<br>only when it's Closed, when trying to Open it, it becomes Opened
<br>only when it's Opened, when trying to Close it, it becomes Closed
<br>only when it's Closed, when trying to Lock it, it becomes Locked
<br>only when it's Locked, when trying to Unlock it, it becomes Closed
<br>If one could visualize it, it might look as follows:<br><img alt="Pasted image 20240912175321.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/attachments/pasted-image-20240912175321.png"><br>
A specification of how a door mechanism could work
<br>Is it too naive example?<br>We need to have a simple problem to focus more on the modeling aspect.<br>Ok, fasten your seat belts, and let's start!<br><br>Our first attempt, for gaining more functional points, will be by using <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Mealy_machine" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Mealy_machine" target="_blank">Mealy machines</a>.<br>As you probably know me a bit, dear Reader, I like to focus on language when modeling and solving problems.<br>Let's then use mathematical language and a bit of the theory to define a Mealy machine:<br>(S,S0​,Σ,Λ,T,G)<br>As we are diving into functional stuff, things must be mathematical in its purest form (as our functions, right?) - and also difficult.<br>Of course, I am joking.<br>We won't go into a theory that much.<br>Let's be pragmatic and focus on our goal - solving a real problem!<br>We could think of a Mealy finite state machine as such that its outputs are determined both by their current state and their current input.<br>A little computer that has some internal state and based on a given input, it change its internal state to one of finite states set.<br>We could represent it as follows:<br>type Mealy&lt;'Input, 'Output&gt; = 'Input -&gt; 'Output * Mealy&lt;'Input, 'Output&gt;
<br>So it basically gets input and does some magic to move to next state, based on some logic, and as an output it returns current state and itself.<br>But it's not really the same "self".<br>Our little machine got transformed by utilizing input and internal state - and we got "new existing" machine.<br>Immutability in its greatness.<br>No mutability, just transformation.<br>Turns out that it's a pretty well-known "problem" of representing Mealy state machines.<br>In one of my discussions with <a data-tooltip-position="top" aria-label="https://twitter.com/Savlambda" rel="noopener nofollow" class="external-link" href="https://twitter.com/Savlambda" target="_blank">borar</a>, he referred to <a data-tooltip-position="top" aria-label="https://github.com/purescript-contrib/purescript-machines" rel="noopener nofollow" class="external-link" href="https://github.com/purescript-contrib/purescript-machines" target="_blank">purescript-machines</a>.<br>It is a PureScript implementation of Mealy machines with halting.<br>Really briefly - halting means that that our machine can "stop" when reaching a termination state (it's on us when we define so).<br>I must admit it was a great resource to understand how to implement it correctly in F#.<br>With <a data-tooltip-position="top" aria-label="https://twitter.com/Savlambda" rel="noopener nofollow" class="external-link" href="https://twitter.com/Savlambda" target="_blank">borar</a>'s help, it was even easier.<br>So let's introduce "stopping a machine" concept and Mealy finite state machine properly:<br>type Step&lt;'Output, 'Continue&gt; =
| Stop
| ContinueWith of ('Output * 'Continue)

type Mealy&lt;'Input, 'Output&gt; = 
    Mealy of ('Input -&gt; Step&lt;'Output, Mealy&lt;'Input,'Output&gt;&gt;)
//       THIS IS OUR CONTINUATION! ______/
<br>In order to utilize "stopping a machine", we need to introduce a concept of Step.<br>It represents intention of how state machine should behave.<br>The most interesting part is ContinueWith because it yields information about the output and a continuation.<br>Typically a continuation is a function that contains description of what needs to be done next.<br>And now we have our precious machinery - Mealy.<br>It keeps a function from an input to a step.<br>An avid Reader might notice that Step, as a second type parameter, uses a continuation.<br>In our example its another Mealy type...Which holds a function.<br>Isn't it weird? Shocking?<br>So our Mealy is keeping a function that produces a pair of output (from a state machine) and a Mealy, that keeps a function that produces a pair of...<br>...I hope you see where it goes.<br>We are going to create a recursive function there.<br>Isn' it too dangerous to build such recursive construct in that particular way?<br>Well, in case we couldn't stop it, it might be a bit interesting to see how stack is exploding.<br>But thanks to Stop case of a Step type, any time our state machine yields Stop - no more steps are possible.<br>But where were we - we have the problem to solve!<br><br>Let's move to the most intersting part - modeling.<br>Nothing gives so much adrenaline like modeling, isn't it?<br>Let's start with possible actions and states that are possible in our little area of activity.<br>Just so you know, from now on I am going to use "commands", instead of actions.<br>type DoorMechanismState = Closed | Opened | Locked

type DoorMechanismCommand = Open | Close | Lock | Unlock

type DoorMechanism = Mealy&lt;DoorMechanismCommand, DoorMechanismState&gt;
<br>Simple, clean, understandable - F#.<br>We have our basic building concepts that work inside our domain.<br>Here is our recursive function we talked about:<br>let rec private continueAs (doorMechanism: DoorMechanismState): DoorMechanism =
    Mealy(fun doorMachineCommand -&gt;
        match doorMechanism, doorMachineCommand with
        | Closed, Open -&gt; ContinueWith(Opened, continueAs Opened)
        | Opened, Close -&gt; ContinueWith(Closed, continueAs Closed)
        | Closed, Lock -&gt; ContinueWith(Locked, continueAs Locked)
        | Locked, Unlock -&gt; ContinueWith(Closed, continueAs Closed)
        | _, _ -&gt; ContinueWith(doorMechanism, continueAs doorMechanism)
//  this is external "output" ____/              \___ here recursion happens!
    )
<br>The most interesting part here is that this function (continueAs) creates a function, that accepts a doorMachineCommand as input (which actually is input for our state machine), and gets "packaged" into a Mealy type.<br>Inside, we are modeling all rules and transitions.<br>As you can see, dear Reader, our little door mechanism is not yielding Stop.<br>In this tiny tale, our state machine will "never terminate".<br>Right now, we can't yet use our state machine.<br>Let's fix it by defining a Send function<br>type Send&lt;'Command, 'State&gt; =
    'Command -&gt; Mealy&lt;'Command, 'State&gt; -&gt; Step&lt;'State, Mealy&lt;'Command, 'State&gt;&gt;
<br>This function takes a command and a Mealy state machine and returns a Step.<br>Provided that state machine returned ContinueWith case, Step is going to keep a continuation function that will enable further state machine "evolution".<br>Let's implement it.<br>let send: Send&lt;DoorMechanismCommand, DoorMechanismState&gt; =
        fun doorMachineCommand (Mealy runDoorMechanismWith) -&gt; 
            runDoorMechanismWith doorMachineCommand
<br>It's not the most advanced piece of the code.<br>"Unwrapping" a Mealy machine in order to "run" our door mechanism with a given command.<br>We're almost there.<br>We somehow need to create a new door mechanism, right?<br>let create () = continueAs Closed
<br><br>Let's create a simple mechanism:<br>let doorMechanism = DoorMechanism.create()
<br>FSI shows the following result (on my local):<br>val doorMechanism: DoorMechanism = Mealy &lt;fun:continueAs@23&gt;
<br>Woah, perfect encapsulation!<br>Ok, let's make it moving by sending Open to it:<br>doorMechanism
|&gt; DoorMechanism.send Open
<br>In return, this gave:<br>val it:
  StateMachines.Mealy.Step&lt;DoorMechanismState,
                           StateMachines.Mealy.Mealy&lt;DoorMechanismCommand,
                                                     DoorMechanismState&gt;&gt; =
  ContinueWith (Opened, Mealy &lt;fun:continueAs@23&gt;)
<br>That's interesting, isn't it?<br>We got a Step that can be continued and it yielded Opened state of a state machine and...<br>...A continuation!<br>As it's opened, let's try to close it.<br>doorMechanism
|&gt; DoorMechanism.send Open
|&gt; DoorMechanism.send Close // ❌ bzzzzt, compile-time error!
<br>Oops, something isn't working correctly.<br>Shouldn't we be able to send another command?<br>Well, yes.<br>But we didn't get a Mealy state machine back!<br>We have information that a state machine can continue processing. What's more, we also got a pair of current state and state machine continuation.<br>Could we just pattern match on ContinueWith and continue processing?<br>Well, not really.<br>We might eventually get Step and then we won't get a continuation back.<br>Hey, we are doing serious functional programming here!<br>What would you say, dear Reader, if we pass a function into Step, instead of "getting a state machine" out of the step?<br>One could say that we "bind" a function to a given Step, so this Step will have the responsibility to run this function inside.<br>Let's then define bind function:<br>let bind f (step: Step&lt;'State, Mealy&lt;'Command,'State&gt;&gt;) =
    match step with
    | Stop -&gt; Stop
    | ContinueWith continueWith -&gt; f continueWith
<br>bind takes a function and a Step and provided that this Step can be continued, it applies a function to a continuation (yet another function).<br>So let's utilize it and fix compile-time error:<br>doorMechanism
|&gt; DoorMechanism.send Open
|&gt; bind (snd &gt;&gt; DoorMechanism.send Close) // ✅ works!
<br>As a result we get:<br>val it:
  StateMachines.Mealy.Step&lt;DoorMechanismState,
                           StateMachines.Mealy.Mealy&lt;DoorMechanismCommand,
                                                     DoorMechanismState&gt;&gt; =
  ContinueWith (Closed, Mealy &lt;fun:continueAs@23-1&gt;)
<br>As expected, our door mechanism is closed.<br>As you probably noticed, dear Reader, we used snd function that takes the second item from a pair.<br>That's because the result of send is a pair - current state machine state and "state machine itself" (in fact it's a continuation function but let's say it's a state machine).<br>So we just "discard" the current state, as we are not using it anywhere.<br>An avid Reader might correlate "bind" verb with Haskell community and well-known &gt;&gt;= operator.<br>Our language, F#, is so powerful that we can define such operator that will enable smoother usage:<br>let (&gt;&gt;=) step f = bind f step
<br>Yup, that's it - we are reordering the arguments.<br>And finally, the usage:<br>doorMechanism
|&gt; DoorMechanism.send Open
&gt;&gt;= (discardOutput &gt;&gt; DoorMechanism.send Close)
&gt;&gt;= (discardOutput &gt;&gt; DoorMechanism.send Lock)
&gt;&gt;= (discardOutput &gt;&gt; DoorMechanism.send Open)
<br>Our final output is as follows:<br>val it:
  StateMachines.Mealy.Step&lt;DoorMechanismState,
                           StateMachines.Mealy.Mealy&lt;DoorMechanismCommand,
                                                     DoorMechanismState&gt;&gt; =
  ContinueWith (Locked, Mealy &lt;fun:continueAs@23-1&gt;)
<br>I added an alias discardOutput for snd so that it communicates the intention (actually, that's the most important principle in software design!)<br><br>Tired?<br>I hope not.<br>We have things to explore!<br>Let's use our Func-o-meter and check how functional we are!<br>How many functional points will we get?!<br><img alt="Pasted image 20240912175449.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/attachments/pasted-image-20240912175449.png"><br>
Showing how much our Mealy implementation is functional
<br>That's just fantastic!<br>Mealy state machine implementation beats <a data-tooltip-position="top" aria-label="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp#solution-3-functional-state-machine-like-approach" rel="noopener nofollow" class="external-link" href="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp#solution-3-functional-state-machine-like-approach" target="_blank">Functional, State Machine-like approach</a>!<br>Hurray!<br>Steady, gradual progress!<br>It's much more functional - we utilized recursion, pattern matching, we encoded invariants what does it mean "to stop a running state machine".<br>That's the engineering we are looking for!<br>Strange, I thought it will be close to High.<br>Can we be more functional?<br><br>Previous solution for sure is functional but is it readable?<br>Could we do a litmus test and show it to any "door mechanisms" expert and ask if we modelled it correctly?<br>Well, recursion does not help, for sure.<br>Maybe we should revisit our problem that we are trying to provide a solution for.<br>Let's bring the rules we started with:<br>
<br>door mechanism starts as Closed
<br>only when it's Closed, when trying to Open it, it becomes Opened
<br>only when it's Opened, when trying to Close it, it becomes Closed
<br>only when it's Closed, when trying to Lock it, it becomes Locked
<br>only when it's Locked, when trying to Unlock it, it becomes Closed
<br>I really believe that modelling "around" the language has superpowers included in it.<br>Ubiquitous language should live not only in spoken language, in models in our heads, but also in the code.<br>Ideally, we should be able to translate those rules into a specification on how the door mechanism works.<br>By using pseudocode, we could achieve something like this:<br>door mechanism:
    starts as Closed
    on Closed when trying to Open then becomes Closed
    on Opened when trying to Close then becomes Opened
    on Closed when trying to Lock then becomes Locked
    on Locked when trying to Unlock then becomes Closed
<br>It's very readable, even non-technical experts will be able to comprehend it.<br>Also, it's very declarative - we state what happens and all "hows" are hidden, "abstracted away".<br>All the words we have used in our specification - they form a domain-specific language (DSL).<br>If we call it a ubiquitous language or domain-specific language - those are just labels for the same concept - efficient communication and smoother collaboration.<br>Ok, enough talking, let's build such declarative DSL in F#!<br>We are going to employ very powerful feature of F# - <a data-tooltip-position="top" aria-label="https://learn.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions" rel="noopener nofollow" class="external-link" href="https://learn.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions" target="_blank">Computation Expressions</a> (or CEs, shortly).<br>Computation Expressions empower us to create domain-specific languages, e.g.:<br>
<br><a data-tooltip-position="top" aria-label="https://compositionalit.github.io/farmer/" rel="noopener nofollow" class="external-link" href="https://compositionalit.github.io/farmer/" target="_blank">Farmer</a> for expressing Infrastructure as Code (IaC)
<br><a data-tooltip-position="top" aria-label="https://github.com/slaveOftime/Fun.Build" rel="noopener nofollow" class="external-link" href="https://github.com/slaveOftime/Fun.Build" target="_blank">Fun.Build</a> for expressing CI/CD build scripts (and pipelines)
<br><a data-tooltip-position="top" aria-label="https://github.com/sleepyfran/sharp-point" rel="noopener nofollow" class="external-link" href="https://github.com/sleepyfran/sharp-point" target="_blank">sharp-point</a> for building presentations
<br>and more!
<br>It sounds like a perfect match - we want to express a specific language, coming from our tiny toy domain.<br><br>Yet again, let's remind ourselves main words (types) used in our "door mechanism area":<br>type DoorState = Closed | Opened | Locked
type DoorCommand = Open | Close | Lock | Unlock
<br>We know what are the core concepts.<br>Our first specification, we want to implement, is creating a state machine that starts in a certain state:<br>let machine = stateMachine "doorMechanism" {
    startsAs Closed
}
<br>We have the following words to cover:<br>
<br>stateMachine
<br>startsAs
<br>stateMachine is actual computation expression, whereas startAs is a custom operator within this computation expression.<br>Let's define the following core building blocks for our DSL:<br>type HandleCommand&lt;'State, 'Command&gt; = 'Command -&gt; 'State
<br>It simply represents a something that we're going to call a command handler.<br>Let's now add low-level contexts that will constitue our state machine:<br>type StateMachineContext&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; =
{
    State: 'State
    Transitions: Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;
}
<br>It should be self explanatory - it's a context with current state and map of all defined transitions.<br>Transitions are modeled as Map of possible states that lead to Map of commands, leading to functions for handling commands.<br>I think being "low-level" here will express the details (not the intentions) might be interesting to see that "there's no magic" (the only magical thing in this tale is our Func-o-meter, remember about it dear Reader).<br>And finally our state machine, that will be created from specification (we are wishing to build):<br>and StateMachine&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; = StateMachine of stateMachine: StateMachineContext&lt;'State, 'Command&gt;
<br>Now we are ready to roll out the implementation for stateMachine computation expression:<br>type StateMachineAssembler&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;(name: string) =
    member this.Yield(_) = StateMachine { CurrentState = Unchecked.defaultof&lt;_&gt;; Transitions = Map.empty } 

    // 👇🏻 more members will come!
<br>Our state machine assembler accepts a name, but it isn't used anywhere.<br>Then we have a first method, that are "natively" supported (and in some cases required) - Yield.<br>It helps returning a value from the computation expression.<br>This partical overloading of Yield says that given any argument, it's going to yield a broken state machine.<br>Why broken?<br>The fact we used Unchecked.defaultof&lt;_&gt; which does not look functional, and safe.<br>"Functional" might mean predictive, readable, safe and explicit.<br>Are we really dealing with StateMachine? Or maybe it's something different?<br>Of course, it turns out that we got a hint from our computation expression - in our lingo, the operation of building a state machine can be named as "assembling a state machine".<br>This pesky and implicit Unchecked.defaultof&lt;_&gt; (which in fact will manifest as dreaded null) expresses a concept which is missing in our code but for sure lives in the area of our problem.<br>Let's make it explicit!<br>type NotAssembledStateMachineContext&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; =
{
    CurrentState: 'State option
    Transitions: Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;
}
and NotAssembledMachine&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; = NotAssembledMachine of stateMachine: NotAssembledStateMachineContext&lt;'State, 'Command&gt;
<br>Let's introduce NotAssembledMachine concept as type - it will be available only when specifying and assembling a state machine.<br>The external world should deal with StateMachine - which is ready to function (pun intended) and provide value to our consumers!<br>type StateMachineAssembler&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;(name: string) =
        member this.Yield(_) = NotAssembledMachine { CurrentState = None; Transitions = Map.empty } 
<br>Sounds like we are both having fun and modeling the domain of our tiny problem - neat!<br><br>It's time for our first custom operator: startsAs:<br>type StateMachineAssembler&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;(name: string) =
    // previous methods
    
    [&lt;CustomOperation "startsAs"&gt;]
    member  this.startsAs ((NotAssembledMachine machine): NotAssembledMachine&lt;'State, 'Command&gt;, state: 'State) : NotAssembledMachine&lt;'State, 'Command&gt; =
        NotAssembledMachine { machine with CurrentState = Some state }
<br>Looks quite simple - it accepts "previously" yielded/returned not assembled machine and makes it current state defined.<br>In fact - that the only requirement we have for each machine - it won't be able to do anything but at least it does no harm.<br>Now let's assemble our machine!<br><br>type StateMachineAssembler&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;(name: string) =
    // previous method
    member this.Run((NotAssembledMachine machine): NotAssembledMachine&lt;'State, 'Command&gt;) =
        match machine.CurrentState with
        | None -&gt; failwith "You need to provide initial state for a state machine using `startsAs` operator."
        | Some state -&gt; StateMachine { CurrentState = state; Transitions = machine.Transitions }
<br>We used another "native" and built-in operator - Run - it executes a computation expression and the type returned from this method is exactly what consumers are getting.<br>So we need to be careful and provide working state machine!<br>As you can see, dear Reader, we check if CurrentState is available.<br>Eventually, we should get correctly assembled StateMachine (or runtime exception, in case we didn't cover initial state).<br>Let's create "an instance" of state machine assembler:<br>let stateMachine&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; (name: string)  =
    StateMachineAssembler&lt;'State, 'Command&gt;  name
<br>When we use our specification:<br>let machine = stateMachine "doorMechanism" {
    startsAs Closed
}
<br>Everything compiles correctly!<br>Hurray!<br>Then, let's see what FSI tells us, when we "run" our machine assembler:<br>val machine: StateMachine&lt;DoorState,DoorCommand&gt; =
    StateMachine { CurrentState = Closed
                   Transitions = map [] }
<br><br>Our machine works but not works.<br>We are able to assemble it, require it to start in certain state aaaand that's it.<br>We need specification of state transitions!<br>Now, we are going to add new computation expression - states that will accept more computation expressions!<br>Our goal is to provide states description in the given form:<br>let machine = stateMachine "doorMechanism" {
    startsAs Closed
    states {
        on Closed {
            whenTryingTo Open (thenBecomes Opened)
        }
    }
}
<br>As you can see, dear Reader, we introduced the following concepts: states, on, whenTryingTo and thenBecomes.<br>Let's start with on computation expression.<br>Yet again, we need to have to have a "context" that will be evolving, as someone is going to use on computation expression.<br>Here's its type definition:<br>and StateTransitions&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; =
{
    OnState: 'State
    Transitions: Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;
}
<br>on computation expression is represented by StateTransitionsBuilder and looks as follows:<br>type StateTransitionsBuilder&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;(fromState: 'State) =
    member _.Yield _ = Map.empty
    
    [&lt;CustomOperation "whenTryingTo"&gt;]
    member _.whenTryingTo (ctx: Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;, command: 'Command, handleCommand:  HandleCommand&lt;'State, 'Command&gt;): Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt; =
        ctx |&gt; Map.add command handleCommand
    
    member this.Run(ctx: Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;): StateTransitions&lt;'State, 'Command&gt; =
        { OnState = fromState; Transitions = ctx }
<br>It looks pretty similar to our StateMachineAssembler.<br>What's worth noticing is that we are able to capture specific concepts through custom operators - as we saw just before.<br>We define whenTryingTo operator - it basically adds a new handle command function by a command to a map of all command handlers Map.<br>Eventually, we return StateTransitions for a particular state.<br>Now, we can define on:<br>let on&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; state =
    StateTransitionsBuilder&lt;'State, 'Command&gt; state
<br>Also, let's define a helper function that nicely captures the language:<br>let thenBecomes nextState command  = nextState
<br>It's dumb simple - just takes the state and returns it.<br>Time to check if our state transitions specification compiles!<br>let onClosedTransitions = on Closed {
    whenTryingTo Open (thenBecomes Opened)
}
<br>All green, we are good!<br>Let's define the last computation expression that will appear in this tiny tale - states<br><br>Now we were dealing with transitions for a single state.<br>Time to handle all state transitions!<br>We're going to start with a builder:<br>type StateMachineStateTransitionsBuilder'&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;() =
    member _.Yield(()) = ()
<br>Nothing fancy - just a "default" Yield that actually will forbid using states computation expression without any state transitions.<br>Let's add another Yield, but this time for single state transitions:<br>type StateMachineStateTransitionsBuilder'&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;() =
    // previous methods

    member _.Yield(stateTransitions: StateTransitions&lt;'State, 'Command&gt;) =
            Map.ofArray [| stateTransitions.OnState, stateTransitions.Transitions |]
<br>Quite verbose.<br>Let's move on.<br>We know that each our state transitions specification might have many different possible state transitions.<br>Then we need to handle "multiple" values being yielded when specying our state machine.<br>type StateMachineStateTransitionsBuilder'&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;() =
    // previous methods

    member _.Delay(f: unit -&gt; Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;) = f()
    member _.Combine(newTransitions: Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;, previousTransitions: Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;) =
            newTransitions |&gt; Map.fold (fun acc key value -&gt; Map.add key value acc) previousTransitions
    member x.For(transitions: Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;, f: unit -&gt; Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;) =
            x.Combine(transitions, f())
<br>Yet again, we need to define three "native" methods - Delay, Combine and For.<br>I omit describing them for brevity.<br>The most important part is the intention behind it - we are merging all incoming transitions for a single state into a map of such descriptions.<br>Eventually, we need to Run our builder to return a value from computation expression:<br>type StateMachineStateTransitionsBuilder'&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;() =
        // previous methods
        member x.Run(transitions: Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;) = transitions 
<br>Time to define states!<br>let states&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt; =
    StateMachineStateTransitionsBuilder&lt;'State, 'Command&gt;()
<br>Finally!<br>Let's use our newly created DSL to assemble a state machine!<br><br>WHAT?!<br>A compile-time error? How could this be?!<br>let machine = stateMachine "doorMechanism" {
    startsAs Closed // ❌ bzzzzt, compile-time error!
    states {
        on Closed {
            whenTryingTo Open (thenBecomes Opened)
        }
    }
}
<br>Inspecting the error message, one could see:<br>This control construct may only be used if the computation expression builder defines a 'For' method
<br>Hmmm, it actually makes sense.<br>Our state machine assembler can handle only single value that gets yielded.<br>Each expression within computation expression returns something - if we put more expressions, we need to handle all the values.<br>Ok, let's come back to our StateMachineAssembler and provide methods for handling multiple values being return from expressions, inside of a computation expression.<br>type StateMachineAssembler&lt;'State, 'Command when 'Command : comparison and 'State : comparison&gt;(name: string) =
        // previous methods
            
        member this.Yield((transitions): Map&lt;'State, Map&lt;'Command, HandleCommand&lt;'State, 'Command&gt;&gt;&gt;) =
            NotAssembledMachine { CurrentState = None; Transitions = transitions }
        
        member this.Delay(f: unit -&gt; NotAssembledMachine&lt;'State, 'Command&gt;) = f()
        member this.For((NotAssembledMachine machine): NotAssembledMachine&lt;'State, 'Command&gt;, f: unit -&gt; NotAssembledMachine&lt;'State, 'Command&gt;) =
            this.Combine(NotAssembledMachine machine, f())
        member this.Combine((NotAssembledMachine newMachine): NotAssembledMachine&lt;'State, 'Command&gt;, (NotAssembledMachine existingMachine): NotAssembledMachine&lt;'State, 'Command&gt;) =
            let newTransitions =
                newMachine.Transitions
                |&gt; Map.fold (fun acc key value -&gt; Map.add key value acc) existingMachine.Transitions
            
            NotAssembledMachine { CurrentState =  newMachine.CurrentState; Transitions = newTransitions  }
<br>It's pretty similar to what we saw in StateMachineStateTransitionsBuilder, when creating states computation expression.<br>All those methods are required to deal with many values.<br>What is worth noticing is the implementation of Combine:<br>// ...rest of computation expression
member this.Combine((NotAssembledMachine newMachine): NotAssembledMachine&lt;'State, 'Command&gt;, (NotAssembledMachine existingMachine): NotAssembledMachine&lt;'State, 'Command&gt;) =
    let newTransitions =
        newMachine.Transitions
        |&gt; Map.fold (fun acc key value -&gt; Map.add key value acc) existingMachine.Transitions
// ...rest of computation expression
<br>It takes two not assembled machines and combines their state transitions - literally it merges them.<br>After handling all the methods, required to support multiple values returned from expressions, it's all green!<br>let machine = stateMachine "doorMechanism" {
    startsAs Closed // ✅ compiles!
    states {
        on Closed {
            whenTryingTo Open (thenBecomes Opened)
        }
    }
}
<br><br>We are ready to express, using our DSL, the rules of a door mechanism:<br>
<br>door mechanism starts as Closed
<br>only when it's Closed, when trying to Open it, it becomes Opened
<br>only when it's Opened, when trying to Close it, it becomes Closed
<br>only when it's Closed, when trying to Lock it, it becomes Locked
<br>only when it's Locked, when trying to Unlock it, it becomes Closed
<br>let machine = stateMachine "doorMechanism" {
    startsAs Closed
    states {
        on Closed {
            whenTryingTo Open (thenBecomes Opened)
            whenTryingTo Lock (thenBecomes Locked)
        } 
        on Opened {
            whenTryingTo Close (thenBecomes Closed)
        }
        on Locked {
            whenTryingTo Unlock (thenBecomes Closed)
        }
    }
}
<br>And it all compiles!<br>It sounds very natural!<br>It is very explicit!<br>F# in its purest form ❤️<br>LET'S RUN OUR STATE MACHINE!<br>val machine: StateMachine&lt;DoorState,DoorCommand&gt; =
    StateMachine
      { CurrentState = Closed
        Transitions =
         map
           [(Closed,
             map [(Open, &lt;fun:machine@86-32&gt;); (Lock, &lt;fun:machine@87-33&gt;)]);
            (Opened, map [(Close, &lt;fun:machine@90-34&gt;)]);
            (Locked, map [(Unlock, &lt;fun:machine@93-35&gt;)])] }
<br>Specification of how our state machine works.<br>Isn't it beautiful?<br>But wait, we are not done yet.<br>We actually need to execute this specification!<br>As in the <a data-tooltip-position="top" aria-label="https://talesfrom.dev/blog/fsm-functional-state-machines#solution-1-mealy-state-machines" rel="noopener nofollow" class="external-link" href="https://talesfrom.dev/blog/fsm-functional-state-machines#solution-1-mealy-state-machines" target="_blank">Solution #1: Mealy State Machines</a>, we are going to define Send action:<br>type Send&lt;'State, 'Command when 'State : comparison and 'Command : comparison&gt; =
    'Command -&gt; StateMachine&lt;'State, 'Command&gt; -&gt; StateMachine&lt;'State, 'Command&gt;
<br>Pretty straightforward.<br>Now implementation of a send function:<br>let send: Send&lt;DoorState, DoorCommand&gt; =
        fun command (StateMachine fsm) -&gt;
            let newState =
                match fsm.Transitions |&gt; Map.tryFind fsm.CurrentState with
                | Some transitions -&gt;
                    match transitions |&gt; Map.tryFind command with
                    | Some transition -&gt; transition command
                    | None -&gt; fsm.CurrentState
                | None -&gt; fsm.CurrentState
            
            StateMachine { fsm with CurrentState = newState }
<br>This function takes a command and finds all specified transitions for a current state, subsequently trying to find a command handler for a given command.<br>In case it was found - it applies a transition function to the command (in our tiny tale it's just returning a new state), else it remains in the same state.<br>Are you ready?<br>This is the final of the all finals!<br>Let's send some commands to our door mechanism state machine:<br>let machine' =
    machine
    |&gt; send Open
    |&gt; send Close
    |&gt; send Lock
    |&gt; send Open
<br>FSI quickly gives us feedback:<br>  val machine': StateMachine&lt;DoorState,DoorCommand&gt; =
    StateMachine
      { CurrentState = Locked
        Transitions =
         map
           [(Closed,
             map [(Open, &lt;fun:machine@86-22&gt;); (Lock, &lt;fun:machine@87-23&gt;)]);
            (Opened, map [(Close, &lt;fun:machine@90-24&gt;)]);
            (Locked, map [(Unlock, &lt;fun:machine@93-25&gt;)])] }
<br>As expected, we can't open a locked door mechanism!<br>Also, we clearly can see all transitions that need to be interpreted (which we already did!), but it nice to have it in a quite readable form.<br><br>Wait!<br>We need to measure how functional is our DSL approach to create and run state machines!<br>Let's me pull Func-o-meter and check it.<br><img alt="Pasted image 20240912175517.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/attachments/pasted-image-20240912175517.png"><br>
Showing how much our DSL implementation is functional
<br>CAN YOU BELIEVE THAT?!<br>We attained functional enlightment, and our work is a masterpiece.<br>DSL-based state machine specification beats everything in the world (in our tiny world of this tiny tale, but still).<br>Are you feeling boosted?<br>Empowered?!<br><br>Well, we experienced and encountered maaaany things, along our journey.<br>We defined the domain of our problem and we tried to provide as functional solutions as possible.<br>We used helpful device called Func-o-meter to measure how functional solutions are:<br>
<br>Mealy State Machines
<br>Domain-specific language for defining state machines
<br>Unfortunately, I need to disappoint you, dear and patient Reader...<br>...Func-o-meter does not exist.<br>I must appologize you - I can't measure how functional the code is.<br>What does it really mean?<br>Recently, more and more languages are getting functional flavors (I am not calling them features, purposefully).<br>And that's good - but its worth keeping that in mind they might be "flavors", add-ins - not core concepts in which the language was forged.<br>Nothing wrong with that!<br>The language is a tool, and of course it shapes the way one is thinking, modeling and reasoning - still, the most important thing is delivering value to our users/customers, with software (or without it, actually).<br>So all the comparisons and measurements our imaginary Func-o-meter did, are farfetched.<br>Still we could define some tenets or attributes, that are attracted by functional-first thinking.<br>As I concluded in <a data-tooltip-position="top" aria-label="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp" rel="noopener nofollow" class="external-link" href="https://talesfrom.dev/blog/many-faces-of-ddd-aggregates-in-fsharp" target="_blank">Many faces of DDD Aggregates in F#</a><br>Conclusion 🔍<br>FP achieves evolvability through immutability<br>And we were able to see this phenomenon, while experiencing our tiny journey.<br>Another one I would add to "functional-first thinking" basket is composition - on all levels - function composition, type composition.<br>Both immutability and composition, in my mind, express the most fundamental part of functional-first thinking - transformative nature.<br>Processes have a transformative nature too.<br>They make one thing become another and we literally saw that, especially in our lovely DSL.<br>Of course, we don't need to model everything in such a way - there's <a data-tooltip-position="top" aria-label="https://talesfrom.dev/blog/the-cost-of-modeling" rel="noopener nofollow" class="external-link" href="https://talesfrom.dev/blog/the-cost-of-modeling" target="_blank">The cost of modeling</a> for each model we build.<br>But I believe functional-first thinking helps us to perform modeling on the "highest" possible level - "becoming" level (you don't know what I am talking about? you might be interested in <a data-tooltip-position="top" aria-label="https://talesfrom.dev/blog/modeling-maturity-levels" rel="noopener nofollow" class="external-link" href="https://talesfrom.dev/blog/modeling-maturity-levels" target="_blank">Modeling Maturity Levels</a>).<br>So what I would suggest is to think in the "functional-first" way (immutability, composition and "transformatively") and not to "be" functional.<br><br>Thank you for your patience, dear Reader.<br>I hope you found it interesting to see the same problem from many different perspectives and to model the solutions on various levels.<br>One thing that is so important and it would be a shame for me if I didn't mention it.<br>We played with the powerful, functional-first language that F# is, but still the most important aspect of our job is understanding the problem we need to solve.<br>This reminds me the following saying:<br>Conclusion 🔍<br>Well-defined problem contains half of the solution<br>As always, there's half truth, half "myth", but it gives the impression that we really need to be careful how we define our problems.<br>I feel it's a nice moment to stop our little journey, dear Reader.<br>Thank you and see you soon!<br>NOTE: source code will be shared soon]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/fsm-functional-state-machines.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSM - Functional State Machines/FSM - Functional State Machines.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:44:55 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/attachments/pasted-image-20240912175130.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsm-functional-state-machines/attachments/pasted-image-20240912175130.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[10 Tips for Productive FSharp Scripting]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:time" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#time</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ndclondon" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ndclondon</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>Scott Hanselman recently had a <a data-tooltip-position="top" aria-label="http://www.hanselman.com/blog/InteractiveCodingWithCAndFREPLsScriptCSOrTheVisualStudioInteractiveWindow.aspx" rel="noopener nofollow" class="external-link" href="http://www.hanselman.com/blog/InteractiveCodingWithCAndFREPLsScriptCSOrTheVisualStudioInteractiveWindow.aspx" target="_blank">nice post on C# and F# REPLs</a>, which reminded me of the time I started using F# scripts. Over time, I found out a couple of small tricks, which helped make the experience productive. I found about them mainly by accident, so I figured, let’s see if I can list them in one place! Some of these are super simple, some probably a bit obscure, but hopefully, one of them at least will make your path towards scripting nirvana an easier one…<br>
Note: these tips are not necessarily ordered by usefulness. For that matter, there might or might not be exactly 10 of them :)
<br><br>You can use the F# Interactive 2 ways: you can directly type code into FSI, the F# Interactive window, or you can write code in an .fsx file, and select pieces of the code you want to execute. I recommend the second approach, for at least two reasons. First, FSI is a very primitive environment, .fsx files provide a much richer experience (IntelliSense). Then this encourages writing clean scripts you can reuse later.<br>
This is not specific to scripts, but… if you are on Visual Studio, do yourself a service and install the <a data-tooltip-position="top" aria-label="http://fsprojects.github.io/VisualFSharpPowerTools/" rel="noopener nofollow" class="external-link" href="http://fsprojects.github.io/VisualFSharpPowerTools/" target="_blank">Visual F# Power Tools</a> - you’ll get nice things such as better code highlighting, refactoring, and more.
<br>To execute code interactively, simply type code in an .fsx file, select a block of code, and hit Alt + Enter. The selected code will be evaluated, and the result will show up in the FSI window. In Visual Studio, you can also select code and right-click “Execute in Interactive”, but shortcuts are way faster.<br>
You can also execute a single-line with Alt + '. I rarely use this option, but this can save you time because you don’t need to select the entire line of code.
<br>
In case the keyboard shortcuts to send code to FSI do not work anymore (ReSharper used to over-write them in the past), you can reset them in Visual Studio, by going to Tools / Options / Environment / Keyboard. The 2 commands you need to map are EditorContextMenus.CodeWindow.ExecuteInInteractive and EditorContextMenus.CodeWindow.ExecuteLineInInteractive.
<br>You can also use these shortcuts from a regular .fs file, which can be handy if you want to validate that a piece of code is behaving the way you want.<br>
Interactive coding is by far my main usage for scripts - I use it extensively to prototype designs, run dumb tasks, or explore data or libraries. I realized recently that a few of my C# friends use LinqPad for the same purpose.
<br><br>While I encourage working primarily from .fsx files, the FSI window is also very helpful. I use it primarily for small verifications. For instance, I might have in my script file code like this:<br>let add x y =
  x + y
<br>Once I send it for evaluation into FSI, I will see the following show up in FSI:<br>val add : x:int -&gt; y:int -&gt; int
&gt;
<br>My function add is now in memory, in my FSI session; I can start typing in the FSI window and use it:<br>&gt; add 1 2;;
val it : int = 3
&gt;
<br>Enter does not trigger execution in FSI. The ;; indicates to FSI “Please execute everything I just typed, up to that point”. This is useful if you want to type multiple lines of code in FSI, and execute them as a block.<br>
it: in our add 1 2 example, the result showed up as it. We simply ran add, but didn’t assign the result to anything. it now contains the result, until we run another expression. If you want to re-use that value, you can assign it in FSI, by doing for instance let x = it;;.′
<br>
Once a value is loaded in your FSI session, it will remain there, available to you until you shadow it (in the example above, x will remain available, until I run for instance let x = 42;;). This is extremely convenient: for instance, you can load a data file once let data = File.ReadAllLines path, and keep using data for as long as you want, without having to reload it between code changes.
<br>
FSI often shows an abbreviated version of values for large items. For instance, [1..999] will show up as val it : int list = [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; ...] - note the … at the end, which indicate that there is more.
<br>What if you inadvertently started a very long computation, or an infinite loop? In Visual Studio, you can either kill the session entirely, by right-clicking over the FSI window and selecting “Reset Interactive Session” or Ctrl + Alt + R, or cancel the latest evaluation you requested (“Cancel Interactive Evaluation”, or Ctrl + Break.).<br><br>Besides interactive scripting, you can also run a script from the command line, by using FSI.exe:<br>&gt;fsi.exe "C:\myscript.fsx"<br>
FSI.exe is typically located at C:\Program Files (x86)\Microsoft SDKs\F#\4.0\Framework\v4.0. You can also install it separately, see <a data-tooltip-position="top" aria-label="http://fsharp.org/" rel="noopener nofollow" class="external-link" href="http://fsharp.org/" target="_blank">fsharp.org/use</a> section for instructions for various platforms.
<br>You can define different behaviors in your script, depending on whether it is run interactively or from the command line, like this:<br>#if INTERACTIVE
let msg = "Interactive"
#else
let msg = "Not Interactive"
#endif

printfn "%s" msg
<br>Updated, Sep 19: thanks Matt Klein for <a data-tooltip-position="top" aria-label="http://stackoverflow.com/q/39581342/114519" rel="noopener nofollow" class="external-link" href="http://stackoverflow.com/q/39581342/114519" target="_blank">pointing the issue</a>.<br>For more information on FSI from the command line, <a data-tooltip-position="top" aria-label="https://msdn.microsoft.com/en-us/library/dd233175.aspx" rel="noopener nofollow" class="external-link" href="https://msdn.microsoft.com/en-us/library/dd233175.aspx" target="_blank">check the reference page here</a>.<br>Updated, Feb 20: <a data-tooltip-position="top" aria-label="https://twitter.com/genTauro42" rel="noopener nofollow" class="external-link" href="https://twitter.com/genTauro42" target="_blank">Ramon Soto Mathiesen</a> points out that <a data-tooltip-position="top" aria-label="https://twitter.com/genTauro42/status/696407757835132928" rel="noopener nofollow" class="external-link" href="https://twitter.com/genTauro42/status/696407757835132928" target="_blank">Tip 9 also applies to the command line</a>.<br><br>Sometimes, your script will reference another resource; for instance, you need to read the contents of a .txt file somewhere. You can use absolute path, as in:<br>File.ReadAllLines @"C:/data/myfile.txt"
<br>
Pre-pending a string with @ makes it a verbatim string, and ignore escape sequences, such as \.
<br>
Use / rather than \, so that path work both on Windows and Mono.
<br>However, if that resource lives in a location relative to your script, consider using relative path, so that you can move your script folder around without breaking it.<br>Relative paths can be a bit tricky; for instance, running the following code interactively…<br>System.Environment.CurrentDirectory
<br>… produces a potentially unexpected result in FSI:<br>val it : string = "C:\Users\Mathias Brandewinder\AppData\Local\Temp"
&gt;
<br>You can avoid these issues by using built-in constants, which refer respectively to the directory where the script lives, the script file name, and the current line of the script:<br>__SOURCE_DIRECTORY__
__SOURCE_FILE__
__LINE__
<br>So if your folder structure was along these lines…<br>root
  /src/script.fsx
  /data/data.txt
<br>… you could refer to the data file data.txt from your script like this:<br>let path = System.IO.Path.Combine(__SOURCE_DIRECTORY__,"..","data/data.txt")
System.IO.File.ReadAllText path
<br><br>By default, FSI loads FSharp.Core and nothing else. If you want to use System.DateTime, you will need to first open System in your script. If you want to use an assembly that is not part of the standard .NET distribution, you will need to reference it first using #r. Imagine for instance that you installed the Nuget package fsharp.data; to use it in your script, you would do something like:<br>#r @"../packages/FSharp.Data.2.2.5/lib/net40/FSharp.Data.dll"
open FSharp.Data
<br>
When you execute open System in interactive, don’t worry if nothing seems to happen: the only result is a new &gt; showing up in FSI.
<br>For assemblies that are part of .NET but not referenced by default, you can use a shorter version:<br>#r @"System.Xaml"
open System.Xaml
<br>
In Visual Studio, you can right-click a reference from Solution Explorer, and send to F# interactive. You can then directly open it, and start using it in FSI.
<br>Updated, Feb 20: <a data-tooltip-position="top" aria-label="https://twitter.com/sergey_tihon" rel="noopener nofollow" class="external-link" href="https://twitter.com/sergey_tihon" target="_blank">Sergey Tihon</a> shared an interesting comment, explaining where Tip 5 can sometimes go wrong. I’d say, try Tip 5 first, but be aware that this might at times not quite work:<br>
<a data-tooltip-position="top" aria-label="https://twitter.com/brandewinder" rel="noopener nofollow" class="external-link" href="https://twitter.com/brandewinder" target="_blank">@brandewinder</a> don't load assemblies like in Tip 5 ) <a rel="noopener nofollow" class="external-link" href="https://t.co/Owft1NmPoo" target="_blank">https://t.co/Owft1NmPoo</a>
— Sergey Tihon (@sergey_tihon) <a data-tooltip-position="top" aria-label="https://twitter.com/sergey_tihon/status/696395229285523456" rel="noopener nofollow" class="external-link" href="https://twitter.com/sergey_tihon/status/696395229285523456" target="_blank">February 7, 2016</a>
<br>Updated, Feb 20: <a data-tooltip-position="top" aria-label="https://twitter.com/dsyme" rel="noopener nofollow" class="external-link" href="https://twitter.com/dsyme" target="_blank">F# open source contributor Don Syme</a> share a related nice trick:<br>
<a data-tooltip-position="top" aria-label="https://twitter.com/jeroldhaas" rel="noopener nofollow" class="external-link" href="https://twitter.com/jeroldhaas" target="_blank">@jeroldhaas</a> <a data-tooltip-position="top" aria-label="https://twitter.com/sergey_tihon" rel="noopener nofollow" class="external-link" href="https://twitter.com/sergey_tihon" target="_blank">@sergey_tihon</a> <a data-tooltip-position="top" aria-label="https://twitter.com/brandewinder" rel="noopener nofollow" class="external-link" href="https://twitter.com/brandewinder" target="_blank">@brandewinder</a> Use <a data-tooltip-position="top" aria-label="https://twitter.com/hashtag/I?src=hash" rel="noopener nofollow" class="external-link" href="https://twitter.com/hashtag/I?src=hash" target="_blank"><code></code></a>#I SOURCE_DIRECTORY, it is wondrous, very satisfying. All relative paths then work
— Don Syme (@dsyme) <a data-tooltip-position="top" aria-label="https://twitter.com/dsyme/status/696429115184955393" rel="noopener nofollow" class="external-link" href="https://twitter.com/dsyme/status/696429115184955393" target="_blank">February 7, 2016</a>
<br><br>The Nuget package manager is useful to consume existing packages. However, by default, Nuget stores assemblies in a folder that includes the package version number. This is very impractical for a script. In our example above, if fsharp.data gets an update, our script reference will be broken once we update the Nuget package:<br>#r @"../packages/FSharp.Data.2.2.5/lib/net40/FSharp.Data.dll"<br>Fixing the script requires manually editing the version number in the path, which quickly becomes a pain. <a data-tooltip-position="top" aria-label="https://fsprojects.github.io/Paket/" rel="noopener nofollow" class="external-link" href="https://fsprojects.github.io/Paket/" target="_blank"><strong></strong></a>Paket provides a better experience, because it stores packages without the version number, in this case, under:<br>#r @"../packages/FSharp.Data/lib/net40/FSharp.Data.dll"<br>Your scripts will now gracefully handle version number changes.<br>If you end up consuming numerous packages, you can make your life even easier, by referencing paths where assemblies might be searched for, using #I:<br>#I @"../packages/
#r @"FSharp.Data/lib/net40/FSharp.Data.dll"
<br>
If your primary goal is to “just script”, consider using <a data-tooltip-position="top" aria-label="https://atom.io/" rel="noopener nofollow" class="external-link" href="https://atom.io/" target="_blank">Atom</a> or <a data-tooltip-position="top" aria-label="https://code.visualstudio.com/" rel="noopener nofollow" class="external-link" href="https://code.visualstudio.com/" target="_blank">VSCode</a>, with the <a data-tooltip-position="top" aria-label="http://ionide.io/" rel="noopener nofollow" class="external-link" href="http://ionide.io/" target="_blank">Ionide plugin</a>. You can create and run free-standing F# scripts, with beautiful <a data-tooltip-position="top" aria-label="http://ionide.io/#paket-integration" rel="noopener nofollow" class="external-link" href="http://ionide.io/#paket-integration" target="_blank">Paket integration</a>.
<br><br>You might want to use the code from an existing file in your script. Suppose that we have a code file Code.fs somewhere, looking like this:<br>namespace Mathias

module Common =
  let hello name = sprintf "Hello, %s" name
<br>You can use that code from your script, by using the #load directive:<br>#load "Code.fs"
open Mathias.Common
hello "World"
<br>
You might have to close and re-open the script file if you end up changing the contents of the file.
<br>
If the file you are attempting to load contains references to other assemblies or files, you might get an error on the #load statement: “One or more errors in loaded files. The namespace or module … is not defined”. Simply reference the missing assemblies above the #load statement, so that your script uses the same dependencies as the file it refers to.
<br><br>Another handy directive, #time, turns on basic profiling. Once it is executed, for every block of code you send for execution you will see timing and garbage collection information. For instance, running this code…<br>#time
[| 1 .. 10000000 |] |&gt; Array.map (fun x -&gt; x * x)
<br>… will produce the following in FSI:<br>--&gt; Timing now on

Real: 00:00:00.887, CPU: 00:00:00.828, GC gen0: 2, gen1: 2, gen2: 2
val it : int [] =
  [|1; 4; 9; 16; 25; 36; 49; // snipped for brevity
<br>We get the wall time and CPU time it took, as well as some information about garbage collection in generations 0, 1 and 2. This would not replace a full-blown profiler, but this is an awfully convenient tool to figure out quickly if there are obvious ways to improve a piece of code.<br>Note that every time you execute #time, the timer will be switched from on to off, or vice-versa. This is not always convenient; you can also explicitly set it to the desired state, like this:<br>#time "on"
// everything now is timed
#time "off"
<br>
If you are interested in profiling, you should take a look at <a data-tooltip-position="top" aria-label="http://www.privateeye.io/" rel="noopener nofollow" class="external-link" href="http://www.privateeye.io/" target="_blank">PrivateEye</a>; check out <a data-tooltip-position="top" aria-label="https://twitter.com/gregyoung" rel="noopener nofollow" class="external-link" href="https://twitter.com/gregyoung" target="_blank">Greg Young</a>’s <a data-tooltip-position="top" aria-label="https://vimeo.com/131637366" rel="noopener nofollow" class="external-link" href="https://vimeo.com/131637366" target="_blank">talk at NDC Oslo 2015</a> to get a feel for what it does.
<br><br>Hat tip to <a data-tooltip-position="top" aria-label="https://twitter.com/rickasaurus" rel="noopener nofollow" class="external-link" href="https://twitter.com/rickasaurus" target="_blank">Rick Minerich</a> for that one. I’ll refer you to his blog post to see how to <a data-tooltip-position="top" aria-label="http://richardminerich.com/2013/03/setting-up-fsharp-interactive-for-machine-learning-with-large-datasets/" rel="noopener nofollow" class="external-link" href="http://richardminerich.com/2013/03/setting-up-fsharp-interactive-for-machine-learning-with-large-datasets/" target="_blank">set FSI to 64 bits to handle large datasets</a>.<br><br>Did you know that you could…<br>
<br><a data-tooltip-position="top" aria-label="https://channel9.msdn.com/Events/Visual-Studio/Visual-Studio-2015-Final-Release-Event/Six-Quick-Picks-from-Visual-F-40" rel="noopener nofollow" class="external-link" href="https://channel9.msdn.com/Events/Visual-Studio/Visual-Studio-2015-Final-Release-Event/Six-Quick-Picks-from-Visual-F-40" target="_blank">debug an F# script? (around 0:12:35 in)</a>
<br><a data-tooltip-position="top" aria-label="http://www.swensensoftware.com/fseye" rel="noopener nofollow" class="external-link" href="http://www.swensensoftware.com/fseye" target="_blank">inspect the objects in your FSI session with <strong></strong>?</a>FsEye
<br>change the FSI font size in Tools/Options/Environment/Fonts and Colors/Show Settings for/F# Interactive?
<br>add your own pretty-printer to FSI, <a data-tooltip-position="top" aria-label="https://github.com/mathnet/mathnet-numerics/blob/master/src/FSharp/MathNet.Numerics.fsx" rel="noopener nofollow" class="external-link" href="https://github.com/mathnet/mathnet-numerics/blob/master/src/FSharp/MathNet.Numerics.fsx" target="_blank">like this</a>?
<br>mess with your coworkers’ mental sanity, by executing (* (opening a multiline comment) in FSI? (credit: <a data-tooltip-position="top" aria-label="https://twitter.com/tomaspetricek" rel="noopener nofollow" class="external-link" href="https://twitter.com/tomaspetricek" target="_blank">Tomas</a>)
<br>simplify loading references with Visual Studio and Power Tools? (credit: <a data-tooltip-position="top" aria-label="https://twitter.com/kitlovesfsharp" rel="noopener nofollow" class="external-link" href="https://twitter.com/kitlovesfsharp" target="_blank">Kit Eason</a>, see details in comments section).
<br>And again… if you are not using the <a data-tooltip-position="top" aria-label="http://fsprojects.github.io/VisualFSharpPowerTools/" rel="noopener nofollow" class="external-link" href="http://fsprojects.github.io/VisualFSharpPowerTools/" target="_blank">Visual F# Power Tools</a>, you are missing out:<br>
"Don't let your friends try <a data-tooltip-position="top" aria-label="https://twitter.com/hashtag/fsharp?src=hash" rel="noopener nofollow" class="external-link" href="https://twitter.com/hashtag/fsharp?src=hash" target="_blank"></a><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> without installing <a data-tooltip-position="top" aria-label="https://twitter.com/FSPowerTools" rel="noopener nofollow" class="external-link" href="https://twitter.com/FSPowerTools" target="_blank">@FSPowerTools</a>." <a data-tooltip-position="top" aria-label="https://twitter.com/dsyme" rel="noopener nofollow" class="external-link" href="https://twitter.com/dsyme" target="_blank">@dsyme</a> at <a data-tooltip-position="top" aria-label="https://twitter.com/hashtag/ndclondon?src=hash" rel="noopener nofollow" class="external-link" href="https://twitter.com/hashtag/ndclondon?src=hash" target="_blank"></a><a href="https://muqiuhan.github.io/wiki?query=tag:ndclondon" class="tag" target="_blank" rel="noopener nofollow">#ndclondon</a>
— Tomas Petricek (@tomaspetricek) <a data-tooltip-position="top" aria-label="https://twitter.com/tomaspetricek/status/687934127627186176" rel="noopener nofollow" class="external-link" href="https://twitter.com/tomaspetricek/status/687934127627186176" target="_blank">January 15, 2016</a>
<br>That’s what I got! I am sure I forgot some - do you have a useful or favorite trick to share?]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/10-tips-for-productive-fsharp-scripting.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/10 Tips for Productive FSharp Scripting.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Dec 2024 12:28:27 GMT</pubDate></item><item><title><![CDATA[Amoeba optimization method using FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>My favorite column in MSDN Magazine is Test Run; it was originally focused on testing, but the author, James McCaffrey, has been focusing lately on topics revolving around numeric optimization and machine learning, presenting a variety of methods and approaches. I quite enjoy his work, with one minor gripe –his examples are all coded in C#, which in my opinion is really too bad, because the algorithms would gain much clarity if written in F# instead.<br>Back in June 2013, he published a piece on <a data-tooltip-position="top" aria-label="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" rel="noopener nofollow" class="external-link" href="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" target="_blank">Amoeba Method Optimization using C#</a>. I hadn’t seen that approach before, and found it intriguing. I also found the C# code a bit too hairy for my feeble brain to follow, so I decided to rewrite it in F#.<br>In a nutshell, the Amoeba approach is a heuristic to find the minimum of a function. Its proper respectable name is the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method" target="_blank">Nelder-Nead method</a>. The reason it is also called the Amoeba method is because of the way the algorithm works: in its simple form, it starts from a triangle, the “Amoeba”; at each step, the Amoeba “probes” the value of 3 points in its neighborhood, and moves based on how much better the new points are. As a result, the triangle is iteratively updated, and behaves a bit like an Amoeba moving on a surface.<br>Before going into the actual details of the algorithm, here is how my final result looks like. You can find the entire code <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Amoeba" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/Amoeba" target="_blank">here on GitHub</a>, with some usage examples in the Sample.fsx script file. Let’s demo the code in action: in a script file, we load the Amoeba code, and use the same function the article does, the <a data-tooltip-position="top" aria-label="http://mathworld.wolfram.com/RosenbrockFunction.html" rel="noopener nofollow" class="external-link" href="http://mathworld.wolfram.com/RosenbrockFunction.html" target="_blank">Rosenbrock function</a>. We transform the function a bit, so that it takes a Point (an alias for an Array of floats, essentially a vector) as an input, and pass it to the solve function, with the domain where we want to search, in that case, [ –10.0; 10.0 ] for both x and y:<br>#load "Amoeba.fs"
 
open Amoeba
open Amoeba.Solver
 
let g (x:float) y =
100. * pown (y - x * x) 2 + pown (1. - x) 2
 
let testFunction (x:Point) =
g x.[0] x.[1]
 
solve Default [| (-10.,10.); (-10.,10.) |] testFunction 1000
<br>Running this in the F# interactive window should produce the following:<br>val it : Solution = (0.0, [|1.0; 1.0|]) 
&gt;
<br>The algorithm properly identified that the minimum is 0, for a value of x = 1.0 and y = 1.0. Note that results may vary: this is a heuristic, which starts with a random initial amoeba, so each run could produce slightly different results, and might at times epically fail.<br>So how does the algorithm work?<br>I won’t go into full detail on the implementation, but here are some points of interest. At each iteration, the Amoeba has a collection of candidate solutions, Points that could be a Solution, with their value (the value of the function to be minimized at that point). These points can be ordered by value, and as such, always have a best and worst point. The following picture, which I lifted from the article, shows what points the Amoeba is probing:<br><img alt="Amoeba" src="https://mathias-brandewinder.github.io//assets/amoeba.png" referrerpolicy="no-referrer"><br>Source: <a data-tooltip-position="top" aria-label="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" rel="noopener nofollow" class="external-link" href="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" target="_blank">“Amoeba Optimization Method in C#”</a><br>The algorithm constructs a Centroid, the average of all current solutions except the worst one, and attempts to replace the Worst with 3 candidates: a Contracted, Reflected and Expanded solution. If none of these is satisfactory (the rules are pretty straightforward in the code), the Amoeba shrinks towards the Best solution. In other words, first the Amoeba searches for new directions to explore by trying to replace its current Worst solution, and if no good change is found, it shrinks on itself, narrowing down around its current search zone towards its current Best candidate.<br>If you consider the diagram, clearly all transformations are a variation on the same theme: take the Worst solution and the Centroid, and compute a new point by stretching it by different values: –50% for contraction, +100% for reflection, and +200% for expansion. For that matter, the shrinkage can also be represented as a stretch of –50% towards the Best point.<br>This is what I ended up with:<br>type Point = float []
type Settings = { Alpha:float; Sigma:float; Gamma:float; Rho:float; Size:int }
 
let stretch ((X,Y):Point*Point) (s:float) =
Array.map2 (fun x y -&gt; x + s * (x - y)) X Y
 
let reflected V s = stretch V s.Alpha
let expanded V s = stretch V s.Gamma
let contracted V s = stretch V s.Rho
<br>I defined Point as an alias for an array of floats, and a Record type Settings to hold the parameters that describe the transformation. The function stretch takes a pair of points and a float (by how much to stretch), and computes the resulting Point by taking every coordinate, and going by a ratio s from x towards y. From then on, defining the 3 transforms is trivial; they just use different values from the settings.<br>Now that we have the Points represented, the other part of the algorithm requires evaluating a function at each of these points. That part was done with a couple types:<br>type Solution = float * Point
type Objective = Point -&gt; float
 
type Amoeba =
{ Dim:int; Solutions:Solution [] } // assumed to be sorted by fst value
member this.Size = this.Solutions.Length
member this.Best = this.Solutions.[0]
member this.Worst = this.Solutions.[this.Size - 1]
 
let evaluate (f:Objective) (x:Point) = f x, x
let valueOf (s:Solution) = fst s
<br>A Solution is a tuple, a pair associating a Point and the value of the function at that point. The function we are trying to minimize, the Objective, takes in a point, and returns a float. We can then define an Amoeba as an array of Solutions, which is assumed to be sorted. Nothing guarantees that the Solutions are ordered, which bugged me for a while; I was tempted to make that type private or internal, but this would have caused some extra hassle for testing, so I decided not to bother with it. I added a few convenience methods on the Amoeba, to directly extract the Best and Worst solutions, and two utility functions, evaluate, which associates a Point with its value, and its counter-part, valueOf, which extracts the value part of a Solution.<br>The rest of the code is really mechanics; I followed the algorithm notation from the Wikipedia page, rather than the MSDN article, because it was actually a bit easier to transcribe, built the search as a recursion (of course), which iteratively transforms an Amoeba for a given number of iterations. For good measure, I introduced another type, Domain, describing where the Amoeba should begin searching, and voila! We are done. In 91 lines of F#, we got a full implementation.<br><br>What I find nice about the algorithm is its relative simplicity. One nice benefit is that it doesn’t require a derivative. Quite often, search algorithms use a gradient to evaluate the slope and decide what direction to explore. The drawback is that first, computing gradients is not always fun, and second, there might not even be a properly defined gradient in the first place. By contrast, the Amoeba doesn’t require anything – just give it a function, and let it probe. In some respects, the algorithm looks to me like a very simple genetic algorithm, maintaining a population of solutions, breeding new ones and letting a form of natural selection operate.<br>Of course, the price to pay for this simplicity is that it is a heuristic, that is, there is no guarantee that the algorithm will find a good solution. From my limited experimentations with it, even in simple cases, failures were not that unusual. If I get time for this, I think it would be fun to try launching multiple searches, and stopping when, say, the algorithm has found the same Best solution a given number of times.<br>Also, note that in this implementation, 2 cases are not covered: the case where the function is not defined everywhere (some Points might throw an exception), and the case where the function doesn’t have a minimum. I will let the enterprising reader think about how that could be handled!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/amoeba-optimization-method-using-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Amoeba optimization method using FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 14 Feb 2025 04:03:59 GMT</pubDate><enclosure url="https://mathias-brandewinder.github.io//assets/amoeba.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://mathias-brandewinder.github.io//assets/amoeba.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Applied Meta-Programming In FSharp With Myriad And Falanx]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <br>
<a rel="noopener nofollow" class="external-link" href="https://dev.to/7sharp9/applied-meta-programming-with-myriad-and-falanx-7l4" target="_blank">https://dev.to/7sharp9/applied-meta-programming-with-myriad-and-falanx-7l4</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/applied-meta-programming-in-fsharp-with-myriad-and-falanx.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Applied Meta-Programming In FSharp With Myriad And Falanx.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Tue, 14 Jan 2025 06:24:45 GMT</pubDate></item><item><title><![CDATA[Baby steps with CNTK and FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:deep-learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#deep-learning</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:deep-learning" class="tag" target="_blank" rel="noopener nofollow">#deep-learning</a><br>So what have I been up to lately? Obsessing over <a data-tooltip-position="top" aria-label="https://www.microsoft.com/en-us/cognitive-toolkit/" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/en-us/cognitive-toolkit/" target="_blank">CNTK, the Microsoft deep-learning library</a>. Specifically, the team released a <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/cognitive-toolkit/cntk-library-managed-api" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/cognitive-toolkit/cntk-library-managed-api" target="_blank">.NET API</a>, which got me interested in exploring how usable this would be from the F# scripting environment. I started a <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp" target="_blank">repository to try out some ideas already</a>, but, before diving into that in later posts, I figure I could start by a simple introduction, to set some context.<br>First, what problem does CNTK solve?<br>Imagine that you are interested in predicting something, and that you have data available, both inputs you can observe (the features), and the values you are trying to predict (the labels). Imagine now that you have an idea of the type of relationship between the input and the output, something along the lines of:<br>labels ≈ function(features, parameters).<br>To make this more concrete, that function could be quite complex, and involve multiple layers of input transformation into the final output (“deep learning”), or it could be quite simple, for instance a traditional linear regression, something along the lines of:<br>car price ≈ car years * coefficient1 + car engine size * coefficient2 + constant.<br>In this particular case, we have 2 features (car years and car engine size), 1 label (car price), and 3 parameters (coefficient1, coefficient2 and constant) - and we would like to find “good” values for the 3 parameters so that the predicted value is in general close to the correct value.<br>The purpose of CNTK is to:<br>
<br>let you specify a function connecting input and output,
<br>let you specify how to read example data to learn from,
<br>learn good parameter values from the example data,
<br>let you learn parameters on CPU or GPU, for large datasets and complex functions.
<br>With that in mind, let’s take a look at a very basic example, a simple linear regression. Using CNTK here is complete overkill, and not worth the overhead; I would not use it for something that simple. Our goal here is simply to illustrate the basics of how CNTK works, from F#. In future posts, we will look into scenarios where CNTK is actually useful. As a secondary goal, I want to discuss some of the aspects that make building a nice F# API on top of the current .NET one tricky.<br><br>First order of business: let’s load this thing into VS Code.<br>CNTK has a few packages on Nuget, based on what environment you want to run on. In our case, we will focus on a <a data-tooltip-position="top" aria-label="https://www.nuget.org/packages/CNTK.CPUOnly/" rel="noopener nofollow" class="external-link" href="https://www.nuget.org/packages/CNTK.CPUOnly/" target="_blank">CPU-only scenario, using the CNTK.CPUOnly 2.3.1 package</a>.<br>We assume that the <a data-tooltip-position="top" aria-label="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-fsharp" rel="noopener nofollow" class="external-link" href="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-fsharp" target="_blank">Ionide-fsharp</a> and <a data-tooltip-position="top" aria-label="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-Paket" rel="noopener nofollow" class="external-link" href="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-Paket" target="_blank">Ionide-Paket</a> extensions are installed in VS Code. Open the Folder where you want to work, and run the Paket: Init command (CTRL+SHIFT+P reveals the available commands). This will create a paket.dependencies file in the folder, where you can now specify what packages are needed, like this:<br>framework:net46
source https://www.nuget.org/api/v2
nuget CNTK.CPUOnly
<br>Run Paket: Install next, and let Paket do its magic, and download the required packages. Once the operation completes, you should see a new folder, packages, with the following structure:<br>packages
  CNTK.CPUOnly
    lib
      net45
        x64
          Cntk.Core.Managed-2.3.1.dll
    support
      x64
        Debug
        Dependency
        Release
<br>Let’s start creating the script we will be working with now, by adding an F# script file CNTK.fsx to our folder. Unfortunately, CNTK depends on a few native libraries to run properly. As a result, the setup is a bit more involved than the usual #r "path/to/library.dll. We’ll follow <a data-tooltip-position="top" aria-label="https://twitter.com/cdrnet" rel="noopener nofollow" class="external-link" href="https://twitter.com/cdrnet" target="_blank">@cdrnet</a> <a data-tooltip-position="top" aria-label="http://christoph.ruegg.name/blog/loading-native-dlls-in-fsharp-interactive.html" rel="noopener nofollow" class="external-link" href="http://christoph.ruegg.name/blog/loading-native-dlls-in-fsharp-interactive.html" target="_blank">approach to load native libraries described here</a>, and add to the PATH every folder that contains the dlls we need, so Cntk.Core.Managed-2.3.1.dll can find them:<br>
Note: I put the <a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" rel="noopener nofollow" class="external-link" href="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" target="_blank">full code used in the post on a gist here</a>
<br>open System
open System.IO

Environment.SetEnvironmentVariable("Path",
    Environment.GetEnvironmentVariable("Path") + ";" + __SOURCE_DIRECTORY__)

let dependencies = [
        "./packages/CNTK.CPUOnly/lib/net45/x64/"
        "./packages/CNTK.CPUOnly/support/x64/Dependency/"
        "./packages/CNTK.CPUOnly/support/x64/Dependency/Release/"
        "./packages/CNTK.CPUOnly/support/x64/Release/"    
    ]

dependencies 
|&gt; Seq.iter (fun dep -&gt; 
    let path = Path.Combine(__SOURCE_DIRECTORY__,dep)
    Environment.SetEnvironmentVariable("Path",
        Environment.GetEnvironmentVariable("Path") + ";" + path)
    )    

#I "./packages/CNTK.CPUOnly/lib/net45/x64/"
#I "./packages/CNTK.CPUOnly/support/x64/Dependency/"
#I "./packages/CNTK.CPUOnly/support/x64/Dependency/Release/"
#I "./packages/CNTK.CPUOnly/support/x64/Release/"

#r "./packages/CNTK.CPUOnly/lib/net45/x64/Cntk.Core.Managed-2.3.1.dll"
open CNTK
<br><br>We can now start using CNTK in our script. Let’s build a function that takes 2 floats as input, and returns a float as an output, multiplying each of the inputs by a parameter.<br>A core element in CNTK is the NDShape, for n-dimensional shape. Think of an NDShape as an n-dimensional array. A vector of size 5 would be an NDShape of dimension [ 5 ] (rank 1), a 12x18 image a NDShape [ 12; 18 ] (rank 2), a 10 x 10 RGB image a NDShape [ 10; 10; 3 channels ] (rank 3), and so on. In our case, the input is an array of size 2, and the output an array of size 1:<br>let inputDim = 2
let outputDim = 1
let input = Variable.InputVariable(NDShape.CreateNDShape [inputDim], DataType.Double, "input")
let output = Variable.InputVariable(NDShape.CreateNDShape [outputDim], DataType.Double, "output")
<br>Which produces the following output:<br>val inputDim : int = 2
val outputDim : int = 1
val input = Variable
val output = Variable
<br>Note how the numeric type of the Variable, DataType.Double, is passed in as a argument, and not generic. Note also how the numeric types are aligned with the C# convention; that is, a DataType.Double is an F# float, and a DataType.Float is an F# single.<br>We can ask a Variable about its shape, for instance input.Shape:<br>val it : NDShape = CNTK.NDShape { Dimensions = seq [2]; (* more stuff *) Rank = 1; }
<br>Let’s create our Function now:<br>let device = DeviceDescriptor.CPUDevice

let predictor =
    let dim = input.Shape.[0]
    let weights = new Parameter(NDShape.CreateNDShape [dim], DataType.Double, 0.0, device, "weights")
    // create an intermediate Function
    let product = CNTKLib.TransposeTimes(input, weights)    
    let constant = new Parameter(NDShape.CreateNDShape [ outputDim ], DataType.Double, 0.0, device, "constant") 
    CNTKLib.Plus(new Variable(product), constant)
<br>val device : DeviceDescriptor
val predictor : Function
<br>A couple of comments here. Our predictor creates a named Parameter weights of dimension and type matching the input Variable, with values initialized at 0.0. We multiply the two shapes together, by calling CNTKLib.TransposeTimes, computing x1 * w1 + x2 * w2, which returns a Function. We then create another Parameter for our constant, and sum them up, using CNTKLib.Plus.<br>Note how we have to explicitly convert product into a Variable in the final step, using new Variable(product). CNTKLib.Plus (and the other functions built in CNTKLib) expects 2 Variable arguments. Unfortunately, a Function is not a Variable, and they do not derive from a common class or interface. The .NET API supports implicit conversion between these 2 types, which works well in C#, where you could just sum these up directly, like this: CNTKLib.Plus(product, constant). F# doesn’t support implicit conversion, and as a result, this requires an annoying amount of explicit manual conversion to combine operations together.<br>Note also how we passed in device, a DeviceDescriptor, to the Parameter constructor. A CNTK Function is intended to run on a device, which must be specified. In this case, we could have omitted the device, in what case it would have picked up by default CPU.<br><br>Now that we have a Function - what can we do with it?<br>Unsuprisingly, we can pass input to a function, and compute the resulting value. We will do that next. However, before doing that, it’s perhaps useful to put things in perspective, to understand why this isn’t as straightforward as you might expect from something named a function. Once an F# function has been instantiated, its whole purpose is to transform an input value into an output value. The intent of a CNTK Function is subtly different: the objective here is to take a function, and modify its Parameters so that when passed in some input, the output it produces is close to some desired output, the Labels. In other words, we want a Function to be “trainable”: we want to be able to pass it known input/output pairs, and adjust the function parameters to fit the data better.<br>With that said, let’s evaluate our predictor function. To do that, we will need to do 3 things:<br>
<br>Supply values to fill in the “input” placeholder shape,
<br>Specify what values we want to observe - we might be interested in the output, but also the weights, for instance,
<br>Specify what device we want the function to run on.
<br>Let’s do that:<br>open System.Collections.Generic

let inputValue = Value.CreateBatch(NDShape.CreateNDShape [inputDim], [| 3.0; 5.0 |], device)
let inputMap = 
    let map = Dictionary&lt;Variable,Value&gt;()
    map.Add(input, inputValue)
    map

let predictedOutput = predictor.Output
let weights = 
    predictor.Parameters () 
    |&gt; Seq.find (fun p -&gt; p.Name = "weights")
let constant = 
    predictor.Parameters () 
    |&gt; Seq.find (fun p -&gt; p.Name = "constant")
let outputMap =
    let map = Dictionary&lt;Variable,Value&gt;()
    map.Add(predictedOutput, null)
    map.Add(weights, null)
    map.Add(constant, null)
    map

predictor.Evaluate(inputMap,outputMap,device)
<br>To evaluate a Function, we pass it the input we care about, a Dictionary&lt;Variable,Value&gt;, which we fill in with input, the Variable we defined earlier. We provide (completely arbitrarily) a value of [3.0;5.0] as an input value. In a similar fashion, we specify what we want to observe: the predicted value, predictor.Output, as well as the 2 named parameters we created, “weights” and “constant”, which we also retrieve from the Function itself. In this case, we set the Value to null, because we have no input to supply. Finally, we run predictor.Evaluate, which will take the inputMap and fill in the missing values in the outputMap.<br>We can now review the outputs:<br>let currentPrediction = 
    outputMap.[predictedOutput].GetDenseData&lt;float&gt;(predictedOutput) 
    |&gt; Seq.map (fun x -&gt; x |&gt; Seq.toArray)
    |&gt; Seq.toArray

let currentWeights = 
    outputMap.[weights].GetDenseData&lt;float&gt;(weights) 
    |&gt; Seq.map (fun x -&gt; x |&gt; Seq.toArray)
    |&gt; Seq.toArray

let currentConstant = 
    outputMap.[constant].GetDenseData&lt;float&gt;(constant) 
    |&gt; Seq.map (fun x -&gt; x |&gt; Seq.toArray)
    |&gt; Seq.toArray
<br>This is not pretty, but… we have values.<br>val currentPrediction : float [] [] = [| [| 0.0 |] |]
val currentWeights : float [] [] = [| [| 0.0; 0.0 |] |] 
val currentConstant : float [] [] = [| [| 0.0 |] |] 
<br>The values we get back are pretty unexciting, but at least they are what we would expect to see. Given that both weights and constant were initialized at 0.0, the function should produce a currentPrediction of 0.0 * 3.0 + 0.0 * 5.0 + 0.0, which is indeed 0.0.<br>Two quick notes here. First, because a value could be of any DataType, we have to manually specify a type when retrieving the values, as in GetDenseData&lt;float&gt;. Then, this is a very stateful model: when we fill in values for the input in the inputMap, we pass in the input instance we initially created to construct the Function. In a similar fashion, we are retrieving values from the instances we passed into the outputMap.<br><br>This was pretty painful. So what is our reward for that pain?<br>As I stated earlier, one defining feature of a Function is that it can be trained. What we mean by that is the following: we can take a Function, supply it batches of input and desired output pairs, and progressively adjust the internal Parameter(s) of the Function so that the values computed by the Function become close(r) to the desired output.<br>Let’s start with a simple illustration. Suppose for a minute that, for our input [ 3.0; 5.0 ], we expected a result of 10.0. Currently, our weights and constant are set to 0.0. By modifying these 3 values, we should be able to tune our predictor to get an answer of 10.0.<br>This is, of course, a silly example. There are many ways I could change the parameters to produce 10.0 - I could set the constant to 10.0, or the second weight to 2.0, or infinitely many other combinations. To get something meaningful, I would need many different input/output pairs. However, we’ll start with this, strictly to illustrate the mechanics involved.<br>Training a Function involves 3 elements:<br>
<br>Supplying a batch of input / output pairs (features and labels),
<br>Defining a measure of fit, that is, how to measure if a value is close to the desired value,
<br>Specifying how parameters should be adjusted to improve the function.
<br>let batchInputValue = Value.CreateBatch(NDShape.CreateNDShape [inputDim], [| 3.0; 5.0 |], device)
let batchOutputValue = Value.CreateBatch(NDShape.CreateNDShape [outputDim], [| 10.0 |], device)

let batch =
    [
        input,batchInputValue
        output,batchOutputValue
    ]
    |&gt; dict

let loss = CNTKLib.SquaredError(new Variable(predictor), output, "loss")
let evaluation = CNTKLib.SquaredError(new Variable(predictor), output, "evaluation")

let learningRatePerSample = new TrainingParameterScheduleDouble(0.01, uint32 1)
let learners = 
    ResizeArray&lt;Learner&gt;(
        [
            Learner.SGDLearner(predictor.Parameters(), learningRatePerSample)
        ]
        )

let trainer = Trainer.CreateTrainer(predictor, loss, evaluation, learners)

for i in 0 .. 10 do
    let _ = trainer.TrainMinibatch(batch, true, device)
    trainer.PreviousMinibatchLossAverage () |&gt; printfn "Loss: %f"
    trainer.PreviousMinibatchEvaluationAverage () |&gt; printfn "Eval: %f"
<br>First, we create a batch of input/output values ([ 3.0; 5.0 ] and [ 10.0 ]), and link them to the input and output Variable(s) we created. Then we define what measure we want to use to determine if a prediction is close or not from the target value. In this case, we use the built-in CNTKLib.SquaredError, which computes the square difference between the predicted value (new Variable(predictor)) and the target value (output). For instance, with the initial weights and constant, the predicted value will be 0.0, and we specified that the desired value was 10.0, so the loss function will evaluate to (0.0 - 10.0)^2, that is, 100.0 - and a perfect prediction of 10.0 would result in a loss of 0.0. Finally, without going into much detail, we specify in learners which strategy to apply when updating the function parameters. In this case, we use the built-in Stochastic Gradient Descent (SGD) strategy, with a learning rate of 0.01 (how aggressively to update the parameters) and a batch size of 1, using only one input/output pair at a time when performing adjustments.<br>We feed all that into a Trainer, and perform 10 updates (trainer.TrainMinibatch), using the same example input/output each time, and writing out the current value of the loss function:<br>Loss: 100.000000
Eval: 100.000000
Loss: 9.000000
Eval: 9.000000
// omitted intermediate results for brevity 
Loss: 0.000000
Eval: 0.000000
Loss: 0.000000
Eval: 0.000000
<br>As you can observe, the prediction error decreases rapidly, from 100.0 initially (as expected), to basically 0.0 after only 10 steps.<br>Let’s make this a bit more interesting, by feeding different examples to the model:<br>let realModel (features:float[]) =
    3.0 * features.[0] - 2.0 * features.[1] + 5.0

let rng = Random(123456)
let batch () =        
    let batchSize = 32        
    let features = [| rng.NextDouble(); rng.NextDouble() |]
    let labels = [| realModel features |]
    let inputValues = Value.CreateBatch(NDShape.CreateNDShape [inputDim], features, device)
    let outputValues = Value.CreateBatch(NDShape.CreateNDShape [outputDim], labels, device)
    [
        input,inputValues
        output,outputValues
    ]
    |&gt; dict
<br>Here we simply create a “true” function, realModel, which we use to generate synthetic data. We then modify our previous example, to feed 1,000 different examples for training:<br>#time "on"

for _ in 1 .. 1000 do
    
    let example = batch ()
    trainer.TrainMinibatch(example,true,device) |&gt; ignore
    trainer.PreviousMinibatchLossAverage () |&gt; printfn "Loss: %f"
<br>On my machine, extracting the weights and constant from the Function after training yields 3.0019, -1.9978 and 4.9975 - pretty close to the correct values of 3.0, -2.0 and 5.0 that we used in realModel.<br>
Note: I put the <a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" rel="noopener nofollow" class="external-link" href="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" target="_blank">full code used in the post on a gist here</a>
<br><br>First, I want to re-iterate that the example we went through is not showcasing a good example of where and how to use CNTK. It is intended primarily as an illustration of CNTK’s building blocks and how they work together. For a trivial linear regression example like this one (shallow learning, if you will), you would be better served with a standard library such as <a data-tooltip-position="top" aria-label="http://accord-framework.net/" rel="noopener nofollow" class="external-link" href="http://accord-framework.net/" target="_blank">Accord.NET</a>. CNTK becomes interesting if you have a deeper, more complex model, and a larger dataset - we’ll explore this in later posts.<br>
As a side-note, my initial intent was to use real batches for the final example, passing in multiple examples at once, but for reasons I couldn’t figure out yet, the code kept crashing.
<br>My second goal was to explore the design of the current .NET API, as a preliminary step before trying to build an F#-scripting friendly layer on top of it.<br>In its current state, the CNTK .NET library is fairly low-level, and rather unpleasant to work with from F#. Ideally, one would like to be able to create re-usable blocks and compose them easily, along the lines of the Keras model, using a DSL to, for instance, define a network by stacking standard transformation layers on top of each other.<br>Such a DSL seems quite possible to achieve in F#, but requires taking into account a few design considerations. First, the choice to use implicit conversion between Variable and Function makes composition of functions in F# painful. This choice is reasonable for C#, but requires re-wrapping every Function into a Variable to string operations together on the F# side.<br>One aspect I am not a fan of in the library is how the DeviceDescriptor leaks all the way down. With the current model, I could create 2 parameters, one on CPU, one on GPU, and combine them together, which doesn’t make a lot of sense. In an ideal world, I would like to define a Function independently of any device, and only then decide whether I want to train that model on a CPU or a GPU.<br>Finally, the fact that a Variable or a Function cannot be named after it was instantiated, as far as I can tell, introduces complications in composing blocks together. If naming was separate from instantiation, we could create a function like named : string -&gt; Function -&gt; Function, which could be inserted anywhere.<br>I haven’t had much time yet to dig into the data readers; so far, most of my efforts have gone into exploring possible directions to address the questions above. If you are interested, the <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp" target="_blank">master branch of my repository</a> contains working, straight conversions of the <a data-tooltip-position="top" aria-label="https://github.com/Microsoft/CNTK/tree/master/Examples/TrainingCSharp/Common" rel="noopener nofollow" class="external-link" href="https://github.com/Microsoft/CNTK/tree/master/Examples/TrainingCSharp/Common" target="_blank">C# examples published by the CNTK team</a>; the results of my explorations can be found in the 3 branches <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-varorfun" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-varorfun" target="_blank">experiment-varorfun</a>, <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-interpreter" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-interpreter" target="_blank">experiment-interpreter</a> and <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-stacking" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-stacking" target="_blank">experiment-stacking</a>.<br>I hope you found something of interest in this post! If you have feedback or suggestions, I would be quite interested to hear about them :) In the meanwhile, I will keep exploring - expect more on the topic in the near future!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/baby-steps-with-cntk-and-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Baby steps with CNTK and FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:44:08 GMT</pubDate></item><item><title><![CDATA[Basic Regression Tree]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:parser" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#parser</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:parser" class="tag" target="_blank" rel="noopener nofollow">#parser</a><br>In our previous installment, we <a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2016/08/06/gradient-boosting-part-1/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2016/08/06/gradient-boosting-part-1/" target="_blank">began exploring Gradient Boosting</a>, and outlined how by combining extremely crude regression models - stumps - we could iteratively create a decent prediction model for the quality of wine bottles, using one Feature, one of the chemical measurements we have available.<br>In and of itself, this is an interesting result: the approach allows us to aggregate mediocre indicators together into a predictor that is better than its individual parts. However, so far, we are using only a tiny subset of the information available. Why restrict ourselves to a single Feature, and not use all of them? And, if the approach works with something as weak as a stump, perhaps we can do better, by aggregating less trivial prediction models?<br>This will be our goal today: we will create a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Decision_tree_learning#Types" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Types" target="_blank">Regression Tree</a>, which we will in a future installment use in place of stumps in our Boosting procedure.<br><br><a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/05683d63bfa67c8b706ce458035c0b81#file-gradient-boosting-2-fsx" rel="noopener nofollow" class="external-link" href="https://gist.github.com/mathias-brandewinder/05683d63bfa67c8b706ce458035c0b81#file-gradient-boosting-2-fsx" target="_blank"><em></em></a>Full code for this post available here as a Gist<br>The Stump model is rather simple: we take a Feature and a split value, the threshold. If the input value is under that threshold, we predict the average output value computed across examples under the threshold, otherwise, we do the opposite:<br><img alt="Stump" src="https://mathias-brandewinder.github.io//assets/2016-08-14-stump.png" referrerpolicy="no-referrer"><br>Or, in code:<br>type Wine = CsvProvider&lt;"data/winequality-red.csv",";",InferRows=1500&gt;

type Observation = Wine.Row

type Feature = Observation -&gt; float

type Example = Observation * float

type Predictor = Observation -&gt; float

let learnStump (sample:Example seq) (feature:Feature) threshold =
    let under = 
        sample 
        |&gt; Seq.filter (fun (obs,lbl) -&gt; feature obs &lt;= threshold)
        |&gt; Seq.averageBy (fun (obs,lbl) -&gt; lbl)
    let over = 
        sample 
        |&gt; Seq.filter (fun (obs,lbl) -&gt; feature obs &gt; threshold)
        |&gt; Seq.averageBy (fun (obs,lbl) -&gt; lbl)
    fun obs -&gt;
        if (feature obs &lt;= threshold)
        then under
        else over
<br>A regression tree extends the idea further. Instead of limiting ourselves to a single threshold, we can further divide each group, and create trees like this one for instance:<br><img alt="Simple Tree" src="https://mathias-brandewinder.github.io//assets/2016-08-14-simple-tree.png" referrerpolicy="no-referrer"><br>Nothing forces us to keep the tree symmetrical, or to use a single Feature, though. This would be a perfectly acceptable tree as well:<br><img alt="Complex Tree" src="https://mathias-brandewinder.github.io//assets/2016-08-14-complex-tree.png" referrerpolicy="no-referrer"><br>The nice thing about trees is, they are pretty flexible, and very easy to interpret. With a tree, we can incorporate multiple features and their interactions. In our example, we are really modelling Quality as a surface, instead of a simple line in the stump example:<br><img alt="Quality Surface" src="https://mathias-brandewinder.github.io//assets/2016-08-14-surface.png" referrerpolicy="no-referrer"><br>The resulting model can be expressed in a very understandable form:<br>
If the Alcohol Level is over 10.5, the Quality is 5.5; Otherwise, check the Volatile Acidity. If it is below 0.8, the Quality is 6.0, otherwise it is 3.0.
<br><br>How can we go about representing and learning a Tree?<br>As it turns out, the representation is fairly straightforward. A Tree can be seen as a recursive data structure: either we reached a terminal Leaf, which gives us a prediction, or we reach a Branch, where, based on a Feature and associated split value, we will find 2 new Trees, one for values under the split value, another for values above the split.<br>That is a match in heaven for a Discriminated Union:<br>type Tree =
    | Leaf of float
    | Branch of (Feature * float) * Tree * Tree
<br>Creating manually the “complex” tree we described above can be done along these lines:<br>let exampleTree =
    // we start with a branch
    Branch(
        // we split on Alcohol level, 10.5
        (``Alcohol Level``, 10.5),
        // if alcohol level is under 10.5, 
        // we have another branch
        Branch(
            // we split on Volatile Acidity, 0.8
            (``Volatile Acidity``, 0.8),
            // if acidity is under 0.8, 
            // we predict 6.0
            Leaf(6.0),
            // otherwise we predict 3.0
            Leaf(3.0)
        ),
        // if alcohol is over 10.5,
        // we predict 5.5
        Leaf(5.5)
    )
<br>How do we go about making predictions with a Tree? We simply walk it down recursively:<br>let rec predict (tree:Tree) (obs:Observation) =
    match tree with
    | Leaf(prediction) -&gt; prediction
    | Branch((feature,split),under,over) -&gt;
        let featureValue = feature obs
        if featureValue &lt;= split
        then predict under obs
        else predict over obs
<br>Let’s try it out on our example:<br>predict exampleTree (reds.Rows |&gt; Seq.head)

&gt; val it : float = 6.0
<br>Note that, if we use partial application:<br>let examplePredictor = predict exampleTree
<br>… we get back a function, examplePredictor, which happens to have exactly the signature we defined earlier for a Predictor:<br>val examplePredictor : (Observation -&gt; float)
<br>As a result, we can immediately re-use the sumOfSquares error function we wrote last time, and evaluate how good our tree is fitting the dataset:<br>let sumOfSquares (sample:Example seq) predictor = 
    sample
    |&gt; Seq.sumBy (fun (obs,lbl) -&gt; 
        pown (lbl - predictor obs) 2)

let redSample = 
    reds.Rows 
    |&gt; Seq.map (fun row -&gt; row, row.Quality |&gt; float)

sumOfSquares redSample examplePredictor 
<br>val it : float = 1617.0
<br>The result is pretty terrible - but then, I picked the tree values randomly. Can we automatically learn a “good” Tree?<br><br>If you recall, the approach we followed to learn a “good” stump was the following: for a given Feature, try out various possible split values, and pick the one that gives us the smallest error, defined as the sumOfSquares between the predicted and actual values.<br>We can use the same idea for a Tree. Instead of stopping once we found a good split, we will simply repeat the same process, and look for a good split in each of the two samples we got after the split. Also, instead of searching for a split on a single Feature, we will now consider all of them, and select the best split across all available Features.<br>That smells like recursion. As a first pass, we will re-use some of the code we wrote last time, the learnStump and evenSplits functions, and whip together a quick-and-dirty tree learning function, disregarding any performance consideration:<br>let rec learnTree (sample:Example seq) (features:Feature list) (depth:int) =
    
    if depth = 0
    then
        // we reached maximum depth, and
        // predict the sample average.
        let avg = sample |&gt; Seq.averageBy snd
        Leaf(avg)
    else
        let (bestFeature,bestSplit) = 
            // create all feature * split combinations
            seq {
                for feature in features do
                    let splits = evenSplits sample feature 10
                    for split in splits -&gt; feature,split
            }
            // find the split with the smallest error
            |&gt; Seq.minBy (fun (feature,split) -&gt; 
                let predictor = learnStump sample feature split
                sumOfSquares sample predictor)
        // split the sample following the split
        let under = 
            sample 
            |&gt; Seq.filter (fun (obs,_) -&gt; 
                bestFeature obs &lt;= bestSplit)
        let over = 
            sample 
            |&gt; Seq.filter (fun (obs,_) -&gt; 
                bestFeature obs &gt; bestSplit)
        // learn the corresponding trees
        let underTree = learnTree under features (depth - 1)
        let overTree =  learnTree over features (depth - 1)
        // and create the corresponding branch
        Branch((bestFeature,bestSplit),underTree,overTree)
<br>Let’s try this out, with a Tree that should be equivalent to the first stump we created last time:<br>let originalStump = learnTree redSample [ ``Alcohol Level`` ] 1
sumOfSquares redSample (predict originalStump)
<br>val it : float = 864.4309287
<br>Good news - we get the same result. Now let’s crank it up a notch:<br>let deeperTree = learnTree redSample [``Alcohol Level``;``Volatile Acidity``] 4
sumOfSquares redSample (predict deeperTree)
<br>val it : float = 680.1290569
<br>This is significantly better that the best result we achieved by ensembling stumps, 811.4601191.<br><br>We have a decent-looking Tree learning algorithm. However, not everything is perfect. For instance, emboldened by our success, we could try to increase the depth a bit.<br>let explodingTree = learnTree redSample [``Alcohol Level``] 5
<br>System.ArgumentException: The step of a range cannot be zero.
Parameter name: step
// long list of F# complaints follows
<br>Uh-oh. What is happening here?<br>As we recurse deeper in the Tree, we split the samples further and further, and have less and less data to train our stump on. One thing which might happen for instance is that we are left only with examples sharing the same label. In that situation, generating even splits is going to cause issues, because the width in [ min + width .. width .. max - width ] (our evenly-spaced splits) will be 0.0.<br>This indicates a first problem, namely, that there might not be any good split to use for a given sample.<br>Beyond that, the design is also a bit problematic. The choice of 10 even splits is quite arbitrary; we might want to use 3, or 42 even splits, or use different strategies altogether (splits of same size, every possible distinct value, …). Our evenSplits function is hard-coded deep inside the algorithm - it would be much nicer if we could inject any split function as an argument.<br>In a similar vein, assuming we are comfortable with using stumps / binary splits, the choice of our error metric is also quite arbitrary. We might want to use something else that the sum of squared prediction errors (Manhattan distance, variance reduction, …). Again, that function is buried deep inside - we would like to use any reasonable cost function we think relevant to the problem.<br>Finally, we are picking the split that yields the best cost. However, that split is not guaranteed to be an improvement. As an example, every observation in the sample could have the same label, in which case no split will improve our predictions. If the resulting cost is the same as before, it is pointless to split, and we might as well spare the algorithm a useless deeper search.<br>In short,<br>
<br>we are not guaranteed to have splits for every sample,
<br>we should split only when strict cost improvements are found,
<br>we would like to decide what splits to use,
<br>we would like to decide what cost metric to use.
<br>We are probably going slightly overboard here; the only real problem we have is the first one. At the same time, why not have a bit of fun!<br>I am going to start with defining a couple of type aliases and utilities:<br>let underOver (sample:Example seq) (feat:Feature,split:float) =
    let under = sample |&gt; Seq.filter (fun (obs,_) -&gt; feat obs &lt;= split)
    let over =  sample |&gt; Seq.filter (fun (obs,_) -&gt; feat obs &gt; split)
    under,over

type Splitter = Example seq -&gt; Feature -&gt; float list

type Cost = Example seq -&gt; float
<br>underOver simply takes a sample, and partitions it into 2 samples, based on a feature and a split value. Splitter is a function that, given a sample and a Feature, will produce a (potentially empty) list of values we could split on. Cost simply measures how good a sample is.<br>Given these elements, we can now rewrite our learnTree function along these lines:<br>let rec learnTree (splitter:Splitter,cost:Cost) (sample:Example seq) (features:Feature list) (depth:int) =
    
    if depth = 0
    then
        let avg = sample |&gt; Seq.averageBy snd
        Leaf(avg)
    else
        let initialCost = cost sample        
        let candidates = 
            // build up all the feature/split candidates,
            // and their associated sample splits
            seq {
                for feature in features do
                    let splits = splitter sample feature
                    for split in splits -&gt; 
                        let under,over = underOver sample (feature,split)  
                        (feature,split),(under,over)
            }
            // compute and append cost of split
            |&gt; Seq.map (fun (candidate,(under,over)) -&gt;
                candidate,(under,over), cost under + cost over)
            // retain only candidates with strict cost improvement
            |&gt; Seq.filter (fun (candidate,(under,over),splitCost) -&gt;
                splitCost &lt; initialCost)

        if (Seq.isEmpty candidates)
        then
            let avg = sample |&gt; Seq.averageBy snd
            Leaf(avg)
        else
            let ((bestFeature,bestSplit),(under,over),spliCost) = 
                candidates 
                |&gt; Seq.minBy (fun (_,_,splitCost) -&gt; splitCost)

            let underTree = learnTree (splitter,cost) under features (depth - 1)
            let overTree =  learnTree (splitter,cost) over features (depth - 1)

            Branch((bestFeature,bestSplit),underTree,overTree)
<br><br>Does it work? Let’s try it out:<br>let evenSplitter n (sample:Example seq) (feature:Feature) = 
    let values = sample |&gt; Seq.map (fst &gt;&gt; feature)
    let min = values |&gt; Seq.min
    let max = values |&gt; Seq.max
    if min = max 
    then []
    else
        let width = (max-min) / (float (n + 1))
        [ min + width .. width .. max - width ]

let sumOfSquaresCost (sample:Example seq) = 
    let avg = sample |&gt; Seq.averageBy snd
    sample |&gt; Seq.sumBy (fun (_,lbl) -&gt; pown (lbl - avg) 2) 

let stableTree = learnTree (evenSplitter 10,sumOfSquaresCost) redSample [``Alcohol Level``;``Volatile Acidity``] 10

sumOfSquares redSample (predict stableTree)
<br>This time, nothing explodes - and the value we get is<br>val it : float = 331.1456491
<br>The nice thing here is that at that point, all it takes to create and try new trees is a specification for the cost and split functions, and a list of features. We can, for instance, create a Tree using every feature we have available:<br>let features = [
    ``Alcohol Level``
    ``Chlorides``
    ``Citric Acid``
    ``Density``
    ``Fixed Acidity``
    ``Free Sulfur Dioxide``
    ``PH``
    ``Residual Sugar``
    ``Total Sulfur Dioxide``
    ``Volatile Acidity``
]

let fullTree = learnTree (evenSplitter 5,sumOfSquaresCost) redSample features 10
<br>The results are pretty decent, too:<br><img alt="Actual vs Predicted" src="https://mathias-brandewinder.github.io//assets/2016-08-14-actual-vs-predicted.PNG" referrerpolicy="no-referrer"><br>Out of curiosity, I also performed a crude training vs. testing analysis, to get a feel for potential over-fitting issues.<br><img alt="Over Fitting" src="https://mathias-brandewinder.github.io//assets/2016-08-14-overfitting.PNG" referrerpolicy="no-referrer"><br>The result we observe is typical of trees: as we increase depth, the error on the training sample steadily decreases, indicating that deeper tree fit the data better and better. However, the testing sample tells a different story: for a little while, error on the training and testing samples match fairly closely, but after we reach a certain depth (here, 3), they start diverging. While our tree fit the training sample better and better, that improvement doesn’t generalize to other samples, as we can see on the testing sample; at that point, we are over-fitting. In our particular case, this means that we shouldn’t put much trust in trees deeper than 3.<br><br>At that point, we have a working regression tree algorithm. It’s not perfect; in particular, we largely ignored any performance consideration. Or, stated more bluntly, performance is terrible ;) Still, the result has a couple nice features, the code is fairly simple, and… it works!<br>Trees are quite an interesting topic, which we only covered very superficially here. Still, we will leave it at that for now, and focus back on our initial goal, gradient boosting. All we needed was something a bit better than stumps to iteratively fit residuals. We have that now, with regression tree that allow us to learn a predictor using every feature we have available. In our next installments, we will look at replacing stumps with trees, and see where that leads us.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/basic-regression-tree.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Basic Regression Tree.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:45:04 GMT</pubDate><enclosure url="https://mathias-brandewinder.github.io//assets/2016-08-14-stump.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://mathias-brandewinder.github.io//assets/2016-08-14-stump.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Building custom fibers library in FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:coroutines" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#coroutines</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:coroutines" class="tag" target="_blank" rel="noopener nofollow">#coroutines</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a><br>Over the course of last few months on this blog post, I've been sharing about internals and how-to of different concurrency patters. We discussed how to <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/build-your-own-actor-model/" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/build-your-own-actor-model/" target="_blank">implement our own actors</a> and specific <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/thread-safety-with-affine-thread-pools/" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/thread-safety-with-affine-thread-pools/" target="_blank">affinity-based thread pool</a>. Today we'll focus of the most dominant pattern present in modern programming nowadays: fibers, also known as coroutines, futures, tasks, green threads or user-space threads.<br>The general idea is simple - we want a fine-grained concurrency primitive, that will let us easily compose chain of operations in sequential manner. Of course we could use threads here, but the question is: are threads fine-grained? In many managed languages with OS threads exposed, they can be quite heavy eg. by default in .NET each thread takes around 1MB of memory and requires calling kernel code to cooperate with other threads, which is an expensive operation on its own.<br>What we're after, are more lightweight structures (less than 1kB), that can live fully in a user space, so that we can have even millions of them cooperating frequently with each other without heavy performance penalties.<br>Before we begin, I think it's good to discuss different designs. We'll cover several different topics to be able to make more informed decisions, that we're up to apply to our own solution.<br><br>Scheduler is a subsystem, which direct responsibility is to assign CPU core processing power to a particular fiber. It's also responsible for coordinating fibers execution. The two most common categories of schedulers are preemptive and cooperative.<br>A preemptive scheduler is the one, that's always in control of fiber execution. It's able to decide on its own, when fiber can be started and stopped. The most obvious example of such is a thread scheduler existing on most operating systems.<br>Preemptive scheduler usually works in one of two ways:<br>
<br>Time based scheduler takes a quant of CPU time and gives it to a given fiber, which ten can execute its logic until it reaches its execution time limit (of course, it can finish earlier). This is how OS thread scheduler, but also how Go goroutine scheduler works.
<br>Another variant is step-based scheduler, which splits fiber's function body into series of (more or less equal) steps. Then each fiber is given a number of steps to execute before preemption occurs. Example of such is Erlang's BEAM - it simply allows each process to execute up to 2000 "reductions", where each reduction is basically a function call. And since in Erlang there are no loops, only tail-recursive functions, this approach works well for long-living iterative processes as well.
<br>One of the problems with preemptive schedulers is that they usually need some kind of involvement from the compiler or hosting virtual machine in order to work. For this reason, most of the fiber libraries use cooperative schedulers to perform their work.<br>A cooperative scheduler doesn't have a concept of preemption - once started by the scheduler, a fiber will execute until it doesn't give back the control willingly. This is often done with dedicated programming constructs, and often is known as yielding, parking or awaiting.<br>In cooperative variant, a fiber body is usually split into series of discrete steps, between which fiber gives control back to the scheduler.<br>Keep in mind that these two are not mutually exclusive - a preemptive scheduler often provides a way for a fiber to return control back to it when it's known that fiber won't be executing any longer eg. because it has been put to sleep for a while.<br><br>A concept, that's somewhat related to a topic above is the idea of stackless and stackful coroutines.<br>A stackful variant is aware of underlying execution stack and can preserve/restore parts of it when yielding/continuing a fiber. Examples of this approach could be Go, Lua, Python asyncio and in the future, also <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=NV46KFV1m-4&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=NV46KFV1m-4&amp;ref=bartoszsypytkowski.com" target="_blank">Java Loom</a> project. Implementing such option (if it's not implemented by a runtime already) usually requires diving deep into low-level internals, since execution stack is not something that most managed languages offers the users to play with, and doing so without coordination with runtime can cause problems - like determining liveness of objects for GC purposes.<br>Stackless coroutine usually captures locals that we want to preserve as part of callback object (lambda), that is allocated as an object on the heap and scheduled on yield continuation. These steps are usually visible directly in code (eg. await in C#, Rust and JavaScript, but also joints of Scala for-comprehensions, bang-suffix in F# or Haskell do-notation), but sometimes can be implicit like in case of Kotlin. Take into account that while many languages offer syntax support for those constructs, it's not explicitly necessary to work - take a look at JavaScript and Promise.then as an example.<br>Stackless coroutines usually construct their logic around one of two concepts:<br>
<br>Finite state machines - this variant is usually faster and can be encoded manually (example of such case is Akka actors), but for a human eye it usually doesn't really read as a sequential step-by-step program execution, unless it has some support from the compiler itself (see: C# and Rust).
<br>Monadic sequencing via bind/flatMap operator, which is very popular in functional languages. While we cover it in more details in the rest of this blog post, for now it's enough to say that it's a way to chain callback-based behaviors together in a way, that resembles standard sequential code.
<br>For sure one of the advantages of stackful coroutines is that they're mono-colored: you can yield/continue coroutine execution from within any other function, while in the stackless variant splits your world into two-colored functions - synchronous and asynchronous - where async one can be only called and yielded safely (without blocking underlying OS thread) from within another async function.<br><br>We already mentioned two important events in fiber execution life cycle - starting and parking. Here I briefly discuss about different design decisions on when to start a fiber execution.<br>Eager execution means, that fiber is started automatically after its creation. An example of such are Scala Future[A] and JavaScript Promise. Since execution process starts right away, we're willingly resign from a certain degree of control over how or when to execute given fiber. Usually this is solved by wrapping a fiber creation into another function or lambda.<br>Lazy execution is much more common and preferred way of work, as it allows us to separate place where we want to define our asynchronous sequence of steps from the place, where the execution details are defined. It's used in C# TPL as well as pretty much in all functional languages implementations (excluding Scala futures mentioned earlier).<br><br>There are also few decisions regarding premature escaping the fiber execution, also known as interruption/cancelation: one of them requires passing special object - a token - between method calls and explicit checking for its completion. It is how C# Tasks work. However putting such requirement onto the API user can be cumbersome and error-prone option. Therefore pretty much every other coroutine library either allows to direcly interrupt a fiber or (like in case of F# Async) passes cancelation tokens and check if they were triggered under the hood.<br><br>Since we talked a bit about various approaches, let's get to the meat of this blog post: implementing our own coroutine library in F#. So, what properties will it have?:<br>
<br>We use cooperative scheduling (we don't want to tweak the compiler) of stackless fibers with support from F# computation expression for nice syntax.
<br>We use simple approach by defining custom bind operator with support from F# computation expressions. No state machines.
<br>We'll use lazy invocation.
<br>We'll make use of implicitly passed cancelation tokens. We'll handle them directly inside the linking code.
<br>All of these give us in very similar approach to that found inside of native F# Async data type. To begin with, we'll simply define the shape of our fiber.<br>Underneath, pretty much every cooperative stackless coroutine approach uses callbacks to drive the flow of synchronous segments of code to be executed one after another. So what we need is a callback which takes a result of previous coroutine and schedules in within some context of execution:<br>type Fiber&lt;'a&gt; = Fiber of (ExecutionContext -&gt; FiberCallback&lt;'a&gt; -&gt; unit)
<br>Here we'll represent Fiber as a simple single-case discriminated union. We could as well define other specialized cases, like:<br>
<br>Situation when coroutine is executed immediately and doesn't need to be awaited on: think about variant of ValueTask from C# Task Parallel Library.
<br>Case when coroutine fails - in that case we might want to store an artificial tracing context that would allow us to create nicely-formatted "stack traces": since .NET Core 2.1, C# already provides similar solution however AFAIK it's been solved differently.
<br>Ok, but what are ExecutionContext and FiberCallback&lt;'a&gt;? Let's start from callback. We can represent it as follows:<br>type FiberCallback&lt;'a&gt; = FiberResult&lt;'a&gt; -&gt; unit
<br>It's just a simple function, which takes result of previous fiber execution and handles it. What's the FiberResult&lt;'a&gt; then?<br>Our fiber can complete successfully (returning a value) or fail with an exception. We'll be conservative here and won't go into more typed world of <a data-tooltip-position="top" aria-label="http://degoes.net/articles/bifunctor-io?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="http://degoes.net/articles/bifunctor-io?ref=bartoszsypytkowski.com" target="_blank">IO bifunctor</a>. We can easy define these possible outputs in F# using Result&lt;'a, exn&gt;.<br>Question is: is that exhaustive? Well... no. As we already mentioned, there's a 3rd state, often overlooked or conflated with failure: a canceled fiber. A canceled fiber doesn't produce any output - since it was canceled before completion. In F# we already know how to represent an absence of value - simply use an option. Therefore our ultimate Fiber result type could look like this:<br>type FiberResult&lt;'a&gt; = Result&lt;'a, exn&gt; option
<br>Now, the ExecutionContext. While it can be compound of many different capabilities throughout the system - even to serve as functional equivalent of dependency injection - here I'll use it only for implicit passing of specific scheduler info and cancelation tokens from one fiber to another.<br>type ExecutionContext = IScheduler * Cancel
<br>IScheduler interface is used to abstract component responsible for running our fibers. At the moment all we need is an ability to schedule fiber execution:<br>[&lt;Interface&gt;]
type IScheduler = 
  abstract Schedule: (unit -&gt; unit) -&gt; unit
<br>While the name and signature imply multithreaded execution model, it doesn't have to be the case. We can even implement scheduler which will simulate everything on a single core.<br>For now, we can simply implement a scheduler API on top of our standard .NET thread pool:<br>module Scheduler

open System.Threading

let shared = 
  { new ISchedule with
      member __.Schedule fn = 
        ThreadPool.QueueUserWorkItem(WaitCallback (fun _ -&gt; fn())) |&gt; ignore  }
<br><br>Now it's a time for cancellation tokens. Of course we could just make use of a flag - conceptually working like native .NET CancellationToken. However given implicit cancellation, it may not be enough. Example:<br>
Imagine, that inside our fiber we're scheduling the race between two other fibers ie. one writing data to a file and other which will complete after timeout. Now, whenever one of them completes first, we want to cancel another one to stop wasting resources for result that no longer matters.
<br>This simple scenario is similar to what .NET Task.WhenAny is used - with a difference that, unlike TPL, we want to actually cancel other executing tasks instead of letting them run (potentially forever) :D<br>Now, since our cancellation is not explicit, we need to deal with few things:<br>
<br>Whenever parent fiber is cancelled, all child fibers it spawned are also cancelled.
<br>Whenever we cancel a fiber that loose the race, we don't want to accidentally cancel a token of its parent.
<br>This behavior implies at least using two separate tokens, however in practice it will be more pragmatic to make our Cancel token work as a tree hierarchy - this way we can easily keep track of things and support more complex scenarios.<br>[&lt;Sealed;AllowNullLiteral&gt;]
type Cancel(parent: Cancel) =
  let mutable flag: int = 0
  let mutable children: Cancel list = []
  new() = Cancel(null)
  /// Check if token was cancelled
  member __.Cancelled = flag = 1
  /// Remove child token
  member private __.RemoveChild(child) = 
    let rec loop child =
      let children' = children
      let nval = children' |&gt; List.filter ((&lt;&gt;) child)
      if not (obj.ReferenceEquals(children', Interlocked.CompareExchange(&amp;children, nval, children')))
      then loop child
    if not (List.isEmpty children) then loop child
  /// Create a new child token and return it.
  member this.AddChild () =
    let rec loop child =
      let children' = children
      if (obj.ReferenceEquals(children', Interlocked.CompareExchange(&amp;children, child::children', children')))
      then child
      else loop child
    loop (Cancel this)
  /// Cancel a token
  member this.Cancel() =
    if Interlocked.Exchange(&amp;flag, 1) = 0 then
      for child in Interlocked.Exchange(&amp;children, []) do child.Cancel()
      if not (isNull parent) then parent.RemoveChild(this)

<br>The general idea is simple: every new cancellation token (except root) may have a parent and a list of children. Canceling parent means canceling its children as well. After cancellation, we need to unpin child from its parent (therefore need for RemveChild operation) to avoid memory leaks.<br><br>What might be confusing for some in the code above, are recursive loops inside of AddChild/RemoveChild operations. This is a good place to introduce lock-free algorithms: we use atomic operations from <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/building-custom-fibers-library-in-f/docs.microsoft.com/en-us/dotnet/api/system.threading.interlocked" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/building-custom-fibers-library-in-f/docs.microsoft.com/en-us/dotnet/api/system.threading.interlocked" target="_blank">Interlocked</a> class to make sure that we can replace field references within a single CPU instruction, therefore making such field update safe without synchronized access. This is also known as Compare-And-Swap semantics.<br>This alone however is not enough, as Interlocked.CompareExchange(&amp;field, new', old) can only safely replace a single field with new value if it contained an old one. This means that you cannot safely add or remove element to the list. So what can we do?<br>
<br>We're taking a value from the field.
<br>Update that value.
<br>Conditionally put it back again. What if in the meantime the field was already replaced by another concurrently running thread? In that case Interlocked.CompareExchange will return field value other that the one we read in step 1. This is why we compare its result with the variable we expected.
<br>If the expectation fails, we'll retry - hence a recursive loop. Eventually even in high contention scenarios we should be able to complete after few retries. Given cheap and idempotent update operation, this still will be way faster than trying to call kernel code to obtain mutex/semaphore lock.
<br>While this may sound like something error prone - we can potentially add the same element multiple times - in practice it's safe, because our collection here is an immutable data structure. Adding the same element multiple times without updating the reference will always produce the same result.<br><br>Now we have pretty much all core structures. We're ready to start building our fiber operators. Starting from the basic ones - a successfully completed fiber and the failed one:<br>let success r = Fiber &lt;| fun (_, c) next -&gt; 
  if c.Cancelled then next None else next (Some (Ok r))

let fail ex = Fiber &lt;| fun (_, c) next -&gt; 
  if c.Cancelled then next None else next (Some (Error ex))
<br>Here, we simply pass a result/error to our Fiber callback:<br>
<br>Cancelled fiber call next callback with None - as fibers cancelled before completion produce output .
<br>Successful call results in passing Some (Ok result) to a callback...
<br>... while failed result can be identified with Some (Error exception).
<br>You'll be able to see a cancellation check made here as preamble of pretty much every operator body, which we'll define. While it may sound cumbersome remember: we do that so that users of our fibers won't have to :)<br>Next very important operation is result mapping - we want to map result of one fiber into something else, returning another (lazy) fiber:<br>let mapResult (fn: Result&lt;'a&gt; -&gt; Result&lt;'b&gt;) (Fiber call) = Fiber &lt;| fun (s, c) next -&gt;
  if c.Cancelled then next None
  else 
    try 
      call (s, c) (fun result -&gt;
        if c.Cancelled then next None
        else next (Option.map fn result))
    with e -&gt; next (Some (Error e))
<br>We can use this function to compose more traditionally-looking map function...<br>let map (fn: 'a -&gt; 'b) fiber = mapResult (Result.map fn) fiber
<br>... however mapResult is more powerful - you could easily imagine using to apply failure recovery (a.k.a try/catch semantics) by simply mapping Error exception → Ok recoveredValue:<br>let catch fn fiber = mapResult (function Error e -&gt; fn e | ok -&gt; ok) fiber
<br>Another must-have function is binding operator (also know as flatMap in other languages like Scala, or Promise.then in JavaScript). It gives us the ability to compose fibers together - we'll also use it when we come up to building a computation expression for our fibers.<br>let bind (fn: 'a -&gt; Fiber&lt;'b&gt;) (Fiber call) = Fiber &lt;| fun (s, c) next -&gt;
   if c.Cancelled then next None 
   else 
     try
       call (s, c) (fun result -&gt;
         if c.Cancelled then next None
         else match result with
              | Some (Ok r) -&gt;
                 let (Fiber call2) = fn r
                 call2 (s, c) next // pass `next` callback over to next fiber
              | None -&gt; next None
              | Some (Error e) -&gt; next (Some(Error e)))
     with e -&gt; next (Some(Error e))
<br>It's simple - we execute one fiber from within another, passing the next callback from outer function as an argument to inner one.<br><br>With these few functions we're already prepared to build a basic computation expression, that will enable us programming with fibers in pleasant way:<br>[&lt;Struct&gt;]
type FiberBuilder =
    member inline __.Zero = Fiber.success (Unchecked.defaultof&lt;_&gt;)
    member inline __.ReturnFrom fib = fib
    member inline __.Return value = Fiber.success value
    member inline __.Bind(fib, fn) = Fiber.bind fn fib

[&lt;AutoOpen&gt;]
module FiberBuilder =

    let fib = FiberBuilder()
<br>While in F# <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions?ref=bartoszsypytkowski.com" target="_blank">there are many more operators</a> we could pack into our computation expression, these are basic ones that will let it work. With such construct, we'll be able to write programs like:<br>let inline millis n = TimeSpan.FromMilliseconds (float n)

let program: Fiber&lt;int&gt; = fib {
  let a = fib {
    do! Fiber.delay (millis 1000) // create some artificial delay
    return 3
  }
  let! b = a |&gt; Fiber.timeout (millis 3000) // execute task within specified timeout
  return b }
<br>Sure, we have neither delay nor timeout operators at the moment, but at least you know where are we heading now :)<br><br>In order to implement delays, we could theoretically just call Thread.Sleep and get over it, but this approach is devastating from any coroutine library point of view. Most user-space thread libraries work by using a predefined fixed pool of OS-level threads and scheduling coroutines on them - you can read more about building thread pools <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/thread-safety-with-affine-thread-pools/" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/thread-safety-with-affine-thread-pools/" target="_blank">here</a>.<br>However, Thread.Sleep(timeout) doesn't know thread pooling mechanism - all it knows about is that we called suspending current OS thread of execution. This means, that this thread will not be awoken by kernel until timeout completes. What it means, is that none of our fibers will be able to use that thread. This is bad, because usually thread pools are made to fit in-line with number of machine CPU cores. In practice, Thread.Sleep may keep one of our CPU cores idle, wasting machine power in the process.<br>For this reason we usually want to build a suspendable fibers, that will respect our thread pool. This however cannot be done without cooperation with scheduler itself. Therefore, we need to extend API of our scheduler:<br>type IScheduler =
  abstract Schedule: (unit -&gt; unit) -&gt; unit
  abstract Delay: TimeSpan * (unit -&gt; unit) -&gt; unit
<br>And our simple implementation of it as well:<br>let shared = 
    { new IScheduler with
      member __.Schedule fn = ....
      member this.Delay (timeout: TimeSpan, fn) = 
        let mutable t = Unchecked.defaultof&lt;Timer&gt;
        let callback = fun _ -&gt; 
          t.Dispose()
          fn()
          ()
        t &lt;- new Timer(callback, null, int timeout.TotalMilliseconds, Timeout.Infinite)
    }
<br>We'll use a .NET timers here to implement our delays. With these in our hands, ourFiber.delay operation is trivial to implement:<br>let delay (timeout): Fiber&lt;unit&gt; =
  Fiber &lt;| fun (s, c) next -&gt;
    if c.Cancelled then next None
    else s.Delay(timeout, fun () -&gt; 
      if c.Cancelled 
      then next None 
      else next (Some (Ok ())))
<br><br>We're slowly getting to the end. What I left for this blog post was to implement two basic operators, that are prevalent in most coroutine libraries:<br>
<br>Fiber.parallel which will schedule multiple fibers to run in parallel and returns a fiber which aggregates their results.
<br>Running two fibers in parallel and returning the result of whichever completes first, while cancelling a second one. We already discussed this approach before. Here I'll call it Fiber.race.
<br><br>We'll start from building a parallel operator, which will change our array of fibers into fiber with an array of results. But let's define the semantics of that operation first:<br>
<br>Our result fiber completes only when all of the aggregated fibers completed with successful result.
<br>If any of the fibers fails, the resulting fiber also fails.
<br>If any of the fibers fails or get cancelled, all pending ones are also cancelled.
<br>The core skeleton of that operation could look like following:<br>let parallel (fibers: Fiber&lt;'a&gt;[]): Fiber&lt;'a[]&gt; =
  Fiber &lt;| fun (s, c) next -&gt;
    if c.Cancelled then next None
    else 
      let child = c.AddChild()
      let successes = Array.zeroCreate remaining
      let mutable remaining = Array.length fibs
      fibers |&gt; Array.iteri (fun idx (Fiber call) -&gt;
        s.Schedule (fun () -&gt; (* to be defined *))
      )
<br>Here, we create a dedicated cancellation token, an array of results and a countdown counter - we're going to decrement it every time one of our fibers completes to know when we're ready to return a complete result. I've left a placeholder for a lambda body that we actually want to schedule. We're going to fill it right away:<br>// defined above: s.Schedule &lt;| fun () -&gt;
call (s, child) (fun result -&gt; 
match result with
| Some (Ok success) -&gt;
  // fill the result array
  successes.[idx] &lt;- success
  if c.Cancelled &amp;&amp; Interlocked.Exchange(&amp;remaining, -1) &gt; 0 then
    next None
  elif Interlocked.Decrement(&amp;remaining) = 0 then
    // if all results have been returned, call the `next` callback
    if c.Cancelled then next None
    else next (Some (Ok successes))
| Some (Error fail) -&gt;
  if Interlocked.Exchange(&amp;remaining, -1) &gt; 0 then 
    child.Cancel() // we failed, cancel other fibers
    if c.Cancelled then next None
    else next (Some (Error fail))
| None -&gt;
  if Interlocked.Exchange(&amp;remaining, -1) &gt; 0 then next None)
<br>As you probably noticed, we're using Interlocked class again - that's because now we have multiple fibers running in parallel, therefore our access to shared mutable values is not thread safe. This includes remaining counter decrement operation. This however doesn't apply to successes.[i] &lt;- success - since every fiber knows and touches only its own index within result array, there's no worry that any other will try to push its result in the same place.<br>What you also can see, we're using a -1 here as a magic value - we'll use it on the counter as a flag to determine if any of the fibers failed/was cancelled - and if so, which one of them will call the next callback.<br><br>With first operator (Fiber.parallel) ready, now it's the time to implement Fiber.race. Since I've discussed it behavior multiple times in this post already, let's dive straight into the code:<br>let race (Fiber left) (Fiber right): Fiber&lt;Choice&lt;'a, 'b&gt;&gt; =
  Fiber &lt;| fun (s, c) next -&gt;
    if c.Cancelled then next None
    else 
      let mutable flag = 0
      let cancelChild = c.AddChild()
      let run fiber choice =
          (* to be described *)
      run left Choice1Of2
      run right Choice2Of2
<br>So again, we want to have shared mutable flag, which we'll use to determine, which of the fibers finished as a first one to be able to call fiber's callback safely and cancel the other. You may see, that our returned fiber uses Choice&lt;,&gt; type - this means, that our left and right fibers can have results of different types. We'll use that soon, but first we need to complete our run function body:<br>let run fiber choice =
  s.Schedule (fun () -&gt;
    fiber (s, cancelChild) (fun result -&gt;
      if Interlocked.Exchange(&amp;flag, 1) = 0 then
        cancelChild.Cancel()
        if c.Cancelled then next None
        else match result with
             | None -&gt; next None
             | Some(Ok v) -&gt; next (Some(Ok(choice v)))
             | Some(Error e) -&gt; next (Some(Error e))))
<br>What we do here is simply trying to race to "reserve" out flag variable - the winner gets his result mapped to corresponding choice, while looser gets cancelled.<br>What's interesting, we can now combine our race and delay functions to easily implement timeout mechanism:<br>let timeout (t: TimeSpan) fiber =
  Fiber &lt;| fun (s, c) next -&gt;
    let (Fiber call) = race (delay t) fiber
    call (s, c) (fun result -&gt;
      if c.Cancelled then next None
      else match result with
           | None -&gt; next None
           | Some(Ok (Choice1Of2 _)) -&gt; next None // timeout won
           | Some(Ok (Choice2Of2 v)) -&gt; next (Some(Ok v))
           | Some(Error e) -&gt; next (Some(Error e))
    )
<br>The one last thing left for us, is to be able to run out fibers on the main thread - otherwise we'd start our program, schedule fibers to run in the background and then close the program without waiting for the results.<br>let blocking (s: IScheduler) (cancel: Cancel) (Fiber fn) =
  use waiter = new ManualResetEventSlim(false)
  let mutable res = None
  s.Schedule(fun () -&gt; fn (s, cancel) (fun result -&gt;
    if not cancel.Cancelled then
      Interlocked.Exchange(&amp;res, Some result) |&gt; ignore
    waiter.Set()))
  waiter.Wait()
  res.Value
<br>It's simple - we'll use standard synchronization primitives provided by .NET runtime, to hold current OS thread until we complete. Sure it's blocking an OS thread, but we'll eventually need that if we don't want our program's main function to finish before all fibers inside the thread pool complete.<br><br>In theory, we could be done here. But, if you managed to read up to this point, we may want to cover one last scenario. Imagine that we'd want to test our fibers. However running tests using standard thread pool scheduler can lead to funky issues:<br>
<br>Sometimes you may trigger some race conditions in your code, that only happen in specific situations (like high CPU contention) and are almost impossible to reproduce during debug sessions.
<br>Other times you may have some lengthy delays/timeouts in your code, like waiting for seconds or even minutes before continuing. Guess what: now your test will wait for just as long.
<br>These are not new problems. They are well known in world of concurrent and distributed systems. What we need, is a simulation of execution environment. If you want to listen more about that concept, I could recommend you <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=4fFDFbi3toc&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=4fFDFbi3toc&amp;ref=bartoszsypytkowski.com" target="_blank">this presentaton</a>. To run our test predictably, we'll create a dedicated test scheduler, which will run our code in deterministic fashion (on a single core) and in a way that's detached from other invariants eg. actual physical clock and random number generator.<br>The idea here is simple - our scheduler will operate on notion of virtual timeline. When we'll try to schedule a new function - to trigger either immediately or after some timeout - we'll store it inside an ordered collection, a timeline. Some of the technical decisions we also made for purposes of this implementation:<br>
<br>Whenever a fiber is going to schedule multiple parallel executions "at the same time", we'll put them all into a single bucket on a timeline. Later on I'll cover, why this is useful.
<br>We'll assume, that single operation execution is instantaneous. It means, it doesn't advance our scheduler's clock. We do it only for delayed executions.
<br>After describing the concept behind the algorithm, the actual implementation really shouldn't be that surprising:<br>type TestScheduler(now: DateTime) =
  let mutable running = false
  let mutable currentTime = now.Ticks
  let mutable timeline = Map.empty
  let schedule delay fn = (* to be defined *)
  let rec run () = (* to be defined *)
  interface IScheduler with
    member this.Schedule fn = 
      schedule 0L fn
      if not running then
        running &lt;- true
        run ()
    member this.Delay (timeout: TimeSpan, fn) = schedule timeout.Ticks fn
<br>We're using a running flag here to not try to invoke run multiple times in nested manner - this would cause non-tailable recursion and potential stack overflow in more expensive tests.<br>The schedule function is pretty simple - calculate expected execution time for a function, then add that function to be executed at that point in time.<br>let schedule delay fn = 
  let at = currentTime + delay
  timeline &lt;-
    match Map.tryFind at timeline with
    | None -&gt; Map.add at [fn] timeline 
    | Some fns -&gt; Map.add at (fn::fns) timeline
<br>Given all of the code we already survived in this blog post, run loop should be pretty simple:<br>let rec run () =
  match Seq.tryHead timeline with
  | None -&gt; running &lt;- false
  | Some (KeyValue(time, bucket)) -&gt;
    timeline &lt;- Map.remove time timeline
    currentTime &lt;- time
    for fn in List.rev bucket do 
      fn ()          
    run ()
<br>We'll try to pick the first entry from the timeline - since here we use F# map, which is sorted in ascending order, we know that first entry is the one with the shortest execution timeout. We update our "current" time to match the expected one we calculated earlier, and finally we execute all functions scheduled at that time and repeat the loop all over until we eventually run out of scheduled actions.<br>Now here's the trick - we use List.rev to execute functions in the same order in which they were scheduled, because we want our tests to be deterministic and our bugs to be reproducible. However this is not the only strategy - since we know that functions in the same bucket could as well be executing in parallel, we could shuffle them around in different permutations for early discovery of some data races! I'll won't dive into it, but leave that idea as food for thoughts for you.<br>One last note about the test scheduler is that isolating it from the actual physical clock means, we cannot trust our time functions (like DateTime.UtcNow) any longer. This shouldn't really be an issue though - because relying on physical time would potentially make our tests indeterministic, we didn't want to use it anyway, right?<br>However, we need to be able to obtain current time from the scheduler, so we need to extend its API:<br>type IScheduler =
  abstract UtcNow: unit -&gt; unit
  // ... other methods
  
type TestScheduler() =
  let mutable currentTime = DateTime.UtcNow.Ticks
  // ... rest of the implementation
  interface IScheduler with
    member __.UtcNow() = DateTime(currentTime)
    // ... other methods
<br>And that's all. As always, if you got confused or have a problems along the way, you can get the entire code <a data-tooltip-position="top" aria-label="https://gist.github.com/Horusiath/9c790691130150b524aaa9ab426ed982?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://gist.github.com/Horusiath/9c790691130150b524aaa9ab426ed982?ref=bartoszsypytkowski.com" target="_blank">here</a>. I wanted to thank to Anthony Lloyd for his initial work on porting Scala <a data-tooltip-position="top" aria-label="https://zio.dev/?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://zio.dev/?ref=bartoszsypytkowski.com" target="_blank">ZIO</a> library to F#, which brought me an inspiration to write this piece.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/building-custom-fibers-library-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Building custom fibers library in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:45:03 GMT</pubDate></item><item><title><![CDATA[Dealing with complex dependency injection in FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <br>Today, we're going to cover different ways of encapsulating capabilities and supplying them between functions using functional programming techniques which can be realized in F#.<br>Managing code dependencies in object oriented languages in 2020 is pretty much one sided problem: dependency injection has won, people use dedicated frameworks to handle that for them, which 99.9% of the time operate using runtime reflection. Of course now you need to learn them as well, potentially misconfigure them and fail at runtime or maybe even discover that not every problem is a stateless web service, but it still better (?) than what we had in the past, and what more can we possibly do anyway?<br>On the other side in functional space, there's no one opinionated solution or approach - various things have been proposed, usually depending on features that languages and compiler have to offer. And since pretty much all functional languages offer this thing known as partial application, for many years it was the most common answer for the problem of managing dependencies.<br>In short we're talking about dependency injection by function parameter, like:<br>// foo requires 2 dependencies to serve the incoming request
let foo bar baz request = ???
// we're providing dependencies by partial application
let wired = foo dependency1 dependency2
// now wired can serve request directly without calling 
// dependencies every time
let response = wired request
<br>It's very simple, doesn't require reflection or dedicated library. However there are several pain points coming with this approach - visible especially as our code base grows and become more complex. However latest approaches popularized by libraries like Scala <a data-tooltip-position="top" aria-label="https://zio.dev/?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://zio.dev/?ref=bartoszsypytkowski.com" target="_blank">ZIO</a> or Haskell's <a data-tooltip-position="top" aria-label="https://youtu.be/idU7GdlfP9Q?t=1394&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://youtu.be/idU7GdlfP9Q?t=1394&amp;ref=bartoszsypytkowski.com" target="_blank">Polysemy</a> challenge this approach.<br><br>There are some design decisions, when partial application doesn't always give a clear answer. Example:<br>
Imagine using a set of methods, that are closely related and - in object oriented world - encapsulated within &nbsp;a single object, like database query/execute or different logging methods (debug/info/warning/error).
<br>Now, given that our function needs to use potentially more than one of these, how should we pass our arguments?:<br>
<br>Functional purist path - pass every dependency as a separate function parameter: let doSmth logError logInfo = ??. While it allows us to precisely describe what this function uses, it would of course lead to explosion of function parameters. Additionally every time you need new function in your existing code, you need to partially apply it at all call sites.
<br>Describe operations using ADT (algebraic data types) and inject a function that will work as an interpreter for them: let doSmth (log: LogEvent -&gt; unit) = ??. While it's easy to mock (you don't need to implement everything, only pattern match on cases that matter for a particular test) and reduces params affinity, it also comes with a lot of indirection, that may lead to harder to grasp, especially during debugging. Sometimes a performance penalty is also to be expected.
<br>Fallback to objects/interfaces and pass them as methods: let doSmth (logger: ILogger) = ??. While interfaces may simplify dependency tree, it's not always obvious when to use it. Mocking story is also more painful + interfaces are not inferred by F# compiler.
<br>These are quite common options I've seen in the wild - each having their own advantages and disadvantages. Which one to use? Good question, as in practice with codebases that are old enough, you usually see 2 or even all 3 of them mixed together. This can lead to some confusion and obscurity over time.<br>What's worse, none of these cases really solves problem of dependency management - all they do is just try to reduce it to a manageable scope. Eventually you'll end up manually wiring - by partial application - dozens of functions and managing all of the dependencies between them by hand.<br><br>In order to get better understanding, we'll use a fairly simple example - changing user password:<br>let fetchUser (db: IDbConnection) userId = 
    db.QueryFirstAsync(Sql.FetchUser, {| userId = userid| })
    
let updateUser (db: IDbConnection) user = db.ExecuteAsync(Sql.UpdateUser, user)

let changePass (logger: ILogger) fetch update = fun req -&gt; task {
    let! user = fetch req.UserId
    if user.Hash = bcrypt user.Salt req.OldPass then
        let salt = generateSalt ()
        let user' = { user with Salt = salt; Hash = bcrypt salt req.NewPass }
        do! update user'
        logger.LogInformation "Password change: user %i" user.Id
        return Ok ()
    else 
        logger.LogError "Password change unauthorized: user %i" user.Id
        return Error "Old password is invalid"
}

<br>Here we have a fairly short snippet with some dependencies? But how many in practice?:<br>
<br>Number of parameters suggest 3, but depending on our choices it could be 4 (if we decide to pass log error and info separately) or 2 (if we conflate fetch/update into dedicated interface).
<br>It's not hard to imagine that in the future our bcrypt hashing function may turn out to be configurable - maybe even per each user. That may need a configurable parameter.
<br>Maybe aside of the logger we may be needing a separate telemetry mechanism to count number of incoming request or password validation failures? That means another parameter.
<br>Salt generation is pseudo-random process - it we want our function to be deterministic, we should probably parametrize it over explicitly passed Random as well.
<br>As you see, what seemed to be simple task at the beginning can quickly blow up out of proportion. As the number of arguments grows, the more nasty our wiring code eventually becomes. Quite common pattern is to hide all of that nastiness under the carpet a.k.a. composition root. However this doesn't have to be the case.<br>Below we'll cover another approach for dealing with dependencies - inspired by Scala <a data-tooltip-position="top" aria-label="https://medium.com/@pascal.mengelt/what-are-the-benefits-of-the-zio-modules-with-zlayers-3bf6cc064a9b?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://medium.com/@pascal.mengelt/what-are-the-benefits-of-the-zio-modules-with-zlayers-3bf6cc064a9b?ref=bartoszsypytkowski.com" target="_blank">ZIO</a> library - using incremental steps, from first principles to monadic bindings.<br><br>Let's start from how our code from above will eventually look like at the end of this step:<br>let changePass env = fun req -&gt; task {
    let! user = Db.fetchUser env req.UserId
    if user.Hash = bcrypt user.Salt req.OldPass then
        let salt = Random.bytes env 32
        do! Db.updateUser env { user with Salt = salt; Hash = bcrypt salt req.NewPass }
        Log.info env "Changed password for user %i" user.Id
        return Ok ()
    else 
        Log.error env "Password change unauthorized: user %i" user.Id
        return Error "Old password is invalid"
}
<br>As you may notice, all of our partially applied parameters disappeared, replaced by some single cryptic env parameter. We'll get there soon. We also packed similar capabilities into corresponding modules (Db/Log/Random). Lets start from defining them:<br>[&lt;Interface&gt;]
type ILogger =
    abstract Debug: string -&gt; unit
    abstract Error: string -&gt; unit 

[&lt;Interface&gt;] type ILog = abstract Logger: ILogger

module Log =
    let debug (env: #ILog) fmt = Printf.kprintf env.Logger.Debug fmt
    let error (env: #ILog) fmt = Printf.kprintf env.Logger.Error fmt
<br>Now we can say something more about env. The secret is in #ILog signature, which means that our environment can be any generic type implementing ILog interface. As soon you'll see, this approach is highly composable, but before that we'll need another module:<br>[&lt;Interface&gt;]
type IDatabase =
    abstract Query: string * 'i -&gt; Task&lt;'o&gt;
    abstract Execute: string * 'i -&gt; Task
	
[&lt;Interface&gt;] type IDb = abstract Database: IDatabase

module Db = 
    let fetchUser (env: #IDb) userId = 
        env.Database.Query(Sql.FetchUser, {| userId = userId |})
    let updateUser (env: #IDb) user = env.Database.Execute(Sql.UpdateUser, user)
<br>Now what will happen if we use functions from both Log and Db modules? As it turns out, F# compiler can properly infer generic constraints over these interfaces. The result env type constraint is inferred to be an union - just like set union, which also means that it handles duplicates for us - of all constraints of other functions using env in its scope:<br>let foo env = // env :&gt; IDb and env :&gt; ILog
    let user = Db.fetchUser env 123	// env :&gt; IDb
    Log.debug env "User: %A" user	// env :&gt; ILog
<br>Now why did we use two separate interfaces (ILog/ILogger) instead of making environment implement ILogger directly? This is more practical approach that will let us isolate capabilities of particular modules rather than putting them flat into our environment. Example:<br>module Log =
    let live : ILogger = ?? // create logger interface

[&lt;Struct&gt;]
type AppEnv = 
    interface ILog with member _.Logger = Log.live
    interface IDb with member _.Database = Db.live connectionString
	
foo (AppEnv())
<br>We cannot eagerly provide a specific implementation of ILog/IDb, because they're yet to be defined as part of by our environment type (which may need to implement many interfaces). To maintain module encapsulation Log module shouldn't be aware of existence or implementation of IDb interface and vice versa for Db module. What we can do however is to provide live implementation of ILogger, which encapsulates capabilities required by the Log module. This way we don't need to know details of ILogger when defining our environment type.<br>Strong sides of this approach?:<br>
<br>We only need to provide a single environment object instead of (potentially) unbounded list of parameters. Since it's always one, it's easier to generalize and compose other functions over it.
<br>Unlike in reflection-based dependency injection frameworks - everything is still safe and checked by the compiler. If our environment type will not implement an interface required somewhere in the call chain, our code will simply not compile.
<br>It's still fairly easy to unit test - each function defines only the generic type constraints that it uses in its own call tree, NOT all of the constraints required by the application.
<br>New dependencies are added implicitly - if your code uses module that requires additional capability, it will be automatically inferred by the compiler and bubble up to our environment type definition. No need to add new function parameters or to pass new argument. Also - unlike the object oriented IoC containers - there's no need to add new dependency as a field or constructor argument.
<br>It gives some opinionated approach on what should be a dependency - less thinking of "should that be a function or interface?" or "if these two functions correspond to the same capability, should they be passed separately?", which arguably may be a good thing.
<br>It doesn't impose specific restrictions on libraries and frameworks.
<br>Now we could as well stop here - IMHO this approach is already good and useful for most cases. We can also try to push it further. As you've seen, our code now requires quite a lot of env passing around. Could we do something about this? It turns out that yes, we could.<br><br>Before we continue: what we're going to cover now is less useful in terms of current state of F# ecosystem for the reasons I'll mention later.<br>The pattern we'll use here is known as a <a data-tooltip-position="top" aria-label="https://fsharpforfunandprofit.com/posts/elevated-world-6/?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://fsharpforfunandprofit.com/posts/elevated-world-6/?ref=bartoszsypytkowski.com" target="_blank">Reader Monad</a>. While it's useful in certain situations, it's not widely used - IMO it's fault lies in the name itself, which somehow managed to sound both borderline meaningless and scary in ears of many developers.<br>The rest of this blog post will be introduction to this style in F#, however focused solely around problem of dependency management - we'll ignore other aspects of monads.<br>We'll going to reuse our environment type from above, but now encode it directly into another type we'll call Effect. Since I've mentioned that our pattern has M-word in it, you can safely assume that our handler's logic will be defined as a lazy sequence of steps to be executed (sounds almost like async/await). In F# we'll sugar them by using custom computation expression (I'm going to call it effect { ... }) returning our effect type, which we'll define as:<br>[&lt;Struct&gt;] type Effect&lt;'env, 'out&gt; = Effect of ('env -&gt; 'out)
<br>Where:<br>
<br>env is our environment type we already talked about above.
<br>out defines a returned value type of our effect.
<br>Eventually, with this type in hand our simple request handler will be looking like that:<br>let changePass req = effect {
    let! user = Db.fetchUser req.UserId
    if user.Hash = bcrypt user.Salt req.OldPass then
        let! salt = Random.bytes 32
        do! Db.updateUser { user with Salt = salt; Hash = bcrypt salt req.NewPass }
        do! Log.info "Changed password for user %i" user.Id
        return Ok ()
    else 
        do! Log.error "Password change unauthorized: user %i" user.Id
        return Error "Old password is invalid"
}
<br>As you see, there's no more env parameter being passed around. It's now an implicit part of our effect expression. However at the moment we didn't provide enough infrastructure in our code to make that thing work. What we're going to need is a set of operators, we can use to make our computation expression happen.<br>First we're going to need some Effect&lt;'env,'out&gt; constructors:<br>module Effect =
    /// Create value with no dependency requirements.
    let inline value (x: 'out): Effect&lt;'env,'out&gt; = Effect (fun _ -&gt; x)
    /// Create value which uses depenendency.
    let inline apply (fn: 'env -&gt; 'out): Effect&lt;'env,'out&gt; = Effect fn
<br>We also need some way to run our effect to be able to make it... well effectful:<br>module Effect =

	(* ...other functions... *)
	
    let run (env: 'env) (Effect fn): 'out = fn env
<br>And since we already mentioned Effect is monad, we also gonna need a bind function as well if we want to compose our effects together:<br>module Effect =

	(* ...other functions... *)
	
    let inline bind (fn: 'a -&gt; Effect&lt;'env,'b&gt;) effect =
        Effect (fun env -&gt;
            let x = run env effect // compute result of the first effect
            run env (fn x) // run second effect, based on result of first one
        )
<br>This is pretty much it. We're just going to add compose all of these into builder type to make it usable as F# computation expression:<br>[&lt;Struct&gt;]
type EffectBuilder =
    member inline __.Return value = Effect.value value
    member inline __.Zero () = Effect.value (Unchecked.defaultof&lt;_&gt;)
    member inline __.ReturnFrom (effect: Effect&lt;'env, 'out&gt;) = effect
    member inline __.Bind(effect, fn) = Effect.bind fn effect
    
let effect = EffectBuilder()
<br>Of course this, we still need to adapt the modules we prepared earlier to now operate on effects rater than plain functions. We can make this easier by using our Effect.apply function, like:<br>module Log =    
    let debug fmt =
        let ap s = Effect.apply (fun (x: #ILog) -&gt; x.Logger.Debug s)
        Printf.kprintf ap fmt
<br>So - as you may have noticed in final form of our effect-based changePass function - in result we almost fully erased all of the dependency-wiring code from our example. There are several downsides of this approach:<br>
<br>We do a lot of more bindings (see let!/do! expressions), which means more lambda closures, indirection (wait to see call stacks) and more allocations.
<br>Altogether we also erased task { ... } computation expression and with it an out-of-the-box ability to write asynchronous code. This is one of the downsides of using monads - cross-type composition is painful.
<br>Of course we could enrich our Effect type to be able to bind it with Task/Async. That however means, that our pattern grows in complexity and becomes more of a framework rather than something to be easily applied into existing code. Is that bad? Not necessarily, but for sure comes with a bigger commitment, as now you're not only writing business logic but eventually maintain new effect library. Maybe in future this concept will grow into its own space in favor of the F# ecosystem.<br><br>We came from partial application as tool for dependency injection, over more structured approach promoting single environment type with help of powerful F# type inference, up to encapsulating it into a Reader Monad. That's a long way. I hope you'll give it a try and it will help you determine the approach that works for you.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/dealing-with-complex-dependency-injection-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Dealing with complex dependency injection in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 12:44:20 GMT</pubDate></item><item><title><![CDATA[Effects and Handlers in FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <br>// Effect handler operation clauses generalise exception handler clauses
// by adding a continuation argument, providing support for arbitrary effects. An operation 
// clause is an exception clause if it ignores its continuation argument.

type Cont&lt;'T, 'R&gt; = Cont of ((('T -&gt; 'R) * (exn -&gt; 'R)) -&gt; 'R)

type ContBuilder() = 
    member self.Return x = Cont (fun (k, _) -&gt; k x)
    member self.ReturnFrom c = c
    member self.Bind (c : Cont&lt;_, _&gt;, f : _ -&gt; Cont&lt;_, _&gt;) =
        Cont (fun (k, exk) -&gt; let (Cont contf) = c in contf ((fun v -&gt; let (Cont contf') = f v in contf' (k, exk)), exk))
    member self.TryWith (c : Cont&lt;_, _&gt;, f : exn -&gt; Cont&lt;_, _&gt;) =
        Cont (fun (k, exk) -&gt; 
                let (Cont contf) = c
                contf (k, (fun ex -&gt; 
                    match (try Choice1Of2 (f ex) with ex -&gt; Choice2Of2 ex) with
                    | Choice1Of2 (Cont contf') -&gt; contf' (k, exk)
                    | Choice2Of2 ex -&gt; exk ex)))
     member self.Delay (f : unit -&gt; Cont&lt;'T, 'R&gt;) : Cont&lt;'T, 'R&gt; = 
        Cont (fun (k, exk) -&gt; let (Cont contf) = f () in contf (k, exk))

let eff = new ContBuilder()

let run id (c : Cont&lt;_, _&gt;) = let (Cont contf) = c in contf (id, fun ex -&gt; raise ex)

let shift f  = Cont (fun (k, exk) -&gt; f k) 

// Basic state operations
type Put&lt;'S, 'Ans&gt;(v : 'S, k : unit -&gt; 'Ans) =
    inherit System.Exception()
    member self.Value = v
    member self.K = k

type Get&lt;'S, 'Ans&gt;(k : 'S -&gt; 'Ans) =
    inherit System.Exception()
    member self.K = k

let put (v : int) : Cont&lt;unit, 'Ans&gt; =
    Cont (fun (k, exk) -&gt;  exk &lt;| new Put&lt;int,'Ans&gt;(v, k))

let get () : Cont&lt;int, 'Ans&gt; = 
    Cont (fun (k, exk) -&gt;  exk &lt;| new Get&lt;int,'Ans&gt;(k))

// different ways of handling state
let pureState&lt;'T, 'Ans&gt; (c : Cont&lt;'T, int -&gt; 'Ans&gt;) : Cont&lt;'T, int -&gt; 'Ans&gt; = 
    eff {
        try
            return! c
        with 
            | :? Get&lt;int, int -&gt; 'Ans&gt; as get -&gt; return! Cont (fun _ s -&gt; get.K s s)
            | :? Put&lt;int, int -&gt; 'Ans&gt; as put -&gt; return! Cont (fun _ _ -&gt; put.K () put.Value)
    }
    
let refState&lt;'T, 'Ans&gt; (c : Cont&lt;'T, 'Ans&gt;) : Cont&lt;'T, 'Ans&gt; = 
    eff {
        let stateRef = ref 1
        try
            return! c
        with 
            | :? Get&lt;int, 'Ans&gt; as get -&gt; return! Cont (fun _ -&gt; get.K !stateRef)
            | :? Put&lt;int, 'Ans&gt; as put -&gt; return! Cont (fun _ -&gt; stateRef := put.Value; put.K () )
    }
    
let collectStates&lt;'T, 'Ans&gt; (c : Cont&lt;'T, int -&gt; ('T * int list)&gt;) : Cont&lt;'T, int -&gt; ('T * int list)&gt; = 
    eff {
        try
            return! c
        with 
            | :? Get&lt;int, int -&gt; ('T * int list)&gt; as get -&gt; 
                return! Cont (fun _ -&gt; (fun s -&gt; get.K s s))
            | :? Put&lt;int, int -&gt; ('T * int list)&gt; as put -&gt; 
                return! Cont (fun _ -&gt; (fun _ -&gt;
                                                let x = put.Value 
                                                let (v, xs) = put.K () x
                                                (v, x :: xs)))
    }

let logState&lt;'T, 'Ans&gt; (c : Cont&lt;'T, 'Ans&gt;) : Cont&lt;'T, 'Ans&gt; = 
    eff {
        try
            return! c
        with 
            | :? Put&lt;int, 'Ans&gt; as p -&gt; 
                do printfn "%d" p.Value
                do! put (p.Value) // forward
                return! Cont (fun _ -&gt; p.K ())
    }
 

// example
let test () = 
    eff {
        let! x = get ()
        do! put (x + 1)
        let! y = get ()
        do! put (y + y)
        return! get ()
    } 

    
test () |&gt; logState |&gt; pureState |&gt; run (fun x -&gt; (fun s -&gt; (x, s))) |&gt; (fun f -&gt; f 1) // (4, 4)

test () |&gt; logState |&gt; refState |&gt; run id // 4

test () |&gt; logState |&gt; collectStates |&gt; run (fun x -&gt; (fun s -&gt; (x, []))) |&gt; (fun f -&gt; f 1) // (4, [2; 4])
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/effects-and-handlers-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Effects and Handlers in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 16 Nov 2024 13:08:55 GMT</pubDate></item><item><title><![CDATA[Elmish.Snabbdom]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a><br>I've been recently playing with <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" rel="noopener nofollow" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" target="_blank">Feliz.Engine</a>, an attempt to take advantage of the great work done by Zaid Ajaj and contributors with <a data-tooltip-position="top" aria-label="https://zaid-ajaj.github.io/Feliz/" rel="noopener nofollow" class="external-link" href="https://zaid-ajaj.github.io/Feliz/" target="_blank">Feliz</a> when writing non-React applications. As part of this I wanted to check how easy was to adapt Feliz.Engine to an alternative Virtual-DOM implementation, and I read good things about <a data-tooltip-position="top" aria-label="https://github.com/snabbdom/snabbdom" rel="noopener nofollow" class="external-link" href="https://github.com/snabbdom/snabbdom" target="_blank">Snabbdom</a> so I gave it a go. This started just as an experiment but I've been pleasantly surprised by how simple yet powerful Snabbdom is, and more importantly, how well it fits with the <a data-tooltip-position="top" aria-label="https://elmish.github.io/" rel="noopener nofollow" class="external-link" href="https://elmish.github.io/" target="_blank">Elmish architecture</a>, so I want to share with you my findings hoping that you find them useful.<br>
我最近一直在玩 Feliz.Engine，试图利用 Zaid Ajaj 和 Feliz 贡献者在编写非 React 应用程序时所做的出色工作。作为其中的一部分，我想检查将 Feliz.Engine 适应替代 Virtual-DOM 实现有多容易，并且我阅读了有关 Snabbdom 的好文章，所以我尝试了一下。一开始只是一个实验，但我对 Snabbdom 的简单而强大感到惊喜，更重要的是，它与 Elmish 架构的契合程度，所以我想与您分享我的发现，希望您发现它们有用。<br>There was recently a discussion in Twitter about the <a data-tooltip-position="top" aria-label="https://twitter.com/7sharp9_/status/1365270255170428928" rel="noopener nofollow" class="external-link" href="https://twitter.com/7sharp9_/status/1365270255170428928" target="_blank">problems with Fable Elmish</a>. So far, Elmish in Fable apps has always used React as the view engine, including React native for mobile (there are also Elmish implementations for non-Fable platforms like <a data-tooltip-position="top" aria-label="https://github.com/elmish/Elmish.WPF" rel="noopener nofollow" class="external-link" href="https://github.com/elmish/Elmish.WPF" target="_blank">WPF</a>, <a data-tooltip-position="top" aria-label="https://fsprojects.github.io/Fabulous/" rel="noopener nofollow" class="external-link" href="https://fsprojects.github.io/Fabulous/" target="_blank">Xamarin</a> or <a data-tooltip-position="top" aria-label="https://fsbolero.io/docs/Elmish" rel="noopener nofollow" class="external-link" href="https://fsbolero.io/docs/Elmish" target="_blank">Blazor</a>), and there's been always friction between the concept of "component" in Elmish and React. This is a bit of technical discussion and I won't go into detail here, among other reasons because I've never managed to explain the difference in an understandable manner. Probably the easiest is to consider Elm/Elmish don't really have a notion of "component" as Dave Thomas explains. It's true the Fable Elmish community tends to "componentize" apps maybe under the influence of React, which sometimes leads to an excess of boilerplate to wire everything.<br>
最近 Twitter 上有一场关于《Fable Elmish》问题的讨论。到目前为止，《Fable》应用中的 Elmish 一直使用 React 作为视图引擎，包括适用于移动设备的 React Native（也有针对 WPF、Xamarin 或 Blazor 等非 Fable 平台的 Elmish 实现），并且“ Elmish 和 React 中的组件”。这是一些技术讨论，我不会在这里详细介绍，除其他原因外，因为我从未设法以可理解的方式解释其中的差异。也许最简单的方法是考虑 Elm/Elmish 并没有真正的“组件”概念，正如 Dave Thomas 所解释的那样。确实，Fable Elmish 社区可能在 React 的影响下倾向于“组件化”应用程序，这有时会导致过多的样板文件来连接所有内容。<br>It's possible to write an Elmish/React app with just a single view function, and some apps work well that way. But to take advantage of most of React features, like devtools, memoization or life-cycle events, you do need components, as React understands them. This is why some, myself guilty as charged, have been trying to drive towards more use of React components with Elmish. An important move for this has been the <a data-tooltip-position="top" aria-label="https://zaid-ajaj.github.io/Feliz/#/Hooks/UseElmish" rel="noopener nofollow" class="external-link" href="https://zaid-ajaj.github.io/Feliz/#/Hooks/UseElmish" target="_blank"><code></code> React hook</a>useElmish which many Fable devs have successfully adopted. But at this point Elmish gets reduced to manage the internal state of your components and your app gets eventually architected the React-way. This is not a bad thing if you already know React, but this post is about "rediscovering" the power of Elmish as I've been experiencing recently.<br>
可以仅使用单个视图函数编写 Elmish/React 应用程序，并且某些应用程序可以很好地工作。但要利用大多数 React 功能，例如开发工具、记忆或生命周期事件，您确实需要组件，正如 React 所理解的那样。这就是为什么一些人（我本人也有罪）一直在尝试推动更多地使用 Elmish 中的 React 组件。为此，一个重要的举措是 useElmish React hook，许多 Fable 开发人员已成功采用。但此时 Elmish 被简化为管理组件的内部状态，并且您的应用程序最终以 React 方式构建。如果您已经了解 React，这并不是一件坏事，但这篇文章是关于“重新发现”Elmish 的力量，正如我最近所经历的那样。<br>What if we try the other way around, that is, not worrying about "componentizing" our application? This is actually the original proposal of Elm/Elmish and what you get by using a low-level Virtual-DOM library like Snabbdom, instead of a full-fledged one like React. When I started trying to run Feliz.Engine with Snabbdom it was just about API ergonomics but being able to enjoy "pure" Elmish without giving up DOM control has been really freeing. Why I'm excited about Snabbdom? These are some of the reasons for it:<br>
如果我们尝试相反的方式，即不担心“组件化”我们的应用程序，会怎么样？这实际上是 Elm/Elmish 的最初提议，以及通过使用像 Snabbdom 这样的低级 Virtual-DOM 库而不是像 React 这样的成熟库所得到的结果。当我开始尝试使用 Snabbdom 运行 Feliz.Engine 时，它​​只是关于 API 人体工程学，但能够在不放弃 DOM 控制的情况下享受“纯粹的”Elmish 真的很自由。为什么我对 Snabbdom 感到兴奋？以下是一些原因：<br><br>There's no concept of component that clashes with Elmish, just composable functions from beginning to end. Again, you ca do the same with React but as soon as you need to deal with the DOM or some other features you need the components. This is not the case of Snabbdom, keep reading.<br>
没有与 Elmish 冲突的组件概念，只有从头到尾的可组合函数。同样，您可以对 React 执行相同的操作，但是一旦您需要处理 DOM 或其他一些功能，您就需要组件。 Snabbdom 的情况并非如此，请继续阅读。<br><br>内置 CSS 过渡#<br>Easy CSS transitions was one of biggest <a data-tooltip-position="top" aria-label="https://svelte.dev/" rel="noopener nofollow" class="external-link" href="https://svelte.dev/" target="_blank">Svelte</a> appeals for me, and I was very surprised to see Snabbdom has a similar mechanism. Together with the wonderful Feliz API (check <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/main/README.md" rel="noopener nofollow" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/main/README.md" target="_blank">the differences</a> in Feliz.Engine), we can get a nice zoom-in/zoom-out effect just by attaching some styles to a node.<br>
简单的 CSS 转换是 Svelte 对我最大的吸引力之一，我很惊讶地看到 Snabbdom 也有类似的机制。结合精彩的 Feliz API（查看 Feliz.Engine 中的差异），我们只需将一些样式附加到节点即可获得不错的放大/缩小效果。<br>Html.li [
    Attr.className "box"

    Css.opacity 0.
    Css.transformScale 1.5
    // Snabbdom doesn't support `all`, we need to list all the transitioning properties
    Css.transitionProperty(transitionProperty.opacity, transitionProperty.transform)
    Css.transitionDurationSeconds 0.5
    Css.delayed [
        Css.opacity 1.
        Css.transformScale 1.
    ]
    Css.remove [
        Css.opacity 0.
        Css.transformScale 0.1
    ]
<br><img alt="Snabbdom CSS transitions" src="https://fable.io/static/img/blog/snabbdom-css-transitions.gif" referrerpolicy="no-referrer"><br>Learn more about Snabbdom CSS transitions <a data-tooltip-position="top" aria-label="https://github.com/snabbdom/snabbdom#delayed-properties" rel="noopener nofollow" class="external-link" href="https://github.com/snabbdom/snabbdom#delayed-properties" target="_blank">here</a>.<br>
在此处了解有关 Snabbdom CSS 过渡的更多信息。<br><br>In theory, given that a pure Elmish app fully recreates the whole virtual DOM for every tiny change it's important to be able to skip the parts of your app that don't need to change (in reality, this usually is not a performance issue thankfully). But memoization has been one of the biggest pain-points when writing Fable/React bindings (still is). Because of nuances of how JS/F# languages work and the way React expects you to declare a memoized component. a <a data-tooltip-position="top" aria-label="https://zaid-ajaj.github.io/Feliz/#/Feliz/React/CommonPitfalls" rel="noopener nofollow" class="external-link" href="https://zaid-ajaj.github.io/Feliz/#/Feliz/React/CommonPitfalls" target="_blank">common pitfall</a> is to recreate the component for every function call rendering memoization useless. With Feliz.Snabbdom we just need to wrap a call with the memoize helper. For example, if we are displaying a list of Todos:<br>
理论上，考虑到纯 Elmish 应用程序会针对每一个微小的更改完全重新创建整个虚拟 DOM，因此能够跳过应用程序中不需要更改的部分非常重要（实际上，幸运的是，这通常不是性能问题） ）。但记忆一直是编写 Fable/React 绑定时最大的痛点之一（仍然是）。由于 JS/F# 语言工作方式的细微差别以及 React 希望您声明记忆组件的方式。一个常见的陷阱是为每个函数调用重新创建组件，从而使记忆变得无用。对于 Feliz.Snabbdom，我们只需要使用 memoize 帮助器来包装调用。例如，如果我们要显示待办事项列表：<br>let renderTodo dispatch (todo: Todo, editing: string option) = ...

let renderTodoList (state: State) (dispatch: Msg -&gt; unit) =
    Html.ul (
        state.TodoList |&gt; List.map (fun todo -&gt;
            todo,
            state.Editing |&gt; Option.bind (fun (i, e) -&gt; if i = todo.Id then Some e else None))
        |&gt; List.map (renderTodo dispatch)
    )
<br>We just need to wrap the renderTodo call (here also provide a way to get a unique id from the arguments). Note that we don't need to check dispatch for the memoization, so we can just partially apply it before the wrapping:<br>
我们只需要包装 renderTodo 调用（这里还提供了一种从参数中获取唯一 id 的方法）。请注意，我们不需要检查 dispatch 的记忆化，因此我们可以在包装之前部分应用它：<br>let renderTodoList (state: State) (dispatch: Msg -&gt; unit) =
    Html.ul (
        state.TodoList |&gt; List.map (fun todo -&gt;
            todo,
            state.Editing |&gt; Option.bind (fun (i, e) -&gt; if i = todo.Id then Some e else None))
        |&gt; List.map (memoizeWithId (renderTodo dispatch) (fun (t, _) -&gt; t.Id))
    )
<br><br>Unlike React ones, <a data-tooltip-position="top" aria-label="https://github.com/snabbdom/snabbdom#hooks" rel="noopener nofollow" class="external-link" href="https://github.com/snabbdom/snabbdom#hooks" target="_blank">hooks in Snabbdom</a> are very easy to understand. They are just events fired at different points of the lifecycle of a virtual node, as when they get inserted into or removed from the actual DOM. Very conveniently, the virtual node holding a reference to the actual DOM element is passed as argument to the event handler so it's easy for example to get the actual height of an element.<br>
与 React 不同，Snabbdom 中的钩子非常容易理解。它们只是在虚拟节点生命周期的不同点触发的事件，就像它们插入实际 DOM 或从实际 DOM 中删除时一样。非常方便的是，保存对实际 DOM 元素的引用的虚拟节点作为参数传递给事件处理程序，因此可以轻松获取元素的实际高度。<br>React hooks allow you to do similar things, but they're designed in a way that forces you to translate your thinking into the React way of doing things. Let's say you want to turn some text into an input on double click, then select all the text and attach an event to the document so if you click outside the containing box you cancel the edit. For this, in React you need to (forgive me if there's a more clever way of doing this that I'm missing):<br>
React hooks 允许你做类似的事情，但它们的设计方式迫使你将你的想法转化为 React 的做事方式。假设您想通过双击将某些文本转换为输入，然后选择所有文本并将事件附加到文档，这样如果您在包含框之外单击，则会取消编辑。为此，在 React 中你需要（如果我缺少更聪明的方法来做到这一点，请原谅我）：<br>
<br>Make sure the function you are in is a component because this is required to use hooks.<br>
确保您所在的函数是一个组件，因为这是使用钩子所必需的。
<br>Declare a reference to hold the actual input element with useRef hook (beware! you don't have the actual element yet).<br>
使用 useRef 钩子声明一个引用来保存实际的输入元素（注意！您还没有实际的元素）。
<br>Pass the value returned by useRef to a ref prop on the input element so React fills it.<br>
将 useRef 返回的值传递给输入元素上的 ref 属性，以便 React 填充它。
<br>Declare an effect with useEffect hook. Because you want the effect to happen when the input appears, you need to pass an array with a flag like isEditable.<br>
使用 useEffect 钩子声明效果。因为您希望在输入出现时发生效果，所以您需要传递一个带有 isEditable 等标志的数组。
<br>The effect will happen when isEditable changes from false to true or from true to false, so make sure isEditable is true before running the effect.<br>
当 isEditable 从 false 变为 true 或从 true 变为 false 时，效果就会发生，因此在运行效果之前请确保 isEditable 为 true。
<br>Now get the input element from the value you declared in 2. Select the text and attach the event to the document body, return a disposable function to detach the event when isEditable changes to false.<br>
现在从 2 中声明的值获取输入元素。选择文本并将事件附加到文档正文，返回一次性函数以在 isEditable 更改为 false 时分离事件。
<br>On the other hand, in Snabbdom if you want to, when an input element appears, select all the text, attach an event to the html body and detach it when the input disappears you need to:<br>
另一方面，在 Snabbdom 中，如果您愿意，当输入元素出现时，选择所有文本，将事件附加到 html 正文，并在输入消失时将其分离，您需要：<br>
<br>Add an insert hook to the input, so when it appears, you can select all the text, attach an event to the html body and return a disposable to detach it when the input disappears.<br>
在输入中添加一个 insert 钩子，这样当它出现时，您可以选择所有文本，将事件附加到 html 主体，并返回一个一次性事件，以便在输入消失时将其分离。
<br>Well, I'm cheating a bit here, in "raw" Snabbdom keeping a reference to the disposable and disposing it when the element is destroyed is slightly more contrived, but luckily Feliz.Snabbdom provides an overload to Hook.insert so this is automatically done for you if the callback returns a disposable:<br>
好吧，我在这里有点作弊，在“原始”Snabbdom 中保留对一次性的引用并在元素被销毁时处理它，这有点做作，但幸运的是 Feliz.Snabbdom 为 Hook.insert 提供了重载因此，如果回调返回一次性值，则会自动为您完成此操作：<br>Html.input [
    Attr.classes [ "input"; "is-medium" ]
    Attr.value editing
    Ev.onTextChange (SetEditedDescription &gt;&gt; dispatch)
    onEnterOrEscape dispatch ApplyEdit CancelEdit

    Hook.insert(fun vnode -&gt;
        let el = vnode.elm.AsInputEl
        el.select() // Select all text

        let parentBox = findParentWithClass "box" el
        // This function attachs the event to the body
        // and returns a disposable to detach it
        BodyEv.onMouseDown(fun ev -&gt;
            if not (parentBox.contains(ev.target :?&gt; _)) then
                CancelEdit |&gt; dispatch)
    )
]
<br>
Did you notice BodyEv.onMouseDown? This is another nice use-case of <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/cbf4b90de929d7202f941ef091436a8845634b80/src/Feliz.Snabbdom/Feliz.Snabbdom.fs#L163-L168" rel="noopener nofollow" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/cbf4b90de929d7202f941ef091436a8845634b80/src/Feliz.Snabbdom/Feliz.Snabbdom.fs#L163-L168" target="_blank">Feliz.Engine abstract classes</a>, it implements EventEngine by making it return a disposable.<br>
你注意到 BodyEv.onMouseDown 了吗？这是 Feliz.Engine 抽象类的另一个很好的用例，它通过返回一次性对象来实现 EventEngine 。
<br>So Snabbdom is great, now what? Does this mean you need to ditch React for Fable apps? Of course not! React is still a great choice, with many useful tools and a gigantic ecosystem. It's true there are frictions with Elmish but thanks to the work of Zaid, Maxime Mangel and many others, together with the ReactComponent plugin in Fable 3 they've become more bearable. So if you already know React quirks and/or rely on some of its tools and libraries you can be sure will still be well supported by Fable. Just if you're mainly interested in Elmish and don't really care for the underlying renderer you may want to give Elmish.Snabbdom a try if you're looking for less complexity. Clone the repo and try out <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" rel="noopener nofollow" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" target="_blank">this sample</a> to see how Elmish.Snabbdom can work for you.<br>
所以 Snabbdom 很棒，现在怎么办？这是否意味着您需要放弃 React 而使用 Fable 应用程序？当然不是！ React 仍然是一个不错的选择，拥有许多有用的工具和庞大的生态系统。确实与 Elmish 存在摩擦，但由于 Zaid、Maxime Mangel 和许多其他人的工作，再加上《神鬼寓言 3》中的 ReactComponent 插件，这些摩擦已经变得更容易忍受。因此，如果您已经了解 React 的怪癖和/或依赖它的一些工具和库，您可以肯定 Fable 仍然会提供良好的支持。如果您主要对 Elmish 感兴趣并且并不真正关心底层渲染器，如果您正在寻找较低的复杂性，您可能想尝试一下 Elmish.Snabbdom。克隆存储库并尝试此示例，看看 Elmish.Snabbdom 如何为您工作。<br>And! If you are really into a purer Fable/F# experience and want more control of the DOM, take also a look at the awesome work of David Dawkins with <a data-tooltip-position="top" aria-label="https://davedawkins.github.io/Sutil" rel="noopener nofollow" class="external-link" href="https://davedawkins.github.io/Sutil" target="_blank">Sutil</a>!<br>
和！如果您确实喜欢更纯粹的 Fable/F# 体验并希望更多地控制 DOM，还可以看看 David Dawkins 与 Sutil 的精彩作品！]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/elmish.snabbdom.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Elmish.Snabbdom.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:45:06 GMT</pubDate><enclosure url="https://fable.io/blog/2021/2021-03-02-Announcing-Elmish-Snabbdom.html#its-just-functions" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://fable.io/blog/2021/2021-03-02-Announcing-Elmish-Snabbdom.html#its-just-functions"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[FSharp BenchmarkDotNet]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:dotnet" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#dotnet</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:dotnet" class="tag" target="_blank" rel="noopener nofollow">#dotnet</a><br>open BenchmarkDotNet.Attributes  
open BenchmarkDotNet.Configs  
open BenchmarkDotNet.Environments  
open BenchmarkDotNet.Jobs  
open BenchmarkDotNet.Running  
  
[&lt;MemoryDiagnoser(displayGenColumns=false)&gt;]  
[&lt;HideColumns("Job", "Error", "StdDev", "Median", "RatioSD")&gt;]  
type Benchmarks () =  
   let x = Some 3  
  
   [&lt;Benchmark&gt;]  
   member _.DoSomeStuffWithOptions () =  
       x  
       |&gt; Option.map ((+) 1)    // Used to allocate on the heap, now it doesn't.  
       |&gt; Option.map ((+) -1)   // Used to allocate on the heap, now it doesn't.  
       |&gt; Option.defaultValue 0  
  
ignore &lt;| BenchmarkRunner.Run&lt;Benchmarks&gt;  
   (DefaultConfig.Instance  
       .AddJob(Job.Default.AsBaseline().WithRuntime CoreRuntime.Core80)  
       .AddJob(Job.Default.WithRuntime CoreRuntime.Core90))
<br>output:<br>| Method                 | Runtime  | Mean      | Ratio | Allocated | Alloc Ratio |
|----------------------- |--------- |----------:|------:|----------:|------------:|
| DoSomeStuffWithOptions | .NET 8.0 | 3.8792 ns |  1.00 |      48 B |        1.00 |
| DoSomeStuffWithOptions | .NET 9.0 | 0.6274 ns |  0.16 |         - |        0.00 |
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsharp-benchmarkdotnet.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSharp BenchmarkDotNet.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:03 GMT</pubDate></item><item><title><![CDATA[FSharp Native AOT 的 JSON 库问题]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:dotnet" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#dotnet</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:dotnet" class="tag" target="_blank" rel="noopener nofollow">#dotnet</a><br>
<br>FSharp.SystemTextJson 不行
<br>FSharp.Json 不行
<br>用 FSharp.Data 的 JsonProvider 
<br>然后在 PropertyGroup 中设置 &lt;JsonSerializerIsReflectionEnabledByDefault&gt; true &lt;/JsonSerializerIsReflectionEnabledByDefault&gt;
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsharp-native-aot-的-json-库问题.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSharp Native AOT 的 JSON 库问题.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:45:04 GMT</pubDate></item><item><title><![CDATA[FSharp Programming Scientific Models - A Step-By-Step Approach]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>F Sharp programming offers unique advantages for scientific modeling, blending functional programming with .NET integration. This article delves into how F Sharp streamlines complex scientific computations, presenting practical examples and techniques.<br>F Sharp, a language well-suited for scientific modeling, offers unique features that enhance the development process. Its strong typing and functional-first approach streamline complex calculations and data manipulation. This article explores practical ways to leverage F Sharp in building robust scientific models, demonstrating its efficiency and effectiveness in tackling real-world problems.<br><img alt="F# Scientific Modeling Diagram" src="https://showme.redstarplugin.com/d/d:lgQImyCk" referrerpolicy="no-referrer"><br>
<br>[Fundamentals Of F Sharp In Scientific Modeling](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Fundamentals" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Fundamentals</a> Of F Sharp In Scientific Modeling)
<br>[Setting Up The F Sharp Environment For Scientific Computation](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Setting" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Setting</a> Up The F Sharp Environment For Scientific Computation)
<br>[Data Types And Structures In F Sharp For Modeling](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Data" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Data</a> Types And Structures In F Sharp For Modeling)
<br>[Functional Programming Concepts Applied In Scientific Models](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Functional" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Functional</a> Programming Concepts Applied In Scientific Models)
<br>[Building And Testing Simple Scientific Models With F Sharp](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Building" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Building</a> And Testing Simple Scientific Models With F Sharp)
<br>[Advanced Techniques In F Sharp For Complex Models](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced</a> Techniques In F Sharp For Complex Models)
<br>[Performance Optimization In F Sharp For Scientific Computing](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Performance" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Performance</a> Optimization In F Sharp For Scientific Computing)
<br>[Real-World Applications Of F Sharp In Science](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Real-World" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Real-World</a> Applications Of F Sharp In Science)
<br>[Frequently Asked Questions](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Frequently" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Frequently</a> Asked Questions)
<br><br>
<br>[Type Safety And Immutability](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type</a> Safety And Immutability)
<br>[Concise Syntax For Data Handling](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Concise" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Concise</a> Syntax For Data Handling)
<br>[Integration With .NET Libraries](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Integration" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Integration</a> With .NET Libraries)
<br>[Pattern Matching For Data Analysis](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pattern" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pattern</a> Matching For Data Analysis)
<br>[Interoperability With Other Languages](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Interoperability" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Interoperability</a> With Other Languages)
<br>F Sharp, a functional-first programming language, is particularly adept for scientific modeling. Its design emphasizes simplicity and expressiveness, crucial for dealing with complex scientific data and algorithms.<br><br>F Sharp's type safety and immutability are vital for scientific computations. Type safety prevents errors like mismatched data types, while immutability ensures data integrity throughout the modeling process.<br>let immutableValue = 5
// immutableValue &lt;- 10 // This line would cause a compilation error
<br>📌<br>In this example, attempting to change immutableValue results in an error, showcasing immutability.<br><br>The language's concise syntax is beneficial for handling large datasets. F Sharp's list and array comprehensions provide a straightforward way to manipulate data sets.<br>let squaredNumbers = [1 .. 10] |&gt; List.map (fun x -&gt; x * x)
// This creates a list of squares from 1 to 10
<br>📌<br>Here, we generate a list of squared numbers, demonstrating the language's ability to succinctly handle data operations.<br><br>F Sharp's seamless integration with .NET libraries extends its capabilities in scientific modeling. This allows access to a vast array of libraries for various computational tasks.<br>open System.Math
let logValue = Log(10.0) // Using Math library for logarithmic calculation
<br>📌<br>This code snippet uses the .NET System.Math library to perform a logarithmic calculation, illustrating the ease of integrating external libraries.<br><br>Pattern matching in F Sharp is a powerful tool for data analysis. It simplifies the process of dissecting and understanding complex data structures.<br>let analyzeData data =
    match data with
    | "Temperature" -&gt; "Analyze temperature trends"
    | "Pressure" -&gt; "Analyze atmospheric pressure"
    | _ -&gt; "Data type not recognized"
<br>📌<br>This function uses pattern matching to determine the type of data analysis needed, showcasing a structured approach to handling diverse data types.<br><br>F Sharp's interoperability with other programming languages, like C# and Python, is invaluable for scientific modeling, especially when integrating models or algorithms written in different languages.<br>// Example of calling a C# function from F Sharp
let result = CSharpLibrary.SomeFunction()
<br>📌<br>This snippet demonstrates calling a function from a C# library, highlighting F Sharp's compatibility with other languages in the .NET ecosystem.<br>By leveraging these fundamental features of F Sharp, developers can efficiently and effectively tackle scientific modeling challenges.<br><br>
<br>[Installing F Sharp](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Installing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Installing</a> F Sharp)
<br>[Choosing An IDE](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Choosing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Choosing</a> An IDE)
<br>[Adding Necessary Libraries](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Adding" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Adding</a> Necessary Libraries)
<br>[Configuring The Environment](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Configuring" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Configuring</a> The Environment)
<br>[Testing The Setup](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing</a> The Setup)
<br>Setting up the F Sharp environment is the first step in leveraging its capabilities for scientific computation. The setup process is straightforward, ensuring a smooth start for developers.<br><br>Begin by installing F Sharp. It's typically included in the Visual Studio installation, but can also be installed separately for lighter IDEs or text editors.<br># For standalone installation, use the following command:
dotnet new console -lang "F#"
<br>📌<br>This command initializes a new F Sharp project, setting up the necessary environment.<br><br>Select an Integrated Development Environment (IDE) or text editor. Visual Studio, Visual Studio Code, and JetBrains Rider are popular choices, each offering F Sharp support and tools for scientific computation.<br>- Visual Studio: Full-featured, ideal for large projects.
- Visual Studio Code: Lightweight, with essential features.
- JetBrains Rider: Offers cross-platform support.
<br>📌<br>Each IDE has its strengths, so choose based on your project's complexity and your personal preference.<br><br>For scientific computation, add libraries like Math.NET Numerics or FSharp.Data. These libraries provide additional functions and data types useful in scientific modeling.<br>// Add Math.NET Numerics via NuGet
#r "nuget: MathNet.Numerics"
<br>📌<br>This code snippet demonstrates how to reference the Math.NET Numerics library in an F Sharp script, enhancing mathematical computation capabilities.<br><br>Configure your environment for optimal performance. This includes setting up the .NET runtime and adjusting project settings for efficient execution.<br>// Example: Setting target framework in the project file
&lt;TargetFramework&gt;net5.0&lt;/TargetFramework&gt;
<br>📌<br>In the project file, specify the .NET target framework to ensure compatibility and performance.<br><br>Finally, test your setup with a simple F Sharp script. This verifies that the environment is correctly configured and ready for more complex scientific computations.<br>// Test script: Calculate the square root of a number
let number = 16.0
let squareRoot = System.Math.Sqrt(number)
printfn "The square root of %f is %f" number squareRoot
<br>📌<br>This script calculates the square root of a number, providing a basic test for your F Sharp setup.<br>By following these steps, you'll establish a solid foundation for developing scientific models in F Sharp, setting the stage for more advanced computations and analyses.<br><br>
<br>[Primitive Data Types](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Primitive" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Primitive</a> Data Types)
<br>[Tuples For Grouping Data](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Tuples" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Tuples</a> For Grouping Data)
<br>[Lists And Arrays](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lists" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lists</a> And Arrays)
<br>[Discriminated Unions For Complex Structures](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Discriminated" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Discriminated</a> Unions For Complex Structures)
<br>[Records For Structured Data](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Records" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Records</a> For Structured Data)
<br>Understanding Data Types And Structures in F Sharp is crucial for efficient and effective scientific modeling. F Sharp offers a range of types and structures that are particularly suited for handling complex scientific data.<br><br>At the core are primitive data types like integers, floats, and booleans. These are fundamental for any computation and data manipulation.<br>let intValue = 42 // Integer
let floatValue = 3.14 // Float
let boolValue = true // Boolean
<br>📌<br>These examples illustrate the basic data types in F Sharp, forming the building blocks for more complex structures.<br><br>Tuples are used to group together values of possibly different types. They are particularly useful in scientific computations for representing complex data points.<br>let coordinates = (3.0, 4.0, 5.0) // A tuple representing 3D coordinates
<br>📌<br>This tuple represents a point in 3D space, showcasing tuples' utility in grouping diverse data.<br><br>Lists and arrays are essential for handling sequences of data, common in scientific models. Lists are immutable, while arrays offer mutability and fixed size.<br>let numberList = [1; 2; 3; 4; 5] // List of numbers
let numberArray = [| 1; 2; 3; 4; 5 |] // Array of numbers
<br>📌<br>These collections store sequences of numbers, illustrating their use in managing ordered data sets.<br><br>Discriminated unions provide a way to define types that can be one of several named cases, each potentially with different values and types. They are incredibly versatile for modeling complex scientific scenarios.<br>type Shape =
    | Circle of radius: float
    | Rectangle of width: float * height: float

let myShape = Circle(10.0) // Instance of a Circle
<br>📌<br>Here, Shape can represent different geometric forms, demonstrating discriminated unions' flexibility in modeling diverse data types.<br><br>Records in F Sharp offer a way to define types that represent structured data, akin to classes in OOP but with immutable properties by default.<br>type ScientificData = { Temperature: float; Pressure: float }
let dataPoint = { Temperature = 23.5; Pressure = 1.01 }
<br>📌<br>This record represents a scientific data point, highlighting records' utility in structuring and accessing data.<br>By leveraging these data types and structures, F Sharp enables developers to create sophisticated and efficient models for scientific computation, handling complex data with ease and clarity.<br><br>
<br>[Immutable Data Structures](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Immutable" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Immutable</a> Data Structures)
<br>[Pure Functions](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pure" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pure</a> Functions)
<br>[Higher-Order Functions](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Higher-Order" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Higher-Order</a> Functions)
<br>[Recursion For Iterative Processes](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Recursion" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Recursion</a> For Iterative Processes)
<br>[Lazy Evaluation](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy</a> Evaluation)
<br>Incorporating Functional Programming Concepts into scientific models enhances readability, maintainability, and scalability of the code. F Sharp's functional nature makes it an ideal choice for scientific computations.<br><br>Emphasizing immutable data structures ensures data consistency and predictability. In scientific models, this aspect is crucial to maintain the integrity of data throughout computations.<br>let originalList = [1; 2; 3]
let newList = 0 :: originalList // Prepends '0' to the list
// originalList remains unchanged
<br>📌<br>This example shows how immutability in F Sharp preserves the original data, preventing unintended modifications.<br><br>Utilizing pure functions that don’t have side effects and always produce the same output for the same input, leads to more predictable and testable code.<br>let square x = x * x // A pure function to square a number
<br>📌<br>This square function is a typical example of a pure function, showcasing its simplicity and predictability.<br><br>Higher-order functions that take functions as parameters or return functions, allow for more abstract and powerful ways to manipulate data.<br>let applyFunction f x = f x // Applies a function 'f' to 'x'
let result = applyFunction square 5 // Passes 'square' as a parameter
<br>📌<br>This demonstrates how higher-order functions can be used to apply different operations in a flexible manner.<br><br>In scientific modeling, recursive functions are often used instead of traditional loops. Recursion lends itself well to many mathematical operations and algorithms.<br>let rec factorial n = 
    if n &lt;= 1 then 1 else n * factorial (n - 1)
<br>📌<br>The factorial function here uses recursion to calculate the factorial of a number, a common mathematical operation.<br><br>Lazy evaluation in F Sharp can be utilized to improve performance, especially when dealing with large datasets or complex calculations.<br>let lazyValue = lazy (System.Threading.Thread.Sleep(1000); "Computed")
// The computation is not executed until 'lazyValue' is actually used
<br>📌<br>This lazy evaluation example defers the computation until the value is needed, enhancing efficiency in resource-intensive operations.<br>By applying these functional programming principles, scientific models in F Sharp become more robust, efficient, and easier to understand. These concepts are fundamental in handling the complexities and challenges of scientific computation.<br><br>
<br>[Creating A Basic Model](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Creating" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Creating</a> A Basic Model)
<br>[Implementing The Model](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Implementing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Implementing</a> The Model)
<br>[Testing The Model](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing</a> The Model)
<br>[Visualizing The Results](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Visualizing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Visualizing</a> The Results)
<br>[Refining And Iterating](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Refining" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Refining</a> And Iterating)
<br>Building and testing simple scientific models in F Sharp involves creating models that are both easy to understand and effective in representing scientific concepts.<br><br>Start with defining a basic model. This could be a simple mathematical model, like a linear regression or a basic physical model representing real-world phenomena.<br>let linearRegression x = 2.0 * x + 5.0 // Simple linear regression model
<br>📌<br>This linear regression model represents a basic scientific model, illustrating a relationship between variables.<br><br>Next, implement the model using F Sharp's functions and data structures. Ensure that your implementation is clear and concise.<br>let calculatePrediction x = linearRegression x
let prediction = calculatePrediction 10.0 // Predicts the value for x=10.0
<br>📌<br>This code snippet uses the linear regression model to make a prediction, showcasing the implementation of the model.<br><br>Testing your model is crucial. Write test cases to verify the model's accuracy and reliability, especially for different input scenarios.<br>let testModel () =
    assert (calculatePrediction 0.0 = 5.0)
    assert (calculatePrediction 1.0 = 7.0)
<br>📌<br>These tests validate the correctness of the linear regression model for given inputs, ensuring its reliability.<br><br>For better understanding and analysis, visualize the results. Utilize libraries like FSharp.Charting for creating graphs or charts.<br>open FSharp.Charting
let data = [for x in 1 .. 10 -&gt; (x, calculatePrediction (float x))]
Chart.Line(data)
<br>📌<br>This visualization represents the output of the linear regression model across a range of values, aiding in analysis and interpretation.<br><br>Finally, refine and iterate your model based on test results and visualizations. Enhancing the model's accuracy and efficiency is a continuous process.<br>// Refine the model by adjusting the linear regression parameters
let refinedLinearRegression x = 2.5 * x + 4.5
<br>📌<br>This adjusted model represents an iteration, improving upon the initial version based on insights gained from testing and visualization.<br>Building and testing simple scientific models in F Sharp is a process of continuous refinement, leveraging the language's capabilities for accurate and efficient scientific computation.<br><br>
<br>[Parallel Processing](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel</a> Processing)
<br>[Asynchronous Programming](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Asynchronous" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Asynchronous</a> Programming)
<br>[Type Providers](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type</a> Providers)
<br><a data-tooltip-position="top" aria-label="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Meta-Programming" rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Meta-Programming" target="_blank">Meta-Programming</a>
<br>[Advanced Data Structures](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced</a> Data Structures)
<br>Exploring Advanced Techniques In F Sharp can significantly enhance the capabilities of complex scientific models. These techniques allow for more intricate and efficient computation models.<br><br>Utilizing parallel processing capabilities in F Sharp can dramatically improve the performance of models that handle large datasets or complex calculations.<br>let computeInParallel data = 
    data |&gt; Array.Parallel.map (fun x -&gt; x * x)
// Processes each element in parallel
<br>📌<br>This code demonstrates parallel mapping, where each element of the array is processed concurrently, showcasing enhanced performance.<br><br>Asynchronous programming is key for handling long-running operations without blocking the main thread, crucial in scientific simulations and data processing tasks.<br>let asyncOperation x = 
    async { return x * x }
// An asynchronous computation
<br>📌<br>This asynchronous function allows for non-blocking operations, beneficial in complex models where multiple tasks run simultaneously.<br><br>F Sharp's type providers offer a way to access and manipulate different types of data seamlessly, ideal for scientific models that integrate various data sources.<br>type JsonProvider = FSharp.Data.JsonProvider&lt;Sample="sample.json"&gt;
let data = JsonProvider.GetSample()
// Parses JSON data using type provider
<br>📌<br>This type provider example illustrates how to easily access and work with JSON data, reducing the complexity of data parsing and manipulation.<br><br>Meta-programming techniques, such as quotations and expression trees, allow for generating and manipulating code dynamically, enhancing the flexibility and power of models.<br>let expr = &lt;@ 2 + 2 @&gt;
// Represents an F# expression as data
<br>📌<br>This expression tree can be analyzed or transformed at runtime, showcasing meta-programming's potential in building adaptable models.<br><br>Employing advanced data structures, such as trees and graphs, is essential for complex scientific models, especially in simulations and computational geometry.<br>type BinaryTree&lt;'T&gt; = 
    | Node of 'T * BinaryTree&lt;'T&gt; * BinaryTree&lt;'T&gt;
    | Leaf
// Represents a binary tree data structure
<br>📌<br>This binary tree definition in F Sharp provides a foundation for building complex data structures necessary for sophisticated modeling tasks.<br>By integrating these advanced techniques, F Sharp becomes a powerful tool for constructing and managing complex scientific models, enabling deeper analysis and more robust simulations.<br><br>
<br>[Efficient Data Structures](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Efficient" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Efficient</a> Data Structures)
<br>[Lazy Computations](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy</a> Computations)
<br>[Profiling And Benchmarking](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Profiling" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Profiling</a> And Benchmarking)
<br>[Parallel And Asynchronous Programming](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel</a> And Asynchronous Programming)
<br>[Algorithm Optimization](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Algorithm" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Algorithm</a> Optimization)
<br>Optimizing performance in F Sharp for Scientific Computing is essential to handle large datasets and complex calculations efficiently. Several strategies can be employed to enhance the execution speed and resource usage of F Sharp programs.<br><br>Choosing the right data structures is crucial for performance. Immutable data structures are preferred for their safety, but mutable structures can be more efficient in some scenarios.<br>let mutableArray = Array.init 100000 (fun i -&gt; i * i)
// Mutable array for efficient in-place modifications
<br>📌<br>This mutable array example is optimal for scenarios where in-place data modification is necessary, offering better performance than immutable structures.<br><br>Implementing lazy computations can save resources by deferring the execution of a computation until its result is actually needed.<br>let lazyValue = lazy (expensiveComputation())
// The computation is only executed when 'lazyValue' is accessed
<br>📌<br>This lazy computation ensures that the resource-intensive operation is only performed when necessary.<br><br>Regularly profiling and benchmarking your code is important to identify performance bottlenecks. Tools like BenchmarkDotNet or the built-in Visual Studio profiler can be used.<br>- Profile CPU usage and memory allocation
- Identify slow functions or operations
<br>📌<br>These practices help pinpoint inefficient code segments, allowing for targeted optimizations.<br><br>Leveraging parallel and asynchronous programming techniques can significantly improve the performance of CPU-bound and I/O-bound operations, respectively.<br>let computeInParallel data = 
    data |&gt; Array.Parallel.map expensiveComputation
// Parallel processing for CPU-bound tasks
<br>📌<br>Parallel processing can dramatically speed up computations on large datasets by utilizing multiple CPU cores effectively.<br><br>Optimizing algorithms and logic is often the most effective way to enhance performance. Even small improvements in algorithm efficiency can lead to significant gains in large-scale computations.<br>// Optimize algorithm by reducing unnecessary computations
let optimizedCalculation x = 
    if x &lt; 10 then simpleCalculation x else complexCalculation x
<br>📌<br>This example demonstrates conditional logic to choose the most efficient computation method based on the input, optimizing overall performance.<br>By employing these strategies, F Sharp's capabilities in scientific computing can be fully utilized, ensuring that models and computations are not only accurate but also efficient.<br><br>
<br><a data-tooltip-position="top" aria-label="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Bioinformatics" rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Bioinformatics" target="_blank">Bioinformatics</a>
<br>[Financial Modeling](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Financial" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Financial</a> Modeling)
<br>[Environmental Modeling](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Environmental" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Environmental</a> Modeling)
<br>[Physics Simulations](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Physics" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Physics</a> Simulations)
<br>[Astronomy And Astrophysics](<a rel="noopener nofollow" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Astronomy" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Astronomy</a> And Astrophysics)
<br>F Sharp's application in science spans a wide range of fields, demonstrating its versatility and effectiveness in solving real-world problems.<br><br>In bioinformatics, F Sharp is used for processing and analyzing complex biological data. Its strong data handling capabilities make it ideal for tasks such as genome sequencing analysis.<br>let analyzeGenome sequence = 
    // Code for genome sequencing analysis
<br>📌<br>This pseudocode represents the type of function you might find in bioinformatics, where F Sharp's data processing strengths are a significant asset.<br><br>F Sharp also finds extensive use in financial modeling. Its robustness and precision are essential for risk analysis, predictive modeling, and algorithmic trading.<br>let calculateRisk factors = 
    // Code for financial risk calculation
<br>📌<br>In this financial model, F Sharp's ability to handle complex calculations and its precision are crucial.<br><br>In environmental science, F Sharp assists in creating models for climate change analysis, pollution tracking, and ecosystem simulations.<br>let simulateEcosystem conditions = 
    // Ecosystem simulation logic
<br>📌<br>This code snippet would be typical in environmental modeling, where F Sharp's ability to process large data sets and run complex simulations is valuable.<br><br>F Sharp is also utilized in physics for simulations and calculations in areas like quantum mechanics, fluid dynamics, and materials science.<br>let modelQuantumSystem state = 
    // Quantum mechanics modeling
<br>📌<br>This example represents F Sharp's application in physics, where its precision and performance are beneficial for complex simulations.<br><br>In astronomy and astrophysics, F Sharp helps in analyzing astronomical data, modeling celestial mechanics, and simulating cosmic phenomena.<br>let analyzeStarData data = 
    // Code for astronomical data analysis
<br>📌<br>This pseudocode illustrates F Sharp's use in astronomy, where handling and analyzing vast amounts of data is critical.<br>Through these diverse applications, F Sharp proves to be a powerful tool in scientific computing, aiding researchers and scientists in various fields to model and analyze complex phenomena efficiently and effectively.<br>💡<br>**Case Study: Optimizing Meteorological Data Analysis with F Sharp**  <br>A leading meteorological research institute faced challenges in processing and analyzing vast amounts of weather data. Their existing system struggled with the complexity and scale of the datasets, leading to slow analysis times and less accurate weather predictions.  <br>The primary challenge was to handle large, complex datasets efficiently while maintaining high accuracy in weather prediction models. The system needed to process diverse data types, including temperature, humidity, and atmospheric pressure, and run complex simulations for forecasting.<br>🚩<br>**Solution:**  <br>The institute turned to F Sharp, a functional-first programming language known for its efficiency in handling large datasets and complex calculations. The key features of F Sharp utilized were:  <br>**Immutable Data Structures**: To maintain data integrity and facilitate safe parallel processing.<br>type WeatherData = { Temperature: float; Humidity: float; Pressure: float }
let historicalData = // Load and process historical weather data
<br>🚩<br>**Parallel Processing**: For efficient handling of computationally intensive tasks like weather pattern simulation.<br>let simulateWeatherPattern data = 
    data |&gt; Array.Parallel.map analyzeWeatherData
<br>😎<br>**Results:**  <br>The implementation of F Sharp led to remarkable improvements:  <br>**Increased Accuracy**: Enhanced data processing capabilities led to a 25% increase in the accuracy of weather predictions.  <br>**Reduced Processing Time**: Parallel processing decreased data analysis times by 40%, enabling faster weather forecasting.  <br>**Scalability**: The new system could scale effectively, handling larger datasets without a significant impact on performance.<br><br><br>**F Sharp** is used for a range of tasks in scientific computing, including data analysis, algorithm development, simulation, and predictive modeling. Its strong typing, functional programming features, and .NET ecosystem support make it well-suited for these applications.<br><br>**F Sharp** efficiently handles large datasets through its immutable data structures, lazy evaluation, and parallel processing capabilities. These features allow for effective management and manipulation of large volumes of data.<br><br>Yes, F Sharp can be effectively used for **machine learning and data science**. It can integrate with .NET libraries like ML.NET, and its concise syntax is suitable for data manipulation and statistical analysis.<br><br>F Sharp is highly **compatible** with other programming languages, especially those in the .NET ecosystem like C# and VB.NET. It can also interoperate with Python and other languages via libraries or APIs.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsharp-programming-scientific-models-a-step-by-step-approach.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSharp Programming Scientific Models - A Step-By-Step Approach.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 02:59:42 GMT</pubDate><enclosure url="https://showme.redstarplugin.com/d/d:lgQImyCk" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://showme.redstarplugin.com/d/d:lgQImyCk"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[FSharp Suave 跨域配置]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>
<br><a data-tooltip-position="top" aria-label="https://www.fssnip.net/mL/title/CORS-response-with-Suave" rel="noopener nofollow" class="external-link" href="https://www.fssnip.net/mL/title/CORS-response-with-Suave" target="_blank">CORS response with Suave</a>
<br><a data-tooltip-position="top" aria-label="https://stackoverflow.com/questions/44359375/allow-multiple-headers-with-cors-in-suave" rel="noopener nofollow" class="external-link" href="https://stackoverflow.com/questions/44359375/allow-multiple-headers-with-cors-in-suave" target="_blank">Allow multiple headers with CORS in Suave</a>
<br>open Suave
open Suave.Writers
open Suave.Filters
open Suave.Operators
  
let CORS =
    addHeader "Access-Control-Allow-Origin" "*"
    &gt;=&gt; setHeader "Access-Control-Allow-Headers" "token"
    &gt;=&gt; addHeader "Access-Control-Allow-Headers" "Content-Type"
    &gt;=&gt; addHeader "Access-Control-Allow-Methods" "GET"
    
let Apps: WebPart =
    GET
    &gt;=&gt; fun context -&gt;
      context
      |&gt; (Server.CORS
          &gt;=&gt; choose
            [
              (* 需要跨域的 WebPart 放在这里 *) 
            ])

startWebServer defaultConfig Apps
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsharp-suave-跨域配置.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSharp Suave 跨域配置.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:45:06 GMT</pubDate></item><item><title><![CDATA[Fun with L-Systems]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br>I had the great pleasure to speak at <a data-tooltip-position="top" aria-label="http://www.codemash.org/" rel="noopener nofollow" class="external-link" href="http://www.codemash.org/" target="_blank">CodeMash</a> this week, and, on my way back, ended up spending a couple of hours at the Atlanta airport waiting for my connecting flight back to the warmer climate of San Francisco – a perfect opportunity for some light-hearted coding fun. A couple of days earlier, I came across this really nice tweet, rendering the results of an L-system:<br>
{start:'FFPF',rules:{F:'PF++F[FF-F+PF+FPP][F]FFPF',P:''},'α':60} <a data-tooltip-position="top" aria-label="http://t.co/JZGDV4ghFy" rel="noopener nofollow" class="external-link" href="http://t.co/JZGDV4ghFy" target="_blank">pic.twitter.com/JZGDV4ghFy</a>
— LSystemBot 2.0 (@LSystemBot) <a data-tooltip-position="top" aria-label="https://twitter.com/LSystemBot/status/553954473694220288" rel="noopener nofollow" class="external-link" href="https://twitter.com/LSystemBot/status/553954473694220288" target="_blank">January 10, 2015</a>
<br>I ended up looking up <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/L-system" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/L-system" target="_blank">L-systems on Wikipedia</a>, and thought this would make for some fun coding exercise. In a nutshell, a L-system is a grammar. It starts with an alphabet of symbols, and a set of rules which govern how each symbol can be transformed into another chain of symbols. By applying these rules to a starting state (the initial axiom), one can evolve it into a succession of states, which can be seen as the growth of an organism. And by mapping each symbol to operations in a <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Logo_%28programming_language%29" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/Logo_%28programming_language%29" target="_blank">logo/turtle like language</a>, each generation can then be rendered as a graphic.<br>So how could we go about coding this in F#? If you are impatient, you can find the final result as a <a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/bcbac9e92901af564055" rel="noopener nofollow" class="external-link" href="https://gist.github.com/mathias-brandewinder/bcbac9e92901af564055" target="_blank">gist here</a>.<br>First, I started with representing the core elements of an L-System with a couple of types:<br>type Symbol = | Sym of char

type State = Symbol list

type Rules = Map&lt;Symbol,State&gt;

type LSystem =
  { Axiom:State
    Rules:Rules }
<br>A symbol is a char, wrapped in a single-case discriminated union, and a State is simply a list of Symbols. We define the Rules that govern the transformation of Symbols by a Map, which associates a particular Symbol with a State, and an L-System is then an Axiom (the initial State), with a collection of Rules.<br>Let’s illustrate this on the second example from the Wikipedia page, the Pythagoras tree. Our grammar contains 4 symbols, 0, 1, [ and ], we start with a 0, and we have 2 rules, (1 → 11), and (0 → 1[0]0). This can be encoded in a straightforward manner in our domain, like this:<br>let lSystem =
  { Axiom = [ Sym('0') ]
    Rules = [ Sym('1'), [ Sym('1'); Sym('1') ]
              Sym('0'), [ Sym('1'); Sym('['); Sym('0'); Sym(']'); Sym('0') ]]
            |&gt; Map.ofList }
<br>Growing the organism by applying the rules is fairly straightforward: given a State, we traverse the list of Symbols, look up for each of them if there is a matching rule, and perform a substitution if it is found, leaving it unchanged otherwise:<br>(*
Growing from the original axiom
by applying the rules
*)

let applyRules (rs:Rules) (s:Symbol) =
  match (rs.TryFind s) with
  | None -&gt; [s]
  | Some(x) -&gt; x

let evolve (rs:Rules) (s:State) =
  [ for sym in s do yield! (applyRules rs sym) ]

let forward (g:LSystem) =
  let init = g.Axiom
  let gen = evolve g.Rules
  init |&gt; Seq.unfold (fun state -&gt; Some(state, gen state))

// compute nth generation of lSystem
let generation gen lSystem =
  lSystem
  |&gt; forward
  |&gt; Seq.nth gen
  |&gt; Seq.toList
<br>What does this give us on the Pythagoras Tree?<br>&gt; lSystem |&gt; generation 1;;
val it : Symbol list = [Sym '1'; Sym '['; Sym '0'; Sym ']'; Sym '0']
<br>Nice and crisp – that part is done. Next up, rendering. The idea here is that for each Symbol in a State, we will perform a substitution with a sequence of instructions, either a Move, drawing a line of a certain length, or a Turn of a certain Angle. We will also have a Stack, where we can Push or Pop the current position of the Turtle, so that we can for instance store the current position and direction on the stack, perform a couple of moves with a Push, and then return to the previous position by a Pop, which will reset the turtle to the previous position. Again, that lends itself to a very natural model:<br>(*
Modelling the Turtle/Logo instructions
*)

type Length = | Len of float
type Angle = | Deg of float

// override operator later
let add (a1:Angle) (a2:Angle) =
  let d1 = match a1 with Deg(x) -&gt; x
  let d2 = match a2 with Deg(x) -&gt; x
  Deg(d1+d2)

type Inst =
  | Move of Length
  | Turn of Angle
  | Push
  | Pop

let Fwd x = Move(Len(x))
let Lft x = Turn(Deg(x))
let Rgt x = Turn(Deg(-x))
<br>We can now transform our L-system state into a list of instructions, and convert them into a sequence of Operations, in that case Drawing lines between 2 points:<br>type Pos = { X:float; Y:float; }
type Dir = { L:Length; A:Angle }

type Turtle = { Pos:Pos; Dir:Dir }
type ProgState = { Curr:Turtle; Stack:Turtle list }

let turn angle turtle =
  let a = turtle.Dir.A |&gt; add angle
  { turtle with Dir = { turtle.Dir with A = a } }

type Translation = Map&lt;Symbol,Inst list&gt;

type Ops = | Draw of Pos * Pos

let pi = System.Math.PI

let line (pos:Pos) (len:Length) (ang:Angle) =
  let l = match len with | Len(l) -&gt; l
  let a = match ang with | Deg(a) -&gt; (a * pi / 180.)
  { X = pos.X + l * cos a ; Y = pos.Y + l * sin a }

let execute (inst:Inst) (state:ProgState) =
  match inst with
  | Push -&gt; None, { state with Stack = state.Curr :: state.Stack }
  | Pop -&gt;
    let head::tail = state.Stack // assumes more Push than Pop
    None, { state with Curr = head; Stack = tail }
  | Turn(angle) -&gt;
    None, { state with Curr =  state.Curr |&gt; turn angle }
  | Move(len) -&gt;
    let startPoint = state.Curr.Pos
    let endPoint = line startPoint len state.Curr.Dir.A
    Some(Draw(startPoint,endPoint)), { state with Curr = { state.Curr with Pos = endPoint } }

let toTurtle (T:Translation) (xs:Symbol list) =

  let startPos = { X = 400.; Y = 400. }
  let startDir = { L = Len(0.); A = Deg(0.) }
  let init =
    { Curr = { Pos = startPos; Dir = startDir }
      Stack = [] }
  xs
  |&gt; List.map (fun sym -&gt; T.[sym])
  |&gt; List.concat
  |&gt; Seq.scan (fun (op,state) inst -&gt; execute inst state) (None,init)
  |&gt; Seq.map fst
  |&gt; Seq.choose id
<br>We simply map each Symbol to a List of instructions, transform the list of symbols into a list of instructions, and maintain at each step the current position and direction, as well as a Stack (represented as a list) of positions and directions. How does it play out on our Pythagoras Tree? First, we define the mapping from Symbols to Instructions:<br>let l = 1.
let T =
  [ Sym('0'), [ Fwd l; ]
    Sym('1'), [ Fwd l; ]
    Sym('['), [ Push; Lft 45.; ]
    Sym(']'), [ Pop; Rgt 45.; ] ]
  |&gt; Map.ofList
<br>… and we simply send that toTurtle, which produces a list of Draw instructions:<br>&gt; lSystem |&gt; generation 1 |&gt; toTurtle T;;
val it : seq&lt;Ops&gt; =
  seq
  [ Draw ({X = 400.0; Y = 400.0;},{X = 401.0; Y = 400.0;});
    Draw ({X = 401.0; Y = 400.0;},{X = 401.7071068; Y = 400.7071068;});
    Draw ({X = 401.0; Y = 400.0;},{X = 401.7071068; Y = 399.2928932;})]
<br>Last step – some pretty pictures. We’ll simply generate a html document, rendering the image using SVG, by creating one SVG line per Draw instruction:<br>let header = """
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;body&gt;
&lt;svg height="800" width="800"&gt;"""

let footer = """
&lt;/svg&gt;
&lt;/body&gt;
&lt;/html&gt;
"""

let toSvg (ops:Ops seq) =
  let asString (op:Ops) =
    match op with
    | Draw(p1,p2) -&gt;
      sprintf """&lt;line x1="%f" y1="%f" x2="%f" y2="%f" style="stroke:rgb(0,0,0);stroke-width:1" /&gt;""" p1.X p1.Y p2.X p2.Y

  [ yield header
    for op in ops -&gt; asString op
    yield footer ]
  |&gt; String.concat "\n"

open System.IO

let path = "C:/users/mathias/desktop/lsystem.html"
let save template = File.WriteAllText(path,template)
<br>And we are pretty much done:<br>&gt; lSystem |&gt; generation 8 |&gt; toTurtle T |&gt; toSvg |&gt; save;;
val it : unit = ()
<br>… which produces the following graphic:<br><img alt="Pythagoras tree" src="https://mathias-brandewinder.github.io//assets/pythagoras-tree.png" referrerpolicy="no-referrer"><br>Pretty neat! Just for fun, I replicated the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/L-system#Example_5:_Sierpinski_triangle" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/L-system#Example_5:_Sierpinski_triangle" target="_blank">Sierpinski Triangle</a> example as well:<br>let sierpinski () =

  let lSystem =
    { Axiom = [ Sym('A') ]
      Rules =
        [ Sym('A'), [ Sym('B'); Sym('&gt;'); Sym('A'); Sym('&gt;'); Sym('B') ]
          Sym('B'), [ Sym('A'); Sym('&lt;'); Sym('B'); Sym('&lt;'); Sym('A') ]]
        |&gt; Map.ofList }

  let l = 1.
  let T =
    [ Sym('A'), [ Fwd l; ]
      Sym('B'), [ Fwd l; ]
      Sym('&gt;'), [ Lft 60.; ]
      Sym('&lt;'), [ Rgt 60.; ] ]
    |&gt; Map.ofList

  lSystem
  |&gt; generation 9
  |&gt; toTurtle T
  |&gt; toSvg
  |&gt; save
<br>… which results in the following picture:<br><img alt="Sierpinski triangle" src="https://mathias-brandewinder.github.io//assets/sierpinski-triangle.png" referrerpolicy="no-referrer"><br>That’s it for tonight! I had a lot of fun coding this (it certainly made the flight less boring), and found the idea of converting code to turtle instructions, with a stack, pretty interesting. Hope you enjoyed it, and if you end up playing with this, share your creations on Twitter and ping me!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fun-with-l-systems.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Fun with L-Systems.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:45:05 GMT</pubDate><enclosure url="https://mathias-brandewinder.github.io//assets/pythagoras-tree.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://mathias-brandewinder.github.io//assets/pythagoras-tree.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Functional Reactive Programming]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <br>Events are everywhere. Almost every program has to handle events, whether it be button clicks in the user interface, listening to sockets in a server, or even a system shutdown notification.<br>And events are the basis of one of the most common OO design patterns: the “Observer” pattern.<br>But as we know, event handling, like concurrency in general, can be tricky to implement. Simple event logic is straightforward, but what about logic like “do something if two events happen in a row but do something different if only one event happens” or “do something if two events happen at roughly the same time”. And how easy is it to combine these requirements in other, more complex ways?<br>Even if you can successfully implement these requirements, the code tends to be spaghetti like and hard to understand, even with the best intentions.<br>Is there an approach that can make event handling easier?<br>We saw in the previous post on message queues that one of the advantages of that approach was that the requests were “serialized” making it conceptually easier to deal with.<br>There is a similar approach that can be used with events. The idea is to turn a series of events into an “event stream”. Event streams then become quite like IEnumerables, and so the obvious next step is to treat them in much the the same way that LINQ handles collections, so that they can be filtered, mapped, split and combined.<br>F# has built in support for this model, as well as for the more traditional approach.<br><br>Let’s start with a simple example to compare the two approaches. We’ll implement the classic event handler approach first.<br>First, we define a utility function that will:<br>
<br>create a timer
<br>register a handler for the Elapsed event
<br>run the timer for five seconds and then stop it
<br>Here’s the code:<br>open System
open System.Threading

/// create a timer and register an event handler,
/// then run the timer for five seconds
let createTimer timerInterval eventHandler =
    // setup a timer
    let timer = new System.Timers.Timer(float timerInterval)
    timer.AutoReset &lt;- true

    // add an event handler
    timer.Elapsed.Add eventHandler

    // return an async task
    async {
        // start timer...
        timer.Start()
        // ...run for five seconds...
        do! Async.Sleep 5000
        // ... and stop
        timer.Stop()
        }
<br>Now test it interactively:<br>// create a handler. The event args are ignored
let basicHandler _ = printfn "tick %A" DateTime.Now

// register the handler
let basicTimer1 = createTimer 1000 basicHandler

// run the task now
Async.RunSynchronously basicTimer1
<br>Now let’s create a similar utility method to create a timer, but this time it will return an “observable” as well, which is the stream of events.<br>let createTimerAndObservable timerInterval =
    // setup a timer
    let timer = new System.Timers.Timer(float timerInterval)
    timer.AutoReset &lt;- true

    // events are automatically IObservable
    let observable = timer.Elapsed

    // return an async task
    let task = async {
        timer.Start()
        do! Async.Sleep 5000
        timer.Stop()
        }

    // return a async task and the observable
    (task,observable)
<br>And again test it interactively:<br>// create the timer and the corresponding observable
let basicTimer2 , timerEventStream = createTimerAndObservable 1000

// register that every time something happens on the
// event stream, print the time.
timerEventStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "tick %A" DateTime.Now)

// run the task now
Async.RunSynchronously basicTimer2
<br>The difference is that instead of registering a handler directly with an event, we are “subscribing” to an event stream. Subtly different, and important.<br><br>In this next example, we’ll have a slightly more complex requirement:<br>Create a timer that ticks every 500ms.
At each tick, print the number of ticks so far and the current time.
<br>To do this in a classic imperative way, we would probably create a class with a mutable counter, as below:<br>type ImperativeTimerCount() =

    let mutable count = 0

    // the event handler. The event args are ignored
    member this.handleEvent _ =
      count &lt;- count + 1
      printfn "timer ticked with count %i" count
<br>We can reuse the utility functions we created earlier to test it:<br>// create a handler class
let handler = new ImperativeTimerCount()

// register the handler method
let timerCount1 = createTimer 500 handler.handleEvent

// run the task now
Async.RunSynchronously timerCount1
<br>Let’s see how we would do this same thing in a functional way:<br>// create the timer and the corresponding observable
let timerCount2, timerEventStream = createTimerAndObservable 500

// set up the transformations on the event stream
timerEventStream
|&gt; Observable.scan (fun count _ -&gt; count + 1) 0
|&gt; Observable.subscribe (fun count -&gt; printfn "timer ticked with count %i" count)

// run the task now
Async.RunSynchronously timerCount2
<br>Here we see how you can build up layers of event transformations, just as you do with list transformations in LINQ.<br>The first transformation is scan, which accumulates state for each event. It is roughly equivalent to the List.fold function that we have seen used with lists. In this case, the accumulated state is just a counter.<br>And then, for each event, the count is printed out.<br>Note that in this functional approach, we didn’t have any mutable state, and we didn’t need to create any special classes.<br><br>For a final example, we’ll look at merging multiple event streams.<br>Let’s make a requirement based on the well-known “FizzBuzz” problem:<br>Create two timers, called '3' and '5'. The '3' timer ticks every 300ms and the '5' timer ticks
every 500ms.

Handle the events as follows:
a) for all events, print the id of the time and the time
b) when a tick is simultaneous with a previous tick, print 'FizzBuzz'
otherwise:
c) when the '3' timer ticks on its own, print 'Fizz'
d) when the '5' timer ticks on its own, print 'Buzz'
<br>First let’s create some code that both implementations can use.<br>We’ll want a generic event type that captures the timer id and the time of the tick.<br>type FizzBuzzEvent = {label:int; time: DateTime}
<br>And then we need a utility function to see if two events are simultaneous. We’ll be generous and allow a time difference of up to 50ms.<br>let areSimultaneous (earlierEvent,laterEvent) =
    let {label=_;time=t1} = earlierEvent
    let {label=_;time=t2} = laterEvent
    t2.Subtract(t1).Milliseconds &lt; 50
<br>In the imperative design, we’ll need to keep track of the previous event, so we can compare them. And we’ll need special case code for the first time, when the previous event doesn’t exist<br>type ImperativeFizzBuzzHandler() =

    let mutable previousEvent: FizzBuzzEvent option = None

    let printEvent thisEvent  =
      let {label=id; time=t} = thisEvent
      printf "[%i] %i.%03i " id t.Second t.Millisecond
      let simultaneous = previousEvent.IsSome &amp;&amp; areSimultaneous (previousEvent.Value,thisEvent)
      if simultaneous then printfn "FizzBuzz"
      elif id = 3 then printfn "Fizz"
      elif id = 5 then printfn "Buzz"

    member this.handleEvent3 eventArgs =
      let event = {label=3; time=DateTime.Now}
      printEvent event
      previousEvent &lt;- Some event

    member this.handleEvent5 eventArgs =
      let event = {label=5; time=DateTime.Now}
      printEvent event
      previousEvent &lt;- Some event
<br>Now the code is beginning to get ugly fast! Already we have mutable state, complex conditional logic, and special cases, just for such a simple requirement.<br>Let’s test it:<br>// create the class
let handler = new ImperativeFizzBuzzHandler()

// create the two timers and register the two handlers
let timer3 = createTimer 300 handler.handleEvent3
let timer5 = createTimer 500 handler.handleEvent5

// run the two timers at the same time
[timer3;timer5]
|&gt; Async.Parallel
|&gt; Async.RunSynchronously
<br>It does work, but are you sure the code is not buggy? Are you likely to accidentally break something if you change it?<br>The problem with this imperative code is that it has a lot of noise that obscures the the requirements.<br>Can the functional version do better? Let’s see!<br>First, we create two event streams, one for each timer:<br>let timer3, timerEventStream3 = createTimerAndObservable 300
let timer5, timerEventStream5 = createTimerAndObservable 500
<br>Next, we convert each event on the “raw” event streams into our FizzBuzz event type:<br>// convert the time events into FizzBuzz events with the appropriate id
let eventStream3  =
   timerEventStream3
   |&gt; Observable.map (fun _ -&gt; {label=3; time=DateTime.Now})

let eventStream5  =
   timerEventStream5
   |&gt; Observable.map (fun _ -&gt; {label=5; time=DateTime.Now})
<br>Now, to see if two events are simultaneous, we need to compare them from the two different streams somehow.<br>It’s actually easier than it sounds, because we can:<br>
<br>combine the two streams into a single stream:
<br>then create pairs of sequential events
<br>then test the pairs to see if they are simultaneous
<br>then split the input stream into two new output streams based on that test
<br>Here’s the actual code to do this:<br>// combine the two streams
let combinedStream =
    Observable.merge eventStream3 eventStream5

// make pairs of events
let pairwiseStream =
   combinedStream |&gt; Observable.pairwise

// split the stream based on whether the pairs are simultaneous
let simultaneousStream, nonSimultaneousStream =
    pairwiseStream |&gt; Observable.partition areSimultaneous
<br>Finally, we can split the nonSimultaneousStream again, based on the event id:<br>// split the non-simultaneous stream based on the id
let fizzStream, buzzStream  =
    nonSimultaneousStream
    // convert pair of events to the first event
    |&gt; Observable.map (fun (ev1,_) -&gt; ev1)
    // split on whether the event id is three
    |&gt; Observable.partition (fun {label=id} -&gt; id=3)
<br>Let’s review so far. We have started with the two original event streams and from them created four new ones:<br>
<br>combinedStream contains all the events
<br>simultaneousStream contains only the simultaneous events
<br>fizzStream contains only the non-simultaneous events with id=3
<br>buzzStream contains only the non-simultaneous events with id=5
<br>Now all we need to do is attach behavior to each stream:<br>//print events from the combinedStream
combinedStream
|&gt; Observable.subscribe (fun {label=id;time=t} -&gt;
                              printf "[%i] %i.%03i " id t.Second t.Millisecond)

//print events from the simultaneous stream
simultaneousStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "FizzBuzz")

//print events from the nonSimultaneous streams
fizzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Fizz")

buzzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Buzz")
<br>Let’s test it:<br>// run the two timers at the same time
[timer3;timer5]
|&gt; Async.Parallel
|&gt; Async.RunSynchronously
<br>Here’s all the code in one complete set:<br>// create the event streams and raw observables
let timer3, timerEventStream3 = createTimerAndObservable 300
let timer5, timerEventStream5 = createTimerAndObservable 500

// convert the time events into FizzBuzz events with the appropriate id
let eventStream3  = timerEventStream3
                    |&gt; Observable.map (fun _ -&gt; {label=3; time=DateTime.Now})
let eventStream5  = timerEventStream5
                    |&gt; Observable.map (fun _ -&gt; {label=5; time=DateTime.Now})

// combine the two streams
let combinedStream =
   Observable.merge eventStream3 eventStream5

// make pairs of events
let pairwiseStream =
   combinedStream |&gt; Observable.pairwise

// split the stream based on whether the pairs are simultaneous
let simultaneousStream, nonSimultaneousStream =
   pairwiseStream |&gt; Observable.partition areSimultaneous

// split the non-simultaneous stream based on the id
let fizzStream, buzzStream  =
    nonSimultaneousStream
    // convert pair of events to the first event
    |&gt; Observable.map (fun (ev1,_) -&gt; ev1)
    // split on whether the event id is three
    |&gt; Observable.partition (fun {label=id} -&gt; id=3)

//print events from the combinedStream
combinedStream
|&gt; Observable.subscribe (fun {label=id;time=t} -&gt;
                              printf "[%i] %i.%03i " id t.Second t.Millisecond)

//print events from the simultaneous stream
simultaneousStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "FizzBuzz")

//print events from the nonSimultaneous streams
fizzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Fizz")

buzzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Buzz")

// run the two timers at the same time
[timer3;timer5]
|&gt; Async.Parallel
|&gt; Async.RunSynchronously
<br>The code might seem a bit long winded, but this kind of incremental, step-wise approach is very clear and self-documenting.<br>Some of the benefits of this style are:<br>
<br>I can see that it meets the requirements just by looking at it, without even running it. Not so with the imperative version.
<br>From a design point of view, each final “output” stream follows the single responsibility principle – it only does one thing – so it is very easy to associate behavior with it.
<br>This code has no conditionals, no mutable state, no edge cases. It would be easy to maintain or change, I hope.
<br>It is easy to debug. For example, I could easily “tap” the output of the simultaneousStream to see if it contains what I think it contains:
<br>// debugging code
//simultaneousStream |&gt; Observable.subscribe (fun e -&gt; printfn "sim %A" e)
//nonSimultaneousStream |&gt; Observable.subscribe (fun e -&gt; printfn "non-sim %A" e)
<br>This would be much harder in the imperative version.<br><br>Functional Reactive Programming (known as FRP) is a big topic, and we’ve only just touched on it here. I hope this introduction has given you a glimpse of the usefulness of this way of doing things.<br>If you want to learn more, see the documentation for the F# <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/ee370313%28v=vs.100%29?redirectedfrom=MSDN" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/ee370313%28v=vs.100%29?redirectedfrom=MSDN" target="_blank">Observable module</a>, which has the basic transformations used above. And there is also the <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/previous-versions/dotnet/reactive-extensions/hh242985%28v=vs.103%29" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/previous-versions/dotnet/reactive-extensions/hh242985%28v=vs.103%29" target="_blank">Reactive Extensions (Rx)</a> library which shipped as part of .NET 4. That contains many other additional transformations.<br>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/functional-reactive-programming.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Functional Reactive Programming.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 02:59:23 GMT</pubDate></item><item><title><![CDATA[Implementing Coroutines (async await) using continuations]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:coroutines" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#coroutines</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:coroutines" class="tag" target="_blank" rel="noopener nofollow">#coroutines</a><br><a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Coroutine" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Coroutine" target="_blank">Coroutines</a> or async/await is not a new concept as it was named in 1958 by Melvin Conway. Coroutines had support in languages such as Simula(1962), Smalltalk(1972) and Modula-2(1978) but I think one can argue that coroutines came into mainstream with C# 5(2012) when Microsoft added async/await to C#.<br><br>A thread can only execute a single subroutine at a time as a subroutine starts and then runs to completion. In a server environment subroutines can be problematic as in order to service connections concurrently we would usually would start a thread per connection, this was the idiom a few years ago. A thread depending on the operating system and settings needs about 1 MiB of user stack space meaning 1,024 threads needs 1 GiB of for just user stack space. In addition there's the cost of kernel stack space, book-keeping and scheduling of threads. This drives cost of VM/hardware. This is sometimes known as the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/C10k_problem" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/C10k_problem" target="_blank">C10K problem</a>.<br><br>A coroutine is like a subroutine except it can also pause/yield the execution when the coroutine begins an asynchronous IO operation to let the executing thread do something else while we are waiting for the IO operation to complete. Once the IO operation is completed the coroutine gets picked up by an idle thread and continues to execute. This enables us to service say 1 million connections by having 1 million coroutines executed by 100 threads.<br>Coroutines are cheaper than threads because the memory footprint requirement is smaller and doesn't require lots of continuous RAM (a rather steep requirement). In addition, the scheduling is simpler because it is cooperative rather than preemptive.<br><br>What the C# community has learnt over the years is that the niceness of coroutines has a price.<br>For one thing the call stack is gone which has huge implications for debugging. Microsoft has improved the tooling over the years to give us back the illusion of call stack.<br>Another common issue is that we sometimes try to fit the round shape of the coroutine peg into the square hole of a subroutine like so:<br>var task = MyFunctionIsAsync(); // Returns a coroutine as a Task
var result = task.Result; // Sometimes result in a dead-lock?!?!
<br>A coroutine might yield execution and to later resume it but by calling Task.Result this thread is now blocked waiting for the coroutine to complete. Depending on the SynchronizationContext it can mean that the coroutine can only execute on the thread that started it but this thread is blocked waiting for the coroutine to finish =&gt; dead-lock.<br>IMHO; it was a mistake to add Task.Result because it made it easy to use coroutines wrongly but what is done is done. It is quite simple; don't call Task.Result or Task.Wait.<br><br>C# has specific compiler support to support coroutines and behind the scenes generates state machines when it can and uses continuation when it can't.<br>I want to demonstrate how to implement coroutines in F# using continuations. This is how F# Async works internally but I will skip exception support and accept that there is potential for stack overflow for long coroutine chains. There are solutions for this which are used by F# Async but I want to keep the code clean to not hide the simple underlying idea.<br>A simple coroutine definition in F# could look like this:<br>type [&lt;Struct&gt;] Coroutine&lt;'T&gt; = Co of (('T -&gt; unit) -&gt; unit)
<br>We start the Coroutine&lt;_&gt; by giving it a continuation/callback function: r: 'T -&gt; unit that the Coroutine&lt;_&gt; invokes when the result 'T is available.<br><br>When the result is available the Coroutine&lt;_&gt; invokes the continuation r immediately like in the result coroutine.<br>let result v =
  Co &lt;| fun r -&gt;
    r v
<br>Sometimes we have to wait on a Task&lt;_&gt; to complete before we can execute the continuation r:<br>// Creates a Coroutine&lt;_&gt; from a Task&lt;_&gt;
let ofTask (t : Task&lt;_&gt;) =
  Co &lt;| fun r -&gt;
    let cw (t : Task&lt;_&gt;) = r t.Result
    t.ContinueWith (Action&lt;Task&lt;'T&gt;&gt; cw) |&gt; ignore

// Creates a Coroutine&lt;unit&gt; from a Task
let ofUnitTask (t : Task) =
  Co &lt;| fun r -&gt;
    let cw (_ : Task) = r ()
    t.ContinueWith (Action&lt;Task&gt; cw) |&gt; ignore
<br>There's an important difference between Task&lt;_&gt; and Coroutine&lt;_&gt;. The task begins executing immediately on creation but Coroutine&lt;_&gt; begins executing once it receives the continuation r.<br><br>In order to complete the Coroutine&lt;_&gt; Monad we define bind and combine:<br>let bind (Co c) f =
  Co &lt;|
    fun r -&gt;
      let cr v =
        let (Co d) = f v
        d r
      c cr

let combine (Co c) (Co d) =
  Co &lt;|
    fun r -&gt;
      let cr _ =
        d r
      c cr
<br><br>In order to support an experience that looks like async/await in C# we can now define a <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions" target="_blank">computation expression builder</a>:<br>type CoroutineBuilder () =
  class
    // To support let!
    member x.Bind       (c, f)  : Coroutine&lt;_&gt; = bind     c               f
    member x.Bind       (t, f)  : Coroutine&lt;_&gt; = bind     (ofTask t)      f
    member x.Bind       (t, f)  : Coroutine&lt;_&gt; = bind     (ofUnitTask t)  f
    // To support do!
    member x.Combine    (c, d)  : Coroutine&lt;_&gt; = combine  c               d
    member x.Combine    (t, d)  : Coroutine&lt;_&gt; = combine  (ofUnitTask t)  d
    // To support return
    member x.Return     v       : Coroutine&lt;_&gt; = result   v
    // To support return!
    member x.ReturnFrom c       : Coroutine&lt;_&gt; = c
    member x.ReturnFrom t       : Coroutine&lt;_&gt; = ofTask   t
    member x.Zero       ()      : Coroutine&lt;_&gt; = result   ()
  end
let coroutine = CoroutineBuilder ()
<br><br>We also need a way to invoke a Coroutine&lt;_&gt;<br>let invoke (Co c) r =
  let wc _ = c r
  ThreadPool.QueueUserWorkItem (WaitCallback wc) |&gt; ignore
<br>The Coroutine&lt;_&gt; will execute in the thread pool to begin with and when done it will call the continuation r with the result.<br>By design we don't expose a .Result property, the only way to receive the result of Coroutine&lt;_&gt; is through the continuation r. This design is to ensure there's no surprise dead-locks from trying to force a coroutine into a subroutine. Note; the continuation r can be executed by any thread.<br><br>All of this enables us to write a simple Coroutine&lt;_&gt; like this:<br>let downloadFromUri uri =
  coroutine {
    let wc    = new WebClient ()
    let! txt  = wc.DownloadStringTaskAsync (Uri uri)
    // Coroutine&lt;_&gt; doesn't support use!
    wc.Dispose ()
    return txt
  }

let exampleDownloadFromGoogle =
  coroutine {
    let! txt = downloadFromUri "https://www.google.com/"
    return txt.Length
  }

[&lt;EntryPoint&gt;]
let main argv =
  invoke exampleDownloadFromGoogle &lt;| printfn "Result: %A"

  printfn "Press any key to exit"
  Console.ReadKey () |&gt; ignore

  0
<br>Prints the size of the google home that at the time of writing was:<br>Press any key to exit
Result: 45782
<br>Note; Press any key to exit is printed before the result, this is because invoke enqueues a Coroutine&lt;_&gt; to be executed by the thread pool and immediately returns. After google responds the Coroutine&lt;_&gt; continues executing on the thread pool and ultimately prints the size of the google homepage.<br><br>We can download from both google &amp; bing:<br>let exampleDownloadFromGoogleAndBing =
  coroutine {
    let! google = downloadFromUri "https://www.google.com/"
    let! bing   = downloadFromUri "https://www.bing.com/"
    return google.Length, bing.Length
  }
<br>Prints:<br>Press any key to exit
Result: (45748, 90632)
<br>This is good because it doesn't block the executing thread while downloading the homepage, this is bad because we download the homepages sequentially. It would be better to download both at the same time.<br><br>Using runInParallel this is easy to do.<br>let exampleDownloadFromGoogleAndBingInParallel =
  coroutine {
    // Start the download coroutines in parallel
    let! cgoogle = runInParallel &lt;| downloadFromUri "https://www.google.com/"
    let! cbing   = runInParallel &lt;| downloadFromUri "https://www.bing.com/"

    // Wait for the coroutines to complete
    let! google  = cgoogle
    let! bing    = cbing

    return google.Length, bing.Length
  }
<br>This downloads the homepages in parallel. How does it work?<br>runInParallel has the signature: Coroutine&lt;'T&gt; -&gt; Coroutine&lt;Coroutine&lt;'T&gt;&gt;. The reason for this is that a Coroutine&lt;_&gt; is started when it is passed the continuation r but in this case we like the continuation to start additional Coroutine&lt;_&gt; before the first Coroutine&lt;_&gt; is done. So runInParallel starts the Coroutine&lt;_&gt; it is passed and then immediately returns a new Coroutine&lt;_&gt; that we can wait on for the result. This enables the continuation to start multiple Coroutine&lt;_&gt; and then wait for the results.<br>This is how Async.StartChild works as well.<br>The implementation of runInParallel is a bit more intricate than the previous functions but the basic idea is this:<br>
<br>Start the Coroutine&lt;_&gt; passed to with a special continuation. This continuation checks a shared state if there's a receiver for the result? If yes, invoke the receiver with the result. Otherwise, store the result.
<br>Create a new Coroutine&lt;_&gt; that once it receives a receiver checks the shared state to see if there's a result available. If yes, invoke the receiver with the value. Otherwise, store the receiver.
<br>We have to account for multiple executing threads which complicates the code even further.<br>let runInParallel (Co c) =
  Co &lt;| fun r -&gt;
    let mutable state = Initial

    // Some of the cases in the matches are error cases but as mentioned
    //  for this blog I avoid handling error cases.

    let cr v =
      let update s =
        match s with
        | Initial       -&gt; ValueNone    , HasValue v
        | HasReceiver r -&gt; ValueSome r  , Done
        | HasValue    _ -&gt; ValueNone    , HasValue v
        | Done          -&gt; ValueNone    , Done
      match cas &amp;state update with
      | ValueSome r -&gt; r v
      | ValueNone   -&gt; ()

    // Starts the coroutine
    c &lt;| cr

    let cco cr =
      let update s =
        match s with
          | Initial       -&gt; ValueNone   , HasReceiver cr
          | HasReceiver _ -&gt; ValueNone   , HasReceiver cr
          | HasValue    v -&gt; ValueSome v , Done
          | Done          -&gt; ValueNone   , Done
      match cas &amp;state update with
      | ValueSome v -&gt; cr v
      | ValueNone   -&gt; ()

    // Passes a new coroutine to the receiver immediately
    r &lt;| Co cco
<br><br>We can make the pattern above a bit more general like so:<br>let exampleDownloadManyPagesInParallel =
  let download uri =
    coroutine {
      let! text = downloadFromUri uri
      return text.Length
    } |&gt; debugf "Download: %s" (string uri)
  coroutine {
    let uris        = Array.init 10 (fun i -&gt; sprintf "https://gist.github.com/mrange?page=%d" (i + 1))
    let! allLengths = uris |&gt; Array.map download |&gt; runAllInParallel 3
    return allLengths
  } |&gt; time
<br>This prints:<br>Press any key to exit
DEBUG - Download: https://gist.github.com/mrange?page=1 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=2 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=3 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=1 - RESULT: 93883
DEBUG - Download: https://gist.github.com/mrange?page=2 - RESULT: 94160
DEBUG - Download: https://gist.github.com/mrange?page=3 - RESULT: 94602
DEBUG - Download: https://gist.github.com/mrange?page=4 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=5 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=6 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=4 - RESULT: 97397
DEBUG - Download: https://gist.github.com/mrange?page=5 - RESULT: 93125
DEBUG - Download: https://gist.github.com/mrange?page=6 - RESULT: 89598
DEBUG - Download: https://gist.github.com/mrange?page=7 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=8 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=9 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=7 - RESULT: 95235
DEBUG - Download: https://gist.github.com/mrange?page=8 - RESULT: 92106
DEBUG - Download: https://gist.github.com/mrange?page=9 - RESULT: 94957
DEBUG - Download: https://gist.github.com/mrange?page=10 - INVOKE
DEBUG - Download: https://gist.github.com/mrange?page=10 - RESULT: 91992
Result: (2342L, [|93883; 94160; 94602; 97397; 93125; 89598; 95235; 92106; 94957; 91992|])
<br>Thanks to debugf we see that it seems tasks are invoked in batches of 3 as intended.<br>debugf is used to debug Coroutine&lt;_&gt;. As mentioned in the prelude a Coroutine&lt;_&gt; implies no call stack (the elimination of the call stack is kind of the point), this makes debugging harder. debugf helps by printing to the console when a named Coroutine&lt;_&gt; is started and completed:<br>let debug nm (Co c) =
  Co &lt;| fun r -&gt;
    printfn "DEBUG - %s - INVOKE" nm
    let cr v =
      printfn "DEBUG - %s - RESULT: %A" nm v
      r v
    c cr

let debugf fmt = kprintf debug fmt
<br>runAllInParallel has the signature: int -&gt; Coroutine&lt;'T&gt; array -&gt; Coroutine&lt;'T array&gt;. That is it runs the coroutines in the input array in parallel. The integer decides how many it runs in parallel at the same time.<br>runAllInParallel works by creating batches of Coroutine&lt;_&gt;. All Coroutine&lt;_&gt; in a batch is executed in parallel, when all Coroutine&lt;_&gt; in the batch has run to completion the next batch is started.<br>A better approach would be to keep always keep the maximum number of Coroutine&lt;_&gt; running at all times but batching was easier to implement for the purpose of this blog:<br>let runAllInParallel parallelism (cos : _ array) =
  let chunks = cos |&gt; Array.indexed |&gt; Array.chunkBySize parallelism
  Co &lt;| fun r -&gt;
    let result = Array.zeroCreate cos.Length
    let rec loop i =
      if i &lt; chunks.Length then
        let chunk = chunks.[i]
        let children = chunk |&gt; Array.map (fun (j, co) -&gt; runInParallel co |&gt; join |&gt; map (fun v -&gt; j, v))
        let mutable remaining = children.Length
        let cr (j, v) =
          result.[j] &lt;- v
          if Interlocked.Decrement &amp;remaining = 0 then loop (i + 1)
        for (Co c) in children do
          c cr
      else
        r result
    loop 0
<br><br>Coroutine&lt;_&gt; is intended for IO bound tasks where the CPU is sitting most of the time waiting on IO. It is possible using Coroutine&lt;_&gt; for CPU bound tasks but you need to explicitly switch to a worker thread. For example; let's say you want to compute the mandelbrot set:<br>let exampleCPUBoundProblem =

  let d = 2048
  let lines (pixels : byte array) f t =
    // Mandelbrot specific code deleted, it's in the full source code below
    ()

  let colines (pixels : byte array) f t =
    coroutine {
      // Switches to a thread pool thread that will do the work
      do! switchToThreadPool
      lines pixels f t
      return ()
    }
  coroutine {
    let pixels = Array.zeroCreate (r*d)

    let  s  = d / 4
    // Run 4 Coroutine&lt;_&gt; in parallel
    let! s0 = runInParallel &lt;| colines pixels (0*s) (1*s)
    let! s1 = runInParallel &lt;| colines pixels (1*s) (2*s)
    let! s2 = runInParallel &lt;| colines pixels (2*s) (3*s)
    let! s3 = runInParallel &lt;| colines pixels (3*s) (4*s)

    do! s0
    do! s1
    do! s2
    do! s3

    let img     = File.Create "mandelbrot.pbm"
    let sw      = new StreamWriter (img)

    do! sw.WriteAsync  (sprintf "P4\n%d %d\n" d d)
    do! sw.FlushAsync  ()
    do! img.WriteAsync (pixels, 0, pixels.Length)

    // Coroutine&lt;_&gt; doesn't support use!
    sw.Dispose ()
    img.Dispose ()

    return ()
  } |&gt; time
<br>The key here is do! switchToThreadPool, without it the colines would execute on the calling thread until completion and then execute the next colines essentially a sequential operation. With do! switchToThreadPool the colines transfer execution to the thread pool and the continuation can start the other colines Coroutine&lt;_&gt; each running in the thread pool.<br>let switchToThreadPool =
  Co &lt;| fun r -&gt;
    // This makes the continuation execute on the thread pool
    //  the calling thread can continue executing other code
    let wc _ = r ()
    ThreadPool.QueueUserWorkItem (WaitCallback wc) |&gt; ignore
<br>With do! switchToThreadPool it prints:<br>Result: (748L, ())
<br>This means the execution took 748 ms.<br>Without do! switchToThreadPool it prints:<br>Result: (1648L, ())
<br>This means the execution took 1648 ms.<br>The speed up we gain from utilizing multiple cores. The reason for the rather poor speed up is that we just split the problem into 4 chunks and starts executing but the way the mandelbrot works the chunks are not even in CPU demand so colines will complete unevenly giving poor usage of the CPU resources. Like mentioned; coroutines are intended for IO bound tasks. For CPU bound tasks we should look for other abstractions such as Parallel.ForEach which utilizes work stealing algorithms to mitigate the issue of uneven CPU demand.<br><br>There you have it, a functioning Coroutine&lt;_&gt; library in F# with the massive caveat that if a continuation throws an exception it all breaks down hard. This is fixable and Async in F# supports exceptions by having special continuation functions for exceptions and cancellation of coroutines.<br>In addition; Async in F# also avoids stack overflows by implementing a common functional pattern called trampolines.<br>As I wanted to keep down the complexity in order to not hide the idea of coroutines using continuations in a forest of error-handling code I just ignored all of it.<br>To summarize, a coroutine based on continuations can look like this:<br>type [&lt;Struct&gt;] Coroutine&lt;'T&gt; = Co of (('T -&gt; unit) -&gt; unit)
<br>The coroutine is started when it is passed a continuation function, when the result is available the continuation function is invoked.<br><br>module MinimalisticCoroutine =

  type [&lt;Struct&gt;] Coroutine&lt;'T&gt; = Co of (('T -&gt; unit) -&gt; unit)

  module Coroutine =
    open FSharp.Core.Printf

    open System
    open System.Diagnostics
    open System.Threading
    open System.Threading.Tasks

    module Details =
      let inline refEq&lt;'T when 'T : not struct&gt; (a : 'T) (b : 'T) = Object.ReferenceEquals (a, b)

      module Loops =
        let rec cas (rs : byref&lt;'T&gt;) (cs : 'T) u : 'U =
          let v, ns   = u cs
          let acs     = Interlocked.CompareExchange(&amp;rs, ns, cs)
          if refEq acs cs then v
          else cas &amp;rs acs u

      let sw =
        let sw = Stopwatch ()
        sw.Start ()
        sw

      let cas (rs : byref&lt;_&gt;) u =
        let cs = rs
        Interlocked.MemoryBarrier ()
        Loops.cas &amp;rs cs u

      type ChildState&lt;'T&gt; =
        | Initial
        | HasReceiver of ('T -&gt; unit)
        | HasValue    of 'T
        | Done

    open Details

    let result v =
      Co &lt;| fun r -&gt;
        r v

    let bind (Co c) f =
      Co &lt;|
        fun r -&gt;
          let cr v =
            let (Co d) = f v
            d r
          c cr

    let combine (Co c) (Co d) =
      Co &lt;|
        fun r -&gt;
          let cr _ =
            d r
          c cr

    let apply (Co c) (Co d) =
      Co &lt;|
        fun r -&gt;
          let cr f =
            let dr v = r (f v)
            d dr
          c cr

    let map m (Co c) =
      Co &lt;| fun r -&gt;
        let cr v = r (m v)
        c cr

    let unfold uf z =
      Co &lt;| fun r -&gt;
        let ra = ResizeArray 16
        let rec cr p =
          match p with
          | None        -&gt; r (ra.ToArray ())
          | Some (s, v) -&gt;
            ra.Add v
            let (Co c) = uf s
            c cr
        let (Co c) = uf z
        c cr

    let debug nm (Co c) =
      Co &lt;| fun r -&gt;
        printfn "DEBUG - %s - INVOKE" nm
        let cr v =
          printfn "DEBUG - %s - RESULT: %A" nm v
          r v
        c cr

    let debugf fmt = kprintf debug fmt

    let time (Co c) =
      Co &lt;| fun r -&gt;
        let before = sw.ElapsedMilliseconds
        let cr v =
          let after = sw.ElapsedMilliseconds
          r (after - before, v)
        c cr

    let switchToNewThread =
      Co &lt;| fun r -&gt;
        let ts () =  r ()
        let t = Thread (ThreadStart ts)
        t.IsBackground  &lt;- true
        t.Name          &lt;- "Coroutine thread"
        t.Start ()

    let switchToThreadPool =
      Co &lt;| fun r -&gt;
        let wc _ = r ()
        ThreadPool.QueueUserWorkItem (WaitCallback wc) |&gt; ignore

    let join (Co c) =
      Co &lt;| fun r -&gt;
        let cr (Co d) = d r
        c cr

    let runInParallel (Co c) =
      Co &lt;| fun r -&gt;
        let mutable state = Initial

        let cr v =
          let update s =
            match s with
            | Initial       -&gt; ValueNone    , HasValue v
            | HasReceiver r -&gt; ValueSome r  , Done
            | HasValue    _ -&gt; ValueNone    , HasValue v
            | Done          -&gt; ValueNone    , Done
          match cas &amp;state update with
          | ValueSome r -&gt; r v
          | ValueNone   -&gt; ()

        c &lt;| cr

        let cco cr =
          let update s =
            match s with
              | Initial       -&gt; ValueNone   , HasReceiver cr
              | HasReceiver _ -&gt; ValueNone   , HasReceiver cr
              | HasValue    v -&gt; ValueSome v , Done
              | Done          -&gt; ValueNone   , Done
          match cas &amp;state update with
          | ValueSome v -&gt; cr v
          | ValueNone   -&gt; ()

        r &lt;| Co cco

    let runAllInParallel parallelism (cos : _ array) =
      let chunks = cos |&gt; Array.indexed |&gt; Array.chunkBySize parallelism
      Co &lt;| fun r -&gt;
        let result = Array.zeroCreate cos.Length
        let rec loop i =
          if i &lt; chunks.Length then
            let chunk = chunks.[i]
            let children = chunk |&gt; Array.map (fun (j, co) -&gt; runInParallel co |&gt; join |&gt; map (fun v -&gt; j, v))
            let mutable remaining = children.Length
            let cr (j, v) =
              result.[j] &lt;- v
              if Interlocked.Decrement &amp;remaining = 0 then loop (i + 1)
            for (Co c) in children do
              c cr
          else
            r result
        loop 0

    let ofTask (t : Task&lt;_&gt;) =
      Co &lt;| fun r -&gt;
        let cw (t : Task&lt;_&gt;) = r t.Result
        t.ContinueWith (Action&lt;Task&lt;'T&gt;&gt; cw) |&gt; ignore

    let ofUnitTask (t : Task) =
      Co &lt;| fun r -&gt;
        let cw (_ : Task) = r ()
        t.ContinueWith (Action&lt;Task&gt; cw) |&gt; ignore

    let invoke (Co c) r =
      let wc _ = c r
      ThreadPool.QueueUserWorkItem (WaitCallback wc) |&gt; ignore

    type Builder () =
      class
        member x.Bind       (c, f)  : Coroutine&lt;_&gt; = bind     c               f
        member x.Bind       (t, f)  : Coroutine&lt;_&gt; = bind     (ofTask t)      f
        member x.Bind       (t, f)  : Coroutine&lt;_&gt; = bind     (ofUnitTask t)  f
        member x.Combine    (c, d)  : Coroutine&lt;_&gt; = combine  c               d
        member x.Combine    (t, d)  : Coroutine&lt;_&gt; = combine  (ofUnitTask t)  d
        member x.Return     v       : Coroutine&lt;_&gt; = result   v
        member x.ReturnFrom c       : Coroutine&lt;_&gt; = c
        member x.ReturnFrom t       : Coroutine&lt;_&gt; = ofTask   t
        member x.Zero       ()      : Coroutine&lt;_&gt; = result   ()
      end
  let coroutine = Coroutine.Builder ()

  type Coroutine&lt;'T&gt; with
    static member (&gt;&gt;=) (c, f) = Coroutine.bind     f c
    static member (&gt;&gt;.) (c, d) = Coroutine.combine  c d
    static member (&lt;*&gt;) (c, d) = Coroutine.apply    c d
    static member (|&gt;&gt;) (c, m) = Coroutine.map      m c

open MinimalisticCoroutine

open System
open System.IO
open System.Net
open System.Threading.Tasks

open Coroutine

let downloadFromUri uri =
  coroutine {
    let wc    = new WebClient ()
    let! txt  = wc.DownloadStringTaskAsync (Uri uri)
    wc.Dispose ()
    return txt
  }

let exampleDownloadFromGoogle =
  coroutine {
    let! txt = downloadFromUri "https://www.google.com/"
    return txt.Length
  }

let exampleDownloadFromGoogleAndBing =
  coroutine {
    let! google = downloadFromUri "https://www.google.com/"
    let! bing   = downloadFromUri "https://www.bing.com/"
    return google.Length, bing.Length
  }

let exampleDownloadFromGoogleAndBingInParallel =
  coroutine {
    // Start the download coroutines in parallel
    let! cgoogle = runInParallel &lt;| downloadFromUri "https://www.google.com/"
    let! cbing   = runInParallel &lt;| downloadFromUri "https://www.bing.com/"

    // Wait for the coroutines to complete
    let! google  = cgoogle
    let! bing    = cbing

    return google.Length, bing.Length
  }

let exampleDownloadManyPagesInParallel =
  let download uri =
    coroutine {
      let! text = downloadFromUri uri
      return text.Length
    } |&gt; debugf "Download: %s" (string uri)
  coroutine {
    let uris        = Array.init 10 (fun i -&gt; sprintf "https://gist.github.com/mrange?page=%d" (i + 1))
    let! allLengths = uris |&gt; Array.map download |&gt; runAllInParallel 3
    return allLengths
  } |&gt; time

let exampleCPUBoundProblem =
  let d = 2048
  let r = d &gt;&gt;&gt; 3
  let m = 250
  let s = 2.0 / float d

  let rec mandelbrot re im cre cim i =
    if i &gt; 0 then
      let re2   = re*re
      let im2   = im*im
      let reim  = re*im
      if re2 + im2 &gt; 4.0 then
        i
      else
        mandelbrot (re2 - im2 + cre) (reim + reim + cim) cre cim (i - 1)
    else
      i

  let lines (pixels : byte array) f t =
    for y = f to (t - 1) do
      let yo  = y*r
      let cim = float y*s - 1.0
      for xo = 0 to (r - 1)  do
        let x0            = xo &lt;&lt;&lt; 3
        let mutable pixel = 0uy
        for p = 0 to 7 do
          let x = x0 + p
          let cre = float x*s - 1.5
          let i   = mandelbrot cre cim cre cim m
          if i = 0 then
            pixel   &lt;-pixel ||| (1uy &lt;&lt;&lt; (7 - p))
        pixels.[yo + xo] &lt;- pixel

  let colines (pixels : byte array) f t =
    coroutine {
      do! switchToThreadPool
      lines pixels f t
      return ()
    }
  coroutine {
    let pixels = Array.zeroCreate (r*d)

    let  s  = d / 4
    let! s0 = runInParallel &lt;| colines pixels (0*s) (1*s)
    let! s1 = runInParallel &lt;| colines pixels (1*s) (2*s)
    let! s2 = runInParallel &lt;| colines pixels (2*s) (3*s)
    let! s3 = runInParallel &lt;| colines pixels (3*s) (4*s)

    do! s0
    do! s1
    do! s2
    do! s3

    let img     = File.Create "mandelbrot.pbm"
    let sw      = new StreamWriter (img)

    do! sw.WriteAsync  (sprintf "P4\n%d %d\n" d d)
    do! sw.FlushAsync  ()
    do! img.WriteAsync (pixels, 0, pixels.Length)

    sw.Dispose ()
    img.Dispose ()

    return ()
  } |&gt; time

[&lt;EntryPoint&gt;]
let main argv =
  Environment.CurrentDirectory &lt;- AppDomain.CurrentDomain.BaseDirectory

  invoke exampleDownloadFromGoogleAndBingInParallel &lt;| printfn "Result: %A"

  printfn "Press any key to exit"
  Console.ReadKey () |&gt; ignore

  0
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/implementing-coroutines-(async-await)-using-continuations.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Implementing Coroutines (async await) using continuations.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 02:59:18 GMT</pubDate></item><item><title><![CDATA[K-Means clustering in FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:machine-learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#machine-learning</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:algorithm" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#algorithm</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:machine-learning" class="tag" target="_blank" rel="noopener nofollow">#machine-learning</a> <a href="https://muqiuhan.github.io/wiki?query=tag:algorithm" class="tag" target="_blank" rel="noopener nofollow">#algorithm</a><br>Machine Learning in Action, in F#<br>Porting <a data-tooltip-position="top" aria-label="http://www.manning.com/pharrington/" rel="noopener nofollow" class="external-link" href="http://www.manning.com/pharrington/" target="_blank">Machine Learning in Action</a> from Python to F#<br>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/07/29/Nearest-Neighbor-Classification-part-1/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/07/29/Nearest-Neighbor-Classification-part-1/" target="_blank">KNN classification (1)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/08/01/Nearest-Neighbor-Classification-part-2/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/08/01/Nearest-Neighbor-Classification-part-2/" target="_blank">KNN classification (2)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/08/05/Decision-Tree-classification/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/08/05/Decision-Tree-classification/" target="_blank">Decision Tree classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/08/18/Naive-Bayes-classification/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/08/18/Naive-Bayes-classification/" target="_blank">Naive Bayes classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/09/30/Logistic-Regression/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/09/30/Logistic-Regression/" target="_blank">Logistic Regression classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/11/25/Support-Vector-Machine-in-FSharp-work-in-progress/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/11/25/Support-Vector-Machine-in-FSharp-work-in-progress/" target="_blank">SVM classification (1)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/12/26/Support-Vector-Machine-in-FSharp/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/12/26/Support-Vector-Machine-in-FSharp/" target="_blank">SVM classification (2)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/12/29/AdaBoost-classifier-in-FSharp/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2012/12/29/AdaBoost-classifier-in-FSharp/" target="_blank">AdaBoost classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2013/02/10/K-Means-Clustering-in-FSharp/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2013/02/10/K-Means-Clustering-in-FSharp/" target="_blank">K-Means clustering</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2013/03/25/Simplify-data-with-SVD-and-MathNET-in-FSharp/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2013/03/25/Simplify-data-with-SVD-and-MathNET-in-FSharp/" target="_blank">SVD</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2013/04/28/Recommendation-Engine-with-SVD-and-MathNET-in-FSharp/" rel="noopener nofollow" class="external-link" href="https://mathias-brandewinder.github.io//2013/04/28/Recommendation-Engine-with-SVD-and-MathNET-in-FSharp/" target="_blank">Recommendation engine</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action" target="_blank">Code on GitHub</a>
<br>And the Journey converting “<a data-tooltip-position="top" aria-label="http://www.manning.com/pharrington/" rel="noopener nofollow" class="external-link" href="http://www.manning.com/pharrington/" target="_blank">Machine Learning in Action</a>” from Python to F# continues! Rather than following the order of the book, I decided to skip chapters 8 and 9, dedicated to regression methods (regression is something I spent a bit too much time doing in the past to be excited about it just right now), and go straight to Unsupervised Learning, which begins with the K-means clustering algorithm. So what is clustering about? In a nutshell, clustering focuses on the following question: given a set of observations, can the computer figure out a way to classify them into “meaningful groups”? The major difference with Classification methods is that in clustering, the Categories / Groups are initially unknown: it’s the algorithm’s job to figure out sensible ways to group items into Clusters, all by itself (hence the word “unsupervised”). Chapter 10 covers 2 clustering algorithms, k-means , and bisecting k-means. We’ll discuss only the first one today. The underlying idea behind the k-means algorithm is to identify k “representative archetypes” (k being a user input), the Centroids. The algorithm proceeds iteratively:<br>
Starting from k random Centroids,<br>
Observations are assigned to the closest Centroid, and constitute a Cluster,<br>
Centroids are updated, by taking the average of their Cluster,<br>
Until the allocation of Observation to Clusters doesn’t change any more.
<br>When things go well, we end up with k stable Centroids (minimal modification of Centroids do not change the Clusters), and Clusters contain Observations that are similar, because they are all close to the same Centroid (The <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" target="_blank">wikipedia page</a> for the algorithm provides a nice graphical representation).<br><br>The Python implementation proposed in the book is both very procedural and deals with Observations that are vectors. I thought it would be interesting to take a different approach, focused on functions instead. The current implementation is likely to change when I get into bisecting k-means, but should remain similar in spirit. Note also that I have given no focus to performance – this is my take on the easiest thing that would work. The entire code can be found <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/KMeansClustering.fs" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/KMeansClustering.fs" target="_blank">here on GitHub</a>. Here is how I approached the problem. First, rather than restricting ourselves to vectors, suppose we want to deal with any generic type. Looking at the pseudo-code above, we need a few functions to implement the algorithm:<br>
<br>to assign Observations of type 'a to the closest Centroid 'a, we need a notion of Distance,
<br>we need to create an initial collection of k Centroids of type 'a, given a dataset of 'as,
<br>to update the Centroids based on a Cluster of 'as, we need some aggregation function.
<br>Let’s create these 3 functions:<br>// the Distance between 2 observations 'a is a float
// It also better be positive - left to the implementer
type Distance&lt;'a&gt; = 'a -&gt; 'a -&gt; float
// CentroidsFactory, given a dataset, 
// should generate n Centroids
type CentroidsFactory&lt;'a&gt; = 'a seq -&gt; int -&gt; 'a seq
// Given a Centroid and observations in a Cluster,
// create an updated Centroid
type ToCentroid&lt;'a&gt; = 'a -&gt; 'a seq -&gt; 'a
<br>We can now define a function which, given a set of Centroids, will return the index of the closest Centroid to an Observation, as well as the distance from the Centroid to the Observation:<br>// Returns the index of and distance to the 
// Centroid closest to observation
let closest (dist: Distance&lt;'a&gt;) centroids (obs: 'a) =
    centroids
    |&gt; Seq.mapi (fun i c -&gt; (i, dist c obs)) 
    |&gt; Seq.minBy (fun (i, d) -&gt; d)
<br>Finally, we’ll go for the laziest possible way to generate k initial Centroids, by picking up k random observations from our dataset:<br>// Picks k random observations as initial centroids
// (this is very lazy, even tolerates duplicates)
let randomCentroids&lt;'a&gt; (rng: System.Random) 
                        (sample: 'a seq) 
                        k =
    let size = Seq.length sample
    seq { for i in 1 .. k do 
            let pick = Seq.nth (rng.Next(size)) sample
            yield pick }
<br>We have all we need – we can now write the algorithm itself:<br>// Given a distance, centroid factory and
// centroid aggregation function, identify
// the k centroids of a dataset
let kmeans (dist: Distance&lt;'a&gt;) 
           (factory: CentroidsFactory&lt;'a&gt;) 
           (aggregator: ToCentroid&lt;'a&gt;)
           (dataset: 'a seq) 
           k =
    // Recursively update Centroids and
    // the assignment of observations to Centroids
    let rec update (centroids, assignment) =
        // Assign each point to the closest centroid
        let next = 
            dataset 
            |&gt; Seq.map (fun obs -&gt; closest dist centroids obs)
            |&gt; Seq.toList
        // Check if any assignment changed
        let change =
            match assignment with
            | Some(previous) -&gt; 
                Seq.zip previous next    
                |&gt; Seq.exists (fun ((i, _), (j, _)) -&gt; not (i = j))
            | None -&gt; true // initially we have no assignment
        if change 
        then 
            // Update each Centroid position:
            // extract cluster of points assigned to each Centroid
            // and compute the new Centroid by aggregating cluster
            let updatedCentroids =
                let assignedDataset = Seq.zip dataset next
                centroids 
                |&gt; Seq.mapi (fun i centroid -&gt; 
                    assignedDataset 
                    |&gt; Seq.filter (fun (_, (ci, _)) -&gt; ci = i)
                    |&gt; Seq.map (fun (obs, _) -&gt; obs)
                    |&gt; aggregator centroid)
            // Perform another round of updates
            update (updatedCentroids, Some(next))
        // No assignment changed, we are done
        else (centroids, next)

    let initialCentroids = factory dataset k
    let centroids = update (initialCentroids, None) |&gt; fst |&gt; Seq.toList        
    let classifier = fun datapoint -&gt; 
        centroids 
        |&gt; List.minBy (fun centroid -&gt; dist centroid datapoint)
    centroids, classifier
<br>The meat of the algorithm is the update function. It takes in a set of current Centroids, and an optional Assignment of Observations to Centroids, represented as a list, mapping each Observation to Centroid indexes and corresponding distance. Note that we could drop the distance for the assignment – it’s never used afterwards, I added it prematurely because it is needed in the bissecting k-means algorithm.<br>The update function is recursive – it computes what Centroid / Cluster each observation will be assigned to next, checks whether any Observation has been assigned to a different Cluster than before (or if there is an assignment at all, to cover the initial case when no assignment has been computed yet). If a change occurred, new Centroids are computed and we go for another round, and otherwise we are done.<br>The outer function calls update, and once it terminates, returns the Centroids that have been identified, as well as a Classifier function, which will return the closest Centroid to an Observation.<br><br>I created two small examples illustrating the algorithm in action: one classic, with numeric observations, and one “just for kicks”, attempting to cluster a collection of strings. Both can be found in the file <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/Chapter10.fsx" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/Chapter10.fsx" target="_blank">Chapter10.fsx</a>.<br>The classic case operates on an artificially created dataset: we generate 3 points in 3 dimensions, and a collection of 50 points randomly generated in spheres around these 3 points:<br>let rng = new System.Random()
let centroids = [ [| 0.; 0.; 0. |]; [| 20.; 30.; 40. |]; [| -40.; -50.; -60. |] ]
// Create 50 points centered around each Centroid
let data = [ 
    for centroid in centroids do
        for i in 1 .. 50 -&gt; 
            Array.map (fun x -&gt; x + 5. * (rng.NextDouble() - 0.5)) centroid ]
<br>If everything works correctly, we expect the algorithm to identify 3 Centroids close to the 3 points we used as anchor points for our data sample. We need to define 2 functions, which are included in the main module: a Distance, and a function to compute a Centroid from a Cluster of Observations:<br>// Euclidean distance between 2 points, represented as float []
let euclidean x y = 
    Array.fold2 (fun d e1 e2 -&gt; d + pown (e1 - e2) 2) 0. x y 
    |&gt; sqrt

// Recompute Centroid as average of given sample
let avgCentroid (current: float []) (sample: float [] seq) =
    let size = Seq.length sample
    match size with
    | 0 -&gt; current
    | _ -&gt;
        sample
        |&gt; Seq.reduce (fun v1 v2 -&gt; 
               Array.map2 (fun v1x v2x -&gt; v1x + v2x) v1 v2)
        |&gt; Array.map (fun e -&gt; e / (float)size)
<br>Armed with this, we can run the algorithm:<br>let factory = randomCentroids&lt;float[]&gt; rng
let identifiedCentroids, classifier = kmeans euclidean factory avgCentroid data 3
printfn "Centroids identified"
identifiedCentroids 
|&gt; List.iter (fun c -&gt; 
    printfn ""
    printf "Centroid: "
    Array.iter (fun x -&gt; printf "%.2f " x) c)
<br>On my machine, this produces the following:<br>Centroids identified  
Centroid: 19.93 30.32 39.89  
Centroid: -39.98 -50.10 -59.69  
Centroid: -0.28 0.43 -0.01
<br>The 3 centroids are exactly what we expect – 3 points close to {20; 30; 40}, {-40; –50; -60} and {0; 0; 0}. Things seem to be working.<br>Now I was curious to see if this would be usable on something completely different, like strings. As usual, in order to make that work, we need a Distance, and a way to reduce a Cluster to a Centroid. The most obvious choice for a Distance between strings is the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/Levenshtein_distance" target="_blank">Levenshtein distance</a>, which measures how many edits are required to transform a string into another. Fortunately for me, someone already provided an <a data-tooltip-position="top" aria-label="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" rel="noopener nofollow" class="external-link" href="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" target="_blank">implementation in F#</a>, which I shamelessly lifted.<br>The Centroid update question required a bit of thinking. Obviously, computing the average of strings isn’t going to work – so how could we find a good “representative string” from a Cluster? I decided to go for something fairly simple: pick the string in the Cluster which has the least worst-case distance to all the others (as an alternative, I also tried picking the string with the lowest sum of squares distance, which produced similar results).<br>Finally, I created a sample, using a collection of 53 words sharing three different roots: “GRAPH”, “SCRIPT” and “GRAM”. Results vary from run to run (not surprisingly, the algorithm often struggles to separate GRAPH and GRAM words), but overall I was pleasantly surprised by the results:<br>Words identified
TELEGRAPHIC
RADIOGRAM
PRESCRIPTIVE

Classification of sample words
AUTOBIOGRAPHER -&gt; TELEGRAPHIC
AUTOBIOGRAPHICAL -&gt; TELEGRAPHIC
AUTOBIOGRAPHY -&gt; TELEGRAPHIC
AUTOGRAPH -&gt; RADIOGRAM
BIBLIOGRAPHIC -&gt; TELEGRAPHIC
BIBLIOGRAPHY -&gt; TELEGRAPHIC
CALLIGRAPHY -&gt; TELEGRAPHIC
CARTOGRAPHY -&gt; RADIOGRAM
CRYPTOGRAPHY -&gt; RADIOGRAM
GRAPH -&gt; TELEGRAPHIC
HISTORIOGRAPHY -&gt; TELEGRAPHIC
PARAGRAPH -&gt; TELEGRAPHIC
SEISMOGRAPH -&gt; TELEGRAPHIC
STENOGRAPHER -&gt; TELEGRAPHIC
TELEGRAPH -&gt; TELEGRAPHIC
TELEGRAPHIC -&gt; TELEGRAPHIC
BIBLIOGRAPHICAL -&gt; TELEGRAPHIC
STEREOGRAPH -&gt; TELEGRAPHIC
DESCRIBABLE -&gt; PRESCRIPTIVE
DESCRIBE -&gt; PRESCRIPTIVE
DESCRIBER -&gt; PRESCRIPTIVE
DESCRIPTION -&gt; PRESCRIPTIVE
DESCRIPTIVE -&gt; PRESCRIPTIVE
INDESCRIBABLE -&gt; PRESCRIPTIVE
INSCRIBE -&gt; PRESCRIPTIVE
INSCRIPTION -&gt; PRESCRIPTIVE
POSTSCRIPT -&gt; PRESCRIPTIVE
PRESCRIBE -&gt; PRESCRIPTIVE
PRESCRIPTION -&gt; PRESCRIPTIVE
PRESCRIPTIVE -&gt; PRESCRIPTIVE
SCRIBAL -&gt; RADIOGRAM
SCRIBBLE -&gt; PRESCRIPTIVE
SCRIBE -&gt; PRESCRIPTIVE
SCRIBBLER -&gt; RADIOGRAM
SCRIPT -&gt; PRESCRIPTIVE
SCRIPTURE -&gt; PRESCRIPTIVE
SCRIPTWRITER -&gt; PRESCRIPTIVE
SUPERSCRIPT -&gt; PRESCRIPTIVE
TRANSCRIBE -&gt; PRESCRIPTIVE
TYPESCRIPT -&gt; PRESCRIPTIVE
TRANSCRIPTION -&gt; PRESCRIPTIVE
DESCRIPTOR -&gt; PRESCRIPTIVE
ANAGRAM -&gt; RADIOGRAM
CABLEGRAM -&gt; RADIOGRAM
CRYPTOGRAM -&gt; RADIOGRAM
GRAMMAR -&gt; RADIOGRAM
GRAMMARIAN -&gt; RADIOGRAM
GRAMMATICAL -&gt; RADIOGRAM
MONOGRAM -&gt; RADIOGRAM
RADIOGRAM -&gt; RADIOGRAM
TELEGRAM -&gt; TELEGRAPHIC
UNGRAMMATICAL -&gt; TELEGRAPHIC
AEROGRAM -&gt; RADIOGRAM
<br>That’s it for today! In our next “ML in Action” episode, we’ll look into the bissecting k-means algorithm, which is a variation on today’s algorithm, and probably revisit the implementation. In the meanwhile, feel free to leave comments or feedback!<br><br><a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/tree/463cc43a5870cc8253bbf8b608800cb8380404b6" rel="noopener nofollow" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/tree/463cc43a5870cc8253bbf8b608800cb8380404b6" target="_blank">Source code on GitHub</a>: the relevant code is in the files KMeansClustering.fs and Chapter10.fsx.<br><a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" target="_blank">K-means algorithm on Wikipedia</a>.<br><a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/Levenshtein_distance" target="_blank">Levenshtein distance on Wikipedia</a>, and an <a data-tooltip-position="top" aria-label="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" rel="noopener nofollow" class="external-link" href="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" target="_blank">F# implementation of Levenshtein distance</a>.<br><a data-tooltip-position="top" aria-label="http://richardminerich.com/2012/09/levenshtein-distance-and-the-triangle-inequality/" rel="noopener nofollow" class="external-link" href="http://richardminerich.com/2012/09/levenshtein-distance-and-the-triangle-inequality/" target="_blank">Interesting discussion on the Levenshtein distance</a> on <a data-tooltip-position="top" aria-label="https://twitter.com/rickasaurus" rel="noopener nofollow" class="external-link" href="https://twitter.com/rickasaurus" target="_blank">@Rickasaurus</a>’ blog.<br>Another <a data-tooltip-position="top" aria-label="http://tech.blinemedical.com/k-means-step-by-step-in-f/" rel="noopener nofollow" class="external-link" href="http://tech.blinemedical.com/k-means-step-by-step-in-f/" target="_blank">K-means implementation in F#,</a> from <a data-tooltip-position="top" aria-label="https://twitter.com/devshorts" rel="noopener nofollow" class="external-link" href="https://twitter.com/devshorts" target="_blank">@DevShorts</a>.<br><a data-tooltip-position="top" aria-label="http://www.learnthat.org/pages/view/roots.html" rel="noopener nofollow" class="external-link" href="http://www.learnthat.org/pages/view/roots.html" target="_blank">Root Words</a>: an intriguing web page, providing help to learn words and vocabulary, which contains a list of words roots. It has one incredibly annoying feature – you can’t copy paste text from the page.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/k-means-clustering-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/K-Means clustering in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:46:09 GMT</pubDate></item><item><title><![CDATA[Learning from mistakes - Winnow algorithm in FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:algorithm" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#algorithm</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:algorithm" class="tag" target="_blank" rel="noopener nofollow">#algorithm</a><br>During some recent meanderings through the confines of the internet, I ended up discovering the <a data-tooltip-position="top" aria-label="http://www.cc.gatech.edu/~ninamf/ML11/lect0906.pdf" rel="noopener nofollow" class="external-link" href="http://www.cc.gatech.edu/~ninamf/ML11/lect0906.pdf" target="_blank">Winnow Algorithm</a>. The simplicity of the approach intrigued me, so I thought it would be interesting to try and implement it in F# and see how well it worked.<br>The purpose of the algorithm is to train a binary classifier, based on binary features. In other words, the goal is to predict one of two states, using a collection of features which are all binary. The prediction model assigns weights to each feature; to predict the state of an observation, it checks all the features that are “active” (true), and sums up the weights assigned to these features. If the total is above a certain threshold, the result is true, otherwise it’s false. Dead simple – and so is the corresponding F# code:<br>type Observation = bool []
type Label = bool
type Example = Label * Observation
type Weights = float []
 
let predict (theta:float) (w:Weights) (obs:Observation) =
    (obs,w) ||&gt; Seq.zip
    |&gt; Seq.filter fst
    |&gt; Seq.sumBy snd
    |&gt; ((&lt;) theta)
<br>We create some type aliases for convenience, and write a predict function which takes in theta (the threshold), weights and and observation; we zip together the features and the weights, exclude the pairs where the feature is not active, sum the weights, check whether the threshold is lower that the total, and we are done.<br>In a nutshell, the learning process feeds examples (observations with known label), and progressively updates the weights when the model makes mistakes. If the current model predicts the output correctly, don’t change anything. If it predicts true but should predict false, it is over-shooting, so weights that were used in the prediction (i.e. the weights attached to active features) are reduced. Conversely, if the prediction is false but the correct result should be true, the active features are not used enough to reach the threshold, so they should be bumped up.<br>And that’s pretty much it – the algorithm starts with arbitrary initial weights of 1 for every feature, and either doubles or halves them based on the mistakes. Again, the F# implementation is completely straightforward. The weights update can be written as follows:<br>let update (theta:float) (alpha:float) (w:Weights) (ex:Example) =
    let real,obs = ex
    match (real,predict theta w obs) with
    | (true,false) -&gt; w |&gt; Array.mapi (fun i x -&gt; if obs.[i] then alpha * x else x)
    | (false,true) -&gt; w |&gt; Array.mapi (fun i x -&gt; if obs.[i] then x / alpha else x)
    | _ -&gt; w
<br>Let’s check that the update mechanism works:<br>&gt; update 0.5 2. [|1.;1.;|] (false,[|false;true;|]);;
val it : float [] = [|1.0; 0.5|]
<br>The threshold is 0.5, the adjustment multiplier is 2, and each feature is currently weighted at 1. The state of our example is [| false; true; |], so only the second feature is active, which means that the predicted value will be 1. (the weight of that feature). This is above the threshold 0.5, so the predicted value is true. However, because the correct value attached to that example is false, our prediction is incorrect, and the weight of the second feature is reduced, while the first one, which was not active, remains unchanged.<br>Let’s wrap this up in a convenience function which will learn from a sequence of examples, and give us directly a function that will classify observations:<br>let learn (theta:float) (alpha:float) (fs:int) (xs:Example seq) =
    let updater = update theta alpha
    let w0 = [| for f in 1 .. fs -&gt; 1. |]   
    let w = Seq.fold (fun w x -&gt; updater w x) w0 xs
    fun (obs:Observation) -&gt; predict theta w obs
<br>We pass in the number of features, fs, to initialize the weights at the correct size, and use a fold to update the weights for each example in the sequence. Finally, we create and return a function that, given an observation, will predict the label, based on the weights we just learnt.<br>And that’s it – in 20 lines of code, we are done, the Winnow is implemented.<br>But… does it work? An example doesn’t prove anything, of course, but I was curious, and cooked up the following idea. Let’s use the Winnow to predict if the next character in a piece of text is going to be a letter, or something else (space, punctuation…), based on the previous characters. In other words, let’s try to predict if we reached the end of a word.<br>To simplify the coding part a bit, I will ignore case, and convert every character to upper case. Obviously, whether a character is upper or lower case is relevant to where we are in a word, but my goal here is just to satisfy my curiosity, so I will ignore that and be lazy. The letters A to Z correspond to char 65 to 90 (that’s an alphabet of 26 characters), and I also want to catch everything that isn’t a letter. One way we can then encode a character so that it fits our requirement of binary features is the following: create an array of 27 slots, and mark with true the slot corresponding to the letter, reserving the last slot for the case “not a letter”.<br>I will readily admit it, the following code is a bit ugly (there is probably a cleaner way to do that), but gets the work done:<br>let letter (c:char) = int c &gt;= 65 &amp;&amp; int c &lt;= 90
 
let encode (c:char) =
    let vec = Array.create (90-65+2) false   
    let x = int c
    if (x &gt;= 65 &amp;&amp; x &lt;= 90)
    then vec.[x-65] &lt;- true
    else vec.[90-65+1] &lt;- true
vec
 
let prepare (cs:char[]) =
    cs |&gt; Seq.map encode |&gt; Array.concat
<br>letter simply recognizes if a char is an uppercase letter, encode creates a vector representing a character, and prepare takes in an array of chars, and returns an array which puts side-by-side each of the encoded characters. As an example,<br>&gt; encode 'B';;
val it : bool [] =
[|false; true; false; false; false; false; false; false; false; false; false;
false; false; false; false; false; false; false; false; false; false;
false; false; false; false; false; false|]
<br>This returns an array of 27 booleans – all of them false, except the second position, which corresponds to B’s position in the alphabet.<br>We will try to predict the next character based not only on the previous one, but rather on the preceding sequence, the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/N-gram" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/N-gram" target="_blank">N-gram</a>. Let’s write a quick and dirty function to transform a string into N-grams, and whether the character that immediately follows is the end of a word, or any letter:<br>let ngrams n (text:string) =
text.ToUpperInvariant()
|&gt; Seq.windowed (n+1)
|&gt; Seq.map (fun x -&gt; x.[n],x.[0..(n-1)])
|&gt; Seq.map (fun (c,cs) -&gt; letter c |&gt; not, prepare cs)
<br>And we are ready to go. What I am really interested in here is not that much how good or bad the classifier is, but whether it actually improves as it gets feds more data. To observe that, let’s do the following: we’ll use a body of text for training, and another one for validation; we will train the classifier on a larger and larger portion of the training text, and measure the quality of the various models by applying it to the validation text.<br>We will train the model on a paragraph by Borges, and validate on some Cicero, both lifted from the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Infinite_monkey_theorem#Origins_and_.22The_Total_Library.22" rel="noopener nofollow" class="external-link" href="http://en.wikipedia.org/wiki/Infinite_monkey_theorem#Origins_and_.22The_Total_Library.22" target="_blank">Total Library section in the Infinite Monkey wikipedia page</a>.<br>Training:<br>
Everything would be in its blind volumes. Everything: the detailed history of the future, Aeschylus’ The Egyptians, the exact number of times that the waters of the Ganges have reflected the flight of a falcon, the secret and true nature of Rome, the encyclopedia Novalis would have constructed, my dreams and half-dreams at dawn on August 14, 1934, the proof of Pierre Fermat’s theorem, the unwritten chapters of Edwin Drood, those same chapters translated into the language spoken by the Garamantes, the paradoxes Berkeley invented concerning Time but didn’t publish, Urizen’s books of iron, the premature epiphanies ofStephen Dedalus, which would be meaningless before a cycle of a thousand years, the Gnostic Gospel of Basilides, the song the sirens sang, the complete catalog of the Library, the proof of the inaccuracy of that catalog. Everything: but for every sensible line or accurate fact there would be millions of meaningless cacophonies, verbal farragoes, and babblings. Everything: but all the generations of mankind could pass before the dizzying shelves—shelves that obliterate the day and on which chaos lies—ever reward them with a tolerable page.
<br>Validation:<br>
He who believes this may as well believe that if a great quantity of the one-and-twenty letters, composed either of gold or any other matter, were thrown upon the ground, they would fall into such order as legibly to form the Annals of Ennius. I doubt whether fortune could make a single verse of them.
<br>Here is how one might go about coding that experiment:<br>let training = ngrams 3 borges
let validation = ngrams 3 cicero
 
let len = Seq.length training
    for l in 25 .. 25 .. (len - 1) do
    let sample = training |&gt; Seq.take l
    let model = learn 0.5 2. (3*(92-65)) sample
    validation
    |&gt; Seq.averageBy (fun (l,o) -&gt;
    if l = model o then 1. else 0.)
    |&gt; printfn "Sample: %i, correct: %.4f" l
<br>Running that code will produce some rather unexciting output:<br>Sample: 25, correct: 0.2168 
Sample: 50, correct: 0.4434 
Sample: 75, correct: 0.5049 
Sample: 100, correct: 0.5955 
Sample: 125, correct: 0.5081 
Sample: 150, correct: 0.6861

// snipped because more of the same

Sample: 1125, correct: 0.7476 
Sample: 1150, correct: 0.6278 
Sample: 1175, correct: 0.6893 
Sample: 1200, correct: 0.7314
<br>Visibly, the quality starts pretty low, with around 21% correct predictions for the smallest sample, climbs up as the sample increases, and ends up oscillating in the 65% – 75% range. This isn’t a proof of anything, of course, but it seems to indicate that the model is “learning”, getting better and better at recognizing word endings as it is fed more 3-grams.<br>And that’s as far as I’ll go on the Winnow. I thought this was an interesting algorithm, if only for its simplicity. It is also suitable for online learning: you don’t need to train your model on a dataset before using it - it can progressively learn on the fly as data is arriving, and the only state you need to maintain is the latest set of weights. The biggest limitation is that it is a linear classifier, which assumes that the data can be cleanly separated along a plane.<br>In any case, I definitely had fun playing with this – I hope you did, too!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/learning-from-mistakes-winnow-algorithm-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Learning from mistakes - Winnow algorithm in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:46:08 GMT</pubDate></item><item><title><![CDATA[Messages and Agents]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <br>In this post, we’ll look at the message-based (or actor-based) approach to concurrency.<br>In this approach, when one task wants to communicate with another, it sends it a message, rather than contacting it directly. The messages are put on a queue, and the receiving task (known as an “actor” or “agent”) pulls the messages off the queue one at a time to process them.<br>This message-based approach has been applied to many situations, from low-level network sockets (built on TCP/IP) to enterprise wide application integration systems (for example RabbitMQ or IBM WebSphere MQ).<br>From a software design point of view, a message-based approach has a number of benefits:<br>
<br>You can manage shared data and resources without locks.
<br>You can easily follow the “single responsibility principle”, because each agent can be designed to do only one thing.
<br>It encourages a “pipeline” model of programming with “producers” sending messages to decoupled “consumers”, which has additional benefits:

<br>The queue acts as a buffer, eliminating waiting on the client side.
<br>It is straightforward to scale up one side or the other of the queue as needed in order to maximize throughput.
<br>Errors can be handled gracefully, because the decoupling means that agents can be created and destroyed without affecting their clients.


<br>From a practical developer’s point of view, what I find most appealing about the message-based approach is that when writing the code for any given actor, you don’t have to hurt your brain by thinking about concurrency. The message queue forces a “serialization” of operations that otherwise might occur concurrently. And this in turn makes it much easier to think about (and write code for) the logic for processing a message, because you can be sure that your code will be isolated from other events that might interrupt your flow.<br>With these advantages, it is not surprising that when a team inside Ericsson wanted to design a programming language for writing highly-concurrent telephony applications, they created one with a message-based approach, namely Erlang. Erlang has now become the poster child for the whole topic, and has created a lot of interest in implementing the same approach in other languages.<br><br>F# has a built-in agent class called MailboxProcessor. These agents are very lightweight compared with threads - you can instantiate tens of thousands of them at the same time.<br>These are similar to the agents in Erlang, but unlike the Erlang ones, they do not work across process boundaries, only in the same process. And unlike a heavyweight queueing system such as RabbitMQ, the messages are not persistent. If your app crashes, the messages are lost.<br>But these are minor issues, and can be worked around. In a future series, I will go into alternative implementations of message queues. The fundamental approach is the same in all cases.<br>Let’s see a simple agent implementation in F#:<br>
let printerAgent = MailboxProcessor.Start(fun inbox-&gt;

    // the message processing function
    let rec messageLoop() = async{

        // read a message
        let! msg = inbox.Receive()

        // process a message
        printfn "message is: %s" msg

        // loop to top
        return! messageLoop()
        }

    // start the loop
    messageLoop()
    )

<br>The MailboxProcessor.Start function takes a simple function parameter. That function loops forever, reading messages from the queue (or “inbox”) and processing them.<br>Here’s the example in use:<br>// test it
printerAgent.Post "hello"
printerAgent.Post "hello again"
printerAgent.Post "hello a third time"
<br>In the rest of this post we’ll look at two slightly more useful examples:<br>
<br>Managing shared state without locks
<br>Serialized and buffered access to shared IO
<br>In both of these cases, a message based approach to concurrency is elegant, efficient, and easy to program.<br><br>Let’s look at the shared state problem first.<br>A common scenario is that you have some state that needs to be accessed and changed by multiple concurrent tasks or threads. We’ll use a very simple case, and say that the requirements are:<br>
<br>A shared “counter” and “sum” that can be incremented by multiple tasks concurrently.
<br>Changes to the counter and sum must be atomic – we must guarantee that they will both be updated at the same time.
<br><br>Using locks or mutexes is a common solution for these requirements, so let’s write some code using a lock, and see how it performs.<br>First let’s write a static LockedCounter class that protects the state with locks.<br>open System
open System.Threading
open System.Diagnostics

// a utility function
type Utility() =
    static let rand = Random()

    static member RandomSleep() =
        let ms = rand.Next(1,10)
        Thread.Sleep ms

// an implementation of a shared counter using locks
type LockedCounter () =

    static let _lock = Object()

    static let mutable count = 0
    static let mutable sum = 0

    static let updateState i =
        // increment the counters and...
        sum &lt;- sum + i
        count &lt;- count + 1
        printfn "Count is: %i. Sum is: %i" count sum

        // ...emulate a short delay
        Utility.RandomSleep()


    // public interface to hide the state
    static member Add i =
        // see how long a client has to wait
        let stopwatch = Stopwatch()
        stopwatch.Start()

        // start lock. Same as C# lock{...}
        lock _lock (fun () -&gt;

            // see how long the wait was
            stopwatch.Stop()
            printfn "Client waited %i" stopwatch.ElapsedMilliseconds

            // do the core logic
            updateState i
            )
        // release lock
<br>Some notes on this code:<br>
<br>This code is written using a very imperative approach, with mutable variables and locks
<br>The public Add method has explicit Monitor.Enter and Monitor.Exit expressions to get and release the lock. This is the same as the lock{...} statement in C#.
<br>We’ve also added a stopwatch to measure how long a client has to wait to get the lock.
<br>The core “business logic” is the updateState method, which not only updates the state, but adds a small random wait as well to emulate the time taken to do the processing.
<br>Let’s test it in isolation:<br>// test in isolation
LockedCounter.Add 4
LockedCounter.Add 5
<br>Next, we’ll create a task that will try to access the counter:<br>let makeCountingTask addFunction taskId  = async {
    let name = sprintf "Task%i" taskId
    for i in [1..3] do
        addFunction i
    }

// test in isolation
let task = makeCountingTask LockedCounter.Add 1
Async.RunSynchronously task
<br>In this case, when there is no contention at all, the wait times are all 0.<br>But what happens when we create 10 child tasks that all try to access the counter at once:<br>let lockedExample5 =
    [1..10]
        |&gt; List.map (fun i -&gt; makeCountingTask LockedCounter.Add i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
<br>Oh dear! Most tasks are now waiting quite a while. If two tasks want to update the state at the same time, one must wait for the other’s work to complete before it can do its own work, which affects performance.<br>And if we add more and more tasks, the contention will increase, and the tasks will spend more and more time waiting rather than working.<br><br>Let’s see how a message queue might help us. Here’s the message based version:<br>type MessageBasedCounter () =

    static let updateState (count,sum) msg =

        // increment the counters and...
        let newSum = sum + msg
        let newCount = count + 1
        printfn "Count is: %i. Sum is: %i" newCount newSum

        // ...emulate a short delay
        Utility.RandomSleep()

        // return the new state
        (newCount,newSum)

    // create the agent
    static let agent = MailboxProcessor.Start(fun inbox -&gt;

        // the message processing function
        let rec messageLoop oldState = async{

            // read a message
            let! msg = inbox.Receive()

            // do the core logic
            let newState = updateState oldState msg

            // loop to top
            return! messageLoop newState
            }

        // start the loop
        messageLoop (0,0)
        )

    // public interface to hide the implementation
    static member Add i = agent.Post i
<br>Some notes on this code:<br>
<br>The core “business logic” is again in the updateState method, which has almost the same implementation as the earlier example, except the state is immutable, so that a new state is created and returned to the main loop.
<br>The agent reads messages (simple ints in this case) and then calls updateState method
<br>The public method Add posts a message to the agent, rather than calling the updateState method directly
<br>This code is written in a more functional way; there are no mutable variables and no locks anywhere. In fact, there is no code dealing with concurrency at all! The code only has to focus on the business logic, and is consequently much easier to understand.
<br>Let’s test it in isolation:<br>// test in isolation
MessageBasedCounter.Add 4
MessageBasedCounter.Add 5
<br>Next, we’ll reuse a task we defined earlier, but calling MessageBasedCounter.Add instead:<br>let task = makeCountingTask MessageBasedCounter.Add 1
Async.RunSynchronously task
<br>Finally let’s create 5 child tasks that try to access the counter at once.<br>let messageExample5 =
    [1..5]
        |&gt; List.map (fun i -&gt; makeCountingTask MessageBasedCounter.Add i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
<br>We can’t measure the waiting time for the clients, because there is none!<br><br>A similar concurrency problem occurs when accessing a shared IO resource such as a file:<br>
<br>If the IO is slow, the clients can spend a lot of time waiting, even without locks.
<br>If multiple threads write to the resource at the same time, you can get corrupted data.
<br>Both problems can be solved by using asynchronous calls combined with buffering – exactly what a message queue does.<br>In this next example, we’ll consider the example of a logging service that many clients will write to concurrently. (In this trivial case, we’ll just write directly to the Console.)<br>We’ll first look at an implementation without concurrency control, and then at an implementation that uses message queues to serialize all requests.<br><br>In order to make the corruption very obvious and repeatable, let’s first create a “slow” console that writes each individual character in the log message and pauses for a millisecond between each character. During that millisecond, another thread could be writing as well, causing an undesirable interleaving of messages.<br>let slowConsoleWrite msg =
    msg |&gt; String.iter (fun ch-&gt;
        System.Threading.Thread.Sleep(1)
        System.Console.Write ch
        )

// test in isolation
slowConsoleWrite "abc"
<br>Next, we will create a simple task that loops a few times, writing its name each time to the logger:<br>let makeTask logger taskId = async {
    let name = sprintf "Task%i" taskId
    for i in [1..3] do
        let msg = sprintf "-%s:Loop%i-" name i
        logger msg
    }

// test in isolation
let task = makeTask slowConsoleWrite 1
Async.RunSynchronously task
<br>Next, we write a logging class that encapsulates access to the slow console. It has no locking or serialization, and is basically not thread-safe:<br>type UnserializedLogger() =
    // interface
    member this.Log msg = slowConsoleWrite msg

// test in isolation
let unserializedLogger = UnserializedLogger()
unserializedLogger.Log "hello"
<br>Now let’s combine all these into a real example. We will create five child tasks and run them in parallel, all trying to write to the slow console.<br>let unserializedExample =
    let logger = UnserializedLogger()
    [1..5]
        |&gt; List.map (fun i -&gt; makeTask logger.Log i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
<br>Ouch! The output is very garbled!<br><br>So what happens when we replace UnserializedLogger with a SerializedLogger class that encapsulates a message queue.<br>The agent inside SerializedLogger simply reads a message from its input queue and writes it to the slow console. Again there is no code dealing with concurrency and no locks are used.<br>type SerializedLogger() =

    // create the mailbox processor
    let agent = MailboxProcessor.Start(fun inbox -&gt;

        // the message processing function
        let rec messageLoop () = async{

            // read a message
            let! msg = inbox.Receive()

            // write it to the log
            slowConsoleWrite msg

            // loop to top
            return! messageLoop ()
            }

        // start the loop
        messageLoop ()
        )

    // public interface
    member this.Log msg = agent.Post msg

// test in isolation
let serializedLogger = SerializedLogger()
serializedLogger.Log "hello"
<br>So now we can repeat the earlier unserialized example but using the SerializedLogger instead. Again, we create five child tasks and run them in parallel:<br>let serializedExample =
    let logger = SerializedLogger()
    [1..5]
        |&gt; List.map (fun i -&gt; makeTask logger.Log i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
<br>What a difference! This time the output is perfect.<br><br>There is much more to say about this message based approach. In a future series, I hope to go into much more detail, including discussion of topics such as:<br>
<br>alternative implementations of message queues with RabbitMQ and TPL Dataflow.
<br>cancellation and out of band messages.
<br>error handling and retries, and handling exceptions in general.
<br>how to scale up and down by creating or removing child agents.
<br>avoiding buffer overruns and detecting starvation or inactivity.
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/messages-and-agents.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Messages and Agents.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 02:58:43 GMT</pubDate></item><item><title><![CDATA[Signaling in WebRTC with Ably and Fable]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:network" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#network</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:dotnet" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#dotnet</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a> <a href="https://muqiuhan.github.io/wiki?query=tag:network" class="tag" target="_blank" rel="noopener nofollow">#network</a> <a href="https://muqiuhan.github.io/wiki?query=tag:dotnet" class="tag" target="_blank" rel="noopener nofollow">#dotnet</a><br>
Hi,<br>I try to combine a WebRTC and Fable for some time in my daily work. I want to describe the process of implementing a signaling mechanism for it.<br>But at first, what is a WebRTC? <a data-tooltip-position="top" aria-label="https://webrtc.org/" rel="noopener nofollow" class="external-link" href="https://webrtc.org/" target="_blank">WebRTC</a> is a technology that allows exchanging data between N peers via UDP or TCP protocols. Exchange means that there is no server in the middle, but peers talk to each other (except situation when Relay connection is used). WebRTC is a native API available in all modern browsers. One of the things that are not ready by default is the signaling process. About which will be this article.<br>Before we go into details of implementation. Let’s explain some basic concepts around signaling. To establish a connection between 2 Peers they need to exchange messages with <a data-tooltip-position="top" aria-label="https://developer.mozilla.org/en-US/docs/Glossary/SDP" rel="noopener nofollow" class="external-link" href="https://developer.mozilla.org/en-US/docs/Glossary/SDP" target="_blank">SDP</a> (Session description protocol) information. The number of those messages depends on:<br>
<br>the number of STUN/TURN server,
<br>type of connection (P2P/Relay).
<br>SDP contains a lot of information needed to start a connection. Messages that would be exchanged are called Offer, Answer, and Candidates. We gonna use a <a data-tooltip-position="top" aria-label="https://webrtcglossary.com/trickle-ice/" rel="noopener nofollow" class="external-link" href="https://webrtcglossary.com/trickle-ice/" target="_blank">Trickle-Ice</a> mechanism. It means that the offer, answer, and candidates will be sent separately in an async manner. Candidates have information about “how to connect to a peer which create them” (more about them <a data-tooltip-position="top" aria-label="https://developer.mozilla.org/en-US/docs/Web/API/RTCIceCandidate" rel="noopener nofollow" class="external-link" href="https://developer.mozilla.org/en-US/docs/Web/API/RTCIceCandidate" target="_blank">here</a>). There is also a simpler way of establishing a connection where Answer and Offer would contain all needed candidates. But I want to focus on an async version because we are using it in our product because of better performance.<br>Switching messages as described above are named “signaling”. When I was looking for some ready to go solution for signaling I find out <a data-tooltip-position="top" aria-label="https://www.ably.io/" rel="noopener nofollow" class="external-link" href="https://www.ably.io/" target="_blank">Ably.io</a>. I decided to give it a try in case of my base application, which should send a file from one peer to another.<br>When I was reading the Ably.io documentation. I realized that the only functions that I will need would be publish and subscribe. Which are used in a message channel context. Because of that, I created an Ably interface in F# which is a port of JS interfaces from their official lib.<br>module AblyMapping =
  type IMessage =
    abstract name: string
    abstract data: string

  type IChannel =
    abstract publish: string * string -&gt; unit
    abstract subscribe: string * (IMessage -&gt; unit) -&gt; unit

  type IChannels =
    abstract ``get``: string -&gt; IChannel

  type IRealTime =
    abstract channels: IChannels

  type IAbly =
    abstract Realtime: (string) -&gt; IRealTime

  let Ably: IAbly = importAll "Ably"
<br>Changes in packages.json.<br>{
  …
  ”ably”: "^1.2.4"
}
<br>I believe that the above code is self-explanatory when we look at the JS example from the Ably site.<br>var ably = new Ably.Realtime('apikey');
var channel = ably.channels.get('gut-tub');

// Publish a message to the gut-tub channel
channel.publish('greeting', 'hello');

var ably = new Ably.Realtime('apikey');
var channel = ably.channels.get('gut-tub');

// Subscribe to messages on channel
channel.subscribe('greeting', function(message) {
  alert(message.data);
});
<br>Initialization of ably could be found in init method.<br>let init (): Model * Cmd&lt;Msg&gt; =
  let channel =
    AblyMapping.Ably.Realtime “api-key”
    |&gt; fun x -&gt; x.channels.get “channel name"
  let model = {
    Role = None
    Channel = channel
    ConnectionState = WebRTC.ConnectionState.NotConnected
    WaitingCandidates = [] }
  model, Cmd.none
<br>Right now, we could go to sending and receiving messages via Ably, but I start with installing the <a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/webrtc-adapter" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/webrtc-adapter" target="_blank">WebRTC-adapter</a> library. This library gives me confidence that what I want to achieve with native API would work the same way in every browser. Since there are some slight differences between implementations.<br>I add the following line to the packages.json with the webrtc-adapter library.<br>{
  …
  "webrtc-adapter": "^7.7.0"
}
<br>Usage in F# code.<br>module WebRTC =
  let private adapter: obj = importAll "webrtc-adapter"
<br>Thanks to the above. I’m sure that the interfaces are compatible across all supported browsers. We could switch to the code which is responsible for handling WebRTC. The code snippet is big because we need to distinguish the sender and receiver of a file. It is because we don’t have an application on the server-side. I started with the creation of a PeerConnection object with a communication server address.<br>let create () =
  let conf =
    [| RTCIceServer.Create( [| "turn:location:443" |], "user", "pass", RTCIceCredentialType.Password ) |]
    |&gt; RTCConfiguration.Create
    |&gt; fun x -&gt;
      x.iceTransportPolicy &lt;- Some RTCIceTransportPolicy.Relay
      x

    let pc = RTCPeerConnection.Create(conf)

    { Peer = pc; Channel = None }
<br>Because I test everything locally, I force using a Relay connection. Otherwise, communication would be done via a local network. That would result in skipping the signaling phase (host candidates would be used). Worth mentioning is that using public STUN/TURN servers is not recommended. We are not sure how the configuration looks like and who owns them.<br>We create a LifecycleModel object which contains a PeerConnection and RTCDataChannel inside. And create an instance of it. It is created as a mutable field instead of part of the Elmish model. It is because of simplicity (default Thoth serializer couldn’t handle serializing PeerConnection also passing the whole big object wouldn’t be something that we want to do).<br>type LifecycleModel = {
  Peer: RTCPeerConnection
  Channel: RTCDataChannel option
}

...

let mutable peer: LifecycleModel = Unchecked.defaultof&lt;_&gt;

...

peer &lt;- WebRTC.create ()
<br>After the creation of the PeerConnection object. We see that in the UI we have a possibility to connect as sender or receiver.<br><img alt="initialize" src="https://mnie.github.com/img/2021_01_05_webrtc_ably/initialize.png" referrerpolicy="no-referrer"><br>It would result in a distinction of how WebRTC is working and a different value for a Role field in a model. When we look at how the sender logic is implemented we start with the creation of RTCDataChannel. It would be used as a transfer channel between users. The creation of a channel is located in the createChannel function.<br>let createChannel pc name =
  let channel = pc.Peer.createDataChannel name
  { pc with Channel = Some channel}
<br>Going to the configuration of messages send and received via Ably. When we are a sender we want to listen to Answer and Candidate(from the receiver) messages. How it is achieved is visible in below code snippet.<br>let subs =
  [
      "answer", fun (msg: AblyMapping.IMessage) -&gt; dispatch (Msg.WebRTC (WebRTC.Msg.Answer msg.data))
      "receiver-candidate", fun (msg: AblyMapping.IMessage) -&gt; dispatch (Msg.WebRTC (WebRTC.Msg.Candidate msg.data))
  ] |&gt; Map.ofList
<br>And combination with Ably channel is visible in init method from Signaling module.<br>let init (channel: IChannel) (subscribers: Map&lt;string, IMessage -&gt; unit&gt;) =
  subscribers
  |&gt; Map.iter (fun subscriberKey subscriberFunc -&gt;
      channel.subscribe (subscriberKey, subscriberFunc)
  )
  channel
<br>As we could see we subscribe to all messages with the given keys answer and receiver-candidate. When they occur we propagate Elmish messages that would be handle in the update method.<br>In the receiver scenario the difference is that we don’t create RTCDataChannel (this is why it is marked as option in LifecycleModel). We gather it when the connection would be in a connecting phase. If it would be created on its own we would receive an invalid state error. This is why we only subscribe to messages Offer and Candidate(from Sender).<br>When a subscription to Ably messages is ready we send an Elmish message Signaling with a ready channel which is updated in the application model. Whereas the WebRTC configuration is updated with callbacks to functions that need to be handled in a connection/data transfer process.<br>Assignment of those callbacks and how they are handled is visible in a subscribe function. Which should:<br>
<br>Initialize RTCDataChannel through which data (file) would be transferred.
<br>let private initReceiverChannel (pc: RTCPeerConnection) msgHandler dispatch =
  let mutable updatedChannel: RTCDataChannel = Unchecked.defaultof&lt;_&gt;
  let callback (ev: RTCDataChannelEvent) =
    let receiveChannel = subscribeChannel ev.channel msgHandler dispatch
    internalLog (sprintf "updating channel: %A" ev.channel.id)
    peer &lt;- { peer with Channel = Some receiveChannel }

  pc.ondatachannel &lt;- callback
  updatedChannel

let updatedChannel =
  if role = Role.Sender then
    match channel with
    | Some c -&gt;
      internalLog (sprintf "initialize channel for: %A" role)
      subscribeChannel c msgHandler dispatch
    | None -&gt; failwith "Channel is not initilized for sender"
  else
      internalLog (sprintf "initialize channel2 for: %A" role)
      initReceiverChannel pc msgHandler dispatch
<br>
<br>Handling connection state (only for diagnostic purposes).
<br>pc.oniceconnectionstatechange &lt;-
  fun _ -&gt;
    internalLog (sprintf "Connection state changed to: %A" pc.iceConnectionState)
<br>
<br>Handling exchange of candidates.
<br>pc.onicecandidate &lt;-
  fun e -&gt;
    match e.candidate with
    | None -&gt; internalLog "Trickle ICE Completed"
    | Some cand -&gt;
      cand.toJSON()
      |&gt; Base64.``to``
      |&gt; Signaling.WebRTCCandidate.Candidate
      |&gt;
        if role = Role.Sender then
            Signaling.Notification.SenderCandidate
        else Signaling.Notification.ReceiverCandidate
      |&gt; onSignal
<br>Configuration of WebRTC is ready on the sender and receiver side.<br><img alt="connect" src="https://mnie.github.com/img/2021_01_05_webrtc_ably/connect.png" referrerpolicy="no-referrer"><br>We could initiate a connection by clicking the Connect button. After clicking it the init method from the WebRTC module would be called.<br>let init connection onSignal =
  let pc, channel = connection.Peer, connection.Channel
  pc.createOffer().``then``(fun desc -&gt;
    pc.setLocalDescription (desc) |&gt; Promise.start
    if isNull desc.sdp then
      internalLog "Local description is empty"
    else
      desc
      |&gt; Base64.``to``
      |&gt; Signaling.WebRTCOffer.Offer
      |&gt; Signaling.Notification.Offer
      |&gt; onSignal)

  |&gt; Promise.catch (sprintf "On negotation needed return error: %A" &gt;&gt; internalLog)
  |&gt; Promise.start

  { Peer = pc
    Channel = channel }
<br>We send Offer via Ably channel and set it as LocalDescription via setLocalDescription method on PeerConnection object. Right now the application flow is not visible when we look at the code at first. The other side of communication should receive Offer via Ably channel which would be then propagated as Offer Elmish message and handle in update method.<br>let setOffer (lifecycle: LifecycleModel) onSignal remote =
  try
    let desc = rtcSessionDescription remote
    internalLog (sprintf "setting offer: %A" desc)
    lifecycle.Peer.setRemoteDescription desc
    |&gt; Promise.catch (sprintf "Failed to set remote description: %A" &gt;&gt; internalLog)
    |&gt; Promise.start
    lifecycle.Peer.createAnswer().``then``(fun desc -&gt;
      lifecycle.Peer.setLocalDescription (desc) |&gt; Promise.start
      if isNull desc.sdp then
        internalLog "Local description is empty"
      else
        internalLog (sprintf "sending answer: %A" desc)
        desc
        |&gt; Base64.``to``
        |&gt; Signaling.WebRTCAnswer.Answer
        |&gt; Signaling.Notification.Answer
        |&gt; onSignal)

    |&gt; Promise.catch (sprintf "On negotation needed errored with: %A" &gt;&gt; internalLog)
    |&gt; Promise.start
  with e -&gt;
    internalLog (sprintf "Error occured while adding remote description: %A" e)
<br>It should set Offer from a sender as a RemoteDescription on a receiver side and in case of success generate Answer. It would be sent to the sender via Ably. The important thing to mention here is that after the creation of Offer/Answer Candidates are generated. They shouldn’t be set on the PeerConnection object before setting Local and Remote descriptions. Because of that Elmish model has a buffer for Candidates that could be received before setting Remote/Local descriptions.<br>Buffor handling:<br>if model.WaitingCandidates.Length &gt; 0 then
  model.WaitingCandidates
  |&gt; List.iter (WebRTC.setCandidate peer )
  { model with
              ConnectionState = WebRTC.ConnectionState.Connecting
              WaitingCandidates = [] }, Cmd.none
else
  { model with ConnectionState = WebRTC.ConnectionState.Connecting }, Cmd.none
<br>Handling a message that contains Candidates looks like that.<br>if model.ConnectionState &lt;&gt; WebRTC.ConnectionState.NotConnected then
  WebRTC.setCandidate peer candidate
  model, Cmd.none
else
  { model with WaitingCandidates = candidate::model.WaitingCandidates }, Cmd.none
<br>ConnectionState is set when Offer or Answer are received or when the DataChannel is open.<br>Handling Answer.<br>| WebRTC (WebRTC.Msg.Answer answer) -&gt;
  WebRTC.setAnswer peer answer
  { model with ConnectionState = WebRTC.ConnectionState.Connecting }, Cmd.none
<br>Going to setAnswer implementation.<br>let setAnswer (lifecycle: LifecycleModel) remote =
  try
    let desc = rtcSessionDescription remote
    internalLog (sprintf "setting answer: %A" desc)
    lifecycle.Peer.setRemoteDescription desc
    |&gt; Promise.catch (sprintf "Failed to set remote description: %A" &gt;&gt; internalLog)
    |&gt; Promise.start
  with e -&gt;
    internalLog (sprintf "Error occured while adding remote description: %A" e)
<br>As we could see, we only set RemoteDescription on a PeerConnection object here.<br>Right now, if everything succeeds, we should have a WebRTC connection ready. DataChannel is open between our peers. We could go to the code which is responsible for sending files. In UI, there should be a textBox that reacts on a file drop.<br><img alt="send" src="https://mnie.github.com/img/2021_01_05_webrtc_ably/send.png" referrerpolicy="no-referrer"><br>Which is handled in the following way.<br>Field.div [
  Field.IsGrouped ] [
  Control.p [ Control.IsExpanded ] [
    div [   
      Class "card border-primary"
      Draggable true
      Style [ Height "100px" ]
      OnDragOver ( fun e -&gt;
        e.preventDefault()
      )
      OnDrop ( fun e -&gt;
        e.preventDefault()
        if e.dataTransfer.files.length &gt; 0 then
          let file = e.dataTransfer.files.[0]
          { Name = file.name; Data = file.slice () }
          |&gt; SendFile
          |&gt; dispatch
      )
    ] []
  ]
]
<br>SendFile message handling.<br>| SendFile file -&gt;
  MessageTransfer.send peer (ChannelMessage.File file.Data)
  model, Cmd.none
<br>Method send in MessageTransfer.<br>let send (lifecycle: LifecycleModel) (msg: ChannelMessage) =
  match msg, lifecycle.Channel with
  | File blob, Some channel -&gt;
    blob
    |&gt; U4.Case2
    |&gt; channel.send
  | _, _ -&gt; log "MessageTransfer" (sprintf "Unable to process: %A channel is: %A" msg lifecycle.Channel)
<br>On a receiver side onmessage handling on DataChannel object looks as follows.<br>channel.onmessage &lt;-
  fun e -&gt;
    internalLog (sprintf "received msg in channel: %A" e.data)
    let blob = e.data :?&gt; Blob
    msgHandler blob
<br>As we could see, just for simplicity we assume that only javascript blob would be sent via DataChannel. We pass there also a msgHandler which is implemented in the following way.<br>let download (data: Blob) =
  let url = Browser.Url.URL.createObjectURL data
  let mutable anchor = document.createElement "a"
  anchor.hidden &lt;- true
  anchor.setAttribute ("href", url)
  anchor.setAttribute ("download", "image.png")
  document.body.appendChild anchor |&gt; ignore
  anchor.click ()
  document.body.removeChild anchor |&gt; ignore
<br>Its responsibility is to receive a FileBlob and immediately download it. We assume that the file would be always a png with the name image.<br>To sum up, thanks to the Ably platform I was able to implement Signaling in a simple way which would be worth consideration during the decision-making phase about “how to achieve Signaling in a most performant way” in my daily work. During some initial tests to compare it with standard HTTP request/response, it looks promising.<br>Another interesting thing is that a user can see how the messages/connections flow and work on the Ably dashboard. There is also an information about used quota.<br><img alt="chart" src="https://mnie.github.com/img/2021_01_05_webrtc_ably/chart.png" referrerpolicy="no-referrer"> <img alt="quota" src="https://mnie.github.com/img/2021_01_05_webrtc_ably/quota.png" referrerpolicy="no-referrer"><br>Because there are no built-in signaling solutions Ably is for sure an almost ready to go alternative which we could use in our scenario. I hope I would be able to compare it with other ways to do signaling in the next articles.<br><a data-tooltip-position="top" aria-label="https://github.com/MNie/WebRTCSignaling" rel="noopener nofollow" class="external-link" href="https://github.com/MNie/WebRTCSignaling" target="_blank">Repository</a><br>I hope you enjoy it. Thanks for reading!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/signaling-in-webrtc-with-ably-and-fable.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Signaling in WebRTC with Ably and Fable.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:45:25 GMT</pubDate><enclosure url="https://mnie.github.com/img/2021_01_05_webrtc_ably/initialize.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://mnie.github.com/img/2021_01_05_webrtc_ably/initialize.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Tips for working with Elmish]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <br>his article is intended for people who are already familiar with the MVU architecture. You can learn more about it on the <a data-tooltip-position="top" aria-label="https://elmish.github.io/elmish/" rel="noopener nofollow" class="external-link" href="https://elmish.github.io/elmish/" target="_blank"><em></em></a>Elmish website.<br>
本文面向已经熟悉 MVU 架构的人员。您可以在 Elmish 网站上了解更多相关信息。<br>I am one of the maintainer of Elmish and I created several <a data-tooltip-position="top" aria-label="http://fable.io" rel="noopener nofollow" class="external-link" href="http://fable.io" target="_blank">Fable</a> / Elmish libraries and applications. When doing so, I needed to understand Elmish deeply and I want to take the opportunity of <a data-tooltip-position="top" aria-label="https://sergeytihon.com/2018/10/22/f-advent-calendar-in-english-2018/" rel="noopener nofollow" class="external-link" href="https://sergeytihon.com/2018/10/22/f-advent-calendar-in-english-2018/" target="_blank"><em></em></a>F# Advent Calendar to share with you my tips and knowledge.<br>
我是 Elmish 的维护者之一，我创建了几个 Fable / Elmish 库和应用程序。这样做时，我需要深入了解 Elmish，我想利用 F# Advent Calendar 的机会与大家分享我的技巧和知识。<br>I will be covering a lot in this article, so grab yourself a cup of tea 🍵, take a deep breath 💨 and let’s get started 🏁.<br>
我将在这篇文章中介绍很多内容，所以给自己喝杯茶🍵，深呼吸💨，让我们开始吧🏁。<br><br>A command is a container for a function that Elmish executes immediately but, may schedule dispatch of message at any time.<br>
命令是 Elmish 立即执行的函数的容器，但可以随时安排消息的发送。<br>For example:&nbsp;例如：<br>
<br>Issue immediately a new Message<br>
立即发出新消息
<br>Make an HTTP call and then return the result in your application via a Message<br>
进行 HTTP 调用，然后通过消息将结果返回到您的应用程序中
<br>Save data in your storage<br>
将数据保存在您的存储中
<br><br>In this section, I will cover all the default functions offered by Elmish and show an example of it.<br>
在本节中，我将介绍 Elmish 提供的所有默认函数并展示它的示例。<br>Use Cmd.none to schedule no Commands.<br>
使用 Cmd.none 不安排任何命令。<br>type Model =
    { Tick : int }
    
type Msg =
    | Tick
    
let private update msg model =
    match msg with
    | Tick -&gt;
        { model with Tick = model.Tick + 1 }, Cmd.none
<br>Use Cmd.ofMsg to schedules another message directly, it can be seen as a way to chain messages.<br>
使用 Cmd.ofMsg 直接调度另一条消息，可以看作是一种链接消息的方式。<br>type Model =
    { Value : string }

type Msg =
    | ChangeValue of string
    | ValidateData
    
let private update msg model =
    match msg with
    | ChangeValue newValue -&gt;
        { model with Value = newValue }, Cmd.ofMsg ValidateData
<br>UseCmd.ofAsync to evaluate an async block and map the result into success or error (of exception).<br>
使用 Cmd.ofAsync 评估 async 块并将结果映射为成功或错误（异常）。<br>type Model =
    { InputValue : string
      UpperValue : string }
      
type Msg =
    | ToUpper of string
    | OnUpperResult of string
    | OnUpperError of exn

let private update (msg : Msg) (model : Model) =
    match msg with
    | ToUpper txt -&gt;
        let asyncUpper (txt : string) =
            async {
                do! Async.Sleep 1000

                return txt.ToUpper()
            }

        model, Cmd.ofAsync asyncUpper txt OnUpperResult OnUpperError

    | OnUpperResult result -&gt;
        { model with UpperValue = result }, Cmd.none

    | OnUpperError error -&gt;
        Browser.console.error error
        model, Cmd.none
<br>Use Cmd.ofFunc to evaluate a simple function and map the result into success or error (of exception).<br>
使用 Cmd.ofFunc 评估一个简单的函数并将结果映射为成功或错误（异常）。<br>type Model =
    { CurrentRoute : Route
      Value : string
      Tick : int
      UpperValue : string }

type Msg =
    | Save of string
    | OnSaveSuccess of bool
    | OnSaveError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | Save txt -&gt;
        let save (txt : string) =         
            // If there is an error, an exception will be thrown and captured by Cmd.ofFunc
            Browser.localStorage.setItem("my-app.input", txt)
            true

        model, Cmd.ofFunc save txt OnSaveSuccess OnSaveError

    | OnSaveSuccess result -&gt;
        // Here we can notify the user that the save succeeded
        model, Cmd.none

    | OnSaveError error -&gt;
        // Here we can notifu the user that the save failed
        Browser.console.error error
        model, Cmd.none
<br>UseCmd.performFunc to evaluate a simple function and map the success to a message discarding any possible error.<br>
使用 Cmd.performFunc 评估一个简单的函数，并将成功映射到一条消息，丢弃任何可能的错误。<br>type Model =
    { IsLoading : bool
      Value : string }

type Msg =
    | Load
    | OnLoadSuccess of bool
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | Load -&gt;
        // This function can never fail so we can use Cmd.performFunc
        let load () =
            let storedValue : string = Browser.localStorage.getItem("my-app.input") :?&gt; string
            if isNull storedValue then
                ""
            else
                storedValue

        { model with IsLoading = true }, Cmd.performFunc load () OnLoadSuccess

    | OnLoadSuccess value -&gt;
        { model with IsLoading = false
                     Value = value }, Cmd.none
<br>UseCmd.attemptFunc to evaluate a simple function and map the error (in case of exception)<br>
使用 Cmd.attemptFunc 计算一个简单的函数并映射错误（如果出现异常）<br>type Model =
    { Value : string }

type Msg =
    | ChangeValue of string
    | OnLogError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | ChangeValue newValue -&gt;
        let log (msg : string) =
            // Send a msg to your logger
            // If there is an error it will throw
            Log.send msg

        let msg = sprintf "Value changed from %s to %s" model.Value newValue        

        { model with Value = newValue }, Cmd.attemptFunc log msg OnLogError

    | OnLogError value -&gt;
        // There was an error during logging, should we do something ?
        model, Cmd.none
<br>Use Cmd.ofPromise to call promise block and map the results.<br>
使用 Cmd.ofPromise 调用 promise 块并映射结果。<br>type Model =
    { Value : int }

type Msg =
    | Submit
    | OnPromiseSuccess of int
    | OnPromiseError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | Submit -&gt;
        let myPromise () =
            promise {
                do! Promise.sleep 1000
                return 10
            }

        model, Cmd.ofPromise myPromise () OnPromiseSuccess OnPromiseError

    | OnPromiseSuccess value -&gt;
        { model with Value = value }, Cmd.none

    | OnPromiseError error -&gt;
        Browser.console.error error
        model, Cmd.none
<br>Use Cmd.ofSub to call the subscriber. This is useful when you are dealing with an API which use callbacks or to listen to an event.<br>
使用 Cmd.ofSub 呼叫订阅者。当您处理使用回调或侦听事件的 API 时，这非常有用。<br>type Model =
    { Position : Browser.Position option
      ErrorMessage : string }

type Msg =
    | GetPosition
    | GetPositionSuccess of Browser.Position
    | GetPositionError of Browser.PositionError

module Sub =
    // Subscriber used to reach gealocation API
    let getPosition onSuccess onError dispatch =
        Browser.navigator.geolocation.getCurrentPosition(
          onSuccess &gt;&gt; dispatch,
          onError &gt;&gt; dispatch
        )

let update (msg:Msg) (model:Model) =
    match msg with
    | GetPosition -&gt;
        model, Cmd.ofSub (Sub.getPosition GetPositionSuccess GetPositionError)
    
    | GetPositionSuccess posision -&gt;
        { model with Position = Some position
                     ErrorMessage = "" }, Cmd.none
    
    | GetPositionError error -&gt;
        { model with Position = None
                     ErrorMessage = error.message }, Cmd.none
<br>Use Cmd.batch to aggregate multiple commands. You can aggregate any of the previous commands together.<br>
使用 Cmd.batch 聚合多个命令。您可以将前面的任何命令聚合在一起。<br>type Model =
    { Data1 : int
      Data2 : int }

type Msg =
    | FetchDatas
    | OnFetchData1 of int
    | OnFetchData2 of int
    | OnPromiseError of exn
    | OnLogError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | FetchDatas -&gt;
        model, Cmd.batch [ Cmd.ofPromise fetchData1 () OnFetchData1 OnPromiseError 
                           Cmd.ofPromise fetchData2 () OnFetchData2 OnPromiseError 
                           Cmd.performFunc log "FetchDatas message has been treated" OnLogError ]

    | OnFetchData1 data1 -&gt;
        { model with Data1 = data1 }, Cmd.performFunc log "FetchData1 data has been received" OnLogError

    | OnFetchData2 data2 -&gt;
        { model with Data2 = data2 }, Cmd.performFunc log "FetchData2 data has been received" OnLogError

    | OnPromiseError error -&gt;
        Browser.console.error("An error occured when fetching data", error)
        model, Cmd.none

    | OnLogError error -&gt;
        Browser.console.error("An error occured when logging", error)
        model, Cmd.none
<br>The functions provided out of the box by Elmish are enough to cover most of the needs of your application. However, when working on a library or with a specific API you could want to create your own commands. We will take a look at it later.<br>
Elmish 提供的开箱即用的功能足以满足您应用程序的大部分需求。但是，当使用库或特定 API 时，您可能想要创建自己的命令。我们稍后会看一下。<br><br>根据您的需求对模型进行建模<br>When working with a Single Page Application, you will have to deal with navigation and need to reflect it in your models.<br>
使用单页应用程序时，您将必须处理导航并需要将其反映在模型中。<br>In general, when I am working with at a page level, I use Discrimination Union so I can store only the active page state.<br>
一般来说，当我在页面级别工作时，我使用歧视联盟，这样我就可以只存储活动页面状态。<br>
For example, if in your application you have a login page you can’t store your authenticated page state in your application because you can’t fetch the resource to display it.<br>
例如，如果您的应用程序中有一个登录页面，您无法在应用程序中存储经过身份验证的页面状态，因为您无法获取资源来显示它。<br>In general my main Model looks like this:<br>
一般来说，我的主要 Model 看起来像这样：<br>
[&lt;RequireQualifiedAccess&gt;]
type AuthPage =
    // Section for the website related to Messages management
    // For example
    // - Display the list of Messages
    // - Create a message
    | Messages of Messages.Model
    // Administration section of the website
    | Administration of Administration.Model

[&lt;RequireQualifiedAccess&gt;]
type Page =
    // The application is loading, display a loader
    | Loading
    // Login page
    | Login of Login.Model
    // From here, the user need to be authenticated
    | AuthPage of AuthPage
    // Global error page like:
    // - Generic error
    // - You need to be connected
    | Errored of Errored.Reason
    // A page that can be accessed when the user is not logged in
    // For example, to reset it's password from an URL
    | ResetAccount of ResetAccount.Model

type Model =
    { // Store the user session information
      // If Some xxx, then the user is signed-in
      // If None, the user isn't connected
      Session : User option
      // Store the active page state
      ActivePage : Page
      // Store the current route information, this is useful if your
      // route have parameters than you need to pass to child later
      CurrentRoute : Router.Route option }```

Using DUs to represent your pages state helps you isolate your logics. For me, the biggest benefit compared to storing all page states in a big record, is that you are not caching your page when navigating.  
使用 DU 表示页面状态可以帮助您隔离逻辑。对我来说，与将所有页面状态存储在大记录中相比，最大的好处是您在导航时不会缓存页面。

Of course sometimes your application needs to maintain state between navigation. For example, if you have a form in several steps you could modelize it like this:  
当然，有时您的应用程序需要在导航之间维护状态。例如，如果您有一个分几个步骤的表单，您可以像这样对其进行建模：
```F#
type Rank =
    | One
    | Two

type Step1 =
    { Login : string
      Password : string }

type Step2 =
    { Age : int 
      Surname : string 
      Firstname : string }

type Model =
    { CurrentRank : Rank
      Step1 : Step1
      Step2 : Step2 }

let private view (model : Model) (dispatch : Msg -&gt; unit) =
    match model.CurrentRank with
    | Rank.One -&gt; // render step n°1 form
    | Rank.Two -&gt; // render step n°2 form
<br><br>One thing we tend to forget when working with Elmish is that everything is a function. The main reason for this forgetfulness is that most of the examples only show you this code:<br>
使用 Elmish 时我们容易忘记的一件事是一切都是函数。造成这种遗忘的主要原因是大多数示例仅向您展示以下代码：<br>type Model =
    { Value : int }

type Msg =
    | SomeAction

let init =
    { Value = 0 }, Cmd.none

let update (msg : Msg) (model : Model) : Model * Cmd&lt;Msg&gt; =
    match msg with
    | SomeAction -&gt;
        model, Cmd.none

let view (model : Model) (dispatch : Msg -&gt; unit) : React.ReactElement =
    div [ ]
        [ str "This is my view" ]
<br>So in our mind update takes two arguments when in fact we should think update takes at least two arguments. The same applies to view and init functions.<br>
因此，在我们看来 update 需要两个参数，而实际上我们应该认为 update 至少需要两个参数。这同样适用于 view 和 init 函数。<br>This idea is linked to the fact that a component’s Model should have all the information needed by the component, but this is not always true.<br>
这个想法与组件的 Model 应该拥有组件所需的所有信息有关，但这并不总是正确的。<br><br>将数据作为参数传递<br>For example, if you have a session in your application in order to make HTTP calls, should you store this session in each component’s Model ?<br>
例如，如果您的应用程序中有一个会话用于进行 HTTP 调用，您是否应该将此会话存储在每个组件的 Model 中？<br>
No, you should store it at one place and then pass your session to the functions that need it.<br>
不，您应该将其存储在一个位置，然后将会话传递给需要它的函数。<br>The next code illustrates this situation, we request a session argument in our init function because we need to make an authenticated http request using Http.Auth.* (custom module used, includes the session info in a request).<br>
下一个代码说明了这种情况，我们在 init 函数中请求 session 参数，因为我们需要使用 Http.Auth.* 发出经过身份验证的 http 请求（使用的自定义模块，包括会话信息要求）。<br>open Elmish
open Fable.Import
open Fable.PowerPack

type Session = 
    { Token : string
      UserId : int }

type Model =
    | Loading
    | Loaded of string
    | Errored

type Msg =
    | OnFetchDataSuccess of string
    | OnFetchDataError of exn

let private fetchData (session : Session) : JS.Promise&lt;string&gt; =
    promise {
        let! res = Http.Auth.postRecord "/api/get-data" session
        return! res.text()
    }

let init (session : Session) =
    Loading, Cmd.ofPromise (fetchData session) OnFetchDataSuccess OnFetchDataError

let private update (msg : Msg) (model : Model) =
    match msg with
    | OnFetchDataSuccess value -&gt;
        Loaded value, Cmd.none

    | OnFetchDataError error -&gt;
        Browser.console.error error
        Errored, Cmd.none
<br><br>将记录作为参数传递<br>If your view function takes several arguments you can use a record as an argument. It will force you to name the arguments and by doing so make your code easier to read and maintain over time.<br>
如果您的视图函数采用多个参数，您可以使用记录作为参数。它将迫使您命名参数，并通过这样做使您的代码随着时间的推移更易于阅读和维护。<br>type private SectionProps =
    { Icon : Fa.IconOption
      Count : int
      Label : string
      IsActive : bool
      ZoneColor : string }
  
let private renderSection (props : SectionProps) =
    // Render the view
    
let view (model : Model) =
    renderSection 
        { Icon = Fa.Solid.Cog
          Count = model.ZoneA.Value + model.ZoneB.Value
          Label = model.Zone.Name
          IsActive = model.IsActive
          ZoneColor = findColor model.Zone.ColorIndex }
<br>It will also make it easier to optimize your code for react using memoBuilder.<br>
它还将使使用 memoBuilder 更轻松地优化代码以进行反应<br>type private SectionProps =
    { Icon : Fa.IconOption
      Count : int
      Label : string
      IsActive : bool
      ZoneColor : string }
  
let private renderSection = 
    memoBuilder "Section" (fun (props : SectionProps) -&gt;
        // Render the view
    )
        
let view (model : Model) =
    renderSection 
        { Icon = Fa.Solid.Cog
          Count = model.ZoneA.Value + model.ZoneB.Value
          Label = model.Zone.Name
          IsActive = model.IsActive
          ZoneColor = findColor model.Zone.ColorIndex }
<br><br>使用辅助函数来处理您的域<br>For example, when an elmish component needs to fetch data from the server often, I like to show a loading animation. Here is how I handle it:<br>
例如，当一个精灵组件需要经常从服务器获取数据时，我喜欢显示加载动画。我是这样处理的：]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/tips-for-working-with-elmish.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Tips for working with Elmish.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 02:58:48 GMT</pubDate></item><item><title><![CDATA[UI programming with Elmish in FSharp]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <br><br>The "Model View Update" (MVU) architecture was made famous by the front-end programming language <a data-tooltip-position="top" aria-label="http://elm-lang.org/" rel="noopener nofollow" class="external-link" href="http://elm-lang.org/" target="_blank">Elm</a> and can be found in many popular environments like <a data-tooltip-position="top" aria-label="http://redux.js.org/" rel="noopener nofollow" class="external-link" href="http://redux.js.org/" target="_blank">Redux</a>. Today it's probably the most famous UI pattern in functional programming. The reason for this is that it clearly separates state changes from UI controls and provides a nice and easily testable way to create modular UIs. Especially for developers that are already familiar with concepts like CQRS or event sourcing many of the things in the MVU architecture will feel natural. Here's what the MVU pattern looks like:<br>
“模型视图更新”（MVU）架构因前端编程语言 Elm 而闻名，并且可以在 Redux 等许多流行环境中找到。如今，它可能是函数式编程中最著名的 UI 模式。原因是它清楚地将状态更改与 UI 控件分开，并提供了一种很好且易于测试的方法来创建模块化 UI。特别是对于已经熟悉 CQRS 或事件源等概念的开发人员来说，MVU 架构中的许多内容都会感觉很自然。 MVU 模式如下所示：<br><img src="https://www.compositional-it.com/wp-content/uploads/2019/09/elmish-1.png" referrerpolicy="no-referrer"><br>Elmish for F# is a project that provides the capability for client-side apps written in F# to follow the MVU architecture. It comes in multiple flavors, so that you can choose between a <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/react/" rel="noopener nofollow" class="external-link" href="https://fable-elmish.github.io/react/" target="_blank">React renderer</a> for HTML, <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/react/react-native.html" rel="noopener nofollow" class="external-link" href="https://fable-elmish.github.io/react/react-native.html" target="_blank">React Native renderer</a> for mobile apps and a <a data-tooltip-position="top" aria-label="https://github.com/Prolucid/Elmish.WPF" rel="noopener nofollow" class="external-link" href="https://github.com/Prolucid/Elmish.WPF" target="_blank">WPF renderer</a> for desktop applications. In this article we will explore its power and modularity by working through a simple example.<br>
Elmish for F# 是一个为用 F# 编写的客户端应用程序提供遵循 MVU 架构的功能的项目。它有多种风格，因此您可以在用于 HTML 的 React 渲染器、用于移动应用程序的 React Native 渲染器和用于桌面应用程序的 WPF 渲染器之间进行选择。在本文中，我们将通过一个简单的示例来探索它的强大功能和模块化性。<br><br>Let's start to illustrate Elmish and the MVU architecture with the very common HTML button counter sample. The following code shows a counter implemented in HTML and JavaScript.<br>
让我们开始用非常常见的 HTML 按钮计数器示例来说明 Elmish 和 MVU 架构。以下代码显示了用 HTML 和 JavaScript 实现的计数器。<br>&lt;html&gt;
    &lt;body&gt;
    &lt;button onclick="--counter; update();"&gt;-&lt;/button&gt;
    &lt;div id="counter"&gt;&lt;/div&gt;
    &lt;button onclick="++counter; update();"&gt;+&lt;/button&gt;
    &lt;script&gt;
        var counter = 0;

        function update() { 
        document.getElementById("counter").textContent = "" + counter;
        }

        update();
    &lt;/script&gt;
    &lt;/body&gt;
&lt;/html&gt;
<br>The code is very straight forward and works as intended, but it has a number of issues:<br>
该代码非常简单并且按预期工作，但它有许多问题：<br>
<br>We are mutating the global variable counter - almost always a dangerous thing to do<br>
我们正在改变全局变量 counter - 几乎总是一件危险的事情
<br>We are directly mutating a DOM element, coupling our "business logic" to the UI.<br>
我们直接改变 DOM 元素，将我们的“业务逻辑”耦合到 UI。
<br>We are referencing the DOM element with its name via a string, which is fragile and can lead to costly knock-on effects.<br>
我们通过字符串引用 DOM 元素及其名称，这是脆弱的，可能会导致代价高昂的连锁反应。
<br>We've embedded some "domain logic" directly in the onclick event<br>
我们直接在 onclick 事件中嵌入了一些“领域逻辑”
<br>These issues prevent us from using this in a modular way. For example, if we wanted to create a list with counters, we could not reuse this code. Instead, we could copy &amp; paste the code a couple of times to create a fixed number of counters, but even then we would need to be very careful that we fix all the references to the corresponding global variables. In the object-oriented world, there are number of patterns that allow you encapsulate this problem of shared mutable state - let's see how to do it in a manner that promotes some FP core practices using the three parts of the MVU pattern:<br>
这些问题阻止我们以模块化的方式使用它。例如，如果我们想创建一个带有计数器的列表，我们就不能重用这段代码。相反，我们可以复制并粘贴代码几次来创建固定数量的计数器，但即使如此，我们也需要非常小心地修复对相应全局变量的所有引用。在面向对象的世界中，有许多模式允许您封装共享可变状态的问题 - 让我们看看如何使用 MVU 模式的三个部分来促进一些 FP 核心实践：<br><br>Let's start with the Model.<br>
让我们从模型开始。<br>type Model = int

type Msg =
| Increment
| Decrement

let init() : Model = 0
<br>In this F# code we capture the current value of the counter in a domain type , before creating a message type which can signal that we want to increment or decrement the counter. We also implement an init function that allows us to create the initial model for our application.<br>
在此 F# 代码中，我们在创建消息类型之前捕获域类型中计数器的当前值，该消息类型可以表明我们想要递增或递减计数器。我们还实现了一个 init 函数，它允许我们为应用程序创建初始模型。<br><br>The View part deals with the question of displaying controls on the screen. This is where we need to decide on a UI framework; in our case we've decided to stick with HTML, and so we will use the React renderer.<br>
View 部分处理在屏幕上显示控件的问题。这是我们需要决定 UI 框架的地方；在我们的例子中，我们决定坚持使用 HTML，因此我们将使用 React 渲染器。<br>let view model dispatch =
    div []
        [ button [ OnClick (fun _ -&gt; dispatch Decrement) ] [ "-" ]
          div [] [ model.ToString() ]
          button [ OnClick (fun _ -&gt; dispatch Increment) ] [ "+" ] ]
<br>This is valid F# code that uses the excellent <a data-tooltip-position="top" aria-label="https://github.com/fable-compiler/fable-react" rel="noopener nofollow" class="external-link" href="https://github.com/fable-compiler/fable-react" target="_blank">Fable.React bindings</a> to convert from F# into React JS. The syntax is still similar to our HTML version from the beginning, but instead of &lt; and &gt;, we are using [ and ]. This syntax is easy to learn and IDE tools like <a data-tooltip-position="top" aria-label="http://ionide.io/" rel="noopener nofollow" class="external-link" href="http://ionide.io/" target="_blank">Ionide</a> provide code completion for it, so it's a natural fit for existing F# developers. An important observation is that we are no longer mutating state from within the OnClick handlers; instead we simply dispatch one of the earlier defined messages into the system. This decouples our model from the view.<br>
这是有效的 F# 代码，它使用出色的 Fable.React 绑定从 F# 转换为 React JS。语法从一开始仍然与我们的 HTML 版本类似，但我们使用 [ 和 ] 代替 &lt; 和 &gt; 。 。这种语法很容易学习，并且 Ionide 等 IDE 工具为其提供了代码补全，因此它非常适合现有的 F# 开发人员。一个重要的观察结果是，我们不再从 OnClick 处理程序内部改变状态；相反，我们只是将先前定义的消息之一发送到系统中。这将我们的模型与视图分离。<br><br>In the Update part, we define a state machine that represents our domain logic (or at least hooks into it).<br>
在更新部分，我们定义一个状态机来表示我们的域逻辑（或至少挂钩它）。<br>let update (msg:Msg) (model:Model) : Model =
    match msg with
    | Increment -&gt; model + 1
    | Decrement -&gt; model - 1
<br>Here, we have defined an update function that will be called by Elmish whenever a message is received. In this very basic scenario there are only two cases to handle, and we can't really see the power of F#'s pattern matching yet. The most interesting observation is that we don't rely on any mutable state. Instead the update function takes a message and the current model, and returns a completely new version of the model. Since all the data is simply inside the model, this is very well testable - we can easily write a set of unit tests around the update function.<br>
在这里，我们定义了一个 update 函数，每当收到消息时 Elmish 都会调用该函数。在这个非常基本的场景中，只有两种情况需要处理，而且我们还不能真正看到 F# 模式匹配的强大功能。最有趣的观察是我们不依赖任何可变状态。相反，更新函数接受消息和当前模型，并返回模型的全新版本。由于所有数据都位于模型内部，因此非常容易测试 - 我们可以轻松地围绕更新函数编写一组单元测试。<br><br>So far, we've not used any functionality from Elmish at all. The Model and Update parts are pure F# code, whilst the View part uses the <a data-tooltip-position="top" aria-label="https://github.com/fable-compiler/fable-react" rel="noopener nofollow" class="external-link" href="https://github.com/fable-compiler/fable-react" target="_blank">Fable.React</a> package. At this point all three parts are completely independent from each other - now Elmish will "wire" these up into an application:<br>
到目前为止，我们还没有使用 Elmish 的任何功能。模型和更新部分是纯 F# 代码，而视图部分使用 Fable.React 包。此时，所有三个部分完全相互独立 - 现在 Elmish 将把它们“连接”到一个应用程序中：<br>Program.mkSimple Counter.init Counter.update Counter.view
|&gt; Program.withConsoleTrace
|&gt; Program.withDebugger
|&gt; Program.withReact "counter-app"
|&gt; Program.run
<br>Elmish's <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/elmish/program.html" rel="noopener nofollow" class="external-link" href="https://fable-elmish.github.io/elmish/program.html" target="_blank">Program</a> abstraction provides "glue" functions like mkSimple to bind the three parts together into an application. Internally, Elmish gives us a message loop and takes care of dispatching messages between the view and the update function. Since the complete state is captured in the model and every state change is explicit by processing a message, this opens a whole new world of debugging features like <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/debugger/" rel="noopener nofollow" class="external-link" href="https://fable-elmish.github.io/debugger/" target="_blank">time travelling</a>.<br>
Elmish 的程序抽象提供了“粘合”函数，例如 mkSimple 将这三个部分绑定到一个应用程序中。在内部，Elmish 为我们提供了一个消息循环，并负责在视图和更新函数之间调度消息。由于模型中捕获了完整的状态，并且通过处理消息来明确每个状态更改，因此这打开了时间旅行等调试功能的全新世界。<br><br>亲子作文<br>In the last section we saw how to build a very basic sample app in the MVU architecture. For this minimal example it's not very clear what the benefit is compared to the original implementation that was using mutation. So, in the following example we'll use the counter as a module and create a list of counters.<br>
在上一节中，我们了解了如何在 MVU 架构中构建一个非常基本的示例应用程序。对于这个最小的示例，与使用突变的原始实现相比，并不清楚有什么好处。因此，在下面的示例中，我们将使用计数器作为模块并创建计数器列表。<br><br>As before let's start with the model:<br>
和之前一样，让我们​​从模型开始：<br>module CounterList
type Model = Counter.Model list

type Msg = 
| Insert
| Remove
| Modify of int * Counter.Msg

let init() : Model =
    [ Counter.init() ]
<br>In this case we have a list of counter models and a new message type. We can signal to:<br>
在本例中，我们有一个计数器模型列表和一个新消息类型。我们可以向以下人员发出信号：<br>
<br>Insert a new counter<br>
插入新计数器
<br>Remove the latest counter<br>
删除最新的计数器
<br>Dispatch a counter message to the corresponding counter in the list.<br>
向列表中相应的计数器发送计数器消息。
<br>When the application starts we will start with one counter as provided by the init function. As you can see, every part of the model refers to the corresponding part of the submodel. CounterList.Model is a collection of Counter.Model, the CounterList.Msg uses the Counter.Msg as payload and the CounterList.init function calls the Counter.init function.<br>
当应用程序启动时，我们将从 init 函数提供的一个计数器开始。正如您所看到的，模型的每个部分都引用子模型的相应部分。 CounterList.Model 是 Counter.Model 的集合， CounterList.Msg 使用 Counter.Msg 作为负载， CounterList.init 函数调用 Counter.init 函数。<br><br>Let's take a look at the View code:<br>
我们看一下 View 代码：<br>let view model dispatch =
    let counters =
        model
        |&gt; List.mapi (fun pos counterModel -&gt; 
            Counter.view
                counterModel
                (fun msg -&gt; dispatch (Modify (pos, msg)))) 

    div [] [ 
        yield button [ OnClick (fun _ -&gt; dispatch Remove) ] [ "Remove" ]
        yield button [ OnClick (fun _ -&gt; dispatch Insert) ] [ "Add" ]
        yield! counters ]
<br>It's not that much different to the view code of the counter itself, except now we render a list of counters and wrap the messages with position information. This code may look a bit unfamiliar at this point, but the F# compiler is helping us here. There is only one way to get it to compile - and that's the correct way! Once you've done this step a few times, it becomes second nature - just like any task that you become familiar with.<br>
它与计数器本身的视图代码没有太大区别，只是现在我们渲染计数器列表并用位置信息包装消息。这段代码此时看起来可能有点陌生，但 F# 编译器在这里为我们提供了帮助。只有一种方法可以编译它 - 这就是正确的方法！一旦您完成此步骤几次，它就会成为第二天性 - 就像您熟悉的任何任务一样。<br><br>As with the Model, we also see a nice symmetry in the Views. But what about the Update part?<br>
与模型一样，我们在视图中也看到了很好的对称性。但是更新部分呢？<br>let update (msg:Msg) (model:Model) =
    match msg with
    | Insert -&gt;
        Counter.init() :: model // append to list
    | Remove -&gt;
        match model with
        | [] -&gt; []              // list is already empty
        | x :: rest -&gt; rest     // remove from list
    | Modify (pos, counterMsg) -&gt;
        model
        |&gt; List.mapi (fun i counterModel -&gt;
            if i = pos then
                Counter.update counterMsg counterModel
            else
                counterModel) }
<br>Now this may come as no surprise, but we have the same situation here: CounterList.update forwards to Counter.update of the corresponding counter! We end up with something really beautiful, since now we have a CounterList component which exposes exactly the same elements as the Counter itself, namely Model, View and Update. This allows us to use the CounterList itself as a component!<br>
现在这可能并不奇怪，但我们这里有同样的情况： CounterList.update 转发到相应计数器的 Counter.update ！我们最终得到了一些非常漂亮的东西，因为现在我们有一个 CounterList 组件，它公开了与 Counter 本身完全相同的元素，即模型、视图和更新。这允许我们使用 CounterList 本身作为组件！<br><br>F# - 非常适合 MVU 架构<br>The MVU architecture was made famous by the Elm language, but can be used in many languages, even in vanilla <a data-tooltip-position="top" aria-label="https://github.com/ccorcos/elmish" rel="noopener nofollow" class="external-link" href="https://github.com/ccorcos/elmish" target="_blank">JavaScript</a>. F# - like Elm - is a language in the ML family of programming language, and provides features such as pattern matching that are extremely powerful and particularly useful for modelling state machines. In conjunction with F# features such as <a data-tooltip-position="top" aria-label="https://fsharpforfunandprofit.com/posts/discriminated-unions/" rel="noopener nofollow" class="external-link" href="https://fsharpforfunandprofit.com/posts/discriminated-unions/" target="_blank">Discriminated Unions</a>, this allows us to create applications in a type-safe manner that cater for all possibilities. The compiler provides us with guidance when corner cases are not dealt with, leading to quicker development time, whilst allowing F# developers to rapidly create UIs in a typesafe manner. The larger and more complex an application becomes, the greater the benefit; the F# compiler (like the Elm compiler) emits warnings for us when we forget to handle all possibilites - even for more complicated patterns - meaning less time spent debugging or writing unit tests, and more time delivering business value.<br>
MVU 架构因 Elm 语言而闻名，但可以在多种语言中使用，甚至可以在普通 JavaScript 中使用。 F# 与 Elm 一样，是 ML 编程语言系列中的一种语言，提供模式匹配等功能，这些功能非常强大，对于状态机建模特别有用。与受歧视联合等 F# 功能相结合，这使我们能够以类型安全的方式创建应用程序，以满足所有可能性。当未处理极端情况时，编译器为我们提供指导，从而缩短开发时间，同时允许 F# 开发人员以类型安全的方式快速创建 UI。应用程序变得越大、越复杂，好处就越大；当我们忘记处理所有可能性时（即使是更复杂的模式），F# 编译器（如 Elm 编译器）会向我们发出警告，这意味着花在调试或编写单元测试上的时间更少，而有更多时间提供业务价值。<br><br>For more information about Elmish and the MVU architecture please see the following resources:<br>
有关 Elmish 和 MVU 架构的更多信息，请参阅以下资源：<br>
<br>"Modern app development with Fable and React Native" from NDC Oslo 2017 - <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=fmaPeUBWZuM" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=fmaPeUBWZuM" target="_blank">Video</a><br>
“使用 Fable 和 React Native 进行现代应用程序开发”来自 NDC Oslo 2017 - 视频
<br>"The Elm Architecture" in the <a data-tooltip-position="top" aria-label="https://guide.elm-lang.org/architecture/" rel="noopener nofollow" class="external-link" href="https://guide.elm-lang.org/architecture/" target="_blank">elm docs</a><br>
elm 文档中的“Elm 架构”
<br>Elmish for F# <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/" rel="noopener nofollow" class="external-link" href="https://fable-elmish.github.io/" target="_blank">docs</a>&nbsp;F# 文档的 Elmish
<br><a data-tooltip-position="top" aria-label="https://github.com/ccorcos/elmish" rel="noopener nofollow" class="external-link" href="https://github.com/ccorcos/elmish" target="_blank">Elmish for vanilla JavaScript
用于原生 JavaScript 的 Elmish</a><br>
<br>Fable compiler <a data-tooltip-position="top" aria-label="http://fable.io/" rel="noopener nofollow" class="external-link" href="http://fable.io/" target="_blank">docs</a>&nbsp;Fable 编译器文档
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/ui-programming-with-elmish-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/UI programming with Elmish in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 20 Dec 2024 02:58:55 GMT</pubDate><enclosure url="https://www.compositional-it.com/wp-content/uploads/2019/09/elmish-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.compositional-it.com/wp-content/uploads/2019/09/elmish-1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Side effects]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <br><br>Side effects are impossible to avoid in imperative code, but they can make reasoning about the behavior of a program very difficult. F# allows us to use imperative side effects, but it's often better not to. How can we avoid side effects while still implementing effectful requirements?<br>As an example, let's take a very simple and common effectful example: logging. Say we're writing a function that computes the length of the hypotenuse of a right triangle from the lengths of the other two sides:  <br>let hypotenuse a b =
    printfn "Side a: %g" a
    printfn "Side b: %g" b
    let c = sqrt &lt;| (a*a + b*b)
    printfn "Side c: %g" c
    c
<br>Every time this function is called, we log the input and output to the console as a side effect. This is fine in a single-threaded application, but what happens if hypotenuse is called simultaneously from two different threads? That's trouble.<br>A common approach in the imperative/OO world is to use dependency injection (or plain old interfaces) to separate the logging API from its implementation. However, the resulting code still causes side effects. Is it possible to define an effectful hypotenuse function that doesn't have any side effects? It sounds almost like a contradiction in terms, but it can be done, and the solution is very interesting.<br><br>The approach is to explicitly declare effects using an Effect type:  <br>type Effect&lt;'result&gt; =
    | Log of string * (unit -&gt; Effect&lt;'result&gt;)
    | Result of 'result
<br>In this simple example, there are only two effects:<br>
<br>Log is what we use to write a string to a logger. This constructor takes an additional continuation function that is to be executed after the string is logged.
<br>Result simply holds a value and doesn't cause an effect.
<br>Note that this type is an example of the <a data-tooltip-position="top" aria-label="https://dev.to/shimmer/monads-for-free-in-f-30dl" rel="noopener nofollow" class="external-link" href="https://dev.to/shimmer/monads-for-free-in-f-30dl" target="_blank">free monad</a>. Log corresponds to the Free constructor and Result corresponds to Pure. The free monad is useful here because it can chain effects together. For example, we could rewrite our calculation like this:  <br>let hypotenuse a b =
    Log ((sprintf "Side a: %g" a), fun () -&gt;
        Log ((sprintf "Side b: %g" b), fun () -&gt;
            let c = sqrt &lt;| (a*a + b*b)
            Log ((sprintf "Side c: %g" c), fun () -&gt;
                Result c)))
<br>It's important to understand that this version of the function returns an Effect&lt;float&gt; rather than a float itself. You can think of this type as an "effectful" computation that will return a float when it is executed. However, until it is executed it does nothing - in particular, it has no side effects. It simply defines a computation.<br><br>In order to actually compute a result, we need some additional code that can "handle" our effects, just like an exception handler handles exceptions (which are also a kind of effect). Let's write a handler that accumulates log messages in a list while performing a calculation:  <br>let handle effect =

    let rec loop log = function
        | Log (str, cont) -&gt;
            let log' = str :: log
            loop log' (cont ())
        | Result result -&gt; result, log

    let result, log = loop [] effect
    result, log |&gt; List.rev
<br>When we pass an effectful computation to handle, we get two things back: the final result of the computation, and a list of all the log messages that were written during the computation:  <br>let c, log =
    hypotenuse a b
        |&gt; handle
<br>Note that handle is also a pure function - it doesn't write anything to the console or perform any other side effect. If we want, we could then write the resulting log to the console, but we'd have to be careful at that point to consider the actual side effects involved. The important thing is that we've successfully separated a pure functional calculation from the impure side effects of writing a log to the console. By solving those two problems separately, we've made it much easier to understand how our program behaves.<br><br>Of course, no one wants to write ugly nested Log invocations like this because they completely distract from the logic of the computation itself. Fortunately, we know that the free monad can help us here by providing a workflow:  <br>let rec bind f = function
    | Log (str, cont) -&gt;
        Log (str, fun () -&gt;
            cont () |&gt; bind f)
    | Result result -&gt; f result

type EffectBuilder() =
    member __.Return(value) = Result value
    member __.Bind(effect, f) = bind f effect

let effect = EffectBuilder()
<br>This is the standard bind implementation for the free monad: it simply passes the binding function down the chain until the end, at which time the two effects are bound together by applying the function.<br>We also need some helper functions that "lift" a log string into the monad:  <br>let log str = Log (str, fun () -&gt; Result ())
let logf fmt = Printf.ksprintf log fmt
<br>Again, this follows the same pattern we've seen before with the free monad. With these tools in hand, we can rewrite our computation much more elegantly:  <br>let hypotenuse a b =
    effect {
        do! logf "Side a: %g" a
        do! logf "Side b: %g" b
        let c = sqrt &lt;| (a*a + b*b)
        do! logf "Side c: %g" c
        return c
    }
<br>This version of the function produces an Effect&lt;float&gt; that is identical to the previous one. It's just much easier to understand, and is essentially no more complex than the original version of hypotenuse that wrote directly to the console.<br><br>We can easily support additional effects by adding union cases to our Effect type. However, this sort of master list of all effects in a system isn't very practical. Ideally, we'd like to modularize effects so that they can be composed together. For example, we'd like to be able to handle log effects separately from exception effects and separately from stateful effects. Unfortunately, this isn't particularly easy to do in F# yet, but there is a library called <a data-tooltip-position="top" aria-label="https://github.com/palladin/Eff" rel="noopener nofollow" class="external-link" href="https://github.com/palladin/Eff" target="_blank"><code></code></a>Eff that serves as a proof of concept. (I wouldn't use it in production, though, because the handlers are rather ugly.)<br>In the future, I expect that algebraic effects will become mainstream, and support for explicit effect types will be baked into both functional and imperative languages. That's still several years away, though, but at least now you know it's (probably) coming.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/why-algebraic-effects-matter-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Why algebraic effects matter in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 16 Nov 2024 13:07:59 GMT</pubDate></item><item><title><![CDATA[Writing high performance Fsharp code]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener nofollow">#fsharp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>While this post is addressed to F# .NET developers, it introduces much wider concepts starting from hardware architecture to overall .NET runtime and JIT compiler optimizations. It shouldn't be a surprise - optimizing the application performance requires us to understand the relationships between our high level code and what actually happens on the hardware.<br>There's a popular opinion that F# code must be slower than equivalent C# code. This opinion is mostly false, however it comes with some rationale. Usually comparison doesn't use equivalent code in both languages, and F# is generally more high level and declarative in nature. "Idiomatic" F# code doesn't always play well with .NET virtual machine. Writing code that is high level, declarative and fast on .NET platform is not an easy task.<br>In the examples below we'll use some common tools that will help us get better insight into nature of F# code:<br>
<br><a data-tooltip-position="top" aria-label="https://sharplab.io/?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://sharplab.io/?ref=bartoszsypytkowski.com" target="_blank">Sharplab</a> allows us to easily inspect generated JIT intermediate representation, assembly or even equivalent C# code (which sometimes is approximate, since not all IL idioms are representable in C#) for a given F# snippet. For assembly code usually some extra mangling with params may be necessary for code to be generated as SharpLab sometimes cannot introspect F# core lib code.
<br><a data-tooltip-position="top" aria-label="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" target="_blank">Object Layout Inspector</a> lets us see how structs and classes will actually be represented in memory.
<br><a data-tooltip-position="top" aria-label="https://benchmarkdotnet.org/articles/overview.html?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://benchmarkdotnet.org/articles/overview.html?ref=bartoszsypytkowski.com" target="_blank">BenchmarkDotNet</a> is very popular library for writing micro benchmarks. We'll use it to show heap allocations and execution times of our code.
<br>A correct profiling of the executing application is crucial before starting any work on optimizing the code - there's no sense in shaving last possible CPU cycles out of the function that's executed for 0.1% of the time.<br>Keep in mind that for most day-to-day business applications, first way to solve performance problems is to reduce obvious mistakes (eg. replacing multiple I/O requests with one, writing more efficient database query etc.). If that was not the case, next step for satisfactory solution can be simply writing more imperative code - to make it easier to reason about for the compiler rather than human - or picking better-suited data structures. This is especially prevalent in F#, where we can observe pervasive usage of types like 't list (tip: if you're looking which collection to use and want good performance, F# list is almost never a good answer). Here we're about to go deeper, into area where we're about to compete with the prefabricated data types and algorithms.<br><br>One of the big performance gains, that .NET runtime uses to take advantage over other managed virtual machines (like JVM) in race for ultimate performance, often comes from using value types. So if we're about to go fast, we first need to understand how they work.<br>.NET structs represent types, which are not allocated separately on the managed memory heap, but rather inlined within the containing scope (instance of the class in case of fields, thread stack for variables, etc.). This means that usually they are cheaper and easier to access in high-allocation scenarios.<br><img alt="class-vs-struct-layout" src="https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-layout.png" referrerpolicy="no-referrer"><br>Historically, F# code was not very promising, when it comes to utilizing value types. Nowadays we got things like struct tuples - struct('a * 'b) which unfortunately are not widely used in F# even though in practice they should be preferable choice when working with tuples - and [&lt;Struct&gt;] attribute, that can be used on records and discriminated unions (we'll return to them later), making use of them became much more feasible.<br>However this doesn't necessarily mean, that replacing all reference types with value types will make our code magically go faster. In fact, this may be quite opposite. Why? Imagine what happens when we want to pass a record as a parameter. How is it done? Usually passing object to a function happens by copying reference to that object, which is either 4B or 8B depending on our OS being x86 or x64, and therefore fits perfectly into standard CPU register.<br>type A() =
  class
    [&lt;DefaultValue&gt;] val mutable x: int
    [&lt;DefaultValue&gt;] val mutable y: int
    [&lt;DefaultValue&gt;] val mutable z: int
  end

let print (value: 't) = System.Console.WriteLine(value.ToString())

let a = A() // sub rsp, 0x28
            // mov rcx, 0x7ff91b23d1a8
            // call 0x00007ff9730aade0 ; allocate A on the heap
print a     // mov rdx, rax            ; copy reference to a to the stack
            // mov rcx, 0x7ff91b23d5f8
            // call _.print[[System.__Canon, System.Private.CoreLib]](System.__Canon)
<br>Now what if we're using structs? For reference types we copy object's reference on the stack - since reference is just a single address, it always can fit into register and be done within a single operation. For value types, we copy entire value instead. If they don't fit into register, we'll have to copy them over in multiple steps.<br>type B =
  struct
    [&lt;DefaultValue&gt;] val mutable x: int
    [&lt;DefaultValue&gt;] val mutable y: int
    [&lt;DefaultValue&gt;] val mutable z: int
  end

let b = B() // sub rsp, 0x38
            // xor eax, eax       ; zero field b.x
            // xor ecx, ecx       ; zero field b.y
            // xor edx, edx       ; zero field b.z
print b     // lea r8, [rsp+0x28]
            // mov [r8], ecx      ; copy field b.x to the stack
            // mov [r8+4], eax    ; copy field b.y to the stack
            // mov [r8+8], edx    ; copy field b.z to the stack
            // lea rcx, [rsp+0x28]
            // call _.print[[_+B, _]](B)
<br>Each of these steps is a machine instruction that takes time to execute. However, sometimes .NET can optimize that - pointer-sized registers are not only ones available in modern machines. We also have a special purpose SIMD (Single Instruction Multiple Data) ones, that are much bigger and can be used as long as passed data fits into them perfectly.<br>type C =
  struct
    [&lt;DefaultValue&gt;] val mutable x: int
    [&lt;DefaultValue&gt;] val mutable y: int
    [&lt;DefaultValue&gt;] val mutable z: int
    [&lt;DefaultValue&gt;] val mutable zz: int
  end
  
let c = C() // sub rsp, 0x48
            // xor eax, eax             ; zero register
            // mov [rsp+0x38], rax      ; init fields b.x and b.y together with zero'ed register
            // mov [rsp+0x40], rax      ; init fields b.z and b.zz together with zero'ed register
print c     // vmovupd xmm0, [rsp+0x38] ; copy all 4 fields together on the stack using SIMD registers
            // vmovupd [rsp+0x28], xmm0
            // lea rcx, [rsp+0x28]
            // call _.print[[_+C, _]](C)
<br>Another thing available in .NET, that allows us addressing inefficiencies of passing structs as arguments are so called by-ref parameters. There are 3 types of these, marked using 't inref, 't outref and 't byref:<br>let print(value: 't inref) = ...
let c = C()
print &amp;c // lea rcx, [rsp+0x28] ; copy address of the struct head onto the stack
         // call _.print[[_+C, _]](C ByRef)
<br>Please, don't confuse by-ref parameters with ref data type:<br>
<br>'a ref is actually an alias for Ref&lt;'a&gt; class, therefore allocated on the heap and passed by reference. In general, using this class in F# very rarely has sense (outside of writing exemplar, idiomatic code).
<br>'a byref is equivalent to C# ref parameter tag - it means that we're passing reference (memory address) to an object or struct. It expects it to be initialized and can be used to change the contents of the underlying value. For this reason F# requires fields and variables passed as byref to be declared with mutable keyword.
<br>'a outref is equivalent to C# out parameter tag - it always must be initialized by the end of the function body. This may sound a bit tricky as F# doesn't put that requirement explicitly. If we didn't make that assignment in any of the code branches, F# compiler will simply initialize it for us with default value (just like using Unchecked.defaultof&lt;_&gt;), which sometimes may lead to null reference exceptions.
<br>'a inref is the youngest of these and is equivalent of C# in parameter - while in C# structs passed as arguments for that parameters don't have to be tagged, F# will always require to mark passing by ref (using &amp; prefix for passed argument) for any parameter marked with byref/inref/outref. inref is basically an optimization technique for what we saw above - it allows us to pass struct into a function using only its memory address, without copying entire struct contents. Additionally inref says that parameter is treated as read only, so it cannot be modified inside of function body. .NET JIT can utilize this information in some cases to reduce number of safety checks, therefore reducing number of instructions to be executed.
<br>While using by-ref parameters is usually good idea when it comes to writing code targeting complex value types, there are several limitations to it.<br>One is that arguments passed using by-ref params cannot be captured by closures/lambdas/anonymous functions, which prevents them from being used in more abstract code:<br>// WRONG!
let doSomething (a: int inref) =
  [1..10]
  |&gt; List.map (fun x -&gt; x + a) // `a` is captured by closure, which is compilation error
  
// RIGHT
let doSomething (a: int inref) =
  let mutable result = []
  for x=10 downto 1 do
    result &lt;- (x + a)::result
  result
<br>This includes problems even for common inlined functions like eg. pipe operator |&gt;. That's the price, we have to pay for speed (at least for now).<br>Second issue is that, at the moment by-ref parameters cannot be involved in building nested functions (regardless if they capture the values from function in outer scope or not). This again makes very inconvenient to use them in cases like tail recursive loop pattern:<br>// WRONG!
let doSomething (a: 'a) =
  let rec loop n (x: 'a inref) = // this nested function won't compile
    if n = 0 then ()
    else loop (n-1) &amp;x
  loop 100 &amp;a

// RIGHT
let rec loop n (x: 'a inref) =
  if n = 0 then ()
  else loop (n-1) &amp;x
  
let doSomething (a: 'a) = loop 100 &amp;a
<br><br>While we talked about by-ref params, .NET (and latest F#) enable us to do something more - we can define so called by-ref structs and readonly structs:<br>[&lt;Struct; IsByRefLike; IsReadOnly&gt;]
type BufWriter&lt;'a&gt; =
  // since BufWriter is by-ref struct it can have by-ref types as fields
  // otherwise it would result in compilation error
  [&lt;DefaultValue&gt;] val buffer: ReadOnlySpan&lt;'a&gt;
  
/// F# records and discriminated unions are marked with IsReadOnly by default
[&lt;Struct; IsByRefLike&gt;]
type BufWriter&lt;'a&gt; = { Buffer: ReadOnlySpan&lt;'a&gt; }
<br>[&lt;IsReadOnly&gt;] attribute let's us define given structure as being readonly. For obvious reasons this also means, that corresponding data type cannot contain any mutable fields within.<br>It's used as a slight optimization technique - sometimes .NET JIT compiler must guarantee that structs contents will not be modified. To do so, it will conservatively copy that structure, even when it has been passed into function using inref parameter. If struct has been marked with [&lt;IsReadOnly&gt;] attribute, compiler can skip this step and avoid building defensive copies. You can read more about it <a data-tooltip-position="top" aria-label="https://devblogs.microsoft.com/premier-developer/avoiding-struct-and-readonly-reference-performance-pitfalls-with-errorprone-net/?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://devblogs.microsoft.com/premier-developer/avoiding-struct-and-readonly-reference-performance-pitfalls-with-errorprone-net/?ref=bartoszsypytkowski.com" target="_blank">here</a>.<br>[&lt;IsByRefLike&gt;] is another attribute. We are talking a lot about passing value types using memory location addresses instead of doing deep copies. Marking struct using this attribute is basically saying "I always want to pass this value by reference". This of course comes with severe limitations: it cannot be boxed (moved to managed heap) and for this reason it can never be captured by closures, implement interfaces or be used as field in classes or other non-by-ref structs.<br>In terms of F# this basically means that this kind of structs are used mostly for code that is executed right away within the function body, with no computation expressions or other indirections. This usually qualifies them to hot paths in our code, where CPU intensive work is expected and allocations are not welcome, like:<br>
<br>for .. in loops - in fact many moderns .NET structures have special variants of GetEnumerator that doesn't allocate any memory and is implemented as by-ref struct. F# also understands that pattern - in fact you can define custom GetEnumerator(): MyEnumerator method for your collection, with MyEnumerator - which can even be a ref struct - having two methods: Current: 'item and MoveNext: unit -&gt; bool, and F# will automatically understand how to use it in loops. You can see an example implementation of it <a data-tooltip-position="top" aria-label="https://github.com/Horusiath/fsharp.core.extensions/blob/62b102e84325e89b0a6c4065b973936c11adee55/src/FSharp.Core.Extensions/Vec.fs?ref=bartoszsypytkowski.com#L147" rel="noopener nofollow" class="external-link" href="https://github.com/Horusiath/fsharp.core.extensions/blob/62b102e84325e89b0a6c4065b973936c11adee55/src/FSharp.Core.Extensions/Vec.fs?ref=bartoszsypytkowski.com#L147" target="_blank">here</a> - it's a part of implementation of persistent vector data type, similar to <a data-tooltip-position="top" aria-label="https://fsprojects.github.io/FSharpx.Collections/PersistentVector.html?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://fsprojects.github.io/FSharpx.Collections/PersistentVector.html?ref=bartoszsypytkowski.com" target="_blank">FSharpX persistent vector</a>, but it's 4.5 times faster and not allocating anything on heap when executed in loops.
<br>Contextual data around byte-shaving operations. All things related to parsing/formatting can make use of that technique to optimize speed and reduce allocations. It's also used inside of all kinds of drivers working with I/O.
<br>While we're using explicit class/struct type definition here, from .NET runtime point of view memory layout for records and struct records is exactly the same (it differs for discriminated unions thou, but we'll cover that soon).<br><br>Another point worth noticing is that .NET have it's own assumptions regarding data size of classes. Let's see that on an example:<br>type A = { x: int; y: int }
type B = { x: int; y: int; z: int }
<br>How do you think, what's the size of A and B? Naively, we could assume that B instance would be 4 bytes bigger than instance of type A. However that's not always true. Let's inspect memory layout of both classes:<br><img alt="padding-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/padding-1.png" referrerpolicy="no-referrer"><br>As you can see both classes start with 16 byte object header and vtable pointer: it's mandatory for every class (and boxed structs). They make things like method overriding or lock calls on objects possible. Then we have actual class content: 2 * sizeof(int) = 8 bytes in case of A and 3 * sizeof(int) = 12 bytes in case of B. However that's not the end. In case of B you can also see 4 extra bytes of padding. Where does it comes from?<br>When managing heap size, .NET GC/allocator makes some simplifications. Namely it assigns blocks of memory that are multiplications of a standard pointer size, which is 4 bytes on 32-bit OS'es and and 8 bytes on 64-bit ones. So, when instantiating objects, GC will always assign them as much space as it's necessary to encapsulate all fields and fit into 4-/8-bytes ceiling: since most servers operate on 64-bits nowadays, we're talking about buckets of 16+8 bytes, 16+16 bytes, 16+24 bytes etc.<br>What's interesting, this padding requirement doesn't concern unmanaged structs (value types consisting only of other value types). If we modify our record B to be a struct:<br>[&lt;Struct&gt;] type B = { x: int; y: int; z: int }
<br>, we'll see that it takes only 12 bytes. If we take into account object header, that's over 2.5 less space than in case of class-based record, with no heap allocations and therefore no need to GC it later. Keep in mind that adding a reference type (eg. string) as struct field will cause it to add padding again. In that case the space saving comes from lack of object header/vtable pointer.<br>Now, if necessary we could also apply padding to structs manually. While in eg. Java you need to add redundant extra fields to do that, in .NET we can hint the runtime about the expected struct size:<br>[&lt;Struct; StructLayout(LayoutKind.Auto, Size=16)&gt;] 
type B = { x: int; y: int; z: int }
<br>StructLayout has many useful properties i.e. it opens the door to manually define the position of each record field within the type. It also exposes the Size property, which we can use to manually say what's the expected size of our struct - in that case when creating it, runtime will explicitly add extra bytes for padding. But what would we need it for? We answer that <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#falsesharing" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#falsesharing" target="_blank">shortly</a>.<br><br>We need to go a little bit deeper and step into hardware territory. Junior programmers often are taught to think about computer memory as a single homogenous block. That's a convenient lie, especially since languages - even as low level like C - rarely expose any primitives to operate on it. From computer architecture classes, you could learn that memory is split into several layers - from RAM to L1-L3 caches.<br><img alt="CPU-architecture" src="https://www.bartoszsypytkowski.com/content/images/2021/02/CPU-architecture.svg" referrerpolicy="no-referrer"><br>Thing is that, access time to L1 can be several dozens times faster than to main memory (RAM). For this reason, when data residing in main memory is about to be used by the CPU, it's first loaded into cache. Hardware does a little bet here: it comes into assumption that most of the data used together resides in the main memory closely to each other. For that reason it doesn't just load a single object reference - it doesn't even know what is it - but instead an entire following block of data, so called cache line. On modern hardware, cache lines are usually 64 bytes long.<br>One of the reasons, why we talked about structs for so long is that collections of entities like A[] behave very differently depending on A being a class or a struct:<br>
<br>If A is a class, it means that A[] contains only references to objects, which actual contents may reside in totally different places of memory. Given nature of .NET GC, when they are created on different threads, you may be pretty sure they will not be placed together. This means that when iterating over them, you may need to load them many times from different places in memory.
<br>If A is a struct, then A[] will contain inlined values of A, with all their contents stored sequentially next to each other.
<br>There's one thing about the cache lines, that can cause misleading results during microbenchmarking of the code. Consider simple operation like sum of the list values: intList |&gt; List.sum. Let's run it twice and check the results:<br><br>In both cases we're talking about the exactly same code over preallocated lists (so list initialization is not part of the benchmark), yet second example takes almost 3 times longer to execute. What has changed then? When setting up the test case I added extra allocation of an object in between appending nodes of the list like so:<br>caseA &lt;- [1..1024] // dataset for TestA
for i = 1024 downto 1 do
    caseB &lt;- i::caseB // dataset for TestB
    unused &lt;- { x = i; y = i; z = i }::unused
<br>Since F# list is implemented as a linked list, it means that its nodes are allocated on a heap and linked together. In first case, even thou suboptimal, those list nodes were still allocated in continuous space in memory, making more efficient use of cache line loads. In second case, our list was fragmented over much bigger space of memory. If elements of our list are value types, we can squash them together by using List. operations over it or just map it into an array. This however won't work for reference types, as we'll only move pointers alone, while objects themselves will stay in their old place.<br>Another way to improve performance of some operations is to revert the field order of the elements stored in collection, eg:<br>[&lt;Struct&gt;] type Point3D = { x: int; y: int; z: int}

type ContainerA(input: Point3D[]) =
  member this.SumX = input |&gt; Array.sumBy (fun a -&gt; a.x)
  member this.Item index = input.[index]

type ContainerB(input: Point3D[]) =
  let x = input |&gt; Array.map (fun a -&gt; a.x)
  let y = input |&gt; Array.map (fun a -&gt; a.y)
  let z = input |&gt; Array.map (fun a -&gt; a.z)
  member this.SumX = Array.sum x
  member this.Item index = { x = x.[index]; y = y.[index]; z = z.[index] }
<br>Now, depending on which operation we care about more - accessing a single element, or computing sum of X coordinates - one or another implementation will have more sense. This approach is even more prevalent if we look into world of databases - a big contributor to performance difference between OLTP databases (oriented towards standard transactional workloads) and OLAP databases (oriented towards analytical data processing) comes exactly from laying out data on a disk by rows vs by columns.<br>PS: In case of ContainerB we can add even better optimization techniques in form of <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#makeuseofvectorization" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#makeuseofvectorization" target="_blank">vectorization</a>, which we'll cover further down the blog post.<br><br>Now the next thing is that L1-L2 caches are residing closely to CPU cores. In fact, as we've shown, every core has it's own cache. This comes with it's own problems: since every CPU has it's own copy of a value, they occasionally need to synchronize and invalidate their caches when that value is accessed from different cores. This is an expensive operation, which we want to avoid.<br>When such accidental sharing may happen? It's not easy to detect in micro benchmarks, and usually needs a dose of profiling and good old fashion trial and error of actual application code. IMO that's why optimizations in this area are not applied so often. Some tips to help build our intuition are:<br>
<br>This can happen when two adjacent fields of the same object are concurrently accessed and modified from different threads. Thing is that unless you configure your types with [&lt;StructLayout(LayoutKind.Explicit)&gt;] you won't know if two fields defined in code one after another will be placed in adjacent memory cells by .NET runtime. Using <a data-tooltip-position="top" aria-label="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" target="_blank">Object Layout Inspector</a> can help you validate your assumptions here.
<br>It can also happen that two different objects/structs will be placed closely inside of collection. This is mostly common for array-backed collections (as they keep elements continuously in memory block) and with small structs (as you may fit more of them inside of single cache line).
<br>We can sometimes reduce risk of false sharing in 2nd case and making it more predictable, by defining type size explicitly to fit exactly into boundaries of cache lines eg. 64B (it's enough to have a class with 6 references/12 int fields or a struct with 8 references/16 int fields). If you know that your objects can be accessed concurrently, but don't fit nicely into into cache lines, you may add extra padding by using [&lt;StructLayout(LayoutKind.Auto, Size=64)&gt;] in your struct definition. While memory usage increases, the overall application performance may improve.<br><br>So far, we only talked about structs in terms of singular elements - when talking about collections, we got pretty much used to the fact, that we have to allocate. This however is not always the case. .NET has a long history of allowing users to allocate collections on stack rather than heap - in C# it's related with stackalloc keyword, in F# it's bit more verbose:<br>open FSharp.NativeInterop

let inline stackalloc&lt;'a when 'a: unmanaged&gt; (length: int): Span&lt;'a&gt; =
  let p = NativePtr.stackalloc&lt;'a&gt; length |&gt; NativePtr.toVoidPtr
  Span&lt;'a&gt;(p, length)
<br>What we returned here is a Span&lt;'a&gt; - a by-ref struct type (meaning: it cannot be used in closures or as a field in most types), that allows us to address its elements just like they existed on the heap. In general, you should avoid allocating too much memory on the stack (in .NET stacks have fixed size that by default is limited to 1MB per thread, allocating over it will cause irrecoverable StackOverflowException). Most common case for using these are short parsing methods, that can be used on the hot paths without producing garbage to be collected later:<br>/// Parse Protocol Buffers style variable length uint32.
let readVarUInt32 (reader: Reader) : uint32 =
  // var int for 32 bit values is never encoded on more than 5 bytes
  let buffer = stackalloc&lt;byte&gt; 5 
  let read = reader.Read buffer
  if read = 0
  then failwith "trying to read var int from empty stream"
  else
    let buffer = buffer.Slice(0, read)
    let mutable decoded = 0u
    let mutable i = 0
    let mutable cont = true
    while cont &amp;&amp; i &lt; buffer.Length do
      let b: byte = buffer.[i]
      decoded &lt;- decoded ||| ((uint32 (b &amp;&amp;&amp; 0x7Fuy)) &lt;&lt;&lt; i * 7)
      i &lt;- i + 1
      if b &lt; 0x80uy then  // check if most significant bit is set
        cont &lt;- false // stop condition reached    
    reader.Advance i
    decoded
<br>Even though we did create a buffer (to avoid cost of multiple virtual calls to reader.Read method), in practice we didn't allocate anything that has to be later collected by the GC.<br>Unfortunately we cannot use spans everywhere eg. as fields of ordinary classes, but there are still situations where we might want to have collections without GC. This often desirable in case of huge number of collections, that most of the time are very small (eg. <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/the-state-of-a-state-based-crdts/#noteaboutvectorclocks" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/the-state-of-a-state-based-crdts/#noteaboutvectorclocks" target="_blank">vector clocks</a>). We can imagine such non-allocating collection like:<br>[&lt;IsReadOnly&gt;]
type HybridMap&lt;'k, 'v&gt; =
    struct
       let count: int  // size: 4B
       // null by default, initialized once we pass over 3 entries
       // size: 8B (reference)
       let inner: Map&lt;'k,'v&gt;
       // inline first 3 entries. size: 48B = 3 * (8B+8B) (assume reference type)
       let entry0: KeyValuePair&lt;'k,'v&gt;   
       let entry1: KeyValuePair&lt;'k,'v&gt;
       let entry2: KeyValuePair&lt;'k,'v&gt;
    end
<br>With map like this, adding first 3 elements produce no garbage. Why only 3? Just like mentioned previously, we prefer our structs to fit into cache lines and this way (assuming both 'k and 'v types are classes) we'll still not surpass 64B (on 64bit OS) or 32B (on 32bit one).<br><br>Discriminated unions are somewhat special citizens, in a sense they have to be represented in terms of .NET reference types (classes) or value types (structs). It means that depending on how they are defined (with or without [&lt;Struct&gt;] attribute), their in memory representation may be very different.<br><img alt="class-vs-struct-union-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-union-1.png" referrerpolicy="no-referrer"><br>A thing you can see in both situations is that fields order in both cases doesn't reflect order in which they were defined. That's because .NET runtime can reorder them in any type unless explicitly forbidden by using [&lt;StructLayout(LayoutKind.Explicit)&gt;] attribute. Moreover, using this attribute is forbidden in case of discriminated unions.<br>Another thing, that you may have noticed is how DUs are represented. A class-based discriminated union is basically equivalent to an empty abstract class with each case being a sealed class inheriting from it. Struct based DU is more complicated. Many other languages with algebraic data types optimize the size according to formula sizeof(tag field) + max([sizeof(ADT case)]). But not F# - what we see here is sizeof(int) + sum([sizeof(ADT case)]):<br><img alt="rust-vs-fsharp-adt-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/rust-vs-fsharp-adt-1.png" referrerpolicy="no-referrer"><br>The reason behind this is a limtation of .NET platform - while technically it's possible to use <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.layoutkind?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.layoutkind?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">LayoutKind.Explicit</a> to implement F# in similar manner to i.e. Rust, it works only as long as we don't try to use it together with generics: .NET cannot make safe guesses in that situation. In result, used memory space for struct-based DU is a sum of all fields defined all cases. For this reason good candidates for struct DU's are usually the ones with very small overall number of fields. F# standard library makes good use of these in form of Result&lt;'t,'e&gt; and 't voption types.<br>Options have somewhat special treatment when it comes to F#. While value options work pretty much in ordinary way, class-based ones have some special magic in them. We could imagine them as:<br>[&lt;Struct&gt;]
type ValueOption&lt;'t&gt; =
  | ValueNone
  | ValueSome of value:'t
  
type Option&lt;'t&gt; = 
  { Value: 't }
  static member Some value = { Value = value }
  static member None = Unchecked.defaultof&lt;Option&lt;'t&gt;&gt; // null
<br>This means, that value option will always have to be initialized and use sizeof(int) + sizeof('t) bytes in memory, while standard option may get optimized away into uninitialized instance (null), which size is always sizeof(IntPtr). So, while you still need to pay for allocating new object for cases where option indeed has value, in some cases where you're working with collections of mostly None values, it may turn out that using standard options is actually more effective approach.<br>A common scenario, where option types are used is when we're dealing with failable operations like trying to find element in a map (which may not be there) or parsing an int. In that case keep in mind:<br>
<br>Using option is the most expensive approach, as eventual success means allocating an extra intermediate object.
<br>Using voption is much cheaper, but at the moment .NET doesn't really know how to pass value types using registers alone, so returning voption may mean copying it through stack in multiple steps, even though we avoided GC allocations.
<br>In practice, the best performing solution is using straight old pattern - popular in C# - of try function definition ending up with 't outref -&gt; bool signature. It can be found in most .NET collections (both F# and C#), as well as parsing methods. In fact this pattern is so popular, that F# can automatically derive tuple out of it eg. let (ok, value) = map.TryGetValue(key). Under normal circumstances this tuple would mean heap allocations, however if you won't capture it and propagate further, but instead use its contents right away like i.e. in match expression, it will let F# compiler to skip allocating an object.
<br>One of the patterns sometimes used by F# programmers is to add more type info to value by wrapping it into DU:<br>type Mileage = Mileage of int 
<br>Patterns like this one are generally devastating for performance - we're allocating 24B of garbage on the heap for every int used. Using units of measure or even type aliases is much better option, since both of them have only compile time representation and are erased by the compiler, they never introduce a runtime overhead.<br>[&lt;Measure&gt;] type miles
type Mileage = int&lt;miles&gt;
<br><br>Did you even wonder, when function is about to call a method on interface parameter, how does it actually know, where to find the method of the underlying object implementing that interface? Runtime resolves actual method to be called by jumping to virtual table of that object (pointer to vtable is part of object header), finding the address of corresponding method (function can have pointers too!) and calling it. As you may imagine all of that indirections can take time. If you think, you're safe because you're not doing object method calls but using module functions instead, check twice - in practice many of them are being inlined as non-static methods.<br><img alt="virtual-call-dispatch-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/virtual-call-dispatch-1.png" referrerpolicy="no-referrer"><br>But how much longer does it actually take? Let's take an example code:<br>/// interface we want to test
type Stub = abstract member DoNothing: unit -&gt; unit
/// class implementing an interface
type A() =
  member _.DoNothing() = ()
  interface Stub with member this.DoNothing() = this.DoNothing()
  
/// struct implementing an interface
[&lt;Struct&gt;]
type B =
  member _.DoNothing() = ()
  interface Stub with member this.DoNothing() = this.DoNothing()

[&lt;MemoryDiagnoser&gt;]
type Benchmark() =
  [&lt;DefaultValue&gt;] val mutable a: A
  [&lt;DefaultValue&gt;] val mutable b: B
  
  static let execute (x: Stub) = x.DoNothing()
  static let executeGeneric (x: #Stub) = x.DoNothing()  
  static let executeDirect (x: A) = x.DoNothing()  
  static let executeDirect2 (x: B) = x.DoNothing()  
  
  [&lt;GlobalSetup&gt;]
  member this.Setup() =
    this.a &lt;- A()
    this.b &lt;- B()        
  [&lt;Benchmark(Baseline=true)&gt;] member this.ExecuteClassDirect() = executeDirect this.a
  [&lt;Benchmark&gt;] member this.ExecuteClass() = execute this.a
  [&lt;Benchmark&gt;] member this.ExecuteClassGeneric() = executeGeneric this.a
  [&lt;Benchmark&gt;] member this.ExecuteStructDirect() = executeDirect2 this.b
  [&lt;Benchmark&gt;] member this.ExecuteStruct() = execute this.b
  [&lt;Benchmark&gt;] member this.ExecuteStructGeneric() = executeGeneric this.b
<br>Example run from BenchmarkDotNet could give us following results:<br><br>There are couple of interesting observations to be made here:<br>
<br>In both cases when either struct or class type parameter is known exactly, the call itself is almost instantaneous (in fact for ExecuteStructDirect method you should receive warning about entire call being optimized away). That's because runtime can say directly which version of the method is going to be called and skip the dispatch to type's virtual table. We call this devirtualization - a process, when runtime is able to replace virtual call dispatch with a direct function call. This is also the reason why you should not pass objects by interface parameters i.e. 't seq instead of 't[] any time when performance matters.
<br>While there exists a small difference between a class instance being called via interface or as generic argument, this is not what we're after. In both cases we can observe similar results - it's because .NET 5 runtime doesn't really specialize generic method calls for reference types. At the moment it can happen sometimes for sealed classes. This means, that usually calling by interface (either explicitly or by using generic type parameter) will require virtual table dispatch anyway.
<br>Passing struct as parameter into a function that expects interface requires boxing - it means that this structure is copied onto heap (hence we can see allocations), prefixed with a header that includes a vtable pointer. From here struct methods look very similar to classes. All of this extra work causes the entire call to be even more expensive. Sometimes when we know that we're about to pass a struct into some callback expecting a generic object (<a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.threading.timer.-ctor?view=net-5.0&amp;ref=bartoszsypytkowski.com#System_Threading_Timer__ctor_System_Threading_TimerCallback_System_Object_System_Int32_System_Int32_" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.timer.-ctor?view=net-5.0&amp;ref=bartoszsypytkowski.com#System_Threading_Timer__ctor_System_Threading_TimerCallback_System_Object_System_Int32_System_Int32_" target="_blank">Timer</a> can be such example) multiple times, it may we worth to eagerly box it ahead and pass boxed version instead, reducing number of allocations. This may also be a sign that reference type is preferred over struct.
<br>An interesting thing happens, once we pass a struct as generic argument. As you can see, this call is several times faster than generic call on the class instance. This is because for structs, .NET JIT uses a generic code specialization - it basically emits machine code for this method call, dedicated for handling this particular type of struct when it's passed as a generic type argument. Since this is specialized branch of code, we don't need to check for vtable every time, as we already can say what function implementation is going to be called.
<br>Generic function specialization may sound a little similar to SRTP (statically resolved type parameters - generics which are erased by F# compiler at compile time), but it's performed by .NET runtime itself. Unlike SRTP, it doesn't prolong our compile times (and .NET JIT is really fast at machine code generation) and can be composed in much better way - you can pass generic functions as parameters themselves over many levels of function calls and let .NET runtime optimize them in a wider context.<br>In some cases we can leverage .NET behavior to introduce something aligned to zero-cost abstractions in our code - it's a term forged by Rust programming language, meaning that we can write abstract code that's as fast as equivalent code written by hand. While .NET and F# offer it in limited scope, we can still use it. Example:<br>type Hasher&lt;'t&gt; = abstract Hash: 't -&gt; int
let inline hash&lt;'h, 't when 'h: struct and 'h :&gt; Hasher&lt;'t&gt;&gt; (value: 't) =
  Unchecked.defaultof&lt;'h&gt;.Hash value

// we should introduce interface for equality check as well

type HashSet&lt;'t, 'h when 'h: struct and 'h :&gt; Hasher&lt;'t&gt;&gt;() =
  member this.Add(item: 't) =
    let h = hash&lt;'h, _&gt; item // 'h cannot be inferred as it has no input parameter
    // ... rest of the implementation

module StringHashers =
  
  [&lt;Struct&gt;]
  type InvariantCultureIgnoreCase =
    interface Hasher&lt;string&gt; with
      [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;]
      member this.Hash(key: string) = StringComparer.InvariantCultureIgnoreCase.GetHashCode key
  
open StringHashers  
let map = HashSet&lt;string, InvariantCultureIgnoreCase&gt;()
<br>Here we managed to introduce two improvements:<br>
<br>Our collection is safer that ordinary .NET HashSet&lt;'t&gt;, as we included information about what hashing method we use. Traditional HashSet&lt;'t&gt; can take IEqualityComparer&lt;'t&gt; as a parameter, but doesn't expose it at type level. That means, that a.Union(b) operation may yield different result than b.Union(a) when a and b use different comparers and we don't get any warnings.
<br>Our collection is also faster, because we provided a precise definition of what hashing method we use at the type level. As mentioned before, this allows .NET runtime to specialize HashSet&lt;'t,'h&gt; methods over 'h, since it's a struct type. Unchecked.defaultof&lt;'h&gt; used here is a pattern that we can use, as we cannot provide static classes (or F# modules) as generic type parameters. However we can provide a struct with no fields instead - it's in-memory representation is 0 bytes, and since it's not boxed (we're providing it as a generic value), .NET will elide its existence completely and compile it to the exact method call (no virtual dispatch is necessary here).
<br>If you're interested more with this approach, I can recommend <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=UybGH0xL5ns&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=UybGH0xL5ns&amp;ref=bartoszsypytkowski.com" target="_blank">this presentation</a> by Frederico Lois.<br><br>We already mentioned registers. As you may (or may not) know, modern day processors offer general purpose registers up to 64bit size. But that's not end of the story. You might have stumbled upon term SIMD (Single Instruction Multiple Data), which was already used here - it's a technique that allows to apply the same operation to multiple values at once. It's a basis for efficient graphical processing and number crunching - hence it's a core building block for GPU programs (eg. shaders).<br>However pretty much every modern day CPU also have dedicated registers - varying in size from 128, 256 to 512 bits atm. of writing this post - that can also be used for this purpose. Their API has been wrapped and exposed in .NET, and it's sometimes used for common operations i.e. finding substrings in provided text or copying structs of certain sizes (we saw that already). Sometimes we call the process of making code use these specialized registers, vectorization.<br>You can also use them on your own. Let's a simple function that's supposed to check if value can be found within given array:<br>
<br>Since vectorized operations can work only over vectors, we first need to create a vector filled in all places with value we try to find.
<br>Next, instead of checking array elements one by one, we load entire chunk of it (as much as we can fit into the vector), and compare it with a previously constructed comparator. This comparison is done over all pairwise vector elements in one instruction.
<br>Since we have to compare all vectors contents or none at all, we need to fallback to standard comparison of elements one by one, once an array remainder is smaller than vector's capacity.
<br>#r "nuget: System.Numerics.Vectors"
open System.Numerics

let inline contains (value: 't) (array: 't[]) =
  let chunkSize = Vector&lt;'t&gt;.Count
  /// ' Use SIMD registers to compare chunks of array at once
  let rec fast (cmp: Vector&lt;'t&gt;) (array: 't[]) (i: int) =
    if i &gt; array.Length - chunkSize then slow value array i
    elif Vector.EqualsAny(cmp, Vector(array, i)) then true // compare entire chunk at once
    else fast cmp array (i+chunkSize)
    
  /// if array remainer size doesn't fit into SIMD register
  /// fallback to check array items one by one
  and slow (value: 't) (array: 't[]) (i: int) =
    if i &gt;= array.Length then false
    elif array.[i] = value then true
    else slow value array (i+1)
    
  // create vector of 't filled with value copies on all positions
  let cmp = Vector(value)
  fast cmp array 0
<br>Limitation here is that this operation works only on numbers - the smaller they are in size, the more of them can we compare at once. But is it fast? Let's check it and compare against standard F# Array.contains 900 [|1..1000|]:<br><br>It's 4 times performance improvement simply by using vectorized operations. There are <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.numerics.vector?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.numerics.vector?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">dozens of operations</a> defined in high-level Vector API. Knowledge on how to make an advantage of SIMD is basis for modern day design and implementation of data structures and algorithms.<br><br>Immutable data structures are prevalent technique of writing programs in functional paradigm and F# is no exception here. Thing is, that immutable structures, be it records or collections, introduce an extra overhead related to copying parts of the code in use. This can be reduced by writing structures that take advantage of <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Persistent_data_structure?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Persistent_data_structure?ref=bartoszsypytkowski.com" target="_blank">structural sharing</a> - which deserves its own blog post - but in general .NET doesn't provide optimizations that could make executing single-threaded, large scale immutable code faster than its mutable equivalent.<br>Often advantage of having immutable data types is not performance related - they just offer simpler way to analyze and debug program, which has fewer moving parts, as you can compare snapshots of previous/current/expected states with each other.<br>One of the bigger advantages - which we're also going to use soon - it's natural idempotency of such structures. Aside of being useful in testing (some of the model checkers and property-based tests make extensive use of it), we can also use it to improve performance. It's not usually visible in simple code microbenchmarking, but rather when we need to combine several different operations at once.<br>Example: imagine, that you have a dictionary, that needs to be updated concurrently, but also checked for size from time to time. In .NET we could simply implement it by using ConcurrentDictionary&lt;'k,'v&gt;, but there's a catch - have you ever wondered, how mutable concurrent dictionary ensures, that during counting process a dictionary size has not changed? Well it does it in the simplest way - it locks entire dictionary until counting completes.<br>open System
open System.Collections.Concurrent

let map = ConcurrentDictionary&lt;_,_&gt;()

// 1st set of workers tries can try to add value to a map
let write k v = map.AddOrUpdate(k, Func&lt;_,_&gt;(fun _ -&gt; v), Func&lt;_,_,_&gt;(fun _ _ -&gt; v))

// another worker performs operation over the elements of the map
let count () = map.Count
<br>This issue doesn't really exists in immutable collections, since there's no risk of changing the collection as it's being iterated - we can simply iterate over old (possibly outdated) snapshot of data, but we won't stop the field/variable from being updated.<br>// shared mutable field
let map = ref Map.empty

let write k v = map := Map.add k v !map

let count () = Map.count !map
<br>Now, the question would be - is it safe? Well... no. Modifying contents of F# ref cells (or even static mutable members) is not threadsafe operation. But we can make it so. How? Old school way would be to fall back to OS-level primitives like semaphores and mutexes, but we still can actually make them faster, thanks to the idempotency of immutable collections.<br><br>We'll again fallback to hardware intrinsic operations, this time exposed as part of <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.threading.volatile?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.volatile?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">Volatile</a> and <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.threading.interlocked?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.interlocked?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">Interlocked</a> classes. We can use first to make sure that our reads and writes are invalidating field values that might be accidentally reordered or cached by other CPU cores. Just like .NET runtime feels free to reorder our fields in defined types, it - of even CPU itself - can decide to reorder our operations if it deems it to be more efficient. The latter provides a set of operations, which can be executed within single processor instruction - without worrying that OS may decide to switch threads in between leaving our shared variable in undefined state.<br>The most powerful method in that toolbox is compare-and-swap in .NET known as Interlocked.CompareExchange, which enables to atomically swap register-sized value or reference to a new one, but only if the existing reference at the moment of swap is equal to expected one. What's important here, we're doing reference-based comparison (basically comparing pointer addresses), not structural one that we know as default in F#.<br>Now, here's how can we use that operation to replace locks:<br>let mutable map = Map.empty

let rec write k v =
  let prev = Volatile.Read &amp;map // read most up-to-date value
  let next = Map.add k v prev   // update operation
  if obj.ReferenceEquals(prev, Interlocked.CompareExchange(&amp;map, next, prev)) 
  then () // we successfully updated the map
  else write k v // retry
<br>What we basically try to do is to retrieve the most recent value, update it and store back using Interlocked.CompareExchange. This operation returns a previously stored value, which should be the same reference as the one, we got prior to making an update. If it's different, it means that another thread concurrently swapped it while we were making an update. It's very rare situation, even when lock contention is fairly high, but if it happens, we just retry a whole operation.<br>Here, we're using Map.add but in practice this could be any function 't -&gt; 't, as long as it satisfies few conditions:<br>
<br>A reference returned by update function must be different than the input - otherwise our if expression will never be able to reach the stop condition. It's one reason, why we prefer immutable data types here.
<br>Update must be idempotent. In case of concurrent conflict, only one of the sides will win, while other will have to repeat, and we don't want to i.e. insert the same item to our list multiple times. That's another reason for using immutable collections.
<br>Update action should be fairly fast. No I/O operations, no number crunching. The longer it takes to execute, the less useful this pattern becomes, as risk of retries grows and their cost may outweigh the cost of acquiring the lock.
<br>All of this puts some strong restrictions on the code that can be used with this pattern, nonetheless it's still used pretty often - especially in combination with standard locking mechanism, where we first try to optimistically use Interlocked primitives to acquire fast locks and on failure fallback to heavier ones. This is how "slim" locks work as well as thread safe queues and unbounded channels (eg. BlockingQueue).<br><br>Inlining is a popular optimization technique, were instead of making function call, we directly emit function body in the outer function. This allows us to avoid costs like putting invocation arguments on the stack or jumps related to returning from function. In F# and .NET, there are several situations, where that happens:<br>
<br>F# function itself can be marked using inline attribute. This will trigger F# code to literally imprint the function body at the callsite. This means that encapsulation rules of such functions must respect .NET encapsulation (eg. you cannot have public inline function calling private function in its body). Like in many other languages, F# inline is optimistic - in some cases when function cannot be inlined eg. because it has been passed as parameter to another non-inlined function, inlining won't occur.
<br>Any function can be forced to be inlined at runtime level by using [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;] attribute. It will tell JIT to emit machine code directly at callsite. This option does not have limitations of F# inline keyword, however it's not always able to introduce some of the optimizations, F# compiler is capable of.
<br>Most of the time inlining happens without our precise knowledge. It can be done by the JIT compiler itself over any function not marked with [&lt;MethodImpl(MethodImplOptions.NoInlining)&gt;] attribute. It's based on a set of heuristic rules, one of which being size of the calling and called functions body - the smaller they are, the higher chance for inlining to happen. Additionally at the current moment, code that explicitly throws an exception is prevented from being inlined at JIT level, so using NoInlining option can also improve speed of your code in some cases - most popular being exception-driven input assertions inside of functions.
<br>As I mentioned, F# inline sometimes can apply optimizations outside of the scope of JIT optimizer alone. Let's take an example:<br>type AtomicRef&lt;'t when 't: not struct&gt;(initValue: 't) =
  let mutable value = initValue
  member this.Value with [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;] get () = Volatile.Read &amp;value
    
  [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;]
  member this.CompareAndSwap(comparand: 't, newValue: 't): bool =
    obj.ReferenceEquals(comparand, Interlocked.CompareExchange(&amp;value, newValue, comparand))
<br>This type is going to present behavior similar to F# ref cell, with the difference that its operations are going to be thread safe in the same sense we described in <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#atomiccompareandswap" rel="noopener nofollow" class="external-link" href="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#atomiccompareandswap" target="_blank">atomic compare-and-swap section</a>. Now imagine, that we'd like to have a generic updating mechanism:<br>module Atomic

let update (modify: 't -&gt; 't) (atom: AtomicRef&lt;'t&gt;) =
  let mutable old = atom.Value
  let mutable newValue = modify old
  while not (atom.CompareAndSwap(old, newValue)) do
    old &lt;- atom.Value
    newValue &lt;- modify old
  newValue
<br>With this we can atomically modify a value within the cell. We could leave it like this, but if you'll benchmark an exemplar snippet like this:<br>// benchmark setup
val a = AtomicRef "hello"
val value = "world"

// benchmarked function body
a |&gt; Atomic.update (fun _ -&gt; value)
<br>You'd discover that this call allocates - It's a result of passing a function argument (in .NET these are realized as objects), that captures value field. Now, we could try to mark Atomic.update function using [&lt;MethodImpl&gt;] attribute or use inline keyword. If we'd try to benchmark these however, the results would be slightly different:<br><br>You may notice, that using attribute might slightly improve speed, but didn't change anything in terms of allocations around capturing lambda parameter. However using F# inline keyword indeed helped here: a lambda argument has been erased, as its behavior was aggressively printed together with inlined function body.<br><br>It's been a long post, but we just touched a tip of an iceberg here. We're didn't really mention optimizations in the area of I/O operations, different flavors of async code execution or <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">dropping .NET safety belt</a> in cases when we wish to omit safe checks even when .NET compiler alone could not. There are also many tricky situations in which one small, seemingly insignificant change in code can throw .NET runtime into pit of deoptimized code. If you're curious about these, you can follow <a data-tooltip-position="top" aria-label="https://twitter.com/badamczewski01?ref=bartoszsypytkowski.com" rel="noopener nofollow" class="external-link" href="https://twitter.com/badamczewski01?ref=bartoszsypytkowski.com" target="_blank">Bartosz Adamczewski</a> on twitter.<br>Ultimately, while many of these tips and behaviors may stay with us for years to come, remember that compilers are still actively developed and just like some of these optimizations are not applied on older runtimes like .NET Full Framework, new ones (like smarter escape analysis, new devirtualization rules etc.) may turn some of the warnings presented here obsolete and shift the optimization techniques to enable us writing code that's both fast and high-level.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/writing-high-performance-fsharp-code.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Writing high performance Fsharp code.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:47:07 GMT</pubDate><enclosure url="https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-layout.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-layout.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[F-bounded polymorphism in Scala]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:scala" class="tag" target="_blank" rel="noopener nofollow">#scala</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a> <br>When we talk about polymorphism in programming, we're referring to the ability of an entity to take on several forms. Among the various approaches to polymorphism, F-bounded polymorphism (or F-bounded quantification) is a particularly advanced technique in the context of object-oriented programming languages. It is characterized by its emphasis on relations between types, thereby combining the advantages of polymorphism and genericity. This concept plays a crucial role in maintaining coherent type hierarchies and promoting consistency in software development. As we explore the depths of type theory and programming, understanding F-bounded polymorphism opens doors to crafting more robust and dependable software systems.<br>In this article, I will start by explaining how I discovered F-bounded polymorphism (somewhat by coincidence) and how it helped me in a specific case. Then, we'll take a closer look at the theory behind polymorphism with quantification (basic and bounded), then F-bounded polymorphism, which we'll finally illustrate with a practical example. Although this concept (and polymorphism in general) exists in many languages, this article uses Scala for the examples.<br><br><br>Let's start with a true story. I recently worked for a client who wanted a platform where multiple versions of information could co-exist. After developing a git-like versioning library (honourable mention to my Scala mentor who will recognize himself) that wasn't perfectly suited to the business, my team and I restarted from scratch with a new history-like approach. For reasons of confidentiality and code propriety, the code examples shown here have nothing to do with the original code, either in terms of naming or implementation (details of which are omitted to focus on F-bounded polymorphism only). Let's start by defining:<br>trait Info[T] {
  def update(t: T): Info[T]

  ...
}
<br>The idea is that Info can remember changes to an object T, and that for any T that needs to be versioned, Info is extended by a concrete class containing these changes field by field. An example would be:<br>case class Foo(foo: String)

case class Bar(foo: Foo, foos: List[Foo])

// definition of class Memory[T] does not matter here
case class FooInfo(fooMemory: Memory[String]) extends Info[Foo] {
  ... // implementation does not matter
}

case class BarInfo(
    fooInfo: FooInfo, 
    foosInfo: ListInfo[Foo]
) extends Info[Bar]
<br>Let's now look at ListInfo, which, as its name suggests, represents the information of a list. To define such a class, we could imagine the following:<br>case class ListInfo[T](
    infos: List[Info[T]]
) extends Info[List[T]] {
  ...
}
<br>However, as the line foosInfo: ListInfo[Foo] suggests, having a single parameter type T is not sufficient here, since the Info[T] type in infos: List[Info[T]] gives no information about the concrete class used. We can therefore modify the class as follows:<br>case class ListInfo[T, InfoType &lt;: Info[T]](
    infos: List[InfoType]
) extends Info[List[T]]
<br>We now know which Info type is used for the item informations. In our example, BarInfo becomes:<br>case class BarInfo(
    fooInfo: FooInfo, 
    foosInfo: ListInfo[Foo, FooInfo]
) extends Info[Bar]
<br>Now imagine that, in ListInfo, we have a method for updating a particular information:<br>case class ListInfo[T, InfoType &lt;: Info[T]](
    infos: List[InfoType]
) extends Info[List[T]] {
  def update(values: List[T]): Info[List[T]] = ???

  // imagine that update() uses this function to update the infos
  def updateInfoAtIndex(index: Int, t: T): ListInfo[T, InfoType] = {
    val updatedInfo = infos(index).update(t)
    copy(infos = infos.updated(index, updatedInfo))
  }
  ...
}
<br>This code doesn't compile. Can you see the problem?<br>Here's the explanation. In the function updateInfoAtIndex(), the type of updatedInfo is the type of the Info trait's update() function, which, as a reminder, is:<br>def update(t: T): Info[T]
<br>However, infos is of type List[InfoType], not List[Info[T]]. The compiler therefore returns the following error:<br>[error]  type mismatch;
[error]  found   : updatedInfo.type (with underlying type Info[T])
[error]  required: InfoType
[error]     copy(infos = infos.updated(index, updatedInfo))
[error]                                       ^
<br>We're faced here with a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Type_erasure" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Type_erasure" target="_blank">type erasure</a> of the type InfoType in its wider type Info[T], which the compiler cannot resolve by itself. Now let's see what solutions are available to us.<br><br><br>The most obvious solution to our problem is type casting:<br>def updateInfoAtIndex(index: Int, t: T): ListInfo[T, InfoType] = {
  val updatedInfo: Info[T] = infos(index).update(t)
  copy(infos = infos.updated(index, updatedInfo.asInstanceOf[InfoType]))
}
<br>However, this solution is sorely lacking in robustness and elegance.<br><br><br>Let's put the problem back at the centre of the table. Here, we lose information on the type of an object of type InfoType &lt;: Info[T], which goes back to its more general nature Info[T]. Intuitively, then, we might wonder whether there is a way of storing this lost information in Info[T] itself, at the type level. A first approach might be to use ClassTag's, but we won't cover this solution in this article. Another approach is to rewrite Info as follows:<br>trait Info[T, InfoType &lt;: Info[T, InfoType]] {
  def update(t: T): InfoType
}
<br>The most remarkable thing here is the recursive definition of Info. Morally, InfoType remains a subtype of Info. However, with recursion, it is also a subtype of the type that "defines" it. This allows us to change the return of the update() function from Info[T] to InfoType. Let's look at the repercussions of this change on the ListInfo class:<br>case class ListInfo[T, InfoType &lt;: Info[T, InfoType]](
    infos: List[InfoType]
) extends Info[List[T], ListInfo[T, InfoType]] {
  def update(values: List[T]): ListInfo[T, InfoType] = ???

  def updateAtIndex(index: Int, t: T): ListInfo[T, InfoType] = {
    val updatedInfo: InfoType = infos(index).update(t) // expected type
    copy(infos = infos.updated(index, infos(index).update(t)))
  }
  ...
}
<br>This code now compiles. Regarding the subtypes of Info[T], these need to be adapted slightly. In our example, we have<br>case class FooInfo(
    fooMemory: Memory[String]
) extends Info[Foo, FooInfo]

case class BarInfo(
    fooInfo: FooInfo, 
    foosInfo: ListInfo[Foo, FooInfo]
) extends Info[Bar, BarInfo]
<br>Without really realizing it, we've just used F-bounded polymorphism. Let's talk about it in more detail.<br><br><br>F-bounded polymorphism is based on relationships between types. It is also nothing other than a special form of polymorphism. This concept is closely linked to that of type in programming languages. Now, you may ask: What is a type? Why do we need them in programming languages? A particularly appealing (and funny) answer comes from the article "On understanding types, data abstraction, and polymorphism" by L. Cardelli and P. Wegner, published in 1986:<br>
A type may be viewed as a set of clothes (or a suit of armor) that protects an underlying untyped representation from arbitrary or unintended use. It provides a protective covering that hides the underlying representation and constrains the way objects may interact with other objects. In an untyped system untyped objects are naked in that the underlying representation is exposed for all to see. Violating the type system involves removing the protective set of clothing and operating directly on the naked representation.
<br>The article concludes, among other things, that types are sets of values. So there are two types of language: those that are not typed, i.e. have only one type, known as monomorphic, and those that are typed, known as polymorphic. Within polymorphic languages, polymorphism can take several forms with which you're no doubt familiar: when a function works or appears to work on several different types (potentially every type), overloading, coercion, subtyping and so on. I urge you to delve into this magnificent article.<br>We'll now take a closer look at some common forms of polymorphism.<br><br><br>In type theory, quantification refers to universally or existentially quantifying type variables:<br>
<br>
Universal Quantification (∀) indicates that a property holds for all permissible types. Type specifications for variables of a universally quantified type have the following form (for any type expression σ(t)): p: ∀t.σ(t). This expresses the property that for every type t, p has the type σ(t).
  In Scala, universal quantification is typically used via generic types, allowing functions and data structures to operate over all types T. For instance, a generic function might be represented as:

<br>
    def identity[T](x: T): T = x

  Here, T is universally quantified: the function should work for any type T. Using the notation above, the identity function is written as: ∀t. t → t.<br>


<br>
Existential Quantification (∃) denotes that there exists at least one type for which a property holds. Formally, existential quantification is written as:
  p: ∃t.σ(t). In Scala, existential types, declared using a wildcard type (placeholder syntax), signify that a type exists without specifying it:<br>


<br>  def printFirst(list: List[_]): Unit = println(list.headOption)
<br>In this function, the type of the list elements is existentially quantified. The function knows there exists some type, but it doesn’t specify or use it explicitly.<br>In reality, the wildcard type (placeholder syntax) is a syntactic sugar for the formal expression of existential types in Scala, which has the form<br>T forSome { Q } where Q is a sequence of type declarations. Type List[_] can therefore be rewritten as:<br>
<br>
    type L = List[t forSome { type t }]


<br>Note that replacing L with L[_] in the left-hand member is also valid. Quick question for you: how would you write the type List[List[_]] with this syntax? Or List[Int]? Or even the type representing any type? Hint: you'll find the answer in one of the sources of this article.<br>

<br>Let's end our explanation of basic quantification with these few wonderful lines, which I found while wandering through the code of the <a data-tooltip-position="top" aria-label="https://github.com/milessabin/shapeless" rel="noopener nofollow" class="external-link" href="https://github.com/milessabin/shapeless" target="_blank">shapeless</a> library:<br>type ¬[T] = T =&gt; Nothing

type ∃[P[_]] = P[T] forSome { type T }
type ∀[P[_]] = ¬[∃[({ type λ[X] = ¬[P[X]]})#λ]]
<br>Reading and reflecting on these lines convinces me that Scala is and always will be my favourite programming language.<br><br><br>Bounded quantification is a conceptual extension of the idea of universal and existential quantification. In essence, it is the notion of constraining the range over which a quantification applies.<br>Bounded quantification essentially introduces a restrictive layer atop basic quantification, enabling explicit definition of permissible type range:<br>
<br>Upper-bounded (T≤B): ∀T: T ≤B Indicates "for all types T that are subtypes of B. In Scala, you can express upper-bounded quantification using the &lt;: symbol in type parameterization:
<br>def maxElement[T &lt;: Ordered[T]](a: T, b: T): T = if (a &lt; b) b else a
<br>Here, T is constrained to be a subtype of Ordered[T], ensuring the elements can be ordered. More generally, if we have types A and B, and A is a subtype of B, it means that any value of type A can also be used in a context that expects type B.<br>
<br>Lower-bounded (B≤T): ∀T: B ≤T Denotes "for all types T that are supertypes of B". Scala represents lower-bounded quantification using the &gt;: symbol:
<br>def prependToSuperTypeList[B, T &gt;: B](element: B, list: List[T]): List[T] = element :: list
<br>Here, T is a supertype of B, ensuring that an element of type B can be prepended to a list of type T.<br>Of course, an existentially quantified parameter type can also be constrained by bounded quantification, for example:<br>def compareElements[A &lt;: Seq[_ &lt;: Comparable[_]]](seq1: A, seq2: A): Boolean = {
  // Comparison logic here
  true
}
<br><br><br>A key concept in F-Bounded Polymorphism is the F-Bound. In bounded quantification, when a type A is F-Bounded with respect to a type B, this means that instances of A are linked by a particular semantic relation to those of B. A formalization of F-bounded polymorphism appeared in 1989 in the article "F-Bounded Polymorphism for Object-Oriented Programming" by Peter Canning et al. This article presents F-bounded polymorphism (or quantification) as a natural extension of bounded quantification.<br>In this article, we can read this definition:<br>
We say that a universally quantified type is F-bounded if it has the form
∀t ⊆ F[t].σ
where F[t] is an expression, generally containing the type variable t.
<br>Let's now break this definition into its components:<br>
<br>∀t: This part represents universal quantification over a type variable t. As stated previously, in programming languages, it means that the statement applies to all possible values of the type variable t.
<br>⊆: This symbol represents the subtype relationship or upper-bounded quantification explained before.
<br>F[t]: This is the type bound associated with the type variable t. It defines a set of types that t must belong to. Importantly, F[t] is expressed in terms of the type variable t itself. This creates a recursive relationship, where the type bound refers to the type variable it is bounding.
<br>σ: This is the actual type expression that is being quantified over and constrained by the F-bounded type system. It represents the type structure that we are trying to define and apply constraints to.
<br>In summary, ∀t ⊆ F[t].σ means that for any type t, the type expression σ is constrained to be a subtype of the type bound F[t] which is defined in terms of the type t itself. In other words, if F[t] is a type of the form F[t] = {aᵢ: σᵢ[t]}, then the condition A ⊆ F[A] says that A must have the methods aᵢ and these methods must have arguments as specified by σᵢ [A], which are defined in terms of A.<br>In Scala, probably the simplest example of F-bounded polymorphism is this one:<br>trait T[U &lt;: T[U]]
<br>Please take a few seconds to admire this.<br><br><br>Let's now explain F-bounded polymorphism with the analogy of musical instruments.<br>
<br>t: A specific musical instrument (e.g., a Guitar)<br>

<br>F[t]: When applied to t, results in an instrument constrained to harmonize with instruments of its own type<br>

<br>σ: { age: InstrumentAge, produceSound: () =&gt; Sound, playInTuneWith: (F[t]) =&gt; Harmony }<br>

<br>In simpler terms, any instrument t is valid only if it can play in tune with another instrument of the same type t and is capable of producing sound.<br>Let's now express the form∀t ⊆ F[t].σ in terms of Scala code:<br>enum Sound {
  case Strumming(instrumentAge: InstrumentAge)
  case Whistling(instrumentAge: InstrumentAge)
  case Harmony(sound1: Sound, sound2: Sound)
  // note that a recursive type is inherently polymorphic in nature,
  // as well as a sum type
}

import Sound._

enum InstrumentAge {
  case New,    // Crisp and clear sound.
         Old,    // Deep and resonating sound.
       Ancient // Faint and mellow sound.
}

trait Instrument[T &lt;: Instrument[T]] {
  val age: InstrumentAge
  def produceSound(): Sound
  def playInTuneWith(instrument: T): Harmony
}

case class Guitar(age: InstrumentAge) extends Instrument[Guitar] {
  def produceSound(): Sound = Strumming(age)

  def playInTuneWith(instrument: Guitar): Harmony = Harmony(produceSound(), instrument.produceSound())
}

case class Flute(age: InstrumentAge) extends Instrument[Flute] {
  def produceSound(): Sound = Whistling(age)

  def playInTuneWith(instrument: Flute): Harmony = Harmony(produceSound(), instrument.produceSound())
}
<br>Here, Instrument is F-bounded. It ensures that a Guitar can only play in tune with another Guitar and a Flute can only play in tune with another Flute. Thus the following code does not compile because the playInTuneWith method expects a Piano.<br>// compilation error
case class Piano(val age: InstrumentAge) extends Instrument[Piano] {
  def produceSound(): Sound = Whistling(age)

  def playInTuneWith(instrument: Guitar): Harmony = Harmony(produceSound(), instrument.produceSound())
}
<br>Tighter constraint with self-type annotation<br>Finally, in our example, the strength of the Scala type system allows us to go even further in our constraints with self-type annotation:<br>trait Instrument[T &lt;: Instrument[T]] { self: T =&gt;
    ...
}
<br>This definition not only asserts that T is a subtype of Instrument[T] but also guarantees that any concrete class or trait extending Instrument[T] is itself of type T . This creates a tighter constraint than the first trait definition. With this annotation, the following code does not compile anymore<br>class BlueBird extends Instrument[Guitar] {
    ...
}
<br>because BlueBird is not of type Guitar.<br><br><br>We have seen that bounded quantification is a powerful tool that helps in expressing more refined and precise relationships between types.<br>In particular, F-bounded polymorphism offers a sophisticated way of shaping type systems and ensuring logical constraints within programming languages and programs. The concept becomes simpler when seen in light of our musical analogy. It is a powerful concept in type theory that lets us constrain a type parameter based on the type itself. This recursive constraint ensures that subclasses adhere to specific type restrictions.<br>Using F-bounded polymorphism in Scala, we achieved a type-safe way to model real-world scenarios, like musical instruments playing in tune. This ensures that mistakes like trying to tune a guitar with a flute are caught during compilation, thus eliminating potential runtime errors.<br>In essence, F-bounded polymorphism offers an expressive and robust way to encapsulate and ensure type relations. It's like the maestro of a symphony, ensuring each instrument plays in perfect harmony, creating a melody that's both beautiful and error-free.<br><br><br>
<br>P. Canning, W. Cook, W. Hill and W. Olthof. F-Bounded Polymorphism for Object-Oriented Programming. Proceedings of the fourth international conference on Functional programming languages and computer architecture. 1989. <a rel="noopener nofollow" class="external-link" href="https://www.cs.utexas.edu/~wcook/papers/FBound89/CookFBound89.pdf" target="_blank">https://www.cs.utexas.edu/~wcook/papers/FBound89/CookFBound89.pdf</a><br>

<br>L. Cardelli and P. Wegner. On understanding types, data abstraction, and polymorphism. Computing Surveys, 17(4):471–522, 1986. <a rel="noopener nofollow" class="external-link" href="http://lucacardelli.name/papers/onunderstanding.a4.pdf" target="_blank">http://lucacardelli.name/papers/onunderstanding.a4.pdf</a><br>

<br>The Scala 2.11 specification of existential types. <a rel="noopener nofollow" class="external-link" href="https://www.scala-lang.org/files/archive/spec/2.11/03-types.html#existential-types" target="_blank">https://www.scala-lang.org/files/archive/spec/2.11/03-types.html#existential-types</a><br>

<br>The Curiously Recurring Template Pattern (CRTP) in C++. See for example <a rel="noopener nofollow" class="external-link" href="https://en.cppreference.com/w/cpp/language/crtp" target="_blank">https://en.cppreference.com/w/cpp/language/crtp</a>.
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/scala/f-bounded-polymorphism-in-scala.html</link><guid isPermaLink="false">Computer Science/Programming Language/JVM/Scala/F-bounded polymorphism in Scala.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:47:37 GMT</pubDate></item><item><title><![CDATA[Overview]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:machine-learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#machine-learning</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:scala" class="tag" target="_blank" rel="noopener nofollow">#scala</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:machine-learning" class="tag" target="_blank" rel="noopener nofollow">#machine-learning</a><br><br>This is the second post in a series about inference of machine learning models using scala. The first post can be found <a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/" rel="noopener nofollow" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/" target="_blank">here</a>. This post will detail how to use a functional streaming library (<a data-tooltip-position="top" aria-label="https://fs2.io/#/" rel="noopener nofollow" class="external-link" href="https://fs2.io/#/" target="_blank">fs2</a>) to perform machine learning model inference using the <a data-tooltip-position="top" aria-label="https://github.com/triton-inference-server/server" rel="noopener nofollow" class="external-link" href="https://github.com/triton-inference-server/server" target="_blank">Triton Inference Server</a> and <a data-tooltip-position="top" aria-label="https://grpc.io/" rel="noopener nofollow" class="external-link" href="https://grpc.io/" target="_blank">gRPC</a>. The post will be broken up into a few different parts. First we will set up our scala, python and docker dependencies. Then we will get Triton up and running using Docker. Finally we will set up fs2 to read from a text file containing image paths. We will use <a data-tooltip-position="top" aria-label="https://opencv.org/" rel="noopener nofollow" class="external-link" href="https://opencv.org/" target="_blank">opencv</a> to format our images into the representation Triton expects. Finally we will load images and send them to Triton in batches, displaying the result to the console.<br>The github repo for these tutorials can be found <a data-tooltip-position="top" aria-label="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" rel="noopener nofollow" class="external-link" href="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" target="_blank">here</a><br><br>It is expected that you have the following tools installed:<br>
<br>scala build tool <a data-tooltip-position="top" aria-label="https://www.scala-sbt.org/" rel="noopener nofollow" class="external-link" href="https://www.scala-sbt.org/" target="_blank">sbt</a>
<br>python build tool <a data-tooltip-position="top" aria-label="https://python-poetry.org/" rel="noopener nofollow" class="external-link" href="https://python-poetry.org/" target="_blank">poetry</a>
<br>Cuda toolkit and Nvidia Docker. More detailed installation tips can be found in the <a data-tooltip-position="top" aria-label="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" rel="noopener nofollow" class="external-link" href="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" target="_blank">github readme</a>
<br><br>First create a new project using the scala 3 giter template/sbt and move to the newly created directory.<br>sbt new scala/scala3.g8 
#   name [Scala 3 Project Template]: scalamachinelearningdeployment 
#   Template applied in ./scalamachinelearningdeployment cd scalamachinelearningdeployment`
<br>Next we will add the fs2 gRPC plugin. Add the following to project/plugins.sbt. This is what will turn our .proto files into code we can use to talk with Triton. We will talk more about .proto files and gRPC later.<br>addSbtPlugin("org.typelevel" % "sbt-fs2-grpc" % "2.7.4")`
<br>We then need to create a module to store our .proto files in, and to run code generation from.<br>mkdir -p protobuf/src/main/protobuf/`
<br>Create a file called downloadprotos.sh and add the following content. These are the proto files provided by the Triton Inference Server. They allow for us to communicate with Triton in any language that can generate code from .proto files.<br>for PROTO in 'grpc_service' 'health' 'model_config' 
do     
  wget -O ./protobuf/src/main/protobuf/$PROTO.proto https://raw.githubusercontent.com/triton-inference-server/common/main/protobuf/$PROTO.proto 
done
<br>Then run the script to download the files.<br>chmod +x downloadprotos.sh
./downloadprotos.sh`
<br>Finally we need to configure our build.sbt. There are a couple key steps to make note of:<br>
<br>Create variables to manage our dependencies
<br>Create a module for the protobuf subdirectory, explicitly stating we depend on the gRPC plugin
<br>Add our dependencies to our root module and make the root module depend to the protobuf module
<br>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/scala/inference-of-machine-learning-models-using-scala.html</link><guid isPermaLink="false">Computer Science/Programming Language/JVM/Scala/Inference of machine learning models using scala.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:47:36 GMT</pubDate></item><item><title><![CDATA[Refinement types in Scala 3]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:scala" class="tag" target="_blank" rel="noopener nofollow">#scala</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br>When familiarizing myself with additions in Scala 3, it was improvements in meta-programming capabilities that caught my eye. I wondered what it would take to implement a simple refinement types library. It is definitely not my plan to end up with a full-blown refinement library as <a data-tooltip-position="top" aria-label="https://github.com/fthomas/refined" rel="noopener nofollow" class="external-link" href="https://github.com/fthomas/refined" target="_blank">refined</a>, but only to understand if the language's new version can offer some improvements in the process.<br>We will use literal types as a basic construct. The introduction of literal types was a subject of <a data-tooltip-position="top" aria-label="https://docs.scala-lang.org/sips/42.type.html" rel="noopener nofollow" class="external-link" href="https://docs.scala-lang.org/sips/42.type.html" target="_blank">SIP-23</a>, and they've been already included in Scala 2.13. They enable you to use literals of primitive types at places where types are expected:<br>val a: 4 = 4
// val b: 5 = a // fails compilation with: 
// Found:    (a : (4 : Int))
// Required: (5 : Int)
<br>What Scala 3 adds to the mix are compile-time operators on literal types. You can found them in the scala.compiletime package:<br>type PlusTwo[T &lt;: Int] = scala.compiletime.ops.int.+[2, T]

val a: PlusTwo[4] = 6
// val bb: PlusTwo[4] = 7 // fails compilation with:
// Found:    (7 : Int)
// Required: (6 : Int)
<br>In the definition of PlusTwo I wanted to stress that + is a type operator, hence the prefix notation. In practice infix operator might be more convenient:<br>import scala.compiletime.ops.int.*

type PlusTwo[T &lt;: Int] = 2 + T
<br>One of operators available in compiletime is comparison operator &lt; which looks particularly useful in scope of refinement types:<br>type &lt;[X &lt;: Int, Y &lt;: Int] &lt;: Boolean
<br>Here are some examples of its usage:<br>val a: 5 &lt; 10 = true
// val b: 15 &lt; 10 = true // fails compilation with:
// Found:    (true : Boolean)
// Required: (false : Boolean)
<br>However, in context of refinement types we would like to use &lt; as a kind of type bound as opposed to just an operator returning Boolean. Therefore, while &lt; is definitely helpful, it's not sufficient by itself to express refinement types. We're striving for something akin to:<br>// val a: Int &lt; 10 = 5 // fails compilation
<br><br>To be able to use comparison operators as a type bound we have to build some minimal structure:<br>trait Validated[PredRes &lt;: Boolean]
given Validated[true] = new Validated[true] {}

trait RefinedInt[Predicate[_ &lt;: Int] &lt;: Boolean]
def validate[V &lt;: Int, Predicate[_ &lt;: Int] &lt;: Boolean]
    (using Validated[Predicate[V]]): RefinedInt[Predicate] = new RefinedInt {}
<br>The idea behind this is that code invoking validate will compile only if Predicate[V] evaluates to type true. Therefore, the whole business of validation will be offloaded to the second type parameter of validate.<br>Such minimal structure is enough to express something like this:<br>type LowerThan10[V &lt;: Int] = V &lt; 10
val lowerThan10: RefinedInt[LowerThan10] = validate[4, LowerThan10]
<br>An equivalent written with type lambda:<br>val lowerThan10: RefinedInt[[V &lt;: Int] =&gt;&gt; V &lt; 10] = validate[4, [V &lt;: Int] =&gt;&gt; V &lt; 10]
<br>I must admit the latter looks uglier, but in general, it is preferred as it allows us to avoid coming up with an unnecessary type name.<br>If you want to see how type lambdas are being used in the wild, I recommend taking a look at <a data-tooltip-position="top" aria-label="https://github.com/lampepfl/dotty/blob/master/library/src/scala/Tuple.scala#L139" rel="noopener nofollow" class="external-link" href="https://github.com/lampepfl/dotty/blob/master/library/src/scala/Tuple.scala#L139" target="_blank">type-level implementations</a> of scala.Tuple higher kinded types of Filter or Fold.<br>This encoding, while very simplistic and not the most convenient to use, is quite flexible. Thanks to operators in compiletime.ops.bool we can build more complicated predicates as the following:<br>import scala.compiletime.ops.boolean.*

validate[7, [V &lt;: Int] =&gt;&gt; V &gt; 5 &amp;&amp; V &lt; 10]
<br>If we try to pass incorrect input we will get a compilation error:<br>validate[4, [V &lt;: Int] =&gt;&gt; V &gt; 5 &amp;&amp; V &lt; 10]
// no implicit argument of type iteration1.Validated[(false : Boolean)] was found for parameter x$2 of method validate in package iteration1
// L25:   validate[4, [V &lt;: Int] =&gt;&gt; V &gt; 5 &amp;&amp; V &lt; 10]
<br>The compilation error is not very helpful, especially compared to the message produced by <a data-tooltip-position="top" aria-label="https://github.com/fthomas/refined" rel="noopener nofollow" class="external-link" href="https://github.com/fthomas/refined" target="_blank">refined</a>:<br>Left predicate of ((4 &gt; 5) &amp;&amp; (4 &lt; 10)) failed: Predicate failed: (4 &gt; 5).
<br>It's something we will work on in iteration 2. That being said, with just a few lines of code we were able to get that basic version working.<br><br>Mechanisms we've used so far, compiletime operators and implicit resolution, are not enough to implement friendly validation errors. That's because the result of implicit resolution is binary - either the implicit had been found or not. We need richer information in case of failure.<br>To do that, we will explore another new feature of Scala 3, which is inline.<br>First, we need to define an ADT for predicates:<br>sealed trait Pred
class And[A &lt;: Pred, B &lt;: Pred]         extends Pred
class Leaf                              extends Pred
class LowerThan[T &lt;: Int &amp; Singleton]   extends Leaf
class GreaterThan[T &lt;: Int &amp; Singleton] extends Leaf
<br>The only notable thing in the above is mixing in Singleton. It restricts type T into being a singleton type, so that LowerThan[Int] will not compile.<br>Then, we have to interpret this ADT at compile-time:<br>import scala.compiletime.*
import scala.compiletime.ops.int.*

trait Validated[E &lt;: Pred]

implicit inline def mkVal[V &lt;: Int &amp; Singleton, E &lt;: Pred](v: V): Validated[E] =
  inline erasedValue[E] match
    case _: LowerThan[t] =&gt;
      inline if constValue[V] &lt; constValue[t]
        then new Validated[E] {}
        else
          inline val vs    = constValue[ToString[V]]
          inline val limit = constValue[ToString[t]]
          error("Validation failed: " + vs + " &lt; " + limit)
    case _: GreaterThan[t] =&gt; // ommited here since it's symmetrical to LowerThan
    case _: And[a, b] =&gt;
      inline mkVal[V, a](v) match
        case _: Validated[_] =&gt;
          inline mkVal[V, b](v) match
            case _: Validated[_] =&gt; new Validated[E] {}
<br>There are a few things worth noting here:<br>
<br>
mkVal has an inline modifier which tells the compiler that it should inline any invocation of this method at compile-time. If it's not possible, compiler will fail the compilation

<br>
erasedValue comes from compiletime package. It's usually used in tandem with inline match. It allows us to match on the expression type, but we cannot access extracted value as that code is executed at compile-time

<br>
You could have noticed a lower-case letter used for the type parameter in case _: LowerThan[t], something against the usual convention. That was not a choice though. In Scala 3 you must use a lower-case identifier for a type being extracted from a pattern match. Using case _: LowerThan[T] would mean that the match would succeed only if LowerThan is parametrized with an already known type T. I like to compare that to term-level pattern match in which there's also a distinction between case a =&gt; and case `a` =&gt;, which in regards to types becomes case _: V[a] and case _: V[A] respectively

<br>
constValue comes from compiletime too. It returns the value of a singleton type

<br>
ToString is a type-level counterpart of toString available only for singleton types of Int, so that val a: ToString[5] = "5" holds

<br>
Calling error fails the compilation with provided message. In Scala 3.0.0 it cannot be invoked with interpolated string, yet it might be <a data-tooltip-position="top" aria-label="https://github.com/lampepfl/dotty/issues/10315" rel="noopener nofollow" class="external-link" href="https://github.com/lampepfl/dotty/issues/10315" target="_blank">possible in future</a>.

<br>
mkVal is defined as an implicit conversion so it will never be called explicitly

<br>Once you got acquainted with these new Scala constructs, the code should not be hard to follow. The great news is that it's all it takes to have reasonable refinements types for Int <a data-tooltip-position="top" aria-label="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote1" rel="noopener nofollow" class="external-link" href="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote1" target="_blank">1</a>. Let's try it out:<br>val a: Validated[LowerThan[10]] = 6
val b: Validated[GreaterThan[5] And LowerThan[10]] = 6

// val y: Validated[GreaterThan[5] And LowerThan[10]] = 1
// fails with:
// Validation failed: 1 &gt; 5
<br><br>Since the core functionality of refined boils down to preventing some code from being compiled, we have to specify negative test cases as code snippets that do not compile. In Scala 3 there's a built-in operation for that: scala.compiletime.testing.typeCheckErrors. We can employ it to write assertions:<br>import scala.compiletime.testing.typeCheckErrors

class IntSpec extends munit.FunSuite:
  test("Those should not compile") {
    val errs = typeCheckErrors("val x: Validated[LowerThan[10]] = 16")
    assertEquals(errs.map(_.message), List("Validation failed: 16 &lt; 10"))
  }
<br>If you're interested in cross-compiling your code you would be better off using munit's compilerErrors which for Scala 3 <a data-tooltip-position="top" aria-label="https://github.com/scalameta/munit/blob/7761b08fcf34396d90b22b1d086bdfd05bb733b0/munit/shared/src/main/scala-3/munit/internal/MacroCompat.scala#L38" rel="noopener nofollow" class="external-link" href="https://github.com/scalameta/munit/blob/7761b08fcf34396d90b22b1d086bdfd05bb733b0/munit/shared/src/main/scala-3/munit/internal/MacroCompat.scala#L38" target="_blank">uses</a> said built-in.<br>Once we have an implementation for Int, it would be interesting to do the same for String.<br><br>We will use the following as a motivating example:<br>val a: String Refined StartsWith["abc"] = "abcd"
<br>We had to add another type parameter in addition to the predicate. To express that that Refined[T, Predicate] type was introduced. You can find the whole code of that <a data-tooltip-position="top" aria-label="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3/src/main/scala/iteration3" rel="noopener nofollow" class="external-link" href="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3/src/main/scala/iteration3" target="_blank">iteration</a> in the accompanying repository. However, most of it is a straightforward structure not related to metaprogramming so we will jump right to the relevant bits instead.<br>What's interesting is how to actually implement StartsWith predicate at compile-time.<br>The first attempt might be to do the same thing that was done with Int:<br>transparent inline def checkPredString[V &lt;: String &amp; Singleton, E &lt;: Pred]: Boolean =
    inline erasedValue[E] match
      case _: StartsWith[t] =&gt;
        inline if constValue[V].startsWith(constValue[t])
        ...
<br>If we try to invoke it, it will end up with such compilation error:<br>Cannot reduce `inline if` because its condition is not a constant value: "abcd".startsWith("abc")
<br>The problem is that we're trying to call non-inline method startsWith from an inline method. Since compiler cannot reduce startsWith it just cannot be invoked there.<br>The solution to that problem is writing a simple macro:<br>transparent inline def startsWith(inline v: String, inline pred: String): Boolean =
    ${ startsWithC('v, 'pred)  }

def startsWithC(v: Expr[String], pred: Expr[String])(using Quotes): Expr[Boolean] =
    val res = v.valueOrError.startsWith(pred.valueOrError)
    Expr(res)
<br>In the macro implementation (i.e. startsWithC) we are not limited to calling only inline methods; therefore, we can call String.startsWith. From my limited experience with Scala 3 macros, the tricky part is to get a value of type T from Expr[T] for non-primitive types.<br>We can freely call macro startsWith from the inline method checkPredString which completes our exercise.<br><br>Another new Scala 3 feature used in the macro definition is modifier transparent. When it's used in inline method signature <a data-tooltip-position="top" aria-label="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote2" rel="noopener nofollow" class="external-link" href="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote2" target="_blank">2</a>, it allows compiler to specialize return type to a more precise type.<br>Getting back to our example, let's take a look at how checkPredString is used:<br>implicit inline def mkValString[V &lt;: String &amp; Singleton, E &lt;: Pred](v: V): Refined[V, E] =
    inline if checkPredString[V, E]
    then Refined.unsafeApply(v)
    else error("Validation failed")
<br>If we remove transparent from checkPredString signature the above code would fail compilation with that message:<br>Cannot reduce `inline if` because its condition is not a constant value: (true:Boolean).&amp;&amp;(true:Boolean):Boolean
<br>Without transparent compiler sees return type of checkPredString as a Boolean which is not enough to inline the code. In contrast to that, with transparent, it would be a concrete type true or false depending on the validation result.<br><br>We've ended up with a code supporting simple compile-time predicates for Int and String with very few lines of code. If you're familiar with Scala 3 metaprogramming building blocks such as inline, the code is straigforward to read. Of course, compared to real refinement types libraries there are many capabilities missing, like predicates inference or runtime lifting to refined types. This is something I explore in <a data-tooltip-position="top" aria-label="https://github.com/note/mini-refined" rel="noopener nofollow" class="external-link" href="https://github.com/note/mini-refined" target="_blank">mini-refined</a>.<br>If you're interested more in Scala 3 metaprogramming capabilities, explore links in the next section.<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3" rel="noopener nofollow" class="external-link" href="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3" target="_blank">full source code</a> of examples presented in this article
<br><a data-tooltip-position="top" aria-label="http://dotty.epfl.ch/docs/reference/metaprogramming/toc.html" rel="noopener nofollow" class="external-link" href="http://dotty.epfl.ch/docs/reference/metaprogramming/toc.html" target="_blank">Scala 3 documentation</a> on metaprogramming
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OPBuCQRgyV4" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=OPBuCQRgyV4" target="_blank">talk</a> by Josh Suereth on inline
<br><a data-tooltip-position="top" aria-label="https://github.com/note/mini-refined" rel="noopener nofollow" class="external-link" href="https://github.com/note/mini-refined" target="_blank">mini-refined</a> - project exploring further ideas proposed in this article
<br>1: One thing might bother you in the method signature itself:<br>implicit inline def mkVal[V &lt;: Int &amp; Singleton, E &lt;: Pred](v: V): Validated[E]
<br>Why do we provide value being validated both on type-level and on term-level? Wouldn't such signature suffice:<br>implicit inline def mkVal[E &lt;: Pred](v: Int &amp; Singleton): Validated[E]
<br>Given the new signature we just need to replace all constValue[V] in the previous implementation with v and that's it, right?<br>The answer is mostly yes. It would compile indeed but whenever you call mkVal with a value failing validation, instead of a nice error message you would get:<br>"A literal string is expected as an argument to `compiletime.error`. Got \"Validation failed: \".+(4).+(\" &lt; \").+(limit)
<br>The issue, again, is lack of <a data-tooltip-position="top" aria-label="https://github.com/lampepfl/dotty/issues/10315" rel="noopener nofollow" class="external-link" href="https://github.com/lampepfl/dotty/issues/10315" target="_blank">constant folding</a> which occurs in this line:<br>error("Validation failed: " + v + " &lt; " + limit)
<br>Reminder - in the previous version we used constValue[V] instead of just v. Therefore, at least for now, we have to duplicate validated value on both type- and term-level. <a data-tooltip-position="top" aria-label="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#afootnote1" rel="noopener nofollow" class="external-link" href="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#afootnote1" target="_blank">↩</a><br>2: transparent can be also used in <a data-tooltip-position="top" aria-label="https://dotty.epfl.ch/docs/reference/other-new-features/transparent-traits.html#transparent-traits" rel="noopener nofollow" class="external-link" href="https://dotty.epfl.ch/docs/reference/other-new-features/transparent-traits.html#transparent-traits" target="_blank">conjunction with traits</a>, in which case it has entirely]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/scala/refinement-types-in-scala-3.html</link><guid isPermaLink="false">Computer Science/Programming Language/JVM/Scala/Refinement types in Scala 3.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:47:36 GMT</pubDate></item><item><title><![CDATA[scala 中 Any、AnyRef、Object、AnyVal 关系以及主要特点分析]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:scala" class="tag" target="_blank" rel="noopener nofollow">#scala</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>
<br>Any 是一个 abstract 类，&nbsp;scala 中的顶级父类
<br>AnyVal 是一个 abstract&nbsp;类，继承 Any，目的是取代 primary 类型
<br>AnyRef 是一个 trait，继承 Any，重写了 Any 中部分方法
<br>Any 和 Object 定义上没有任何关系
<br>AnyRef 和 Object 定义上没有任何关系
<br>scala 的继承体系是通过 Any、AnyRef&nbsp; 实现的。为了兼容 java 的继承体系，scala 编译器将 AnyRef 置于与 Object 同等地位，二者的&nbsp; Class 类型相同。因此凡是 Object 继承体系的子类，都是&nbsp;&nbsp;AnyRef&nbsp; 的子类；而 AnyRef 继承体系生成的子类，也是&nbsp;Object 的子类。这些子类既是 AnyRef 又是 Object，这样 scala 保证定义的类能够被 jvm 加载，而编码时我们按照 scala 语言书写即可。
<br>scala 让编程者感觉 Any 类是&nbsp; scala 的顶级父类。作为&nbsp;&nbsp;jvm 来说，Object 才是顶级父类，scala 编译器必然将 Any、AnyRef&nbsp;编译为 Object 的子类型，这是 scala 编译器来实现的。 编程者完全无感。
<br>scala 类构造同时结合了 c++ 和 javascript 特点，使用方式类似于 javascript。
<br>总结：scala 语法规定了自己的继承体系(Any)，本质跟 java 不同，兼容 java。<br>package scala

abstract class Any {
  def equals(that: Any): Boolean // 值比较
  def hashCode(): Int // hash 值
  def toString(): String

  final def getClass(): Class[_] = sys.error("getClass")

  final def ==(that: Any): Boolean = this equals that // 值比较，参数可为 null
  final def !=(that: Any): Boolean = !(this == that) // 值比较
  final def ## : Int = sys.error("##") // hash 值，参数可为 null
  final def isInstanceOf[T0]: Boolean = sys.error("isInstanceOf") //是否为 T0 实例
  final def asInstanceOf[T0]: T0 = sys.error("asInstanceOf") //强转为 T0
}

abstract class AnyVal extends Any {
  def getClass(): Class[_ &lt;: AnyVal] = null
}

trait AnyRef extends Any

![复制代码](https://assets.cnblogs.com/images/copycode.gif)

 继承示例：

![复制代码](https://assets.cnblogs.com/images/copycode.gif)

// 父类  
class PersonFather(name: String, age: Int) {
  println("PersonRather is created!")

  def walk(): Unit = {
    println("Person walking ...")
  }
}
  
// 继承
class StudentSon(name: String, age: Int, var stuNo: Int) extends PersonFather(name, age) {
  println("StudentSon is created!")

  override def walk(): Unit = {
    super.walk()
    println("Student walking ......")
  }
}
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/scala/scala-中-any、anyref、object、anyval-关系以及主要特点分析.html</link><guid isPermaLink="false">Computer Science/Programming Language/JVM/Scala/scala 中 Any、AnyRef、Object、AnyVal 关系以及主要特点分析.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:47:36 GMT</pubDate></item><item><title><![CDATA[Overview]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:machine-learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#machine-learning</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:algorithm" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#algorithm</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:scala" class="tag" target="_blank" rel="noopener nofollow">#scala</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:machine-learning" class="tag" target="_blank" rel="noopener nofollow">#machine-learning</a> <a href="https://muqiuhan.github.io/wiki?query=tag:algorithm" class="tag" target="_blank" rel="noopener nofollow">#algorithm</a><br><br>This is the first in series of posts which will cover how one might setup a server to perform efficient inference of neural network models on both CPUs and GPUs using the <a data-tooltip-position="top" aria-label="https://www.scala-lang.org/" rel="noopener nofollow" class="external-link" href="https://www.scala-lang.org/" target="_blank">Scala</a> programming language. This entry will introduce key concepts at a high level as well as introduce the baseline neural network model we will use throughout the series.<br>Note that throughout this series certain concept or technologies will be mentioned but not explained in detail. This is because most of these technologies are deep and complex in their own right and there is simply not enough time to discuss them here. Instead we will provide a brief description, a link to find more information and a justification as to why that technology is important.<br>One such topic is machine learning and neural networks as a whole. The motivation of this series is not to become an expert on machine learning or neural networks. In fact the training of neural networks will not be covered in any capacity. We are simply interested with the integration of a pre-trained network into a live application. We model a given network as a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Pure_function" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Pure_function" target="_blank">pure function</a> which accepts an input tensor A as well as weights W, and returns an output B.<br><br>These posts will be geared towards those with some experience with Scala, specifically with experience with the <a data-tooltip-position="top" aria-label="https://typelevel.org/" rel="noopener nofollow" class="external-link" href="https://typelevel.org/" target="_blank">Typelevel</a> ecosystem. The idea is that this series will help provide a happy path to getting started with high performance machine learning inference, for developers using this stack. However if you do not have a ton of experience with Scala or Typelevel, you should still be able to follow along as we will link to relevant documentation, and tools like Triton, gRPC and ONNX are language agnostic, so the knowledge you gain on these topics will be transferable to other languages.<br><br>The motivation for these posts is two-fold. First is that neural networks are increasingly common part of solutions in just about every technical domain, and thus it is important to leverage them in a efficient manner. Solutions such as <a data-tooltip-position="top" aria-label="https://aws.amazon.com/pm/sagemaker/" rel="noopener nofollow" class="external-link" href="https://aws.amazon.com/pm/sagemaker/" target="_blank">AWS Sagemaker</a>, <a data-tooltip-position="top" aria-label="https://www.paperspace.com/" rel="noopener nofollow" class="external-link" href="https://www.paperspace.com/" target="_blank">Digital Ocean Paperspace</a> and <a data-tooltip-position="top" aria-label="https://azure.microsoft.com/en-us/products/machine-learning/" rel="noopener nofollow" class="external-link" href="https://azure.microsoft.com/en-us/products/machine-learning/" target="_blank">Azure Machine Learning</a> exist to fill this gap, but there are reasons you would not want to use those services, whether it be organization rules on <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Data_governance" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Data_governance" target="_blank">data governance</a>, the avoidance of vendor lock or simply that those services aren’t an appropriate solution to your specific problem. The second motivation is simply to learn. Taking a problem, approaching it from multiple angles and then continuously refining and analyzing our solutions is a great way to gain a deep understanding of a certain domain.<br><br>We will use the well studied problem of image classification as our baseline problem. Image classification has uses in everywhere from self-driving vehicles to <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2202.08546" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2202.08546" target="_blank">medical image analysis</a>. <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1506.02640" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/1506.02640" target="_blank">YOLO</a> (You Only Look Once) is a family of vision models can be used to address tasks such as <a data-tooltip-position="top" aria-label="https://docs.ultralytics.com/tasks/" rel="noopener nofollow" class="external-link" href="https://docs.ultralytics.com/tasks/" target="_blank">classification, detection and segmentation</a>. At the time of writing the most recent version of YOLO is YOLOv8 and that is the model we will use throughout this series. <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.09972" rel="noopener nofollow" class="external-link" href="https://arxiv.org/abs/2305.09972" target="_blank">YOLOv8</a> was trained on the <a data-tooltip-position="top" aria-label="https://www.image-net.org/" rel="noopener nofollow" class="external-link" href="https://www.image-net.org/" target="_blank">ImageNet</a>, which is a dataset of hierarchically organized concepts into nodes in a tree. Each node has around 1000 images that relate to it. Our systems will take an image as input and return the top K predictions for the label that best describes the image, along with the probability assigned to that label. The diagram below shows an example of the inference process at a high level. A user sends a picture of a golden retriever as an HTTP request via curl, the service processes the request and returns a map from the image name to an ordered list of tuples where the first element is the assigned probability that the image belongs to the label, which is the second element.<br>Simplified Inference Process. A request is made to our live service and a mapping of the image to a sorted list of pairs is returned.<br><br><br>The total amount of requests that a system can process over a period of time. Systems are often measured using throughput as high throughput systems scale better in general. Consider a case in which you expect to be be receiving ~1000 requests/second for a sustained period of time and you want each request to take no more than one second. If your system has a measured throughput of 100 requests/second you now know that you will need to run and load balance along at least 10 instances of your hypothetical service. Increasing the throughput of your system will reduce the number of instances you need to run.<br><br>The total time in-between when a network request is sent to a system, and when a response is received. As programmers we generally want to develop low latency systems. What is determined as low enough depends on a use case. For a search engine like google or an internet database like <a data-tooltip-position="top" aria-label="https://www.imdb.com/" rel="noopener nofollow" class="external-link" href="https://www.imdb.com/" target="_blank">IMDB</a>, a few fractions of a second of latency is often low enough, as humans tend to perceive that as instantaneous. However for something like a self driving car or an application that deals with high frequency financial data, a few milliseconds of latency may be the target. We care about latency as that will be one of the metrics we use to benchmark our systems.<br><br>Central Processing Unit. The main processor on a given machine. Unless explicitly specified otherwise the instructions generated by a programming language will run on this processor.<br><br>Graphical Processing Unit. A multi-core processor capable of efficiently performing <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Embarrassingly_parallel" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Embarrassingly_parallel" target="_blank">embarrassingly parallel</a> tasks.&nbsp; We care about them as tensor operations commonly found in neural networks are often embarrassingly parallel and thus the training and evaluation of neural networks is greatly accelerated by a GPU.<br><br><a data-tooltip-position="top" aria-label="https://protobuf.dev/" rel="noopener nofollow" class="external-link" href="https://protobuf.dev/" target="_blank">Protocol Buffers are language-neutral, platform-neutral extensible mechanisms for serializing structured data</a>. We care about them as they are a building block for both gRPC and the ONNX file format, and they allow for type safe network RPC calls.<br><br><a data-tooltip-position="top" aria-label="https://grpc.io/" rel="noopener nofollow" class="external-link" href="https://grpc.io/" target="_blank">Googles Remote Procedure Call framework</a> is a generalized method for different systems to communicate with each other. Data is serialized using Protocol Buffers and is then sent across network boundaries in an extremely efficient way. Aside from being very performant gRPC is also driven by a specification language designed to have code generation tools built around it. This means that you can expose a gRPC server written in Scala and then generate a client (stub) in python, Ruby, Go or any other language to interact with the Scala service. It is important to note that gRPC is not supported by browsers and is generally used by back-end services to communicate with each other.<br><br><a data-tooltip-position="top" aria-label="https://onnx.ai/" rel="noopener nofollow" class="external-link" href="https://onnx.ai/" target="_blank">Open Neural Network Exchange</a> is an open format used to describe a neural network. This format is de-coupled from the actual runtime that executes the network. The ability to swap out back-ends is very powerful as different problems will have different hardware constraints. If you do not have GPUs available you can use the default CPU runtime. If you do have access to GPUs then you can use TensorRT or a <a data-tooltip-position="top" aria-label="https://developer.nvidia.com/cuda-toolkit" rel="noopener nofollow" class="external-link" href="https://developer.nvidia.com/cuda-toolkit" target="_blank">CUDA</a> runtime. As advances in the machine learning field advance more performant runtimes may be developed and if they choose to support ONNX then you will be able to use them with little to no refactoring.&nbsp; Most frameworks for developing and training neural networks such as PyTorch, TensorFlow or MXNet support exporting to ONNX format. This means you can have multiple different projects using different frameworks to develop your deliverables and by exporting them to ONNX you wont have to change your deployment strategy.<br><br>An SDK developed by Nvidia to optimize and accelerate inference on GPUs. Built on top of CUDA libraries. We want to use it as it can provide state of the art latency and throughput.<br><br>When performing model inference a neural network can either process one tensor at a time, or it can process a batch of tensors. Due to the parallel nature of neural computation processing a batch of tensors is often much more efficient. When deploying a live service that can take requests from multiple sources, we may not have enough data to “fill” a batch with any given request. In-flight batching is the process of taking requests that arrive at roughly the same time and dynamically batch them together.<br><br>A model deployment server developed by Nvidia. It takes a model and creates either HTTP or gRPC endpoints to serve requests. It can target different back-ends such as ONNX or TensorRT. It also has the ability to perform inflight batching.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html</link><guid isPermaLink="false">Computer Science/Programming Language/JVM/Scala/Setup a server to perform efficient inference of neural network models.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:47:36 GMT</pubDate></item><item><title><![CDATA[Understanding JVM Garbage Collector Performance]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:jvm" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#jvm</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:gc" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#gc</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:jvm" class="tag" target="_blank" rel="noopener nofollow">#jvm</a> <a href="https://muqiuhan.github.io/wiki?query=tag:gc" class="tag" target="_blank" rel="noopener nofollow">#gc</a><br>
<a rel="noopener nofollow" class="external-link" href="https://mill-build.org/blog/6-garbage-collector-perf.html" target="_blank">https://mill-build.org/blog/6-garbage-collector-perf.html</a>
<br><br>从理论上来说，GC的性能主要集中在两个方面，一是程序用于收集垃圾的时间百分比，而不是实际工作，这个值越低越好。二是程序在收集垃圾时完全暂停的最长时间，这个值也是越低越好。<br>分开这两个指标的原因是：<br>
<br>有些程序只关心自身的运行，例如，如果一个程序只关心完成大批量分析需要多长时间，而不关心 GC 是否会导致它中途暂停。
<br>其他程序只关心暂停时间，例如，玩电子游戏的人并不关心它是否能跑得比他们的眼睛能感知的更快，但他们关心它不会在爽玩时暂停明显的时间。
<br>从这些有限描述中可以对简单垃圾回收器的性能做出一些理论上的推断：<br>
<br>暂停时间应与 live-set 的大小成正比。这是因为集合涉及跟踪、复制和更新 live-set 中的引用。
<br>暂停时间不取决于要收集的垃圾量。收集器根本没有花时间查看或扫描垃圾对象，它们所在的堆在垃圾收集后会被直接擦除。
<br>集合之间的间隔与可用内存成反比。只需要在分配的垃圾填满了程序在存储 live-set 所需的 “额外” 堆内存时运行垃圾收集。
<br>GC 开销是暂停时间除以间隔，或与额外内存成正比，与实时集大小和堆大小成反比
<br>换言之：<br>
<br>allocation_cost = O(1)
<br>gc_pause_time = O(live-set)
<br>gc_interval = O(heap-size - live-set)
<br>gc_overhead = gc_pause_time / gc_interval
<br>gc_overhead = O(live-set / (heap-size - live-set))
<br>从这个结论中，可以看到一些不直观的结果：<br>
<br>更多的内存不会减少暂停时间: gc_pause_time = O（live-set），因此暂停时间不取决于你有多少堆大小。
<br>提供更多内存不会改善 GC 开销:  gc_overhead = O(live-set / (heap-size - live-set)) ，因此提供越大的堆大小才能拥有更少的 GC 开销（将更大百分比的程序时间花在有用的工作上）。
<br>相反，提供与程序完全相同的内存是最坏的情况！ gc_overhead = O(live-set / (heap-size - live-set)) 什么时候 heap-size = live-set 表示 gc_interval = 0 且 gc_overhead = 无穷大：程序将不断需要运行开销极大的收集器，并且没有时间执行实际工作。因此，垃圾回收器需要额外的内存才能使用，此外，还需要分配程序中的所有对象所需的内存。
<br><br>Garbage collectors are a core part of many programming languages. While they generally work well, on occasion when they go wrong they can fail in very unintuitive ways. This article will discuss the fundamental design of how garbage collectors work, and tie it to real benchmarks of how GCs perform on the Java Virtual Machine. You should come away with a deeper understanding of how the JVM garbage collector works and concrete ways you can work to improve its performance in your own real-world projects.<br><br>To understand how real-world JVM garbage collectors works, it is best to start by looking at a simple example garbage collector. This will both give an intuition for how things work in general, and also help you notice when things diverge from this idealized example.<br><br>At its core, a garbage collector helps manage the free memory of a program, often called the heap. The memory of a program can be modelled as a linear sequence of storage locations, e.g. below where we have 16 slots in memory:<br><img alt="Pasted image 20250111190234.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111190234.png"><br>
These storage locations can contain objects (below named foo, bar, qux, baz) that take up memory and may reference other objects (solid arrows). Furthermore, the values may be referenced from outside the heap (dashed lines), e.g. from the "stack" which represents local variables in methods that are currently being run (shown below) or from static global variables (not shown). We keep a free-memory pointer to the first empty slot on the right.<br>If we want to allocate a new object new1, we can simply put it at the location of the free-memory pointer (green below), and bump the pointer 1 slot to the right:<br><img alt="Pasted image 20250111190404.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111190404.png"><br>
Similarly, objects may stop being referenced, e.g. bar below no longer has a reference pointing at it from the stack. This may happen because a local variable on the stack is set to null, or because a method call returned and the local variables associated with it are no longer necessary:<br>
<img alt="Pasted image 20250111190441.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111190441.png"><br>
For the purposes of this example, we show all objects on the heap taking up 1 slot, but in real programs the size of each object may vary depending on the fields it has or if it’s a variable-length array.<br><br>The simplest kind of garbage collector splits the 16-slot heap we saw earlier into two 8-slot halves. If we want to allocate 4 more objects (new2, to new5), but there are only 3 slots left in that half of the heap, we will need to do a collection:<br>
<img alt="Pasted image 20250111190820.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111190820.png"><br>
To do a collection, the GC first starts from all non-heap references (e.g. the STACK references above) often called "GC roots". It then traces the graph of references, highlighted red below:<br>
<img alt="Pasted image 20250111190859.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111190859.png"><br>
Here, we can see that foo is not referenced ("garbage"), qux and new1 are referenced directly from the STACK, and baz is referenced indirectly from qux. bar is referenced by foo, but because foo is itself garbage we can count bar as garbage as well.<br>We then copy all objects we traced (often called the live-set) from HALF1 to HALF2, adjust all the references appropriately. Now HALF2 is the half of the heap in use, and HALF1 can be reset to empty:<br><img alt="Pasted image 20250111191028.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111191028.png"><br>
This collection has freed up 5 slots, so we now have space to allocate the 4 new2 to new5 objects we wanted (green) starting from our free-memory pointer:<br>
<img alt="Pasted image 20250111191058.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111191058.png"><br>You may notice that the objects foo and bar disappeared. This is because foo and bar were not referenced directly or indirectly by any GC roots: they were unreachable, and thus considered "garbage". These garbage objects were not explicitly deleted, but simply did not get copied over from HALF1 to HALF2 during collection, and thus were wiped out when HALF1 was cleared.<br>As your program executes, the methods actively running may change, and thus the references (both from stack to heap and between entries on your heap) may change. For example, we may stop referencing qux, which also means that baz is now unreachable:<br><img alt="Pasted image 20250111191142.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111191142.png"><br>Although qux and baz are now "garbage", they still take up space in the heap. Thus, if we want to allocate two new objects (e.g. new6 and new7), and there is only one slot left on the heap (above), we need to repeat the garbage collection process: tracing the objects transitively reachable (new1, new2, new3, new4, new5), copying them from HALF2 to HALF1, adjusting any references to now use HALF1 as the new heap, and clearing anything that was left behind in HALF2. This then gives us enough space to allocate new6 and new7 (below in green):<br><img alt="Pasted image 20250111191255.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111191255.png"><br>This process can repeat as many times as necessary: as long as there are some objects that are unreachable, you can run a collection and copy the "live" objects to the other half of the heap, freeing up some space to allocate new objects. The only reason this may fail is that if you run a collection and there still isn’t enough space to allocate the objects you want; that means your program has run out of memory, and will fail with an OutOfMemoryError or similar.<br>Even this simplistic GC has a lot of interesting properties, and you may have heard these terms or labels that can apply to it:<br>
<br>semi-space garbage collector, because of the way it splits the heap into two halves
<br>copying garbage collector, because it needs to copy the heap objects back and forth between HALF1 and HALF2
<br>tracing garbage collector, because of the way it traverses the graph of heap references in order to decide what to copy.
<br>stop the world garbage collector, because while this whole trace-copy-update-references workflow is happening, we have to stop the program to avoid race conditions between the garbage collector and the program code.
<br>compacting garbage collector, because every time we run a GC, we copy everything to the left-most memory, avoiding the memory fragmentation that occurs with other memory management techniques such as <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Reference_counting" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Reference_counting" target="_blank">Reference Counting</a>.
<br>Most modern GCs are considerably more complicated than this: e.g. they may have optimizations to avoid wasting half the heap by leaving it empty, or they may have optimizations for handling short-lived objects, but at their heart this is still what they do. And understanding the performance characteristics of this simple, naive GC can help give you an intuition in how GCs compare to other memory management strategies, and how modern GCs behave in terms of performance.<br><br><a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Reference_counting" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Reference_counting" target="_blank">Reference Counting</a> is another popular memory management strategy that Garbage Collection is often compared to. Reference counting works by keeping track of how many incoming references each object has, and when that number reaches zero the object can be collected. This approach has a few major differences from that of a tracing GC. We discuss a few of them below:<br><br>Program that use reference counting tend to find their heap getting more and more fragmented over time We can see this in the heap diagrams: the tracing garbage collector heaps above always had a single block of empty space to the right, and had the new objects allocated in ascending order from left-to-right:<br>
<img alt="Pasted image 20250111191658.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111191658.png"><br>
In contrast, reference counted heaps (e.g. below) tend to get fragmented, with free space scattered about, and the allocated objects jumbled up in no particular order:<br><img alt="Pasted image 20250111191743.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111191743.png"><br>There are two main ways this affect performance:<br>
<br>With garbage collection all the free memory is always on the right in one contiguous block, so an allocation just involves putting the object at the free-pointer location and moving free-pointer one slot to the right. Furthermore, newly allocated objects (which tend to be used together) are placed next to each other, making them more cache-friendly and improving access performance.
<br>With reference counting objects are usually freed in-place, meaning that the free space is scattered throughout the heap, and you may need to scan the entire heap from left-to-right in order to find a spot to allocate something. There are data structures and algorithms that can make allocation faster than a linear scan, but they will never be as fast as the single pointer lookup necessary with a GC.
<br><br>Objects that reference each other cyclically can thus cause memory leaks when their objects never get collected, resulting in the program running out of memory even though much of the heap could be cleaned up by a tracing garbage collector.<br>For example, consider the following heap, identical to the one we started with, but with an additional edge from bar to foo (green), and with the edge from the stack to bar removed:<br><img alt="Pasted image 20250111191936.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111191936.png"><br>
With reference counting, even though foo and bar cannot be reached by any external reference - they are "garbage" - each one still has a reference pointing at it from the other. Thus they will never get collected.<br>But with a tracing garbage collector, a collection can traverse the reference graph (red), and copy qux and baz to the other half of the heap, leaving foo and bar behind as garbage, despite the reference cycle between them.<br>Garbage Collection and Reference Counting have very different characteristics, and neither is strictly superior to the other in all scenarios. Many programming languages (e.g. Python) that use reference counting also have a backup tracing garbage collector that runs once in a while to clean up unreachable reference cycles and compact the heap, and most modern GCs (e.g. ZGC discussed below) use some reference-counting techniques as part of their implementation.<br><br>Typically, GC performance focuses on two main aspects:<br>
<br>Overhead: what % of the time your program is spent collecting garbage, rather than real work. Lower is better.
<br>Pause Times: what is the longest time your program is completely paused while collecting garbage. Lower is better.
<br>These two metrics are separate:<br>
<br>Some programs only care about throughput, e.g. if you only care about how long a big batch analysis takes to complete, and don’t care if it pauses in the middle to GC: you just want it to finish as soon as possible.
<br>Other programs only care about pause times, e.g. someone playing a videogame doesn’t care if it can run faster than their eye can perceive, but they do care that it does not freeze or pause for noticeable amounts of time while you are playing it.
<br>Even from the limited description above, we can already make some interesting inferences about how the performance of a simple garbage collector will be like.<br>
<br>Allocations in garbage collectors are cheap: when the heap is not yet full, we can just allocate things on the first empty slots on the right side of the heap and bump free-pointer, without having to scan the heap to find empty slots.
<br>Pause times should be proportional to the size of the live-set. That is because a collection involves tracing, copying, then updating the references within the live-set.
<br>Pause times would not depend on the amount of garbage to be collected. The collection we looked at above spend no time at all looking at or scanning for garbage objects, they simply all disappeared when their half of the heap was wiped out following a collection.
<br>Interval between collections is inversely proportional to free memory. We only need to run a collection when the garbage we allocate fills up the "extra" heap memory our program has on top of what is necessary to store the live-set.
<br>GC overhead is the pause time divided by the interval, or proportional to the extra memory and inversely proportional to the live-set size and heap size.
<br>In other words:<br>
<br>allocation_cost = O(1)
<br>gc_pause_time = O(live-set)
<br>gc_interval = O(heap-size - live-set)
<br>gc_overhead = gc_pause_time / gc_interval
<br>gc_overhead = O(live-set / (heap-size - live-set))
<br>Even from this small conclusions, we can already see some unintuitive results:<br>
<br>More memory does not reduce pause times! gc_pause_time = O(live-set), and so pause times do not depend on how much heap-size you have.
<br>There is no point at which providing more memory does not improve GC overhead! gc_overhead = O(live-set / (heap-size - live-set)), so providing larger and larger heap-sizes means less and less GC overhead, meaning a larger % of your program time is spent on useful work.
<br>Conversely, providing exactly as much memory as the program is the worst case possible! gc_overhead = O(live-set / (heap-size - live-set)) when heap-size = live-set means gc_interval = 0 and gc_overhead = infinity: the program will constantly need to run an expensive collections and have no time left to do actual work. Garbage collectors therefore need excess memory to work with, on top of the memory you would expect to need to allocate all the objects in your program.
<br>Even from this theoretical analysis, we have already found a number of surprising results in how GCs perform over time. Let’s now see how this applies to some real-world garbage collectors included with the Java Virtual Machine.<br><br>Now that we have run through a theoretical introduction and analysis of how GCs work and how we would expect them to perform, let’s look at some small Java programs and monitor how garbage collection happens when using them. For this benchmark, we’ll be using the following Java program:<br>public class GC {
    public static void main(String[] args) throws Exception{
        final long liveSetByteSize = Integer.parseInt(args[0]) * 1000000L;
        final int benchMillis = Integer.parseInt(args[1]);
        final int benchCount = Integer.parseInt(args[2]);
        // 0-490 array entries per object, * 4-bytes per entry,
        // + 20 byte array header = average 1000 bytes per entry
        final int maxObjectSize = 490;
        final int averageObjectSize = (maxObjectSize / 2) * 4 + 20;

        final int liveSetSize = (int)(liveSetByteSize / averageObjectSize);

        long maxPauseTotal = 0;
        long throughputTotal = 0;

        for(int i = 0; i &lt; benchCount + 1; i++) {
            int chunkSize = 256;
            Object[] liveSet = new Object[liveSetSize];
            for(int j = 0; j &lt; liveSetSize; j++) liveSet[j] = new int[j % maxObjectSize];
            System.gc();
            long maxPause = 0;
            long startTime = System.currentTimeMillis();

            long loopCount = 0;
            java.util.Random random = new java.util.Random(1337);
            int liveSetIndex = 0;

            while (startTime + benchMillis &gt; System.currentTimeMillis()) {
                if (loopCount % liveSetSize == 0) Thread.sleep(1);
                long loopStartTime = System.currentTimeMillis();
                liveSetIndex = random.nextInt(liveSetSize);
                liveSet[liveSetIndex] = new int[liveSetIndex % maxObjectSize];
                long loopTime = System.currentTimeMillis() - loopStartTime;
                if (loopTime &gt; maxPause) maxPause = loopTime;
                loopCount++;
            }
            if (i != 0) {
                long benchEndTime = System.currentTimeMillis();
                long bytesPerLoop = maxObjectSize / 2 * 4 + 20;
                throughputTotal += (long) (1.0 * loopCount * bytesPerLoop / 1000000 / (benchEndTime - startTime) * averageObjectSize);
                maxPauseTotal += maxPause;
            }

            System.out.println(liveSet[random.nextInt(liveSet.length)]);
        }

        long maxPause = maxPauseTotal / benchCount;
        long throughput = throughputTotal / benchCount;

        System.out.println("longest-gc: " + maxPause + " ms, throughput: " + throughput + " mb/s");
    }
}
<br>This is a small Java program designed to do a rough benchmark of Java garbage collection performance. For each benchmark, it:<br>
<br>Starts off allocating a bunch of int[] arrays of varying size in liveSet, on average taking up 1000 bytes each.
<br>Loops continuously to allocate more int[]s and over-writes the references to older ones.
<br>Tracks how long each allocation takes to run: ideally it should be almost instant, but if that allocation triggers a GC it may take some time.
<br>Lastly, we print out the two numbers we care about in a GC: the maxPause time in milliseconds, and the throughput it is able to handle in megabytes per second (throughput being the opposite of overhead we mentioned earlier).<br>To be clear, this benchmark is rough. Performance will vary between runs, and on what hardware and software you run it (I ran it on a M1 Macbook Pro running Java 23). But the results should be clear even if the exact numbers will differ between runs.<br>You can run this program via:<br>&gt; java -Xmx1g GC.java 800 10000 5 # Default is -XX:+UseG1GC
&gt; java -Xmx1g -XX:+UseParallelGC GC.java 800 10000 5
&gt; java -Xmx1g -XX:+UseZGC GC.java 800 10000 5
<br>Above, -Xmx1g sets the heap size, the -XX: flags set the garbage collector, 800 sets the liveSet size (in megabytes), and 10000 and 5 set the duration and number of iterations to run the benchmark (here 10 seconds, 5 iterations). The measured pause times and allocation rate are averaged over those 5 iterations.<br>I used the following Java program to run the benchmark for a range of inputs to collect the numbers shown below:<br>package mill.main.client;

import java.util.ArrayList;
import java.util.List;
import java.util.Optional;

public class GCBenchmark {
    public static void main(String[] args) throws Exception {
        String[][] javaGcCombinations = { {"23", "G1"}, {"23", "Z"} };

        for (String[] combination : javaGcCombinations) {
            String javaVersion = combination[0];
            String gc = combination[1];

            System.out.println("Benchmarking javaVersion=" + javaVersion + " gc=" + gc);

            int[] liveSets = {400, 800, 1600, 3200, 6400};
            int[] heapSizes = {800, 1600, 3200, 6400, 12800};

            List&lt;List&lt;String[]&gt;&gt; lists = new ArrayList&lt;&gt;();

            for (int liveSet : liveSets) {
                List&lt;String[]&gt; innerList = new ArrayList&lt;&gt;();

                for (int heapSize : heapSizes) {
                    if (liveSet &gt;= heapSize) innerList.add(new String[]{"", ""});
                    else innerList.add(runBench(liveSet, heapSize, javaVersion, gc));
                }

                lists.add(innerList);
            }

            renderTable(liveSets, heapSizes, lists, 0);
            renderTable(liveSets, heapSizes, lists, 1);
        }
    }

    static String[] runBench(int liveSet, int heapSize, String javaVersion, String gc) throws Exception {
        System.out.println("Benchmarking liveSet=" + liveSet + " heapSize=" + heapSize);

        String javaBin = "/Users/lihaoyi/Downloads/amazon-corretto-" + javaVersion + ".jdk/Contents/Home/bin/java";

        ProcessBuilder processBuilder = new ProcessBuilder(
            javaBin, "-Xmx" + heapSize + "m", "-XX:+Use" + gc + "GC", "GC.java", "" + liveSet, "10000", "5"
        );

        Process process = processBuilder.start();
        process.waitFor();


        List&lt;String&gt; outputLines =
            new String(process.getInputStream().readAllBytes()).lines().toList();

        Optional&lt;String[]&gt; result = outputLines.stream()
            .filter(line -&gt; line.startsWith("longest-gc: "))
            .map(line -&gt; {
                String[] parts = line.split(", throughput: ");
                return new String[]{
                    parts[0].split(": ")[1].trim(),
                    parts[1].trim()
                };
            })
            .findFirst();

        return result.orElse(new String[]{"error", "error"});
    }

    static void renderTable(int[] liveSets, int[] heapSizes, List&lt;List&lt;String[]&gt;&gt; lists, int columnIndex) {
        StringBuilder header = new StringBuilder("| live-set\\heap-size | ");
        for (int heapSize : heapSizes) header.append(heapSize).append(" mb | ");
        System.out.println(header);
        for (int i = 0; i &lt; liveSets.length; i++) {
            StringBuilder row = new StringBuilder("| ").append(liveSets[i]).append(" mb | ");
            for (String[] pair : lists.get(i)) row.append(pair[columnIndex]).append(" | ");
            System.out.println(row);
        }
    }
}
<br><br>Running this on the default GC (G1), we get the followings numbers:<br>Pause Times:<br><img alt="Pasted image 20250111193311.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111193311.png"><br>Throughput:<br><img alt="Pasted image 20250111193326.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111193326.png"><br>Some things worth noting with ZGC:<br>
<br>In the lower heap-size benchmarks - with heap-size twice live-set - ZGC has worse pause times than the default G1GC (10s to 100s of milliseconds) but and worse throughput (2300-2600 mb/s rather than the 2800-3100 mb/s of G1GC).
<br>For larger heap-sizes - 4 times the live-set and above - ZGC’s pause times drop to single-digit milliseconds (1-10 ms), much lower than those of G1GC.
<br>As mentioned in the discussion on Theoretical GC Performance, for most garbage collectors pause times are proportional to the live set, and increasing the heap size does not help at all (and according to our G1 Garbage Collector Benchmarks, may even make things worse!). This can be problematic, because there are many use cases that cannot tolerate long GC pause times, but at the same time may require a significant amount of live data to be kept in memory, so shrinking the live-set is not possible.<br>ZGC provides an option here, where if you are willing to provide significantly more memory than the default G1GC requires, perhaps twice as much, you can get your pause times from 10-100s of milliseconds down to 1-2 milliseconds. These pause times remain low for a wide range of heap sizes and live set sizes, and can be beneficial for a lot of applications that cannot afford to just randomly stop for 100ms at a time. But the extra memory requirement means it’s not a strict improvement, and it really depends on your use case whether the tradeoff is worth it.<br><br>Now that we’ve studied garbage collections in theory, and looked at some concrete numbers, there are some interesting conclusions. First, the unintuitive things:<br>
<br>
Adding more memory does not improve GC pause times. It may even make things worse! This is perhaps the most unintuitive thing about garbage collectors: it seems so obvious that problems with memory management would be solved by adding more memory, but we can see from our theoretical analysis above why that is not the case, and we verified that empirically in benchmarks.

<br>
Caching data in-process can make garbage collection pause times worse! If you have problems with GC pause times then caching things in-memory will increase the size of your live-set and therefore make your pause times even worse! "LRU" caches in particular are the worst case for garbage collectors, which are typically optimized for collecting recently-allocated short-lived objects. In contrast, caching things out of process does not have this problem. Caching can be worthwhile to reduce redundant computation, but it is not a solution to garbage collection problems.

<br>
There will never be an exact amount of memory that a garbage-collected application needs. You can always reduce-overhead/increase-throughput by providing more memory, to make GCs less and less frequent, leaving more time to do useful work. And you can usually provide less memory, at the cost of more and more frequent GCs. Exactly how much memory to provide is thus something you tweak and tune rather than something you can calculate exactly.

<br>
Fewer larger processes can have worse GC performance than more smaller processes! There are many ways in which consolidating smaller processes into larger ones can improve efficiency: less per-process overhead, eliminating <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Inter-process_communication" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Inter-process_communication" target="_blank">inter-process communication</a> cost, etc. But GC pause times scale with total live set size, so combining two smaller processes into one large one can make pause times worse than they were before. Even if the large process does the same thing as the smaller processes, it can suffer from worse GC pause times.

<br>
You can reduce pause times by reducing the live-set. If you have very large in-process data structures, moving them somewhere else (e.g. into <a data-tooltip-position="top" aria-label="https://www.sqlite.org/" rel="noopener nofollow" class="external-link" href="https://www.sqlite.org/" target="_blank">SQLite</a>, <a data-tooltip-position="top" aria-label="https://github.com/redis/redis" rel="noopener nofollow" class="external-link" href="https://github.com/redis/redis" target="_blank">Redis</a>, or <a data-tooltip-position="top" aria-label="https://memcached.org/" rel="noopener nofollow" class="external-link" href="https://memcached.org/" target="_blank">Memcached</a>) would reduce the amount of objects the GC needs to trace and copy every collection, and reduce the pause times

<br>
Shorter-lived objects are faster to collect, due to most GCs being generational. This also ties into (1) above: caches tend to keep lots of long-lived objects in memory, which apart from slowing down collections due to the size of the live-set, also slows them down by missing out on the GC’s optimizations for short-lived objects.

<br>
Switch to the Z garbage collector lets you trade off memory for pause times. JVM programs are by default already very memory hungry compared to other languages (Go, Rust, etc.) and ZGC requires perhaps another 2x as much memory to work. But if you are willing to pay the cost, ZGC can bring pause times down from 50-500ms down to 1-5ms, which may make a big different for latency-sensitive applications.

<br>The Java benchmarks above were run on one particular set of hardware on one version of the JVM, and the exact numbers will differ when run on other hardware or JVM versions. Nevertheless, the overall trends that you can see would remain the same, as would the take-aways of what you need to know to understand garbage collector performance.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/understanding-jvm-garbage-collector-performance.html</link><guid isPermaLink="false">Computer Science/Programming Language/JVM/Understanding JVM Garbage Collector Performance/Understanding JVM Garbage Collector Performance.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 11:51:21 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111190234.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/programming-language/jvm/understanding-jvm-garbage-collector-performance/attachments/pasted-image-20250111190234.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[OCaml News 2023 - 3]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a><br>o-新鲜事儿<br>
<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ocaml-5-1-0-released/13021" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ocaml-5-1-0-released/13021" target="_blank">OCaml 5.1.0 released</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-dune-3-14/14096" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-dune-3-14/14096" target="_blank">[ANN] dune 3.14</a>

<br>
<a data-tooltip-position="top" aria-label="https://ocaml.codes/search/" rel="noopener nofollow" class="external-link" href="https://ocaml.codes/search/" target="_blank">code search</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-ocaml-codes-code-search-for-opam-packages/14092" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-ocaml-codes-code-search-for-opam-packages/14092" target="_blank">[ANN] ocaml.codes, code search for OPAM packages</a><br>


<br>
用 livegrep 基于 opam 包的源码做的代码搜索，还挺方便的。

<br>
<a data-tooltip-position="top" aria-label="https://melange.re/blog/posts/announcing-melange-3" rel="noopener nofollow" class="external-link" href="https://melange.re/blog/posts/announcing-melange-3" target="_blank">Announcing Melange 3</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-melange-3-0/14102" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-melange-3-0/14102" target="_blank">[ANN] Melange 3.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/learn-ocaml-1-0-is-out/14100" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/learn-ocaml-1-0-is-out/14100" target="_blank">Learn-OCaml 1.0 is out!</a><br>


<br>
<a rel="noopener nofollow" class="external-link" href="https://ocaml-sf.org/learn-ocaml-public/" target="_blank">https://ocaml-sf.org/learn-ocaml-public/</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/owlbarn/owl" rel="noopener nofollow" class="external-link" href="https://github.com/owlbarn/owl" target="_blank">GitHub - owlbarn/owl: Owl - OCaml Scientific Computing @ http://ocaml.xyz</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/owl-project-concluding/14117" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/owl-project-concluding/14117" target="_blank">Owl project concluding</a><br>


<br>
经过八年的维护，Owl项目即将终止<br>


<br><br>o-视频<br>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=jvQ7fj9LlVA" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=jvQ7fj9LlVA" target="_blank">Inferring Locality in OCaml | OCaml Unboxed</a><br>

<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=AGu4AO5zO8o" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=AGu4AO5zO8o" target="_blank">OCaml Locals Save Allocations | OCaml Unboxed</a><br>

<br><a data-tooltip-position="top" aria-label="https://watch.ocaml.org/w/qQzb94X9WM7zLif7FynPyN" rel="noopener nofollow" class="external-link" href="https://watch.ocaml.org/w/qQzb94X9WM7zLif7FynPyN" target="_blank">Ocsigen: Developing Web and mobile applications in OCaml – Jérôme Vouillon &amp; Vincent Balat</a><br>

<br><a data-tooltip-position="top" aria-label="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" rel="noopener nofollow" class="external-link" href="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" target="_blank">Verifying an Effect-Based Cooperative Concurrency Scheduler in Iris by Adrian Dapprich</a><br>

<br><br>o-博客 / 文章 / 帖子<br>
<br>
<a data-tooltip-position="top" aria-label="https://priver.dev/blog/dbcaml/dbcaml/" rel="noopener nofollow" class="external-link" href="https://priver.dev/blog/dbcaml/dbcaml/" target="_blank">Introducing DBCaml, Database toolkit for OCaml</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://priver.dev/blog/dbcaml/building-a-connnection-pool/" rel="noopener nofollow" class="external-link" href="https://priver.dev/blog/dbcaml/building-a-connnection-pool/" target="_blank">Building a Connnection Pool for DBCaml on top of riot</a>

<br>
<a data-tooltip-position="top" aria-label="https://ocamlpro.com/blog/2021_09_02_generating_static_and_portable_executables_with_ocaml/" rel="noopener nofollow" class="external-link" href="https://ocamlpro.com/blog/2021_09_02_generating_static_and_portable_executables_with_ocaml/" target="_blank">Generating static and portable executables with OCaml</a><br>


<br>
OCaml编译器没有内置生成静态可移植可执行文件的特性，这里提到了一些技巧

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/analog-of-promise-any-for-multicore-ocaml/14145" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/analog-of-promise-any-for-multicore-ocaml/14145" target="_blank">Analog of Promise.any() for Multicore OCaml</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/printf-vs-format/14130" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/printf-vs-format/14130" target="_blank">Printf vs. Format?</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/how-to-represent-tuples-in-ast/14095" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/how-to-represent-tuples-in-ast/14095" target="_blank">How to represent tuples in AST?</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/how-do-i-pass-an-unsigned-char-an-array-of-bytes-representing-binary-data-from-c-to-ocaml/14074" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/how-do-i-pass-an-unsigned-char-an-array-of-bytes-representing-binary-data-from-c-to-ocaml/14074" target="_blank">How do I pass an unsigned char * (an array of bytes representing binary data) from C to OCaml?</a><br>


<br><br>o-未来<br>
<br>
<a data-tooltip-position="top" aria-label="https://docs.google.com/forms/d/e/1FAIpQLSe1U_5KanTeKt1h9t5vjYohYXepXDhPCru4tsms4OcI5k0Fkw/viewform?pli=1" rel="noopener nofollow" class="external-link" href="https://docs.google.com/forms/d/e/1FAIpQLSe1U_5KanTeKt1h9t5vjYohYXepXDhPCru4tsms4OcI5k0Fkw/viewform?pli=1" target="_blank">How do we want to present OCaml to the World on OCaml.org?</a><br>


<br>
一个问卷，用于更好的改进 <a data-tooltip-position="top" aria-label="http://ocaml.org/" rel="noopener nofollow" class="external-link" href="http://ocaml.org/" target="_blank">ocaml.org</a> 有关学术和工业应用板块的内容。

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/feedback-help-wanted-upcoming-ocaml-org-cookbook-feature/14127" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/feedback-help-wanted-upcoming-ocaml-org-cookbook-feature/14127" target="_blank">Feedback / Help Wanted: Upcoming OCaml.org Cookbook Feature</a><br>


<br>
<a data-tooltip-position="top" aria-label="http://ocaml.org/" rel="noopener nofollow" class="external-link" href="http://ocaml.org/" target="_blank">ocaml.org</a> 准备上线一个cookbook页面，放一些如何用OCaml的生态解决常见需求的资源

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/state-of-compaction-in-ocaml-5/14121/1" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/state-of-compaction-in-ocaml-5/14121/1" target="_blank">State of compaction in OCaml 5?</a><br>


<br>
OCaml 5.2 的 compact heap 会将未使用的内存返回给操作系统。在 OCaml 5 的 GC 中，小于 128byte 的块用大小隔离池进行管理，比如有一个池，处理大小为 3byte 的分配，另一个池处理大小为 4byte 的分配等等，这样的池在每个Domain里都有。用这个方法分配速度很快，因为不用找合适的内存间隙了，只要找正确的池大小就行。<br>


<br><br>o-值得被注意的项目<br>
<br>
<a data-tooltip-position="top" aria-label="https://github.com/mbarbin/vcs" rel="noopener nofollow" class="external-link" href="https://github.com/mbarbin/vcs" target="_blank">GitHub - mbarbin/vcs: A versatile OCaml library for Git interaction</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/a-versatile-ocaml-library-for-git-interaction-seeking-community-feedback/14155" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/a-versatile-ocaml-library-for-git-interaction-seeking-community-feedback/14155" target="_blank">A Versatile OCaml Library for Git Interaction - Seeking Community Feedback</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/dbcaml/dbcaml" rel="noopener nofollow" class="external-link" href="https://github.com/dbcaml/dbcaml" target="_blank">GitHub - dbcaml/dbcaml: DBCaml is a database library for OCaml</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/dbcaml-a-new-database-toolkit-built-on-riot/14150" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/dbcaml-a-new-database-toolkit-built-on-riot/14150" target="_blank">DBcaml, a new database toolkit built on Riot</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/c-cube/fuseau" rel="noopener nofollow" class="external-link" href="https://github.com/c-cube/fuseau" target="_blank">GitHub - c-cube/fuseau: [alpha] lightweight fiber library for OCaml 5</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-fuseau-0-1/14157" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-fuseau-0-1/14157" target="_blank">[ANN] fuseau 0.1</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/issuu/ocaml-protoc-plugin" rel="noopener nofollow" class="external-link" href="https://github.com/issuu/ocaml-protoc-plugin" target="_blank">GitHub - issuu/ocaml-protoc-plugin: ocaml-protoc-plugin</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/taking-over-maintanence-of-a-stale-project/14156" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/taking-over-maintanence-of-a-stale-project/14156" target="_blank">Taking over maintanence of a stale project</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/darrenldl/docfd" rel="noopener nofollow" class="external-link" href="https://github.com/darrenldl/docfd" target="_blank">GitHub - darrenldl/docfd: TUI multiline fuzzy document finder</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-docfd-tui-multiline-fuzzy-document-finder-2-2-0/14109/1" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-docfd-tui-multiline-fuzzy-document-finder-2-2-0/14109/1" target="_blank">[ANN] Docfd: TUI multiline fuzzy document finder 2.2.0</a>

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2023-3.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2023 - 3.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:47:08 GMT</pubDate></item><item><title><![CDATA[OCaml News 2024 - 1]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a><br><br>
<br><a data-tooltip-position="top" aria-label="https://tarides.com/blog/2023-12-20-ocaml-survey-developers-perception-interest-and-perceived-barriers/" rel="noopener nofollow" class="external-link" href="https://tarides.com/blog/2023-12-20-ocaml-survey-developers-perception-interest-and-perceived-barriers/" target="_blank">OCaml Survey: Developers' Perception, Interest, and Perceived Barriers</a>
<br><a data-tooltip-position="top" aria-label="https://tarides.com/blog/2023-12-29-announcing-the-orchide-project-powering-satellite-innovation/" rel="noopener nofollow" class="external-link" href="https://tarides.com/blog/2023-12-29-announcing-the-orchide-project-powering-satellite-innovation/" target="_blank">Announcing the ORCHIDE Project: Powering Satellite Innovation</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12885" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/12885" target="_blank">Dynarrays, unboxed (with local dummies) #12885</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12871" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/12871" target="_blank">Stdlib priority queues #12871</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12596" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/12596" target="_blank">Compile recursive bindings in Lambda #12596</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12828" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/12828" target="_blank">Add short syntax for dependent functor types #12828</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12508" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/12508" target="_blank">[shapes] Add support for project-wide occurrences #12508</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/1802" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/1802" target="_blank">Make the character set for OCaml source code officially UTF-8. #1802</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12719" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/12719" target="_blank">Add thread local storage #12719</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=zG7JejHlQoM" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=zG7JejHlQoM" target="_blank">"Melange: The next frontier in type-safe web development" by Dillon Mulroy - RVAJS 2023</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=R-XJzUrP7bQ" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=R-XJzUrP7bQ" target="_blank">Trying out OCaml TUI framework Mint Tea!</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=XyDbG9FGR1o" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=XyDbG9FGR1o" target="_blank">A TUI chat in OCaml 🐫</a>
<br>[Building a Game Engine... with OCaml ?! [Part 1]](<a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=1XpUaTnssQE" target="_blank">https://www.youtube.com/watch?v=1XpUaTnssQE</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://sancho.dev/blog/whats-possible-with-melange" rel="noopener nofollow" class="external-link" href="https://sancho.dev/blog/whats-possible-with-melange" target="_blank">What's possible with Melange</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/access-inferred-types/13805" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/access-inferred-types/13805" target="_blank">Access inferred types</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/using-menhir-to-parse-into-idiomatic-js-typescript-structures/13809" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/using-menhir-to-parse-into-idiomatic-js-typescript-structures/13809" target="_blank">Using Menhir to parse into idiomatic JS (TypeScript) structures</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/why-constructors-are-not-curried/13792" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/why-constructors-are-not-curried/13792" target="_blank">Why constructors are not curried?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/practical-example-of-applicative-vs-generative-functors/13777" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/practical-example-of-applicative-vs-generative-functors/13777" target="_blank">Practical example of applicative vs generative functors?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/compiler-optimization-on-flattening-adt-for-less-boxing/13764" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/compiler-optimization-on-flattening-adt-for-less-boxing/13764" target="_blank">Compiler optimization on flattening ADT for less boxing?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/how-to-express-koka-home-page-example/13748" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/how-to-express-koka-home-page-example/13748" target="_blank">How to express Koka home page example?</a>
<br><a data-tooltip-position="top" aria-label="https://practicalocaml.com/parsing-with-binary-string-pattern-matching/" rel="noopener nofollow" class="external-link" href="https://practicalocaml.com/parsing-with-binary-string-pattern-matching/" target="_blank">Parsing with Binary String Pattern Matching</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/backtracking/grid" rel="noopener nofollow" class="external-link" href="https://github.com/backtracking/grid" target="_blank">grid: A tiny library for two-dimensional arrays</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/minttea" rel="noopener nofollow" class="external-link" href="https://github.com/leostera/minttea" target="_blank">A fun little TUI framework for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/terrateamio/ocaml-ts-mode" rel="noopener nofollow" class="external-link" href="https://github.com/terrateamio/ocaml-ts-mode" target="_blank">ocaml-ts-mode: Ocaml mode for emacs using treesitter</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/dmmulroy/create-melange-app" rel="noopener nofollow" class="external-link" href="https://github.com/dmmulroy/create-melange-app" target="_blank">create-melange-app: An example app created by create-melange-app</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/andersfugmann/ppx_protocol_conv" rel="noopener nofollow" class="external-link" href="https://github.com/andersfugmann/ppx_protocol_conv" target="_blank">ppx_protocol_conv: Pluggable serialization and deserialization of ocaml data strucures based on type_conv</a>
<br><a data-tooltip-position="top" aria-label="https://spatial-shell.app/" rel="noopener nofollow" class="external-link" href="https://spatial-shell.app/" target="_blank">spatial-shell: Spatial Shell is a daemon implementing a spatial model inspired by Material Shell, for i3 and sway. More precisely, it organizes your windows within a grid whose rows are the workspaces of your favorite WM.</a>
<br><a data-tooltip-position="top" aria-label="https://codeberg.org/marcc/fixgen" rel="noopener nofollow" class="external-link" href="https://codeberg.org/marcc/fixgen" target="_blank">Fixgen: A language agnostic fixture generator</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/tweag/opam-nix" rel="noopener nofollow" class="external-link" href="https://github.com/tweag/opam-nix" target="_blank">opam-nix: Turn opam-based OCaml projects into Nix derivations</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/blink" rel="noopener nofollow" class="external-link" href="https://github.com/leostera/blink" target="_blank">Blink: A pure OCaml HTTP client for Riot</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/colors" rel="noopener nofollow" class="external-link" href="https://github.com/leostera/colors" target="_blank">colors: A pure OCaml library for manipulating colors in different color spaces.</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/nationalarchives/miiify" rel="noopener nofollow" class="external-link" href="https://github.com/nationalarchives/miiify" target="_blank">miilfy: A web annotation server built with the same principles as Git</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/riot" rel="noopener nofollow" class="external-link" href="https://github.com/leostera/riot" target="_blank">Roit v0.0.7: An actor-model multi-core scheduler for OCaml 5 🐫</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/castore" rel="noopener nofollow" class="external-link" href="https://github.com/leostera/castore" target="_blank">CAStore: A portable pure OCaml CA Store, with no dependencies, inspired by Elixir's [:castore](https://github.com/elixir-mint/castore).</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/learn-ocaml-1-0-approaching-call-for-testers/13621" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/learn-ocaml-1-0-approaching-call-for-testers/13621" target="_blank">Lean-OCaml 1.0: A Web Application for Learning OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/cmarkit" rel="noopener nofollow" class="external-link" href="https://erratique.ch/software/cmarkit" target="_blank">cmarkit 0.3.0" CommonMark parser and renderer for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://ocaml.org/p/dream-html/latest" rel="noopener nofollow" class="external-link" href="https://ocaml.org/p/dream-html/latest" target="_blank">dream-html 2.0.0: A library for generating HTML</a>
<br><a data-tooltip-position="top" aria-label="https://git.frama-c.com/pub/caisar/" rel="noopener nofollow" class="external-link" href="https://git.frama-c.com/pub/caisar/" target="_blank">Caisar: A platform under active development at CEA LIST, aiming to provide a wide range of features to characterize the safety and robustness of artificial intelligence based software.</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/jserot/rfsm" rel="noopener nofollow" class="external-link" href="https://github.com/jserot/rfsm" target="_blank">RFSM 2.0: A toolset for describing and simulating StateChart-like state diagrams.</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/chshersh/zbg/tree/main" rel="noopener nofollow" class="external-link" href="https://github.com/chshersh/zbg/tree/main" target="_blank">Zbg 2.0: <code></code> (short for <strong></strong>ero <strong></strong>ullshit <strong></strong>it) is a CLI tool for using <code></code> efficiently.</a>zbgZBGgit
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2024-1.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2024 - 1.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:47:06 GMT</pubDate></item><item><title><![CDATA[OCaml News 2024 - 2]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a><br><br>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-preview-play-with-project-wide-occurrences-for-ocaml/13814" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-preview-play-with-project-wide-occurrences-for-ocaml/13814" target="_blank">[ANN][PREVIEW] Play with project-wide occurrences for OCaml!</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/dune/pull/8784" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/dune/pull/8784" target="_blank">[Dune]: Add link flags ocamlmklib when using ctypes stubs. #8784</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ocaml-software-foundation-january-2024-update/13828" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ocaml-software-foundation-january-2024-update/13828" target="_blank">OCaml Software Foundation: January 2024 update</a>
<br>[Apprendre à programmer avec OCaml](Apprendre à programmer avec OCaml)
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/call-for-speakers-for-the-2024-carolina-code-conference-is-open-until-april-15th/13827" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/call-for-speakers-for-the-2024-carolina-code-conference-is-open-until-april-15th/13827" target="_blank">[Call for Speakers for the 2024 Carolina Code Conference is open until April 15th](https://discuss.ocaml.org/t/call-for-speakers-for-the-2024-carolina-code-conference-is-open-until-april-15th/13827)</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/discussions/11924" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/discussions/11924" target="_blank">[OCaml]: feature request: better errors #11924</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ocsigen-summary-of-recent-releases/13817" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ocsigen-summary-of-recent-releases/13817" target="_blank">Ocsigen: summary of recent releases</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/benchmark-between-open-addressing-and-closed-addressing-hashtbl/13882" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/benchmark-between-open-addressing-and-closed-addressing-hashtbl/13882" target="_blank">Benchmark between open-addressing and closed-addressing hashtbl</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/examples-of-caqti-infix/13878" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/examples-of-caqti-infix/13878" target="_blank">Examples of Caqti infix?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/why-can-t-i-create-a-project-with-non-ascii-characters/13865" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/why-can-t-i-create-a-project-with-non-ascii-characters/13865" target="_blank">Why can’t I create a project with non-ASCII characters? </a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/old-self-taught-vs-uni-debate-landscape-for-former-jane-street/13851" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/old-self-taught-vs-uni-debate-landscape-for-former-jane-street/13851" target="_blank">Old self-taught vs. uni debate; Landscape for former; Jane Street</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/toml-file-parser/13854" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/toml-file-parser/13854" target="_blank">Is there an easy way to read the values from a toml file?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/generate-typed-ast-fragments/13824" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/generate-typed-ast-fragments/13824" target="_blank">Generate typed AST fragments</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/partially-apply-function-accepting-multiple-modules-and-keep-polymorphic-types/13823" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/partially-apply-function-accepting-multiple-modules-and-keep-polymorphic-types/13823" target="_blank">Partially apply function accepting multiple modules and keep polymorphic types</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/printing-unicode-characters-on-different-platforms/13813" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/printing-unicode-characters-on-different-platforms/13813" target="_blank">Printing Unicode Characters on Different Platforms</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" rel="noopener nofollow" class="external-link" href="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" target="_blank">Verifying an Effect-Based Cooperative Concurrency Scheduler in Iris by Adrian Dapprich</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/advice-for-combining-multiple-monads/10409" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/advice-for-combining-multiple-monads/10409" target="_blank">Advice for combining multiple monads</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/writing-ctypes-bindings-to-system-shared-libraries-for-bytecode-targets-via-the-dune-ctypes-stanza/13844" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/writing-ctypes-bindings-to-system-shared-libraries-for-bytecode-targets-via-the-dune-ctypes-stanza/13844" target="_blank">Writing ctypes bindings to system shared libraries for bytecode targets via the dune ctypes stanza</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/johnyob/grace" rel="noopener nofollow" class="external-link" href="https://github.com/johnyob/grace" target="_blank">Grace: A fancy diagnostics library that allows your compilers to exit with grace</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/thierry-martinez/metapp" rel="noopener nofollow" class="external-link" href="https://github.com/thierry-martinez/metapp" target="_blank">meta-pp: Meta-preprocessor for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/stedolan/ppx_stage" rel="noopener nofollow" class="external-link" href="https://github.com/stedolan/ppx_stage" target="_blank">ppx_stage: Staged metaprogramming in stock OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/cmarkit" rel="noopener nofollow" class="external-link" href="https://erratique.ch/software/cmarkit" target="_blank">Cmarkit is an [OCaml](http://ocaml.org) libary for parsing the [CommonMark](https://spec.commonmark.org/) specification.</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/brr" rel="noopener nofollow" class="external-link" href="https://erratique.ch/software/brr" target="_blank">Brr is a toolkit for programming browsers in [OCaml](http://ocaml.org) with the [js_of_ocaml](http://ocsigen.org/js_of_ocaml/) compiler. It provides:</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/zipc" rel="noopener nofollow" class="external-link" href="https://erratique.ch/software/zipc" target="_blank">Zipc is an in-memory [ZIP](https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT) archive and [deflate](https://www.rfc-editor.org/rfc/rfc1951) compression codec for [OCaml](http://ocaml.org).</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/lukstafi/ocaml-gccjit" rel="noopener nofollow" class="external-link" href="https://github.com/lukstafi/ocaml-gccjit" target="_blank">ocaml-gccjit: OCaml bindings for libgccjit</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/whitequark/ocaml-m17n" rel="noopener nofollow" class="external-link" href="https://github.com/whitequark/ocaml-m17n" target="_blank">ocaml-m17n: Multilingualization for the OCaml source code</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/hackwaly/ocamlearlybird" rel="noopener nofollow" class="external-link" href="https://github.com/hackwaly/ocamlearlybird" target="_blank">ocamlearlybird: OCaml debug adapter</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/dmbaturin/otoml" rel="noopener nofollow" class="external-link" href="https://github.com/dmbaturin/otoml" target="_blank">otoml: TOML parsing, manipulation, and pretty-printing library for OCaml (fully 1.0.0-compliant)</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/mattjbray/ocaml-decoders" rel="noopener nofollow" class="external-link" href="https://github.com/mattjbray/ocaml-decoders" target="_blank">ocaml-decoders: Elm-inspired decoders for Ocaml</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/OCamlPro/ez_toml" rel="noopener nofollow" class="external-link" href="https://github.com/OCamlPro/ez_toml" target="_blank">ez_toml: A library to parse and print TOML files</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="http://cambium.inria.fr/~fpottier/oma/doc/oma/Oma/index.html" rel="noopener nofollow" class="external-link" href="http://cambium.inria.fr/~fpottier/oma/doc/oma/Oma/index.html" target="_blank">Oma: OCaml implementation of 'Two Simplified Algorithms for Maintaining Order in a List'</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-first-release-of-oma/13845" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-first-release-of-oma/13845" target="_blank">[ANN] First release of oma</a>
<br><a data-tooltip-position="top" aria-label="https://erikdemaine.org/papers/DietzSleator_ESA2002/paper.pdf" rel="noopener nofollow" class="external-link" href="https://erikdemaine.org/papers/DietzSleator_ESA2002/paper.pdf" target="_blank">Two Simplified Algorithms for Maintaining Order in a List</a>


<br><a data-tooltip-position="top" aria-label="https://github.com/hackwaly/ocamlearlybird" rel="noopener nofollow" class="external-link" href="https://github.com/hackwaly/ocamlearlybird" target="_blank">ocamlearlybird: v1.3.0</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-ocamlearlybird-just-got-ability-to-inspect-opaque-abstract-values/13852" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-ocamlearlybird-just-got-ability-to-inspect-opaque-abstract-values/13852" target="_blank">[ANN] Ocamlearlybird just got ability to inspect opaque/abstract values</a>


<br><a data-tooltip-position="top" aria-label="https://github.com/Axot017/validate" rel="noopener nofollow" class="external-link" href="https://github.com/Axot017/validate" target="_blank">validate: OCaml Data Validation library</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-validate-a-new-library-for-data-validation/13861" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-validate-a-new-library-for-data-validation/13861" target="_blank">[ANN] Validate - A New Library for Data Validation</a>


<br><a data-tooltip-position="top" aria-label="https://github.com/gborough/sarif" rel="noopener nofollow" class="external-link" href="https://github.com/gborough/sarif" target="_blank">sarif v2.1.0: Static Analysis Results Interchange Format (SARIF) For OCaml</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-sarif-0-1-0-static-analysis-results-interchange-format-sarif-for-ocaml/13821" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-sarif-0-1-0-static-analysis-results-interchange-format-sarif-for-ocaml/13821" target="_blank">[ANN] sarif 0.1.0 - Static Analysis Results Interchange Format (SARIF) For OCaml</a>


<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-new-release-of-menhir-20231231/13816" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/ann-new-release-of-menhir-20231231/13816" target="_blank">[ANN] New release of Menhir (20231231)</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2024-2.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2024 - 2.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:47:07 GMT</pubDate></item><item><title><![CDATA[OCaml News 2024 - 6]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a><br>       ^o3
~/\_/\_|)
|/=_=\|
"     "
<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13275" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13275" target="_blank">Modular explicits #13275</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12828" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12828" target="_blank">Add short syntax for dependent functor types #12828</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13310" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13310" target="_blank">Add Pair module to standard library #13310</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13272" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13272" target="_blank">Allow maximum number of domains to be specified as a OCAMLRUNPARAM parameter #13272</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13097" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13097" target="_blank">Immutable arrays #13097</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13161" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13161" target="_blank">Restore native armv7 support for NetBSD 10.0 #13161</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12309" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12309" target="_blank">Add effect syntax #12309</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12114" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12114" target="_blank">Add ThreadSanitizer support #12114</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13195" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13195" target="_blank">A new abstract data type of enumerations in Set.Make(Ord).Enum #13195</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12871" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12871" target="_blank">Stdlib priority queues #12871</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/9080" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/9080" target="_blank">Aliasing == and != with explicit names #9080</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12964" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12964" target="_blank">Memory cleanup at exit #12964</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13169" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13169" target="_blank">A document type for error messages #13169</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13318" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13318" target="_blank">Fix GC alarm regression #13318</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13326" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13326" target="_blank">Implement O_APPEND on windows #13326</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13296" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13296" target="_blank">Add missing functions from Array to Dynarray #13296</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12182%23discussion_r1678134261" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12182%23discussion_r1678134261" target="_blank">Improve the type clash error message #12182</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12298" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12298" target="_blank">Emphasize that Bigarray.int refers to the OCaml int type, and not the C int type #12298</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/effects-with-lwt-a-dead-end-for-now/15002" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/effects-with-lwt-a-dead-end-for-now/15002" target="_blank">Effects with Lwt, a dead end for now?</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/changes-in-handling-of-gc-parameters-and-alarms-in-5-2-0/14986" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/changes-in-handling-of-gc-parameters-and-alarms-in-5-2-0/14986" target="_blank">Changes in handling of Gc parameters and alarms in 5.2.0</a>

<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mopsa-1-0-modular-open-platform-for-static-analysis/15013" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mopsa-1-0-modular-open-platform-for-static-analysis/15013" target="_blank">[ANN] Mopsa 1.0 -- Modular Open Platform for Static Analysis</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-a-small-extension-of-bigarray-genarray-adding-iteration-mapping-and-folding/15005" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-a-small-extension-of-bigarray-genarray-adding-iteration-mapping-and-folding/15005" target="_blank">[ANN] A small extension of Bigarray.Genarray adding iteration, mapping and folding</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-cudajit-bindings-to-the-cuda-and-nvrtc-libraries/15010" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-cudajit-bindings-to-the-cuda-and-nvrtc-libraries/15010" target="_blank">[ANN] cudajit: Bindings to the <code></code> and <code></code> libraries</a>cudanvrtc

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ocaml-lsp-1-18-0/14952" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ocaml-lsp-1-18-0/14952" target="_blank">[ANN] OCaml LSP 1.18.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ortac-0-3-0-dynamic-formal-verification-made-easy/14936" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ortac-0-3-0-dynamic-formal-verification-made-easy/14936" target="_blank">[ANN] Ortac 0.3.0 Dynamic formal verification made easy</a>

<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//hal.sorbonne-universite.fr/hal-02890500v1/document" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//hal.sorbonne-universite.fr/hal-02890500v1/document" target="_blank">Combinations of Reusable Abstract Domains for a Multilingual Static Analyzer</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-21-fixing-living/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-21-fixing-living/" target="_blank">Fighting Mutation with Mutation in Living</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" target="_blank">Type system and polymorphic let's</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/using-docusaurus-to-document-an-ocaml-project/13359" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/using-docusaurus-to-document-an-ocaml-project/13359" target="_blank">Using Docusaurus to document an OCaml project</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" target="_blank">Exploring the Docusaurus+Odoc combo</a><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" target="_blank">Type system and polymorphic let's</a><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" target="_blank">Exploring the Docusaurus+Odoc combo</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-11-a-possibly-safer-interface-to-the-ctypes-ffi/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-11-a-possibly-safer-interface-to-the-ctypes-ffi/" target="_blank">A (Possibly) Safer Interface to the Ctypes FFI</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-09-ocaml-ffi-sharp-edges-and-how-to-avoid-them/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-09-ocaml-ffi-sharp-edges-and-how-to-avoid-them/" target="_blank">OCaml FFI Sharp Edges -- and How to Avoid them!</a>

<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//ocaml.libvirt.org/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//ocaml.libvirt.org/" target="_blank">ocaml-libvirt OCaml bindings for libvirt</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//gitlab.com/mopsa/mopsa-analyzer/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//gitlab.com/mopsa/mopsa-analyzer/" target="_blank">Gitlab - MOPSA/MOPSA analyzer: stands for Modular and Open Platform for Static Analysis.</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/Heyji2/GenArrayIter" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/Heyji2/GenArrayIter" target="_blank">GitHub - Heyji2/GenArrayIter: Adding iteration, mapping and folding to the ocaml BigArray.Genarrays module which provides arrays of arbitrary dimensions</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/mbarbin/bopkit" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/mbarbin/bopkit" target="_blank">GitHub - mbarbin/bopkit: An educational project for digital circuits programming</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/dx3mod/rpmfile" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/dx3mod/rpmfile" target="_blank">GitHub - dx3mod/rpmfile: A library for reading metadata from RPM packages.</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/gildor478/ocaml-fileutils" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/gildor478/ocaml-fileutils" target="_blank">Github - gildor478/ocaml-fileutils: OCaml API to manipulate real files (POSIX like) and filenames</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/NathanReb/ocaml-api-watch" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/NathanReb/ocaml-api-watch" target="_blank">Github - NathanReb/ocaml-api-watch: Libraries and tools to keep watch on you OCaml lib's API changes</a>

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2024-6.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2024 - 6.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:47:09 GMT</pubDate></item><item><title><![CDATA[OCaml News 2024 - 7]]></title><description><![CDATA[ 
 <br>       ^o3
~/\_/\_|)
|/=_=\|
"     "

^~~~~~~~~~~~这其实是用 ASCII 画的一个骆驼。
<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ocaml-5-2-1-released/15634" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ocaml-5-2-1-released/15634" target="_blank">OCaml 5.2.1 released</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/feedback-help-wanted-upcoming-ocaml-org-cookbook-feature/14127" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/feedback-help-wanted-upcoming-ocaml-org-cookbook-feature/14127" target="_blank">Feedback / Help Wanted: Upcoming OCaml.org Cookbook Feature</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/second-beta-release-of-ocaml-5-3-0/15700" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/second-beta-release-of-ocaml-5-3-0/15700" target="_blank">Second beta release of OCaml 5.3.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/terrateams-open-source-ocaml-repository/15645" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/terrateams-open-source-ocaml-repository/15645" target="_blank">Terrateam's open source Ocaml repository</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mlx-syntax-dialect/15035" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mlx-syntax-dialect/15035" target="_blank">[ANN] .mlx syntax dialect</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-opam-2-3-0-is-out/15609" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-opam-2-3-0-is-out/15609" target="_blank">[ANN] opam 2.3.0 is out!</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//fun-ocaml.com/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//fun-ocaml.com/" target="_blank">Fun OCaml 2024 - Berlin - September 16+17, 2024</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/%40FUNOCaml" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/%40FUNOCaml" target="_blank">Fun OCaml is an open source hacking event dedicated to OCaml enthusiasts and professionals around the globe!</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13526" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13526" target="_blank">Simplify the build of cross compilers by shym · Pull Request #13526 · ocaml/ocaml</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13580" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13580" target="_blank">"Mark-delay" performance improvement to major GC by NickBarnes · Pull Request #13580 · ocaml/ocaml</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13416" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13416" target="_blank">Use WinAPI concurrency primitives on Windows ports (remove winpthreads) by MisterDA · Pull Request #13416 · ocaml/ocaml</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13404" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13404" target="_blank">Atomic record fields by clef-men · Pull Request #13404 · ocaml/ocaml</a>

<br><br>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/playlist%3Flist%3DPLP3MfTGqcNVJMFcWWDF6VSPJB7zuKN5yQ" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/playlist%3Flist%3DPLP3MfTGqcNVJMFcWWDF6VSPJB7zuKN5yQ" target="_blank">All the talk recordings from FUN OCaml 2024 in Berlin, September 16 + 17, 2024.</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DwbrELQrzwQk" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DwbrELQrzwQk" target="_blank">[ICFP24] Oxidizing OCaml with Modal Memory Management</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D7yYC6EGYg10" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D7yYC6EGYg10" target="_blank">[OCaml24] Priodomainslib: Prioritized Fine-grained Parallelism for Multicore OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DKEkmcXVtFi0" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DKEkmcXVtFi0" target="_blank">[OCaml24] ChorCaml: Functional Choreographic Programming in OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dvanyv3ZEto8" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dvanyv3ZEto8" target="_blank">[OCaml24] Saturn: a library of verified concurrent data structures for OCaml 5</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DmUzOBC3V0ds" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DmUzOBC3V0ds" target="_blank">Beyond the Basics of LSP: Advanced IDE services for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DBUpRPSzkrH0" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DBUpRPSzkrH0" target="_blank">Why Terrateam chose OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D8zTN3rtcED4" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D8zTN3rtcED4" target="_blank">A Crash Course in OCaml Modules • Tim McGilchrist • YOW! 2015</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//watch.ocaml.org/w/peT3MdWjS1BYYMbowEJ1gv" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//watch.ocaml.org/w/peT3MdWjS1BYYMbowEJ1gv" target="_blank">Outreachy May 2024 Demo</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DJFZvnGD_hV8" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DJFZvnGD_hV8" target="_blank">Octane: A Query Builder for OCaml</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/what-are-the-biggest-reasons-newcomers-give-up-on-ocaml/10958" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/what-are-the-biggest-reasons-newcomers-give-up-on-ocaml/10958" target="_blank">What are the biggest reasons newcomers give up on OCaml?</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/do-you-use-return-to-describe-a-function/15615" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/do-you-use-return-to-describe-a-function/15615" target="_blank">Do you use "return" to describe a function?</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ocaml-cycle-detection-and-garbage-collector/15757" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ocaml-cycle-detection-and-garbage-collector/15757" target="_blank">OCaml Cycle detection and Garbage Collector</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/modeling-rust-trait-objects-subtyping-relation-via-ocaml-polymorphic-variants/15644" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/modeling-rust-trait-objects-subtyping-relation-via-ocaml-polymorphic-variants/15644" target="_blank">Modeling Rust trait objects subtyping relation via OCaml polymorphic variants</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ocamldebug-using-dune/15595" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ocamldebug-using-dune/15595" target="_blank">Ocamldebug using dune</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/typesafe-sql-with-octane-ml/15582" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/typesafe-sql-with-octane-ml/15582" target="_blank">Typesafe SQL with Octane.ml</a>
<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/using-map-and-ppx-yojson/12524" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/using-map-and-ppx-yojson/12524" target="_blank">Using Map and ppx yojson</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/correct-syntax-to-add-compare-ppx/15764" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/correct-syntax-to-add-compare-ppx/15764" target="_blank">Correct syntax to add compare ppx</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/tiny-educational-concurrent-i-o-and-promises-library/15703" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/tiny-educational-concurrent-i-o-and-promises-library/15703" target="_blank">Tiny educational concurrent I/O and promises library</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/good-example-of-handwritten-lexer-recursive-descent-parser/15672" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/good-example-of-handwritten-lexer-recursive-descent-parser/15672" target="_blank">Good example of handwritten Lexer + Recursive Descent Parser?</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/new-part-pragmatic-category-theory-part-2-published/15056" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/new-part-pragmatic-category-theory-part-2-published/15056" target="_blank">[NEW PART] Pragmatic Category Theory: Part 2 published!</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//dev.to/chshersh/pragmatic-category-theory-part-1-semigroup-intro-1ign" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//dev.to/chshersh/pragmatic-category-theory-part-1-semigroup-intro-1ign" target="_blank">Pragmatic Category Theory | Part 1: Semigroup Intro</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//dev.to/chshersh/pragmatic-category-theory-part-2-composing-semigroups-87" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//dev.to/chshersh/pragmatic-category-theory-part-2-composing-semigroups-87" target="_blank">Pragmatic Category Theory | Part 2: Composing Semigroups</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/installing-ocaml-on-a-mac-m1-mini/15642" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/installing-ocaml-on-a-mac-m1-mini/15642" target="_blank">Installing OCaml on a mac m1 mini</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/a-case-for-feminism-in-programming-language-design/15478" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/a-case-for-feminism-in-programming-language-design/15478" target="_blank">« A Case for Feminism in Programming Language Design »</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/pdf/10.1145/3689492.3689809" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/pdf/10.1145/3689492.3689809" target="_blank">A Case for Feminism in Programming Language Design.pdf</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//www.lexifi.com/blog/ocaml/decoders/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//www.lexifi.com/blog/ocaml/decoders/" target="_blank">Vdom 0.3 brings Elm-style decoders to OCaml</a>

<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/re2ocaml-regexp-compiler/15669" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/re2ocaml-regexp-compiler/15669" target="_blank">Re2ocaml regexp compiler</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/eliom-11-1-towards-web-assembly-support/15704" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/eliom-11-1-towards-web-assembly-support/15704" target="_blank">Eliom 11.1: Towards Web Assembly support</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-new-releases-of-merlin-and-ocaml-lsp/15752" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-new-releases-of-merlin-and-ocaml-lsp/15752" target="_blank">[ANN] New releases of Merlin and OCaml-LSP</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/new-release-of-baby/15754" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/new-release-of-baby/15754" target="_blank">New release of baby</a>

<br>
babyis an OCaml library that offers persistent sets and maps based on balanced binary search trees. It offers replacements for OCaml’sSetandMapmodules.

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-dune-3-17/15770" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-dune-3-17/15770" target="_blank">[ANN] Dune 3.17</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-jsont-0-1-0-declarative-json-data-manipulation-for-ocaml/15702" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-jsont-0-1-0-declarative-json-data-manipulation-for-ocaml/15702" target="_blank">[ANN] Jsont 0.1.0 – Declarative JSON data manipulation for OCaml</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-release-of-saturn-1-0/15763" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-release-of-saturn-1-0/15763" target="_blank">[ANN] Release of Saturn 1.0</a>

<br>Saturn is a collection of concurrent-safe data structures designed for OCaml 5.


<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-areas-and-adversaries/15706/3" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-areas-and-adversaries/15706/3" target="_blank">[ANN] Areas and Adversaries</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-release-of-cppo-1-8-0/15749" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-release-of-cppo-1-8-0/15749" target="_blank">[ANN] Release of cppo 1.8.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-capnp-rpc-2-0/15739" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-capnp-rpc-2-0/15739" target="_blank">[ANN] capnp-rpc 2.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-advent-of-code-project-template/13539" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-advent-of-code-project-template/13539" target="_blank">[ANN] Advent of Code project template</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-testo-0-1-0-a-new-testing-framework-for-ocaml/15624" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-testo-0-1-0-a-new-testing-framework-for-ocaml/15624" target="_blank">[ANN] Testo 0.1.0 - a new testing framework for OCaml</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-vdom-0-3-functional-ui-applications-now-with-custom-event-handlers/13298" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-vdom-0-3-functional-ui-applications-now-with-custom-event-handlers/13298" target="_blank">[ANN] vdom 0.3: functional UI applications now with custom event handlers</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/visualizing-dependencies-between-ocaml-modules/15254" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/visualizing-dependencies-between-ocaml-modules/15254" target="_blank">Visualizing dependencies between Ocaml modules</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mariadb-1-2-0/15709" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mariadb-1-2-0/15709" target="_blank">[ANN] mariadb 1.2.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-new-release-of-monolith/15701" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-new-release-of-monolith/15701" target="_blank">[ANN] New release of Monolith</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-first-release-of-hachis/15309" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-first-release-of-hachis/15309" target="_blank">[ANN] First release of hachis</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-liquidsoap-2-3-0/15677" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-liquidsoap-2-3-0/15677" target="_blank">[ANN] Liquidsoap 2.3.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-dream-html-pure-html-3-5-2/14808/3" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-dream-html-pure-html-3-5-2/14808/3" target="_blank">[ANN] dream-html &amp; pure-html 3.5.2</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-js-of-ocaml-5-9-0/15674" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-js-of-ocaml-5-9-0/15674" target="_blank">[ANN] Js_of_ocaml 5.9.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ppx-deriving-ezjsonm/15637" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ppx-deriving-ezjsonm/15637" target="_blank">[ANN] ppx_deriving_ezjsonm</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-smaws-preview-release-an-aws-sdk-for-ocaml-using-eio/15635" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-smaws-preview-release-an-aws-sdk-for-ocaml-using-eio/15635" target="_blank">[ANN] smaws preview release, an AWS SDK for OCaml using eio</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-aws-s3-4-0-1/2451" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-aws-s3-4-0-1/2451" target="_blank">[ANN] aws-s3 4.0.1</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/first-release-of-cmdlang/15616" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/first-release-of-cmdlang/15616" target="_blank">First release of cmdlang</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-eliom-11-and-ocsigen-start-7/15487" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-eliom-11-and-ocsigen-start-7/15487" target="_blank">[ANN] Eliom 11 and Ocsigen Start 7</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-jane-street-ocaml-extensions-now-with-developer-tooling/15597" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-jane-street-ocaml-extensions-now-with-developer-tooling/15597" target="_blank">[ANN] Jane Street OCaml extensions – now with developer tooling!</a>

<br><br>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/tjdevries/octane.ml" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/tjdevries/octane.ml" target="_blank">GitHub - tjdevries/octane.ml: The fastest, hottest</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/semgrep/testo" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/semgrep/testo" target="_blank">Github - semgrep/testo: Test framework for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/sim642/odep" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/sim642/odep" target="_blank">GitHub - sim642/odep: Dependency graphs for OCaml modules, libraries and packages</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/LexiFi/ocaml-vdom" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/LexiFi/ocaml-vdom" target="_blank">GitHub - LexiFi/ocaml-vdom: Elm architecture and (V)DOM for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml-community/ocaml-mariadb" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml-community/ocaml-mariadb" target="_blank">GitHub - ocaml-community/ocaml-mariadb: OCaml bindings to MariaDB, supporting the nonblocking API</a>
<br><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//cambium.inria.fr/~fpottier/monolith/doc/monolith/Monolith/" rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//cambium.inria.fr/~fpottier/monolith/doc/monolith/Monolith/" target="_blank">Monolith (monolith.Monolith)</a>: Monolith offers facilities for testing an OCaml library by comparing its implementation (known as the candidate implementation) against a reference implementation.
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2024-7.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2024 - 7.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:45:38 GMT</pubDate></item><item><title><![CDATA[A hack to implement efficient TLS (thread-local-storage)]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>Currently OCaml 5 provides a <a data-tooltip-position="top" aria-label="https://v2.ocaml.org/api/Domain.DLS.html" rel="noopener nofollow" class="external-link" href="https://v2.ocaml.org/api/Domain.DLS.html" target="_blank"><code></code> 2</a>Domain.DLS module for domain-local storage.<br>Unfortunately,<br>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/11770" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/issues/11770" target="_blank">there is no corresponding <code></code> 7</a>Thread.TLS for (sys)thread-local storage, and
<br>the current implementation of Domain.DLS is not thread-safe.
<br>I don’t want to spend time to motivate this topic, but for many of the use cases of Domain.DLS, what you actually want, is to use a Thread.TLS. IOW, many of the uses of Domain.DLS are probably “wrong” and should actually use a Thread.TLS, because, when using Domain.DLS, the implicit assumption is often that you don’t have multiple threads on the domain, but that is typically decided at a higher level in the application and so making such an assumption is typically not safe.<br><br>I mentioned that the current implementation of Domain.DLS is not thread-safe. What I mean by that is that <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/12677" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/issues/12677" target="_blank">the current implementation is literally not thread-safe at all</a> in the sense that unrelated concurrent Domain.DLS accesses can actually break the DLS. That is because the state updates performed by Domain.DLS contain safe-points during which the OCaml runtime may switch between (sys)threads.<br>Consider <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L120-L127" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L120-L127" target="_blank">the implementation of <code></code> 1</a>Domain.DLS.get:<br>  let get (idx, init) =
    let st = maybe_grow idx in
    let v = st.(idx) in
    if v == unique_value then
      let v' = Obj.repr (init ()) in
      st.(idx) &lt;- (Sys.opaque_identity v');
      Obj.magic v'
    else Obj.magic v
<br>If there are two (or more) threads on a single domain that concurrently call get before init has been called initially, then what might happen is that init gets called twice (or more) and the threads get different values which could e.g. be pointers to two different mutable objects.<br>Consider <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L98-L111" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L98-L111" target="_blank">the implementation of <code></code></a>maybe_grow:<br>  let maybe_grow idx =
    let st = get_dls_state () in
    let sz = Array.length st in
    if idx &lt; sz then st
    else begin
      let rec compute_new_size s =
        if idx &lt; s then s else compute_new_size (2 * s)
      in
      let new_sz = compute_new_size sz in
      let new_st = Array.make new_sz unique_value in
      Array.blit st 0 new_st 0 sz;
      set_dls_state new_st;
      new_st
    end

<br>Imagine calling get (which calls maybe_grow) with two different keys from two different threads concurrently. The end result might be that two different arrays are allocated and only one of them “wins”. What this means, for example, is that effects of set calls may effectively be undone by concurrent calls of get.<br>In other words, the Domain.DLS, as it is currently implemented, is not thread-safe.<br><br>If you dig into the implementation of threads, you will notice that the opaque <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/thread.mli#L18" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/thread.mli#L18" target="_blank"><code></code> type</a>Thread.t is actually a heap block (record) of three fields. You can see the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L66-L68" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L66-L68" target="_blank"><code></code> accessors</a>Thread.t:<br>#define Ident(v) Field(v, 0)
#define Start_closure(v) Field(v, 1)
#define Terminated(v) Field(v, 2)
<br>and the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L335-L346" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L335-L346" target="_blank"><code></code> allocation 1</a>Thread.t:<br>static value caml_thread_new_descriptor(value clos)
{
  CAMLparam1(clos);
  CAMLlocal1(mu);
  value descr;
  /* Create and initialize the termination semaphore */
  mu = caml_threadstatus_new();
  /* Create a descriptor for the new thread */
  descr = caml_alloc_3(0, Val_long(atomic_fetch_add(&amp;thread_next_id, +1)),
                       clos, mu);
  CAMLreturn(descr);
}
<br>The second field, Start_closure, is used to pass the closure to the thread start:<br>static void * caml_thread_start(void * v)
{
  caml_thread_t th = (caml_thread_t) v;
  int dom_id = th-&gt;domain_id;
  value clos;
  void * signal_stack;

  caml_init_domain_self(dom_id);

  st_tls_set(caml_thread_key, th);

  thread_lock_acquire(dom_id);
  restore_runtime_state(th);
  signal_stack = caml_init_signal_stack();

  clos = Start_closure(Active_thread-&gt;descr);
  caml_modify(&amp;(Start_closure(Active_thread-&gt;descr)), Val_unit);
  caml_callback_exn(clos, Val_unit);
  caml_thread_stop();
  caml_free_signal_stack(signal_stack);
  return 0;
}

<br>and, as seen above, <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L575" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L575" target="_blank">it is overwritten with the unit value</a> before the closure is called.<br>What this means is that when you call Thread.self () and get a reference to the current Thread.t, the Start_closure field of that heap block will be the unit value:<br>assert (Obj.field (Obj.repr (Thread.self ())) 1 = Obj.repr ())
<br>Let’s hijack that field for the purpose of implementing an efficient TLS!<br>Here is the full hack:<br>module TLS : sig
  type 'a key
  val new_key : (unit -&gt; 'a) -&gt; 'a key
  val get : 'a key -&gt; 'a
  val set : 'a key -&gt; 'a -&gt; unit
end = struct
  type 'a key = { index : int; compute : unit -&gt; 'a }

  let counter = Atomic.make 0
  let unique () = Obj.repr counter

  let new_key compute =
    let index = Atomic.fetch_and_add counter 1 in
    { index; compute }

  type t = { _id : int; mutable tls : Obj.t }

  let ceil_pow_2_minus_1 n =
    let n = n lor (n lsr 1) in
    let n = n lor (n lsr 2) in
    let n = n lor (n lsr 4) in
    let n = n lor (n lsr 8) in
    let n = n lor (n lsr 16) in
    if Sys.int_size &gt; 32 then n lor (n lsr 32) else n

  let[@inline never] grow_tls t before index =
    let new_length = ceil_pow_2_minus_1 (index + 1) in
    let after = Array.make new_length (unique ()) in
    Array.blit before 0 after 0 (Array.length before);
    t.tls &lt;- Obj.repr after;
    after

  let[@inline] get_tls index =
    let t = Obj.magic (Thread.self ()) in
    let tls = t.tls in
    if Obj.is_int tls then grow_tls t [||] index
    else
      let tls = (Obj.magic tls : Obj.t array) in
      if index &lt; Array.length tls then tls else grow_tls t tls index

  let get key =
    let tls = get_tls key.index in
    let value = Array.unsafe_get tls key.index in
    if value != unique () then Obj.magic value
    else
      let value = key.compute () in
      Array.unsafe_set tls key.index (Obj.repr (Sys.opaque_identity value));
      value

  let set key value =
    let tls = get_tls key.index in
    Array.unsafe_set tls key.index (Obj.repr (Sys.opaque_identity value))
end
<br>The above achieves about 80% of the performance of Domain.DLS allowing roughly 241M TLS.gets/s (vs 296M Domain.DLS.gets/s) on my laptop.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/a-hack-to-implement-efficient-tls-(thread-local-storage).html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/A hack to implement efficient TLS (thread-local-storage).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:47:05 GMT</pubDate></item><item><title><![CDATA[Advanced C binding using ocaml-ctypes and dune]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>I was working on a OCaml binding for <a data-tooltip-position="top" aria-label="https://github.com/Haivision/srt" rel="noopener nofollow" class="external-link" href="https://github.com/Haivision/srt" target="_blank">libsrt</a> last summer, to add support for SRT real-time input and output to <a data-tooltip-position="top" aria-label="https://github.com/savonet/liquidsoap" rel="noopener nofollow" class="external-link" href="https://github.com/savonet/liquidsoap" target="_blank">liquidsoap</a>, and came across the need to access the [sys/socket.h](https://pubs.opengroup.org/onlinepubs/7908799/xns/syssocket.h.html) C API.<br>I had already decided to use the very elegant [ocaml-ctypes](https://github.com/ocamllabs/ocaml-ctypes) module for the SRT binding so I went with it and created a [ocaml-sys-socket](https://github.com/toots/ocaml-sys-socket) module using it as well. It was a very interesting experience that I would like to describe here!<br><br>The idea behind OCaml ctypes is to create a binding against a C library without having to write C code, or as least as possible. The most straight-forward way of using it is via [libffi](https://github.com/libffi/libffi) , providing access to dynamically-loaded libraries.<br>The second way of using it is by letting the module generate the basic C stubs required to build and link against a shared library. This is the mode that we’re going to use here. In this mode, the programmer has to describe the C headers of the library they intent to bind to using dedicated OCaml modules, operators and types. From that description, ocaml-ctypes is able to generate the required glue for the binding.<br>One advantage of using ocaml-ctypes is that the created bindings make as few assumptions as possible about the <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/intfc.html" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/intfc.html" target="_blank">OCaml C interfacing API</a>. This is pretty nice, in particular since the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml" target="_blank">OCaml compiler</a> is moving pretty quickly these days (which is awesome!) and also if, perhaps one day, <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/ocaml-multicore" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/ocaml-multicore" target="_blank">support for multi-core</a> is added to the compiler, which will undoubtedly change the C interface API quite a bit.<br><br>[dune](https://github.com/ocaml/dune) (formally jbuilder ) is a build system for OCaml projects that has recently raised to much popularity, particularly due to its tight integration with the rest of the OCaml ecosystem, such as [ocamlfind](http://projects.camlcity.org/projects/findlib.html) and [opam](https://opam.ocaml.org/) .<br>My personal motto in programming in general is that “Simple things should be simple, but complex things should be possible”. dune certainly does not fit into that category but, rather, makes some complex things extremely easy to setup. It’s the kind of tool that will make your life incredibly easier when what you intent to do fits well within their workflow but might not be easy to bend to some very specific niche use. We will see one such case below.<br>At any rate, it’s been an amazing experience getting to learn how to use dune and the resulting code and build system is remarkably short and elegant, yet very powerful.<br><br>socket.h is the Unix header that describes the C API to various socket operations, IP version 4 and 6 as well as unix file sockets. There is also a windows API mimicking it, which makes most code using it easily portable to&nbsp;windows.<br>Most network-based C libraries refer to socket.h to describe the type of socket that can be used with their API so it’s an important entry point for a lot of network operations and one that would be nice to support as generically as possible in OCaml.<br>The catch, though, is that, most likely for historical reasons¹, the <a data-tooltip-position="top" aria-label="https://pubs.opengroup.org/onlinepubs/7908799/xns/syssocket.h.html" rel="noopener nofollow" class="external-link" href="https://pubs.opengroup.org/onlinepubs/7908799/xns/syssocket.h.html" target="_blank">POSIX specifications</a> only partially defines some of the required data structures and types, which makes it possible to write C code using them but does not give enough information to write C bindings without having to use the compiler to parse the actual system-specific headers of the running host.<br>For instance, here’s how the sockaddr structure is specified:<br>The &lt;sys/socket.h&gt; header defines the sockaddr structure that includes at least the following members:sa_family_t   sa_family       address family<br>
char          sa_data[]       socket address (variable-length data)<br>Likewise, here’s what is specified about the size of the socklen_t data type:<br>&lt;sys/socket.h&gt; makes available a type, socklen_t, which is an unsigned opaque integral type of length of at least 32 bits.<br>Thus, in order to know the exact offset of sa_family inside the sockaddr structure or the actual size of a socklen_t integer, one has to include the OS-specific header, parse its definitions for that specific OS and, only then, is it possible to compute that offset or data size. Let’s see how it’s done in our binding now!<br><br>The C binding requires 4 separate passes:<br>
<br>The [constants](https://github.com/toots/ocaml-sys-socket/tree/master/src/sys-socket/constants) pass, which computes and exports some specific constant and data sizes, computed from the C headers
<br>The [types](https://github.com/toots/ocaml-sys-socket/tree/master/src/sys-socket/types) pass, which, given the system-specific constants and sizes exported in the previous phase, defines the actual C data structure bindings.
<br>The [stubs](https://github.com/toots/ocaml-sys-socket/tree/master/src/sys-socket/stubs) pass, where we define the actual bindings to the C functions that we wish to export in our API.
<br>Finally, the <a data-tooltip-position="top" aria-label="https://github.com/toots/ocaml-sys-socket/blob/master/src/sys-socket/sys_socket.mli" rel="noopener nofollow" class="external-link" href="https://github.com/toots/ocaml-sys-socket/blob/master/src/sys-socket/sys_socket.mli" target="_blank">last pass</a> does a cleanup of the stubs pass to export a relevant and OCaml- (and ocaml-ctypes) specific public API that is to be used by users of the module.
<br>dune makes each of these steps fairly easy to integrate into the next one, defining compilation elements and binaries to build before moving to the next pass.<br><br>During that pass, we compute and export all required C values defined in the headers. We also add our own constants, which give us the sizes that the POSIX specifications leave up to the OS. Here’s the OCaml code for it:<br>module Def (S : Cstubs.Types.TYPE) = struct
  let af_inet = S.constant "AF_INET" S.int
  let af_inet6 = S.constant "AF_INET6" S.int
  let af_unix = S.constant "AF_UNIX" S.int
  let af_unspec = S.constant "AF_UNSPEC" S.int
  let sa_data_len = S.constant "SA_DATA_LEN" S.int
  let sa_family_len = S.constant "SA_FAMILY_LEN" S.int
  let sock_dgram = S.constant "SOCK_DGRAM" S.int
  let sock_stream = S.constant "SOCK_STREAM" S.int
  let sock_seqpacket = S.constant "SOCK_STREAM" S.int
  let socklen_t_len = S.constant "SOCKLEN_T_LEN" S.int
  let ni_maxserv = S.constant "NI_MAXSERV" S.int
  let ni_maxhost = S.constant "NI_MAXHOST" S.int
  let ni_numerichost = S.constant "NI_NUMERICHOST" S.int
  let ni_numericserv = S.constant "NI_NUMERICSERV" S.int
end
<br>Pretty straightforward! Some of these constants are defined by the POSIX headers and some are custom defined for our needs, for instance SOCKLEN_T_LEN . Here’s how they are extracted, using the dune build configuration for [gen_constants_c](https://github.com/toots/ocaml-sys-socket/blob/master/src/sys-socket/generator/gen_constants_c.ml):<br>let c_headers = "
#ifdef _WIN32
  #include &lt;winsock2.h&gt;
  #include &lt;ws2tcpip.h&gt;
#else
  #include &lt;sys/socket.h&gt;
  #include &lt;sys/un.h&gt;
  #include &lt;netdb.h&gt;
#endif
#define SA_DATA_LEN (sizeof(((struct sockaddr*)0)-&gt;sa_data))
#define SA_FAMILY_LEN (sizeof(((struct sockaddr*)0)-&gt;sa_family))
#define SOCKLEN_T_LEN (sizeof(socklen_t))
#ifndef NI_MAXHOST
  #define NI_MAXHOST 1025
#endif
#ifndef NI_MAXSERV
  #define NI_MAXSERV 32
#endif
"
let () =
  let fname = Sys.argv.(1) in
  let oc = open_out_bin fname in
  let format =
    Format.formatter_of_out_channel oc
  in
  Format.fprintf format "%s@\n" c_headers;
  Cstubs.Types.write_c format (module Sys_socket_constants.Def);
  Format.pp_print_flush format ();
  close_out oc
<br>This OCaml code makes use of ocaml-ctypes to build a binary that exports the OCaml interface defined by Sys_socket_constants.Def . Once compiled, its output looks like this:<br>include Ctypes
let lift x = x
open Ctypes_static

let rec field : type t a. t typ -&gt; string -&gt; a typ -&gt; (a, t) field =
  fun s fname ftype -&gt; match s, fname with
  | View { ty }, _ -&gt;
    let { ftype; foffset; fname } = field ty fname ftype in
    { ftype; foffset; fname }
  | _ -&gt; failwith ("Unexpected field "^ fname)

let rec seal : type a. a typ -&gt; unit = function
  | Struct { tag; spec = Complete _ } -&gt;
    raise (ModifyingSealedType tag)
  | Union { utag; uspec = Some _ } -&gt;
    raise (ModifyingSealedType utag)
  | View { ty } -&gt; seal ty
  | _ -&gt;
    raise (Unsupported "Sealing a non-structured type")

type 'a const = 'a
let constant (type t) name (t : t typ) : t = match t, name with
  | Ctypes_static.Primitive Cstubs_internals.Int, "NI_NUMERICSERV" -&gt;
    8
  | Ctypes_static.Primitive Cstubs_internals.Int, "NI_NUMERICHOST" -&gt;
    2
  | Ctypes_static.Primitive Cstubs_internals.Int, "NI_MAXHOST" -&gt;
    1025
  | Ctypes_static.Primitive Cstubs_internals.Int, "NI_MAXSERV" -&gt;
    32
  | Ctypes_static.Primitive Cstubs_internals.Int, "SOCKLEN_T_LEN" -&gt;
    4
  | Ctypes_static.Primitive Cstubs_internals.Int, "SOCK_STREAM" -&gt;
    1
  | Ctypes_static.Primitive Cstubs_internals.Int, "SOCK_STREAM" -&gt;
    1
  | Ctypes_static.Primitive Cstubs_internals.Int, "SOCK_DGRAM" -&gt;
    2
  | Ctypes_static.Primitive Cstubs_internals.Int, "SA_FAMILY_LEN" -&gt;
    1
  | Ctypes_static.Primitive Cstubs_internals.Int, "SA_DATA_LEN" -&gt;
    14
  | Ctypes_static.Primitive Cstubs_internals.Int, "AF_UNSPEC" -&gt;
    0
  | Ctypes_static.Primitive Cstubs_internals.Int, "AF_UNIX" -&gt;
    1
  | Ctypes_static.Primitive Cstubs_internals.Int, "AF_INET6" -&gt;
    30
  | Ctypes_static.Primitive Cstubs_internals.Int, "AF_INET" -&gt;
    2
  | _, s -&gt; failwith ("unmatched constant: "^ s)

let enum (type a) name ?typedef ?unexpected (alist : (a * int64) list) =
  match name with
  | s -&gt;
    failwith ("unmatched enum: "^ s)
<br>The files used to describe how to build this binary using dune are located in a separate [generator](https://github.com/toots/ocaml-sys-socket/tree/master/src/sys-socket/generator) directory. Here’s the entry to build this one:<br>(executable
 (name gen_constants_c)
 (modules gen_constants_c)
 (libraries sys-socket.constants ctypes.stubs))

(rule
 (targets gen_constants.c)
 (deps    (:gen ./gen_constants_c.exe))
 (action  (run %{gen} %{targets})))

(rule
 (targets gen_constants_c)
 (deps    (:c_code ./gen_constants.c))
 (action  (run %{ocaml-config:c_compiler} -I %{lib:ctypes:} -I %{ocaml-config:standard_library} -o %{targets} %{c_code})))
<br>This executable is compiled during the next phase. Let’s move into it now!<br><br>During that phase, we use the constants exported during the previous phase to describe the various C structures and types. This is by far the most complex part of the code, making use of first-class modules and several OCaml tricks.<br>First, let’s look at how we tell dune that we need to generate the .ml file exporting our required constants from the previous pass:<br>(rule
 (targets sys_socket_generated_constants.ml)
 (deps    (:exec ../generator/exec.sh)
          (:gen ../generator/gen_constants_c))
 (action  (with-stdout-to %{targets}
            (system "%{exec} %{ocaml-config:system} %{gen}"))))
<br>With only this information, if the code refers to a Sys_socket_generated_constants module, dune will know that this module needs to be generated and how to do it. We will explain later the use of the exec.sh wrapper here.<br>Now that we can make use of the exported constants in our OCaml code, let’s see how we define the Socklen module, exporting abstract types and interface to use socklen_t integers:<br>module Constants = Sys_socket_constants.Def(Sys_socket_generated_constants)

module type Socklen = functor (S : Cstubs.Types.TYPE) -&gt; sig
  type socklen
  val socklen_t : socklen S.typ
  val int_of_socklen : socklen -&gt; int
  val socklen_of_int : int -&gt; socklen
end

let socklen : (module Socklen)  =
    match Constants.socklen_t_len with
      | 4 -&gt; (module functor (S : Cstubs.Types.TYPE) -&gt; struct
                 type socklen = Unsigned.uint32
                 let socklen_t = S.uint32_t
                 let int_of_socklen = Unsigned.UInt32.to_int
                 let socklen_of_int = Unsigned.UInt32.of_int
               end)
      | 8 -&gt; (module functor (S : Cstubs.Types.TYPE) -&gt; struct
                 type socklen = Unsigned.uint64
                 let socklen_t = S.uint64_t
                 let int_of_socklen = Unsigned.UInt64.to_int
                 let socklen_of_int = Unsigned.UInt64.of_int
               end)
      | _ -&gt; assert false

module Socklen = (val socklen : Socklen)
<br>As you can see, we make use of first-order modules and the size of the socklen_t integer to define the right API for the compiling host. Now let’s see how we define the sockaddr interface:<br>module type SaFamily = sig
  type sa_family
  val int_of_sa_family : sa_family -&gt; int
  val sa_family_of_int : int -&gt; sa_family
  
  module T : functor (S : Cstubs.Types.TYPE) -&gt; sig
    val t : sa_family S.typ
  end
end

let saFamily : (module SaFamily)  =
    match Constants.sa_family_len with
      | 1 -&gt;  (module struct
                 type sa_family = Unsigned.uint8
                 let int_of_sa_family = Unsigned.UInt8.to_int
                 let sa_family_of_int = Unsigned.UInt8.of_int                 
                 module T (S : Cstubs.Types.TYPE) = struct
                   let t = S.uint8_t
                 end
               end)
...

module SaFamily = (val saFamily : SaFamily)

module Def (S : Cstubs.Types.TYPE) = struct
  include Constants

  include Socklen(S)

  include SaFamily

  module SaFamilyT = SaFamily.T(S)

  let sa_family_t = S.typedef SaFamilyT.t "sa_family_t"

  module Sockaddr = struct
    type t = unit
    let t = S.structure "sockaddr"
    let sa_family = S.field t "sa_family" sa_family_t
    let sa_data = S.field t "sa_data" (S.array sa_data_len S.char)
    let () = S.seal t
  end
  
  ...
end
<br>Here, too, we make use of the size of sa_family as exported previously to define the right structure fields.<br>Next step, we need to compile this interface again to export the right offset for the various structures that have been defined. That’s dune’s job again!<br>First, the generator code:<br>let c_headers = "
#ifdef _WIN32
  #include &lt;winsock2.h&gt;
  #include &lt;ws2tcpip.h&gt;
#else
  #include &lt;sys/socket.h&gt;
  #include &lt;sys/un.h&gt;
  #include &lt;netinet/in.h&gt;
  #include &lt;netdb.h&gt;
#endif
"

let () =
  let fname = Sys.argv.(1) in
  let oc = open_out_bin fname in
  let format =
    Format.formatter_of_out_channel oc
  in
  Format.fprintf format "%s@\n" c_headers;
  Cstubs.Types.write_c format (module Sys_socket_types.Def);
  Format.pp_print_flush format ();
  close_out oc
<br>And the build instructions:<br>(executable
 (name gen_types_c)
 (modules gen_types_c)
 (libraries sys-socket.types ctypes.stubs))

(rule
 (targets gen_types.c)
 (deps    (:gen ./gen_types_c.exe))
 (action  (run %{gen} %{targets})))

(rule
 (targets gen_types_c)
 (deps    (:c_code ./gen_types.c))
 (action  (run %{ocaml-config:c_compiler} -I %{lib:ctypes:} -I %{ocaml-config:standard_library} -o %{targets} %{c_code})))
<br>Once, compiled, the exported .ml looks like this:<br>include Ctypes
let lift x = x
open Ctypes_static

let rec field : type t a. t typ -&gt; string -&gt; a typ -&gt; (a, t) field =
  fun s fname ftype -&gt; match s, fname with
...
  | Struct ({ tag = "sockaddr"} as s'), "sa_data" -&gt;
    let f = {ftype; fname; foffset = 2} in
    (s'.fields &lt;- BoxedField f :: s'.fields; f)
  | Struct ({ tag = "sockaddr"} as s'), "sa_family" -&gt;
    let f = {ftype; fname; foffset = 1} in
    (s'.fields &lt;- BoxedField f :: s'.fields; f)
  | View { ty }, _ -&gt;
    let { ftype; foffset; fname } = field ty fname ftype in
    { ftype; foffset; fname }
  | _ -&gt; failwith ("Unexpected field "^ fname)

let rec seal : type a. a typ -&gt; unit = function
...
  | Struct ({ tag = "sockaddr_storage"; spec = Incomplete _ } as s') -&gt;
    s'.spec &lt;- Complete { size = 128; align = 8 }
  | Struct ({ tag = "sockaddr"; spec = Incomplete _ } as s') -&gt;
    s'.spec &lt;- Complete { size = 16; align = 1 }
  | Struct { tag; spec = Complete _ } -&gt;
    raise (ModifyingSealedType tag)
  | Union { utag; uspec = Some _ } -&gt;
    raise (ModifyingSealedType utag)
  | View { ty } -&gt; seal ty
  | _ -&gt;
    raise (Unsupported "Sealing a non-structured type")

type 'a const = 'a
let constant (type t) name (t : t typ) : t = match t, name with
  | _, s -&gt; failwith ("unmatched constant: "^ s)

let enum (type a) name ?typedef ?unexpected (alist : (a * int64) list) =
  match name with
  | s -&gt;
    failwith ("unmatched enum: "^ s)
<br>As you can see, this exports all the offsets required to access the fields inside a sockaddr_t structure. We’re now ready to move to the final stage, which is the actual binding stubs!<br><br>First step in this pass, just like with the previous ones, we need to configure dune to be able to build the exported .ml code from the types pass:<br>(rule
 (targets sys_socket_generated_types.ml)
 (deps    (:exec ../generator/exec.sh)
          (:gen ../generator/gen_types_c))
 (action  (with-stdout-to %{targets}
            (system "%{exec} %{ocaml-config:system} %{gen}"))))
<br>And we can now define the proper bindings. Here’s how it looks like:<br>open Ctypes

module Def (F : Cstubs.FOREIGN) = struct
  open F

  module Types = Sys_socket_types.Def(Sys_socket_generated_types)

  open Types

  let getnameinfo = foreign "getnameinfo" (ptr sockaddr_t @-&gt; socklen_t @-&gt; ptr char @-&gt; socklen_t @-&gt; ptr char @-&gt; socklen_t @-&gt; int @-&gt; (returning int))

...
end
<br>As you can see, we’re exporting the getnameinfo function, taking various arguments, including a pointer to a sockaddr_t structure and a couple of socklen_t integers, making use of all the various data types and structures previously defined. The exact specifications of this function can be found <a data-tooltip-position="top" aria-label="https://pubs.opengroup.org/onlinepubs/009695399/functions/getnameinfo.html" rel="noopener nofollow" class="external-link" href="https://pubs.opengroup.org/onlinepubs/009695399/functions/getnameinfo.html" target="_blank">here</a>. We can now define out top-level API..<br><br>Building upon the previous modules, we export various OCaml idiomatic APIs that the binding user can now use to build new bindings against the socket.h APIs.<br>Just like with the previous steps, first we need to configure the build system:<br>(rule
 (targets sys_socket_generated_stubs.ml)
 (deps    (:gen ./generator/gen_stubs.exe))
 (action  (run %{gen} ml %{targets})))

(rule
 (targets sys_socket_generated_stubs.c)
 (deps    (:gen ./generator/gen_stubs.exe))
 (action  (run %{gen} c %{targets})))
<br>This time, we need ocaml-ctypes to generate two compilation units: a .ml file describing the API exported during the stubs phase, as well as the C code to glue it with the C APIs. Here’s the code for that generator:<br>let c_headers = "
#ifdef _WIN32
  #include &lt;winsock2.h&gt;
  #include &lt;ws2tcpip.h&gt;
#else
  #include &lt;sys/socket.h&gt;
  #include &lt;netinet/in.h&gt;
  #include &lt;arpa/inet.h&gt;
  #include &lt;netdb.h&gt;
#endif
#include &lt;string.h&gt;
"

let () =
  let mode = Sys.argv.(1) in
  let fname = Sys.argv.(2) in
  let oc = open_out_bin fname in
  let format =
    Format.formatter_of_out_channel oc
  in
  let fn =
    match mode with
      | "ml" -&gt; Cstubs.write_ml
      | "c"  -&gt;
         Format.fprintf format "%s@\n" c_headers;
         Cstubs.write_c
      | _    -&gt; assert false
  in
  fn ~concurrency:Cstubs.unlocked format ~prefix:"sys_socket" (module Sys_socket_stubs.Def);
  Format.pp_print_flush format ();
  close_out oc
<br>The exported .ml and .c files are omitted here for simplicity but the reader can generated them themselves from the [ocaml-sys-socket](https://github.com/toots/ocaml-sys-socket) repository if they are curious about their actual content.<br>We can now export our top-level API:<br>open Ctypes

include Sys_socket_types.SaFamily

include Sys_socket_stubs.Def(Sys_socket_generated_stubs)

type socklen = Types.socklen
let socklen_t = Types.socklen_t
let int_of_socklen = Types.int_of_socklen
let socklen_of_int = Types.socklen_of_int

module Sockaddr = struct
  include Types.Sockaddr
  let from_sockaddr_storage = from_sockaddr_storage t
  let sa_data_len = Types.sa_data_len
end

let getnameinfo sockaddr_ptr =
  let maxhost = Types.ni_maxhost in
  let s = allocate_n char ~count:maxhost in
  let maxserv = Types.ni_maxserv in
  let p = allocate_n char ~count:maxserv in
  match getnameinfo sockaddr_ptr (socklen_of_int (sizeof sockaddr_t))
                    s (socklen_of_int maxhost) 
                    p (socklen_of_int maxserv)
                    (Types.ni_numerichost lor
                     Types.ni_numericserv)  with
    | 0 -&gt;
      let host =
        let length =
          Unsigned.Size_t.to_int
            (strnlen s (Unsigned.Size_t.of_int maxhost))
        in
        string_from_ptr s ~length
      in
      let port =
        let length =
          Unsigned.Size_t.to_int
            (strnlen p (Unsigned.Size_t.of_int maxserv))
        in
        let port =
          string_from_ptr p ~length
        in
        try
          int_of_string port
        with _ -&gt;
          match getservbyname p null with
            | ptr when is_null ptr -&gt; failwith "getnameinfo"
            | ptr -&gt;
               Unsigned.UInt16.to_int
                 (ntohs (!@ (ptr |-&gt; Types.Servent.s_port)))
      in
      host, port
    | _ -&gt; failwith "getnameinfo"

...
<br>open Ctypes

(** Ctypes routines for C type socklen_t. *)
type socklen
val socklen_t : socklen typ
val int_of_socklen : socklen -&gt; int
val socklen_of_int : int -&gt; socklen

(** Generic sockaddr_t structure. *)
module Sockaddr : sig
  type t
  val t : t structure typ
  val sa_family : (sa_family, t structure) field
  val sa_data : (char carray, t structure) field
  val sa_data_len  : int

  val from_sockaddr_storage : SockaddrStorage.t structure ptr -&gt; t structure ptr
end

(** IP address conversion functions. *)
val getnameinfo : sockaddr ptr -&gt; string * int

...
<br>That’s it! We now have ocaml-ctypes specific data types and structures that can be used to interface with the host’s native socket.h APIs. Note that we also worked on top of the original low-level binding to getnameinfo to export a higher-level function more idiomatic to the OCaml language.<br><br>On windows platforms, liquidsoap is compiled using [ocaml-cross-windows](https://github.com/ocaml-cross/opam-cross-windows) and, since windows does have compatible socket APIs, we wanted to also look at cross-compiling for the windows target, which is where we hit a snag on the current dune support.<br>The problem is that, at each intermediary steps, in the case of a cross-compilation, the compiled binaries need to use the target’s OS headers and not the host’s headers, otherwise we end up using offsets specific to e.g. Debian but for a windows binary.<br>In this case, this means that the compiled .exe binaries need to be windows binaries and that we need to execute them as windows native binaries, using [wine](https://www.winehq.org/) .<br>dune has a truly amazing <a data-tooltip-position="top" aria-label="https://dune.readthedocs.io/en/latest/cross-compilation.html" rel="noopener nofollow" class="external-link" href="https://dune.readthedocs.io/en/latest/cross-compilation.html" target="_blank">support for cross-compiling</a>, which we do not cover here, but, unfortunately, its primitives for building and executing binaries do not yet cover this use case. Thus we had to trick it into compiling things the way we wanted to do, which why we are using the exec.sh wrapper. Here’s its code:<br>#!/bin/sh

SYSTEM=$1
CMD=$2
ARG=$3

if test "${SYSTEM}" = "mingw"; then
  wine $CMD $ARG
elif test "${SYSTEM}" = "mingw64"; then
  wine64 $CMD $ARG
else
  $CMD $ARG
<br>Now, you can go back to the previous dune files and see how this wrapper allows to execute binaries according to the system that the corresponding ocamlopt compiler has been configured to build for.<br><br>It’s been a fun time working on this binding! It’s amazing to see the level of details that can be built through ocaml-ctypes using their provided primitives. Ultimately, the binding is very clean and elegant, with very few low-level assumptions.<br>Likewise, the simplicity and power of the dune build system makes this very fluid to build. Without it, each of the described steps above would have been much more painful to execute and compile.<br>[1]: My bet is that, at the time the POSIX specifications were being written, there we already several inconsistent socket.h headers out in the wild among the various historical UNIX flavors..]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/advanced-c-binding-using-ocaml-ctypes-and-dune.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Advanced C binding using ocaml-ctypes and dune.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:47:06 GMT</pubDate></item><item><title><![CDATA[Folding in Parallel]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <br>We describe interesting ways of representing an inherently sequential accumulation (fold) as the composition of a map and a monoid reduction. As the result, some seemingly sequential algorithms can run not just in parallel but embarrassingly in parallel: The input sequence can be arbitrarily partitioned (and recursively sub-partitioned, if desired) among workers, which would run in parallel with no races, dependencies or even memory bank conflicts. Such embarrassing parallelism is ideal for multi-core, GPU or distributed processing.<br>The general principle is well-known; also well-known is that its specific applications require ingenuity.<br>
<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#intro" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#intro" target="_blank">Introduction: Fold v. Monoid Reduction</a>
<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#trivial" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#trivial" target="_blank">Fold as map-reduce, trivially</a>
<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#simple" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#simple" target="_blank">Fold as map-reduce, more interestingly</a>
<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#Horner" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#Horner" target="_blank">Generalized Horner rule</a>
<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#BM" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#BM" target="_blank">Boyer-Moore majority voting</a>
<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#conclusions" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/map-monoid-reduce.html#conclusions" target="_blank">Conclusions</a>
<br><br><br>At ICFP 2009, Guy Steele gave a keynote calling ``foldl and foldr considered slightly harmful'' and advocating (map)reduce instead.<br>Recall, folding over a sequence is an inherently sequential stateful accumulation; using lists for concreteness, it is defined as<br>fold_left  : ('z -&gt; 'a -&gt; 'z) -&gt; 'z -&gt; 'a list -&gt; 'z
fold_right : ('a -&gt; 'z -&gt; 'z) -&gt; 'a list -&gt; 'z -&gt; 'z
<br>Actually, there are two operations: the left and the right fold. Their meaning should be clear from the following example:<br>fold_left (+) 0 [1;2;3;4]  ≡ (((0 + 1) + 2) + 3) + 4

fold_right (+) [1;2;3;4] 0 ≡ 1 + (2 + (3 + (4 + 0)))
<br>In this case, of the folding function being addition, the results are identical: both expressions sum the list. Generally, left and right folds produce different results: try, for example, subtraction as the folding function. The types of the list elements 'a and of the accumulator 'z need not be the same: for example,<br>fold_left (fun z _ -&gt; z + 1) 0 l
<br>computes the length of any list,<br>fold_left (fun z x -&gt; x :: z) [] l
<br>reverses the list l and<br>fold_right (fun x z -&gt; if p x then x::z else z) l []
<br>filters the list: omits the elements for which the predicate p returns false. Many other operations on lists (in fact, all of them) can be expressed as folds. Fold is indeed the general pattern of sequential stateful processing of a sequence.<br>The are alternative definitions of fold: sometimes the last two arguments of the right fold are swapped. If the arguments of the right folding function are likewise swapped, then the left and the right fold have the same signature. Their behavior, the association pattern, is still different. Olivier Danvy, see below, traces the history of list folds and the argument order.<br>For concreteness we showed folds over lists, but similar operations exist over arrays, streams, files, trees, dictionaries and any other collections.<br>In this article, by reduce we always mean the reduce over a monoid. A monoid is a set (called `carrier set') with an associative binary operation which has a unit (also called zero) element. Concretely, in OCaml<br>type 'a monoid = {zero: 'a; op: 'a -&gt; 'a -&gt; 'a}
<br>where 'a is the type of monoid elements, op must be associative and<br>op zero x = op x zero = x
<br>must hold for every element x of the monoid. In Google MapReduce, the operation op is also taken to be commutative. We do not impose such requirement.<br>Reduce over a sequence (here, a list) is the operation<br>reduce : 'a monoid -&gt; 'a list -&gt; 'a
<br>with the behavior that can be illustrated as<br>reduce monoid []  ≡ monoid.zero
reduce monoid [x] ≡ x             (* for any x and monoid *)

reduce {zero=0;op=(+)} [1;2;3;4] ≡ 1 + 2 + 3 + 4
<br>One may say that reduce wedges in' the monoid operation between the consecutive elements of the sequence. Since op` is associative, the parentheses are not necessary. Therefore, unlike the left and the right fold, there is only one reduce.<br>We shall see that reduce is often pre-composed with map. The two operations may always be fused, into<br>map_reduce : ('a -&gt; 'z) -&gt; 'z monoid -&gt; 'a list -&gt; 'z
<br>Nevertheless, we will write map and reduce separately, for clarity&nbsp;-- assuming that actual implementations use the efficient, fused operation.<br>Fold has to be evaluated sequentially, because of the data dependency on the accumulator. In fact, the left fold is just another notation for a for-loop:<br>type 'a array_slice = {arr:'a array; from:int; upto:int} 
let fold_left_arr : ('z -&gt; 'a -&gt; 'z) -&gt; 'z -&gt; 'a array_slice -&gt; 'z = 
  fun f z {arr;from;upto} -&gt;
  let acc = ref z in
  for i=from to upto do
    acc := f !acc arr.(i)
  done;
  !acc
<br>Here, for variety, we use an array slice as a sequence.<br>On the other hand, reduce has a variety of implementations. It may be performed sequentially:<br>let seqreduce_arr (m: 'a monoid) (arrsl: 'a array_slice) : 'a =
  fold_left_arr m.op m.zero arrsl
<br>Or it can be done in parallel:<br>let rec parreduce_arr (m: 'a monoid) {arr;from;upto} : 'a =
  match upto+1-from with
  | 0 -&gt; m.zero
  | 1 -&gt; arr.(from)
  | 2 -&gt; m.op arr.(from) arr.(from+1)
  | 3 -&gt; m.op (m.op arr.(from) arr.(from+1)) arr.(from+2)
  | n -&gt; let n' = n / 2 in
         (* Here, the two parreduce_arr invocations can be done in parallel! *)
         m.op 
           (parreduce_arr m {arr;from;upto=from+n'-1})
           (parreduce_arr m {arr;from=from+n';upto})
<br>The two parreduce_arr invocations in the recursive case can run in parallel&nbsp;-- embarrassingly in parallel, with no races or even read dependencies. Whether they should be done in parallel is another question&nbsp;-- something that we can decide case-by-case. For example, if the array slice is short, the two parreduce_arr invocations are better done sequentially (since the ever-present overhead of parallel evaluation would dominate.) If the two halves of the slice are reduced also in parallel, we obtain a hierarchical decomposition: binary-tree--like processing.<br>We do not have to recursively decompose the slices. We may cut the input array into a sequence of non-overlapping slices, arbitrarily, and assign them to available cores. The cores may do their assigned work as they wish, without any synchronization with the others. At the end, we combine their results using the monoid operation.<br>Since the (left or right) fold commits us to sequential evaluation, Guy Steele called it `slightly harmful'. He urged using reduce, as far as possible, because it is flexible and decouples the algorithm from the execution strategy. The strategy (sequential, parallel, distributed) and data partitioning can be chosen later, depending on circumstances and available resources.<br>Thus the main question is: can we convert fold into reduce? The rest of the article answers it.<br>References<br>Guy Steele: Organizing Functional Code for Parallel execution or, foldl and foldr considered slightly harmful<br>
August 2009 (ICFP 2009, Keynote)<br>Olivier Danvy: ``Folding left and right matters: direct style, accumulators, and continuations''<br>
Journal of Functional Programming, vol 33, e2. Functional Pearl, February 2023<br>
Appendix A: A brief history of folding left and right over lists<br>According to Danvy, the first instances of fold_left and fold_right were investigated by Christopher Strachey in 1961(!). Reduce is part of APL (Iverson, 1962), where it is called /': hence +/xsums the arrayx`. Goedel recursor R in System T is a more general version of fold (called now para-fold). Church numerals are folds.<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/monoid_reduce.ml" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/monoid_reduce.ml" target="_blank">monoid_reduce.ml</a>&nbsp;[12K]<br>
Complete code for the article<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Streams.html#zip-folds" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Streams.html#zip-folds" target="_blank">How to zip folds</a><br>
A complete library of fold-represented lists, demonstrating that all list processing operations can be expressed as folds<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Scheme/xml.html#Papers" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Scheme/xml.html#Papers" target="_blank">Accumulating tree traversals, a better tree fold</a> with applications to XML parsing<br><br>The main question is expressing fold_left and fold_right in terms of reduce. In this section we see two trivial answers. In fact, we see that fold can always be expressed in terms of reduce&nbsp;-- but in a way that is not useful or interesting.<br>If the folding function, the first argument of fold, is associative (which implies that the type of sequence elements is the same as the accumulator/result type) and has a zero element, then fold is trivially an instance of reduce. We have seen the example already:<br>fold_left (+) 0 l ≡ fold_right (+) l 0 ≡ reduce {op=(+);zero=0} l
<br>for any list l.<br>The second trivial answer is that a fold can always be expressed in terms of a monoid reduce, unconditionally:<br>let compose_monoid = {op=(fun g h -&gt; fun x -&gt; g (h x)); zero=Fun.id}
fold_right f l z ≡ List.map f l |&gt; reduce compose_monoid |&gt; (|&gt;) z
<br>for any f, z, l of the appropriate types. The very similar expression can be written for the left fold (left as an exercise to the reader). The key idea is that function composition is associative.<br>Unfortunately, this trivial answer is of little practical use. If f:'a -&gt; 'z -&gt; 'z, then map f l creates a list of 'z -&gt; 'z closures, which reduce composes together into a (big) 'z -&gt; 'z closure, which is finally applied to z. If the list l is of size N, the result of the reduction is the composition of N closures, and hence has the size proportional to N (with a fairly noticeable proportionality constant: a closure takes several words of heap space). In effect we have built an intermediate data structure of unbounded size. Although composing the closures can be parallelized, the useful work is done only when the big closure composition is finally applied to the initial accumulator z&nbsp;-- at which point the folding function f is applied step-by-step, sequentially, just like in the original fold_right f l z. This trivial reduction of fold only wastes time and space.<br>The problem hence is not merely to express fold as reduce, but do so efficiently, without undue and unbounded overhead. Only then we can profitably fold in parallel.<br><br>Thus our problem is to efficiently express fold as reduce, without intermediary data of unbound size and without taking undue time. In other words, if folding can be done in constant time per sequence element and in constant (working) space, so should reduce.<br>Sometimes the problem can be solved simply. We have already seen one such case: the folding function is associative and has the unit element. The left and the right fold are instances of reduce then. A little more interesting is the case of the folding function f that can be factored out in terms of two other functions op and g as<br>f z x = op z (g x)
<br>for any sequence element x and accumulator z. Here op is an associative operation that has a zero element. As an example, remember finding the length of a list, expressed as fold as<br>List.fold_left (fun z _ -&gt; z + 1) 0 l
<br>The folding function indeed factors out<br>z + 1 = z + (Fun.const 1 x)
<br>and so length can be re-written as<br>map (Fun.const 1) l |&gt; reduce {op=(+);zero=0}
<br>or, as map_reduce. Length, therefore, may be computed in parallel.<br>Such a factorization is an instance of the general principle, called ``Conjugate Transform'' in Guy Steele's talk. The principle calls for representing fold as<br>fold_left (f:'z-&gt;'a-&gt;'a) (z:'z) l = map g l |&gt; reduce m |&gt; h
<br>and similarly for the right fold. Here m:'u monoid is a monoid for some type 'u, and g:'a-&gt;'u and h:'u-&gt;'z are some functions. They generally depend on f and z. Informally, the principle recommends we look for a `bigger'' type 'uthat can embed both'aand'z` and admits a suitable associative operation with a unit element.<br>One may always represent fold in such a way, as we saw in the previous section: we chose compose_monoid as m, with 'z-&gt;'z as the type 'u.<br>The conjugate transform is a principle, or a schema. It does not tell how to actually find the efficient monoid, or if it even exists. In fact, finding the efficient monoid is often non-trivial and requires ingenuity. As an example, consider a subtractive folding, mentioned in passing earlier. Subtraction is not associative and therefore fold_left (-) is not an instance of reduce. A moment of thought however shows that<br>fold_left (-) z l = z - fold_left (+) 0 l = z - reduce {op=(+);zero=0} l
<br>In terms of the conjugate transform schema, the function g is negation and h is adding z. The second moment of thought shows that fold_right (-) can not be reduced just as easily (or at all?). Actually, the right-subtractive-folding can also be carried out efficiently in parallel, as a monoid reduction. The reader is encouraged to find this monoid&nbsp;-- to appreciate the difficulty and ingenuity.<br><br>Horner rule, or schema, is a widely-used sequential algorithm to efficiently evaluate a polynomial. A similar accumulating pattern also frequently occurs in parsing. Although the algorithm is inherently sequential, on the face of it, it turns out representable as a monoid reduction. Our reduction is more general and flexible than the other known ways to parallelize the Horner rule.<br>As an illustrating example, we take the conversion of a sequence of digits (most-significant first) to the corresponding number&nbsp;-- a simple parsing operation. It is an accumulating sequential operation and can be written as fold:<br>let digits_to_num = 
  List.fold_left (fun z x -&gt; 10*z + (Char.code x - Char.code '0')) 0
<br>For example, digits_to_num ['1'; '7'; '5'] gives 175. One can easily factor it as a composition of a map and<br>List.fold_left (fun z x -&gt; 10*z + x) 0
<br>which is essentially the Horner rule: evaluating the polynomial Σ di bi at b=10. The folding function fun z x -&gt; 10*z + x is not associative, however.<br>Still, there is a way to build a monoid. It is the monoid with the operation<br>let op (x,b1) (y,b2) = (x*b2+y, b1*b2)
<br>which is clearly associative (but not commutative):<br>((x,b1) `op` (y,b2)) `op` (z,b3)
= (x*b2+y, b1*b2) `op` (z,b3)
= (x*b2*b3+y*b3+z, b1*b2*b3)
= (x,b1) `op` (y*b3+z,b2*b3)
= (x,b1) `op` ((y,b2) `op` (z,b3))
<br>with the unit (0,1):<br>(x,b) `op` (0,1) = (0,1) `op` (x,b) = (x,b)
<br>Therefore, digits_to_num can be written as<br>let digits_to_num = 
    List.map (fun x -&gt; (Char.code x - Char.code '0')) &gt;&gt;
    List.map (fun x -&gt; (x,10)) &gt;&gt;
    reduce {op;zero=(0,1)} &gt;&gt; fst
<br>where &gt;&gt; is the left-to-right functional composition. Our construction is also an instance of the conjugate transform, with g being fun x -&gt; (x,10) and and h being fst.<br>Thus, the Horner rule can be expressed as map-reduce and can hence be evaluated embarrassingly in parallel.<br>Our construction is general and applies to any folding function f : 'z -&gt; 'a -&gt; 'z of the form<br>f z x = m.op (h z) x
<br>where m.op is associative and has a zero element (that is, an operation of some monoid m). This factorization looks very similar to the one in the previous section; the difference, however slight, makes finding the monoid quite more difficult. It does exists:<br>let hmonoid (m:'a monoid) : ('a * ('a-&gt;'a)) monoid = 
  {op = (fun (x,h1) (y,h2) -&gt; (m.op (h2 x) y, h1 &gt;&gt; h2));
   zero = (m.zero, Fun.id)}
<br>Its carrier is the set of pairs (x,h) of monoid m elements and functions on them. We assume the composition h1&gt;&gt;h2 is represented efficiently. For example, when h is a multiplication by a constant, it can be represented by that constant; the composition of such functions is then representable by the product of the corresponding constants.<br>The Horner rule has the feel of the parallel prefix problem and the famous parallel scan of Guy Blelloch (PPoPP 2009). Horner rule does not require reporting of intermediate results, however. Our monoid reduction differs from Blelloch's even-odd interleaving. It also differs from monoid-cached trees from Guy Steele's ICFP09 keynote. We do not rely on any special data structures: We can work with plain arrays, partitioning them across available cores.<br>Since Horner method is pervasive in polynomial evaluation, there are approaches to parallelize it. The most common is even/odd splitting of polynomial coefficients. Although it may be well suitable for SIMD processing, the even/odd splitting is bad for multicore since it creates read contention (bank conflicts) and wastes cache space. Our monoid reduce lets us assign different memory banks to different cores, for their exclusive, conflict-free access.<br>On the other hand, Estrin's scheme may be seen as a very particular instance of our monoid construction: binomial grouping. Our monoid reduction allows for arbitrary grouping, hierarchically if desired, and of not necessarily of the same size.<br>References<br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/monoid_reduce.ml" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/monoid_reduce.ml" target="_blank">monoid_reduce.ml</a>&nbsp;[12K]<br>
Complete code for the article<br><br>Boyer-Moore majority voting is the algorithm for finding the majority element in a sequence&nbsp;-- that is, the element that occurs more than half of the time. The algorithm requires one pass over the sequence and the constant amount of working space. It is important to keep in mind that the return value is the majority element if it exists. The algorithm in general cannot tell if the sequence has majority: if majority does not exist, the return value is an arbitrary element, not necessarily the most frequently occurring. In general, therefore, one needs the second pass over the sequence to count the occurrences of the returned element and verify it is indeed the majority. The second pass may be avoided or be unnecessary in some cases.<br>The Boyer-Moore algorithm is called a prototypical streaming algorithm and is inherently sequential. It may be surprising, therefore, that it may be presented as monoid reduction and hence performed just as efficiently in parallel&nbsp;-- in fact, embarrassingly in parallel. The monoid reduction implementation is actually simple. What is complex is convincing ourselves that it indeed works and does the right thing, in all cases.<br>The Boyer-Moore algorithm is a state machine processing, ingesting input element-by-element. It can hence be implemented as fold (assuming the input is given as a non-empty list):<br>let bm : 'a list -&gt; 'a = function h::t -&gt; 
  let fsm (c,m) x =
      if c = 0 then (1,x)
      else if m = x then (c+1,m)
      else (c-1,m)
  in List.fold_left fsm (1,h) t &gt;&gt; snd
<br>The state is the counter c and the candidate majority m. At the end of the algorithm, when the entire sequence is scanned, m is the majority element (if the sequence has one). If the counter c at the end is more than half of the sequence length, the sequence has majority and m is the majority element. Otherwise, we have to make another pass through the sequence to verify that m is indeed the majority. The verification pass may be unnecessary if we have a prior knowledge that majority exists&nbsp;-- or if we are satisfied with any element if it does not.<br>Although the algorithm seems inherently sequential, it can in fact be represented as monoid reduction. The monoid is simple and efficient:<br>let bm_monoid (z:'a) : (int * 'a) monoid = 
  {zero = (0,z);
   op = fun (c1,m1) (c2,m2) -&gt;
     if c1 = 0 then (c2,m2) else
     if c2 = 0 then (c1,m1) else
     if m1 = m2 then (c1+c2,m1) else
     if c1 &gt; c2 then (c1-c2, m1) else
     (c2-c1, m2)}
<br>Monoid elements (the carrier set) are pairs: of a natural number c and of a sequence element m. When c is zero, m may be arbitrary; in the above code we use z for such an arbitrary element. We could have avoided specifying it by defining a more sophisticated data type for monoid carrier, or using 'a option. The reasoning below would get messier; therefore, we keep the above definition for the sake of clarity.<br>One can see that op is commutative and that zero is indeed the zero of op. However, is op associative? That is, is bm_monoid really a monoid? It is easy to check:<br>let m1 = (2,10) and m2 = (3,9) and m3 = (2,8)
op (op m1 m2) m3  ⇝ (1, 8) 
op m1 (op m2 m3)  ⇝ (1, 10)
<br>Unfortunately, bm_monoid is not a monoid.<br>To cut the suspense, it turns out that bm_monoid is `morally' a monoid, and reduction with it indeed gives the right result, in all cases. Proving it however is not that straightforward.<br>For the sake of proof, we extend bm_monoid to:<br>let bm_monoid_ext (z:'a) : (int * 'a * ('a*'a) list) monoid = 
  {zero = (0,z,[]);
   op = fun (c1,m1,l1) (c2,m2,l2) -&gt;
     if c1 = 0 then (c2,m2,l1@l2) else
     if c2 = 0 then (c1,m1,l1@l2) else
     if m1 = m2 then (c1+c2,m1,l1@l2) else
     if c1 &gt; c2 then (c1-c2, m1, repeat c2 (m1,m2) @ l1 @ l2) else
     (c2-c1, m2, repeat c1 (m1,m2) @ l1 @ l2)}
<br>Here, the infix operation @ is list concatenation and repeat (n:int) (x:'a) : 'a list produces the list with of length n with elements all equal to x.<br>The carrier set of bm_monoid_ext is a set of triples: a natural number c, a sequence element m, and a list of pairs with distinct components (that is, fst of any pair is not equal to its snd). We note that op preserves the invariant that all pairs in l have distinct components.<br>Let us introduce the relation ≈ on that set, defined as follows: x ≈ y just in case flatten x is equal to flatten y modulo permutation, where<br>let flatten : (int * 'a * ('a*'a) list) -&gt; 'a list = fun (c,m,l) -&gt;
  repeat c m @ List.map fst l @ List.map snd l
<br>That is, flatten x collects all elements mentioned within x in a multiset. Therefore, x ≈ y iff the multiset of elements of x is equal to the multiset of elements of y. One easily sees that the relation ≈ is reflexive, commutative and associative. That is, it is an equivalence relation. It also has useful for us properties, as follows.<br>Proposition: bm_monoid_ext is a monoid modulo ≈. That is,<br>op (0,z,[]) (c,m,l) ≈ op (c,m,l) (0,z,[]) ≈ (c,m,l)
op (c1,m1,l1) (op (c2,m2,l2) (c3,m3,l3)) ≈ op (op (c1,m1,l1) (c2,m2,l2)) (c3,m3,l3)
<br>The associativity property follows from the fact op preserves elements occurring in (c,m,l), including their multiplicities (and the multiset equality is associative).<br>Moreover, ≈ is adequate for the task of finding the majority.<br>Proposition (Adequacy): if (c1,m1,l1) ≈ (c2,m2,l2) and the multiset flatten (c1,m1,l1) has majority, then c1&gt;0, c2&gt;0, and m1=m2. Informally, ≈ preserves the found majority (if the majority exists). The proposition is the consequence of the fact that majority is invariant to sequence permutation, and the following representation lemma.<br>Lemma (Representation): if a sequence (multiset) flatten (c,m,l) has majority, then c&gt;0 and m is the majority element.<br>For the proof we note that if the list of distinct pairs l has n elements, it has 2*n of values. Not all of them are distinct. However, there is no value that occurs more than n times; otherwise, by the pigeonhole principle, some pair in l would have had equal components. Therefore, no value occurring in (c,m,l) that is distinct from m (if c&gt;0) can be the majority element. If the majority exists, it has to be m, and c&gt;0.<br>Our proofs are inspired by Boyer and Moore's original proofs. They however used induction (which is appropriate for the sequential algorithm). We, in contrast, used equational reasoning (and the pigeonhole principle) -- but no induction.<br>As an aside, it may seem that induction is the only proof method used in computer science&nbsp;-- many widely used textbooks present nothing but induction. In my opinion, induction is overused. Mathematics has many other proof methods.<br>The final step is to observe that the part of bm_monoid_ext that we actually care about&nbsp;-- the element m and the count c&nbsp;-- is computed with no regard to the list l. The list is a ghost' list, so to speak, needed only for proving correctness but not for computing the majority. Therefore, we may omit it and recover the bm_monoid`.<br>In the upshot, the majority voting may hence be performed as map-reduce: mapping with (fun x -&gt; (1,x)) and then reducing over the monoid bm_monoid. It is just as efficient as the original Boyer-Moore algorithm: one pass over the sequence using constant and small working space. However, the input may be split arbitrarily among available workers, who would do the search in parallel, without any interference or dependencies. Majority voting can be done embarrassingly in parallel.<br>References<br><a data-tooltip-position="top" aria-label="%5Bhttps://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_majority_vote_algorithm%5D(https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_majority_vote_algorithm)" rel="noopener nofollow" class="external-link" href="https://muqiuhan.github.io/wiki/[https://en.wikipedia.org/wiki/Boyer–Moore_majority_vote_algorithm](https://en.wikipedia.org/wiki/Boyer–Moore_majority_vote_algorithm)" target="_blank">[https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_majority_vote_algorithm](https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_majority_vote_algorithm)</a><br><a data-tooltip-position="top" aria-label="https://okmij.org/ftp/Algorithms/monoid_reduce.ml" rel="noopener nofollow" class="external-link" href="https://okmij.org/ftp/Algorithms/monoid_reduce.ml" target="_blank">monoid_reduce.ml</a>&nbsp;[12K]<br>
Complete code for the article<br><br>In his ICFP'09 keynote, Guy Steele exhorted us to use reduce (or, map_reduce) rather than fold, as far as possible. Unlike fold, reduce does not commit us to a particular evaluation strategy. It can be performed sequentially, embarrassingly parallel, or in a tree-like fashion. Like the Σ notation in Math, it specifies what should be summed up, but not how or in which sequence.<br>Guy Steele conclusions apply to the present article as well. Especially his final thoughts:<br>
<br>Associative combining operators are a VERY BIG DEAL
<br>Inventing (and proving) new combining operators is a very, very big deal
<br><br><br>This site's top page is <a data-tooltip-position="top" aria-label="http://okmij.org/ftp/" rel="noopener nofollow" class="external-link" href="http://okmij.org/ftp/" target="_blank"><strong></strong></a>http://okmij.org/ftp/  <br>oleg-at-okmij.org<br>
Your comments, problem reports, questions are very welcome!<br>Generated by MarXere]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/folding-in-parallel.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Folding in Parallel.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:07 GMT</pubDate></item><item><title><![CDATA[Generalised signature]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>
This post presents a technique for defining more reusable OCaml signatures, helping to maintain consistent APIs with minimal boilerplate. We'll work through a few examples, which you can check out <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/generalised-signatures" rel="noopener nofollow" class="external-link" href="https://github.com/CraigFe/generalised-signatures" target="_blank">on GitHub</a>.
<br><br>Consider the following definition of an iter function for some container type t:<br>let iter f t =
  for i = 0 to length t - 1 do
    f (get t i)
  done
<br>iter requires only that t comes with functions get and length. Many useful operations can be derived in terms of such indexing functions. To take advantage of this, let's move iter into a functor and provide some other useful operations too:<br>module type Indexable1 = sig
  type 'a t

  val get    : 'a t -&gt; int -&gt; 'a
  val length : _ t -&gt; int
end

module Foldable_of_indexable1 (I : Indexable1) : sig
  open I

  val iter      :        ('a -&gt; unit) -&gt; 'a t -&gt; unit
  val iteri     : (int -&gt; 'a -&gt; unit) -&gt; 'a t -&gt; unit
  val fold_left : ('acc -&gt; 'a -&gt; 'acc) -&gt; 'acc -&gt; 'a t -&gt; 'acc
  val exists    : ('a -&gt; bool) -&gt; 'a t -&gt; bool
  val for_all   : ('a -&gt; bool) -&gt; 'a t -&gt; bool
  val is_empty  : _ t -&gt; bool
  (* ... *)
end
<br>For many types, including array, the get-based definitions are identical to their hand-optimised equivalents (modulo functor application). We can imagine avoiding a lot of standard-library boilerplate – and potential for API inconsistency – by using many such functors <a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fn-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fn-1" target="_blank">1</a>. We'd end up defining exactly one iter function that suffices for all Indexable types.<br>All good so far. Now, let's consider the string type.<br>A string is also an indexable container with length and get functions, albeit one that can only contain char values. It's natural to expect to be able to re-use Foldable_of_indexable1 in some way: indeed, our definition of iter above is exactly equal to the one in Stdlib.String.iter. Unfortunately, our Indexable1 module type can only describe parametric containers:<br>module _ : (Indexable1 with type 'a t := string) = Stdlib.String
<br>Error: Signature mismatch:
       ...
       Values do not match:
         val get : t -&gt; int -&gt; char
       is not included in
         val get : t -&gt; int -&gt; 'a
       File "string.mli", line 52, characters 0-57: Actual declaration
<br>We're unable to tell the type system something like<br>
'a t = string &nbsp;&nbsp; implies &nbsp;&nbsp; 'a = char
<br>as part of our substitution. This means that many types – including string, bytes, unboxed arrays and unboxed vectors – can't benefit from our Foldable_of_iterable1 definitions, even though their own definitions will be identical!<br>When we wrapped our code in the Foldable_of_indexable1 functor, we needed to give it specific input and output module types, and the ones we picked artificially limited its usefulness. This is a hazard of functorising highly-generic code. As ever, we could solve the problem with copy-paste: a new Indexable0 module type for non-parametric containers, and a new functor Foldable_of_indexable0 with exactly the same implementations as our previous one.<br>(* Non-parametric indexable types *)
module type Indexable0 = sig
  type t
  type elt

  val get    : t -&gt; int -&gt; elt
  val length : t -&gt; int
end

module Foldable_of_indexable0 (I : Indexable0) : sig
  (* All with the same implementation as before... *)
end
<br>This definition suffers from the dual problem when we try to apply it to parameterised containers like 'a array:<br>module _ : (Indexable0 with type t := 'a array) = Stdlib.Array
<br>Error: The type variable 'a is unbound in this type declaration.
<br>This time, we wanted to be able to say something like<br>
elt = 'a &nbsp;&nbsp; implies &nbsp;&nbsp; t = 'a array &nbsp;&nbsp; (where 'a is universally quantified),
<br>which is even more nonsensical than our previous attempt. Neither Indexable0 nor Indexable1 can be expressed in terms of the other. We need something more general.<br><br>Interestingly, it's possible to generalise Indexable0 and Indexable1 with <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" target="_blank">another layer of indirection</a> by making elt a type operator:<br>module type IndexableN = sig
  type 'a t
  type 'a elt

  val get    : 'a t -&gt; int -&gt; 'a elt
  val length : _ t -&gt; int
end
<br>elt carries the type equalities needed for the Indexable1 case, without forbidding the non-parametric implementation needed for the Indexable0 case. Arrays can set 'a elt := 'a, and strings can set 'a elt := char. Indeed, we can do this in the general case:<br>(** [Indexable0] is a special-case of [IndexableN] *)
module Indexable0_to_N = functor
  (T : Indexable0) -&gt;
  (T : IndexableN with type 'a t := T.t and type 'a elt := elt)

(** [Indexable1] is a special-case of [IndexableN] *)
module Indexable1_to_N = functor
  (T : Indexable1) -&gt;
  (T : IndexableN with type 'a t := 'a T.t and type 'a elt := 'a)
<br>Now we can define a single Foldable_of_indexableN functor (with exactly the same implementations as before), and it will work for polymorphic and monomorphic containers. Neat!<br><img alt="A lattice showing Indexable0 and Indexable1 being generalised by IndexableN." src="https://www.craigfe.io/posts/generalised-signatures/dag-indexable.png" referrerpolicy="no-referrer"><br>In the general case, when you notice that different signatures are sharing common functions, it's often possible to unify them under a common interface with the following two steps:<br>
<br>generalise. Convert pure type variables into type operators (as in 'a → 'a elt), to support use-cases like instantiating those variables to fixed types. Add type parameters to existing types to carry type equalities between them (as in 'a t / 'a elt), to support use-cases where these types depend on each other.
<br>specialise. Use destructive substitution (:=) to eliminate those types and type parameters when they're not needed. We're taking advantage of the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/792" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/pull/792" target="_blank">more powerful destructive substitution</a> offered by OCaml 4.06, which allows us to freely undo our generalisation step.
<br>The truly magical part of this trick is that – with better support for destructive type substitutions recently added to Odoc – it can be made completely invisible<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fn-2" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fn-2" target="_blank">2</a> in documentation!<br>module type Indexable1 = sig
  type _ t

  val get : 'a t -&gt; int -&gt; 'a
  val length : _ t -&gt; int
end

(** This module gets identical documentation to the one above! *)
module type Indexable1' = sig
  include IndexableN with type 'a elt := 'a (** @inline *)
end
<br><br>One unavoidable limitation is in what sort of operations we can put in the Foldable_of_indexable functor. Suppose our initial attempt at generalising containers included a sum function:<br>let sum : int t -&gt; t = fold_left ( + ) 0
<br>sum requires a container that can hold int values, which is clearly not possible for strings as the type system will happily tell us:<br>   |   let sum = fold_left ( + ) 0
                           ^^^^^
Error: This expression has type int -&gt; int -&gt; int
       but an expression was expected of type int -&gt; 'a elt -&gt; int
       Type int is not compatible with type 'a elt
<br>To state the obvious, we can't rely on parametricity in our container functions if we want them to work on non-parametric containers. The natural solution here would be to define such parametric-only functions in a separate functor.<br><br>Indexable containers aren't the only example of generalised signatures in the real world. Indeed, many other data-structures and design patterns have APIs that can be unified in this way. Consider the case of hashtables, which have a huge space of possible implementations:<br>
<br>key types can be left polymorphic by using a magic hash function like caml_hash (as in <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.html" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.html" target="_blank"><code></code></a>Stdlib.Hashtbl), or fixed by a user-specified hash function (as in <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.Make.html" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.Make.html" target="_blank"><code></code></a>Stdlib.Hashtbl.Make).<br>

<br>value types can be left polymorphic, fixed by the user (as in persistent hashtables like <a data-tooltip-position="top" aria-label="https://mirage.github.io/index/index/Index/Make/index.html" rel="noopener nofollow" class="external-link" href="https://mirage.github.io/index/index/Index/Make/index.html" target="_blank"><code></code></a>Index), or even determined by the keys used to index them (as in universal maps like <a data-tooltip-position="top" aria-label="https://erratique.ch/software/hmap/doc/Hmap" rel="noopener nofollow" class="external-link" href="https://erratique.ch/software/hmap/doc/Hmap" target="_blank"><code></code></a>Hmap).<br>

<br>Initially, it looks like these different hashtables will each require their own hand-written signature (and this is what the standard library does with its hashtables). However, with enough type parameters, these different implementations can all be unified under a single Hashtbl_generalised module type:<br>module type Hashtbl_generalised = sig
  (** We have three types ([t], [key] and [value]) and three type variables:

      - ['k]/['v] allow the hashtable to determine key/value types;
      - ['a] is carried from keys to corresponding values, allowing the key to
        determine the types of values. *)

  type ('k, 'v) t
  type ('k, 'a) key
  type ('v, 'a) value

  val create : int -&gt; (_, _) t
  val replace : ('k, 'v) t -&gt; ('k, 'a) key -&gt; ('v, 'a) value -&gt; unit
  val remove : ('k, _) t -&gt; ('k, _) key -&gt; unit
  val find_opt : ('k, 'v) t -&gt; ('k, 'a) key -&gt; ('v, 'a) value option
  (* ... *)
end
<br>We can then implement our different hashtable signatures as specialisations:<br><img alt="A lattice showing four different `Hashtbl` module types being generalised by `Hashtbl_generalised`." src="https://www.craigfe.io/posts/generalised-signatures/dag-hashtables.png" referrerpolicy="no-referrer"><br>For instance, for the regular polymorphic hashtable:<br>module type Poly_hash = sig
  include Hashtbl_generalised
    with type ('k, _) key := 'k
     and type ('v, _) value := 'v (** @inline **)
end
<br>The other specialisations are very similar (see <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/generalised-signatures/blob/main/examples/hashtbl.ml" rel="noopener nofollow" class="external-link" href="https://github.com/CraigFe/generalised-signatures/blob/main/examples/hashtbl.ml" target="_blank">here</a> for the specifics).<br>What is it that makes Hashtable_generalised a good parent interface for these four flavours of hashtable? To get some insight, we can notice that each of the type parameters ('k, 'v, and 'a) connects its own pair of types:<br>hashtbl_generalised<br><img src="https://www.craigfe.io/posts/generalised-signatures/dep-hashtbl_generalised.png" referrerpolicy="no-referrer"><br>Framed this way, the type parameter 'k exists solely to carry type information between hashtables and their keys (using a type equality at call sites). Similarly, 'v bridges between hashtables and values, and 'a between keys and values. From here, each of our hashtable variants uses destructive subsitution (:=) to prune away unnecessary bridges and express some sort of dependency relation between the types:<br><br>In this case, it's not feasible for all these data structures to share the same implementation, but it's still valuable for them to implement a common core API: it ensures consistency of the user-facing functions, allows sharing of documentation, and may even allow these implementations to share a common test suite.<br><br>The full code for our Indexable and Hashtbl examples, including explicit definitions of each of the module types, can be found in the <a data-tooltip-position="top" aria-label="https://github.(com/CraigFe/generalised-signatures)" rel="noopener nofollow" class="external-link" href="https://github.(com/CraigFe/generalised-signatures)" target="_blank"><code></code> repository</a>generalised-signatures. This repository also contains and a <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/generalised-signatures/blob/main/examples/monads.ml" rel="noopener nofollow" class="external-link" href="https://github.com/CraigFe/generalised-signatures/blob/main/examples/monads.ml" target="_blank">third demonstration</a> of this technique being used to express monad-like signatures. The auto-generated documentation for these examples can be <a data-tooltip-position="top" aria-label="https://craigfe.github.io/generalised-signatures/generalised_signatures/Generalised_signatures/index.html" rel="noopener nofollow" class="external-link" href="https://craigfe.github.io/generalised-signatures/generalised_signatures/Generalised_signatures/index.html" target="_blank">viewed online</a>.x<br>Thanks for making it to the end; I hope you picked up something useful. If you think it would help others in your network, I'd appreciate it if you <a data-tooltip-position="top" aria-label="https://twitter.com/share?url=NaN&amp;text=%E2%80%9CGeneralised%20signatures%E2%80%9D%2C%20a%20post%20by%20Craig%20Ferguson.%20&amp;via=_craigfe" rel="noopener nofollow" class="external-link" href="https://twitter.com/share?url=NaN&amp;text=%E2%80%9CGeneralised%20signatures%E2%80%9D%2C%20a%20post%20by%20Craig%20Ferguson.%20&amp;via=_craigfe" target="_blank">shared it</a> with them.<br><br><br>The typeclasses in Haskell's <a data-tooltip-position="top" aria-label="https://hackage.haskell.org/package/base-4.14.0.0/docs/Data-Foldable.html" rel="noopener nofollow" class="external-link" href="https://hackage.haskell.org/package/base-4.14.0.0/docs/Data-Foldable.html" target="_blank">base</a> have the same "polymorphic-instances-only" property as our Indexable1 signature (unsurprising, since it doesn't provide any unboxed container types).<br>class Indexable1 f where        -- Polymorphic instances only
  get    :: f a -&gt; Int -&gt; a
  length :: f a -&gt; Int
<br>A similar trick can be performed there to generalise the typeclass instances for monomorphic containers like Text:<br>{-# LANGUAGE TypeFamilies #-}

type family   Elt container     -- Relate containers to their element type
type instance Elt [a]  = a
type instance Elt Text = Char

class IndexableN c where
  get    :: c -&gt; Int -&gt; Elt c
  length :: c -&gt; Int

instance IndexableN [a] where   -- Polymorphic instance
  get    = (!!)
  length = Prelude.length

instance IndexableN Text where  -- Monomorphic instance
  get    = Text.index
  length = Text.length
<br>As in the OCaml version, we use an Elt type operator to carry the equality needed for the monomorphic case. This time we used type families to specify the relations explicitly, but we could have used multi-parameter type classes for something more akin to the OCaml functor implementation. See the <a data-tooltip-position="top" aria-label="https://hackage.haskell.org/package/mono-traversable" rel="noopener nofollow" class="external-link" href="https://hackage.haskell.org/package/mono-traversable" target="_blank">mono-traversable</a> package for more of this sort of trickery in Haskell.<br><br>
<br>This is the approach taken by Jane Street's <a data-tooltip-position="top" aria-label="https://github.com/janestreet/base" rel="noopener nofollow" class="external-link" href="https://github.com/janestreet/base" target="_blank">base</a>, and is very similar to the Haskell notion of building standard libraries from type-class instances.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fnref-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fnref-1" target="_blank">↩</a>
<br>This example uses the (** @inline *) tag to ensure that Odoc doesn't leak that Indexable1' is implemented in terms of IndexableN.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fnref-2" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fnref-2" target="_blank">↩</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/generalised-signature.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Generalised signature.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:48:12 GMT</pubDate><enclosure url="https://www.craigfe.io/posts/generalised-signatures/dag-indexable.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.craigfe.io/posts/generalised-signatures/dag-indexable.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Interfacing OCaml and PostgreSQL with Caqti]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:database" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#database</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:postgresql" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#postgresql</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:database" class="tag" target="_blank" rel="noopener nofollow">#database</a> <a href="https://muqiuhan.github.io/wiki?query=tag:postgresql" class="tag" target="_blank" rel="noopener nofollow">#postgresql</a><br>
On dealing with dependencies in your Dune-powered OCaml app and interfacing with the most popular DBMS in town.
<br>This article is part of Hands-on OCaml, a series of articles that I’m working on that is focusing on doing web app development with OCaml.<br>The project we will be building throughout the series is a To-Do List app, which connects to a PostgreSQL database as its datastore. In the previous article, we have covered initializing and bootstrapping our project with Dune; if you haven’t seen it, check out the link below:<br>This tutorial will build upon the foundation we have laid out in the previous article in the series. While you can of course follow along without actually doing the tutorial, it is recommended to give the article a read first to be sure that we have the required knowledge in place.<br>In this second article of Hands-on OCaml series, we will explore how to manage dependencies in OCaml project; in particular, we will bring in and use a third-party library to deal with DB operations.<br><br>Assuming you followed the previous article, you should have both opam and jbuilder installed. Verify their installation as follows:<br>$ opam --version  
1.2.2
$ jbuilder --version  
1.0+beta20
<br>We would also need to have a local PostgreSQL instance up and running. I’m assuming readers have had prior experience with PostgreSQL, so I’m not going to expand about it here, but you should be able to install it via your OS package manager (e.g. apt on Ubuntu or brew on MacOS) or via a Docker container. Verify that it is running and and you can connect to it via psql. At the time of this writing, I am using locally installed PostgreSQL 10.4.<br><br>In this section we’ll see how we will prepare our project. Again, this tutorial assumes that you have done the initial <a data-tooltip-position="top" aria-label="https://medium.com/@bobbypriambodo/starting-an-ocaml-app-project-using-dune-d4f74e291de8" rel="noopener nofollow" class="external-link" href="https://medium.com/@bobbypriambodo/starting-an-ocaml-app-project-using-dune-d4f74e291de8" target="_blank">Dune setup tutorial</a>. If you haven’t, do check it out. You might also want to remove the previously created .mli and .ml files from bin and lib since we won’t need them anymore.<br><br>First off, we will fetch some dependencies! Unlike the previous tutorial where we install packages directly, we are going to use a different and cleaner method of installing dependencies: through opam files.<br>Make sure you’re in the todolist directory, create a file named todolist.opam with the following contents:<br>opam-version: "1.2"
name: "todolist"
version: "1.0.0"
maintainer: "Your Name &lt;email@example.com&gt;"

depends: [
  "jbuilder" {build}
  "lwt"
  "lwt_ppx"
  "caqti"
  "caqti-lwt"
  "caqti-driver-postgresql"
]
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/interfacing-ocaml-and-postgresql-with-caqti.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Interfacing OCaml and PostgreSQL with Caqti.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:48:10 GMT</pubDate></item><item><title><![CDATA[OCaml Lwt or Async]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:multi-thread" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#multi-thread</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:multi-thread" class="tag" target="_blank" rel="noopener nofollow">#multi-thread</a> <br><br>
<br>社区偏好: OCaml 社区更倾向于使用 Lwt。
<br>错误处理差异: Lwt的 Lwt.t 允许异常阻止异步值的计算，而 Async 则提供了两种处理错误的方式：模仿 Lwt 的方式 或 构建监控层次结构。
<br>绑定行为: Lwt 在绑定时如果异步值已确定，则会立即执行，而 Async 则保证代码在绑定之间不会被中断，以便于推理竞态条件。
<br>技术偏好: 从技术角度更倾向于 Async，但社区的偏好和 Lwt 的可移植性使其成为更实际的选择。
<br>支持两种库的挑战: 尝试同时支持 Async 和 Lwt 会增加实现复杂性，限制可用库，并增加构建系统和测试的复杂性。
<br>Lwt 的优势: Lwt在多个平台上的支持更广泛，包括 js_of_ocaml、mirage 和 Windows，且拥有更活跃的社区和更好的文档。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-lwt-or-async.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml Lwt or Async.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:07 GMT</pubDate></item><item><title><![CDATA[OCaml Preprocessors (PPX) 概述]]></title><description><![CDATA[ 
 <br><br>OCaml 的预处理器是指在编译时调用的程序，用于在实际编译之前修改程序。它们在处理文件包含、条件编译、样板生成或扩展语言的时候很有用。<br>一个例子:<br>Printf.printf "This program has been compiled by user: %s" [%get_env "USER"]
<br>当使用预处理器编译代码时，它会将 [%get_env "USER"] 替换为 USER 环境变量的内容。例如，如果 USER 环境变量设置为 "JohnDoe"，则该行将变为：<br>Printf.printf "This program has been compiled by user: %s" "JohnDoe"
<br>通过这种修改，预处理器在编译时将[%get_env "USER"]替换为USER环境变量的内容，这段代码在大多数系统上应该可以正常工作，而无需额外配置。编译时，预处理器会将[%get_env&nbsp;"USER"]替换为包含编译程序的用户的用户名的字符串。由于这是在编译时发生的，因此在运行时，USER变量的值不会产生影响，因为它仅用于编译程序中的信息目的。<br><br>某些语言内置支持预处理，意味着这类语言有一部分可以在编译时执行。例如，C语言的预处理器语法和语义是语言的一部分；Rust则有其宏系统。<br>OCaml 没有宏系统，所有预处理器都是独立的程序。尽管它不是语言的一部分，但 OCaml 平台官方支持用于编写此类预处理器的库。<br>具体来说，OCaml 支持两种类型的预处理器：一种在源级别工作（如C），另一种在 AST 上工作。后者称为"PPX"，即预处理器扩展（Pre-Processor eXtension）的缩写。<br>虽然这两种预处理都有其使用场景，但在 OCaml 中，建议尽可能使用 PPX，原因有几个：<br>
<br>与 Merlin 和 Dune 集成得很好，不会干扰编辑器中的错误报告和 Merlin 的“跳转到定义”功能。
<br>性能好，易于组合。
<br>与 OCaml 特性相辅相成。
<br><br>
<br><a data-tooltip-position="top" aria-label="https://ocaml.org/docs/metaprogramming#metaprogramming" rel="noopener nofollow" class="external-link" href="https://ocaml.org/docs/metaprogramming#metaprogramming" target="_blank">Preprocessors and PPXs</a>
<br><a data-tooltip-position="top" aria-label="https://www.victor.darvariu.me/jekyll/update/2018/06/19/ppx-tutorial.html" rel="noopener nofollow" class="external-link" href="https://www.victor.darvariu.me/jekyll/update/2018/06/19/ppx-tutorial.html" target="_blank">A Tutorial to OCaml -ppx Language Extensions</a>
<br><a data-tooltip-position="top" aria-label="https://ocaml-ppx.github.io/ppxlib/ppxlib/quick_intro.html" rel="noopener nofollow" class="external-link" href="https://ocaml-ppx.github.io/ppxlib/ppxlib/quick_intro.html" target="_blank">ppxlib: quick_intro</a>
<br><a data-tooltip-position="top" aria-label="https://tarides.com/blog/2019-05-09-an-introduction-to-ocaml-ppx-ecosystem/" rel="noopener nofollow" class="external-link" href="https://tarides.com/blog/2019-05-09-an-introduction-to-ocaml-ppx-ecosystem/" target="_blank">An introduction to OCaml ppx ecosystem</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-preprocessors-(ppx)-概述.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml Preprocessors (PPX) 概述.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:45:39 GMT</pubDate></item><item><title><![CDATA[Off to the Races Using ThreadSanitizer in OCaml]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>OCaml Multicore opened up a new world of performance for developers, something that <a data-tooltip-position="top" aria-label="https://tarides.com/blog/2022-12-20-how-nomadic-labs-used-multicore-processing-to-create-a-faster-blockchain/" rel="noopener nofollow" class="external-link" href="https://tarides.com/blog/2022-12-20-how-nomadic-labs-used-multicore-processing-to-create-a-faster-blockchain/" target="_blank">Nomadic Labs has tested with great results.</a> Rather than relying on one core to do everything, the program can take advantage of multiple cores simultaneously for a significant performance boost.<br>With new programming possibilities come new classes of bugs, which require updated detection methods. One of these types of bugs is called a data race. A data race is a race condition that occurs when two accesses are made to the same memory location, at least one is a write, and no order is enforced between them.<br>Data races can be dangerous as they are easy to miss and capable of yielding unexpected results. Consequently, integrating a tool to detect data races has been a high priority for the teams working on OCaml 5.0 with Multicore support. Whilst data races in OCaml are less problematic than in many other languages (for example, data races in OCaml do not cause crashes and do not constitute undefined behaviour), developers still want to be made aware of possible data races so that they can remove them from their programs. More about this below.<br><br><br><a data-tooltip-position="top" aria-label="https://clang.llvm.org/docs/ThreadSanitizer.html" rel="noopener nofollow" class="external-link" href="https://clang.llvm.org/docs/ThreadSanitizer.html" target="_blank">ThreadSanitizer</a>, or TSan, is an open-source tool that reliably detects data races at runtime. It consists of instrumenting programs with calls to a dedicated runtime that performs the detection.<br>Support for TSan will officially be part of the OCaml 5.2 release, and there is already a backport for OCaml 5.1.<br>This blog post will demonstrate the benefits of using TSan, offer insight into how TSan works, and outline the challenges of integrating it with OCaml. For a more practically oriented guide on how to use TSan in your own projects, the <a data-tooltip-position="top" aria-label="https://ocaml.org/docs/multicore-transition" rel="noopener nofollow" class="external-link" href="https://ocaml.org/docs/multicore-transition" target="_blank">tutorial on using TSan with OCaml Multicore</a> is a great place to start.<br>We will begin by examining what a data race looks like, both before and after using TSan.<br><br><br>Let us consider how a data race might occur. Say an OCaml programmer writes code to populate a table of clients from several sources. They decide to make it Multicore to improve performance by using two Domains for two data sources:<br>let clients = Hashtbl.create 16
let free_id = Atomic.make 0

let clients1 = (* Some data source *)

let clients2 = (* Some data source *)

let record_clients =
  Seq.iter
    (fun c -&gt; Hashtbl.add clients (Atomic.fetch_and_add free_id 1) c)

let () =
  let d = Domain.spawn (fun () -&gt; record_clients clients1) in
  record_clients clients2;
  Domain.join d
<br>As we can tell, each incoming client is bound to a unique ID. The programmer correctly used the Atomic module for ID generation, ensuring the IDs are truly unique. However, they have failed to use a domain-safe module designed for concurrency, instead opting for Hashtbl. Unfortunately, this module is unsafe for concurrent use: using Hashtbl.t in parallel can cause data races and lead to surprising results.<br>For example, when two domains add elements in parallel it may cause some elements to be silently dropped. To make matters worse, the resulting bugs would be non-deterministic and as such be hard to detect and track down. Furthermore, if the programmer's project depends on libraries that use Hashtbl, it would make them unsafe to use in parallel without it necessarily being clear from their documentation.<br>If, however, the programmer were to build their program on a special opam switch with a TSan-enabled compiler like this:<br>$ opam switch create 5.1.0+tsan
$ opam install dune
$ dune exec ./clients.exe
<br>(Side note: the 5.1.0+tsan switch is the most convenient way to use TSan with OCaml at the time of writing. Once OCaml 5.2 is released, the blessed command will be opam switch create &lt;switch name&gt; ocaml-option-tsan.)<br>All memory accesses would be instrumented with calls to the TSan runtime, and TSan would detect the data race condition and output a data race report:<br>==================
WARNING: ThreadSanitizer: data race (pid=790576)
  Write of size 8 at 0x7f42b37f57e0 by main thread (mutexes: write M86):
    #0 caml_modify runtime/memory.c:166 (clients.exe+0x58b87d)
    #1 camlStdlib__Hashtbl.resize_749 stdlib/hashtbl.ml:152 (clients.exe+0x536766)
    #2 camlStdlib__Seq.iter_329 stdlib/seq.ml:76 (clients.exe+0x4c8a87)
    #3 camlDune__exe__Clients.entry /workspace_root/clients.ml:9 (clients.exe+0x4650ef)
    #4 caml_program &lt;null&gt; (clients.exe+0x45fefe)
    #5 caml_start_program &lt;null&gt; (clients.exe+0x5a0ae7)

  Previous read of size 8 at 0x7f42b37f57e0 by thread T1 (mutexes: write M90):
    #0 camlStdlib__Hashtbl.key_index_1308 stdlib/hashtbl.ml:507 (clients.exe+0x53a625)
    #1 camlStdlib__Hashtbl.add_1312 stdlib/hashtbl.ml:511 (clients.exe+0x53a6f8)
    #2 camlStdlib__Seq.iter_329 stdlib/seq.ml:76 (clients.exe+0x4c8a87)
    #3 camlStdlib__Domain.body_703 stdlib/domain.ml:202 (clients.exe+0x50bf60)
    #4 caml_start_program &lt;null&gt; (clients.exe+0x5a0ae7)
    #5 caml_callback_exn runtime/callback.c:197 (clients.exe+0x56917b)
    #6 caml_callback runtime/callback.c:293 (clients.exe+0x569cb0)
    #7 domain_thread_func runtime/domain.c:1100 (clients.exe+0x56d37f)
    [...]

SUMMARY: ThreadSanitizer: data race runtime/memory.c:166 in caml_modify
==================
[...]
ThreadSanitizer: reported 2 warnings
<br>Above is a truncated view of what the TSan report, warning of a data race, looks like in this case. TSan has detected two memory accesses, a write and a read, made to one memory location. As they are also unordered, this constitutes a data race and TSan reports it along with the backtraces of both accesses.<br>In this case it would be evident that something has gone wrong with Hashtbl.add – a big hint to the programmer.<br><br><br>Now that we know what TSan is used for, it's time to explore how it works. Compiling a program with TSan enabled causes the executable to be instrumented with calls to the TSan runtime library. The runtime library tracks memory accesses and ordering relations between these accesses.<br>Internally, the TSan runtime assigns a vector clock to each OCaml domain or system thread. Each thread holds a vector clock – a vector clock being an array of n integers, where n is the number of threads – and increments its clock upon each event (memory access, mutex operation, etc.). Certain operations like mutex locks, atomic reads, and so on, will synchronise clocks between threads.<br><a data-tooltip-position="top" aria-label="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/798d4/vector-clocks.png" rel="noopener nofollow" class="external-link" href="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/798d4/vector-clocks.png" target="_blank"></a><img alt="A mutex lock synchronising the clock between two threads." src="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/c5bb3/vector-clocks.png" referrerpolicy="no-referrer"><br>Comparing vector clocks allows TSan to establish an order between events, so-called <a data-tooltip-position="top" aria-label="https://jameshfisher.com/2017/02/10/happened-before/" rel="noopener nofollow" class="external-link" href="https://jameshfisher.com/2017/02/10/happened-before/" target="_blank">happens-before relations.</a> TSan reports a data race every time two memory accesses are made to overlapping memory regions, if:<br>
<br>At least one of them is a write, and
<br>There is no established happens-before relation between them.
<br><br><br>Let us look at this process in more detail. Each word of application memory is associated with one or more 'shadow words'. Each shadow word contains information about a recent memory access to that word. This information points to the vector clock's state at the moment the access was performed.<br><a data-tooltip-position="top" aria-label="https://tarides.com/static/805211c514eae2a5f8a8410fd26f476a/d125e/shadow-state.png" rel="noopener nofollow" class="external-link" href="https://tarides.com/static/805211c514eae2a5f8a8410fd26f476a/d125e/shadow-state.png" target="_blank"></a><img alt="A box labelled application with an arrow to a box labeled shadow state." src="https://tarides.com/static/805211c514eae2a5f8a8410fd26f476a/c5bb3/shadow-state.png" referrerpolicy="no-referrer"><br>This information (called the 'shadow state') is updated at every instrumented memory access: TSan compares the accessor's clock with each existing shadow word, and checks the following:<br>
<br>Do the accesses overlap?
<br>Is one of them a write?
<br>Are the thread IDs different?
<br>Are they unordered by happens-before?
<br>If these conditions are met, TSan detects and reports a data race.<br>In addition to memory access, operations like Domain.spawn and Domain.join (as well as mutex operations) are relevant for operation ordering. As such, TSan also instruments these operations.<br><br><br>The core of TSan support is instrumentation of memory acceses with calls to the TSan runtime. The OCaml compiler performs this instrumentation in a dedicated pass.<br><br><br>For TSan to show a backtrace of past events, function entries and exits must also be instrumented. This is done as part of the instrumentation pass.<br>However, in OCaml, a function can also be exited by an <a data-tooltip-position="top" aria-label="https://github.com/fabbing/obts_exn" rel="noopener nofollow" class="external-link" href="https://github.com/fabbing/obts_exn" target="_blank">exception</a>, bypassing part of the instrumentation. When that happens, for TSan’s view of the backtrace to remain up-to-date, the OCaml runtime informs TSan about every exited function.<br><br><br><a data-tooltip-position="top" aria-label="https://v2.ocaml.org/manual/effects.html" rel="noopener nofollow" class="external-link" href="https://v2.ocaml.org/manual/effects.html" target="_blank">Effect handlers</a> are a generalisation of exception handlers. Performing an effect results in a jump to the associated effect handler, and then a delimited continuation makes it possible to resume the computation. In the same way as with exceptions, the OCaml runtime must signal to TSan which functions are exited when an effect is performed and re-entered when a continuation is resumed.<br><br><br>Each language specifies how memory behaves in parallel programs using what is known as a memory model. Incidentally, what counts as a data race in a given language also depends on its memory model.<br>TSan can detect data races in programs that follow the C memory model. OCaml 5's memory model is different from the C model, however, and it offers more guarantees: data races in C and C++ cause undefined behaviour (i.e., anything can happen), which is not the case in OCaml. OCaml's semantics are “fully defined” (see the <a data-tooltip-position="top" aria-label="https://v2.ocaml.org/manual/memorymodel.html" rel="noopener nofollow" class="external-link" href="https://v2.ocaml.org/manual/memorymodel.html" target="_blank">manual page</a> about the memory model). In particular, a program with data races in OCaml will not crash, unlike in C++. In addition, there can be no <a data-tooltip-position="top" aria-label="https://www.hboehm.info/c++mm/thin_air.html" rel="noopener nofollow" class="external-link" href="https://www.hboehm.info/c++mm/thin_air.html" target="_blank">out-of-thin-air values</a>: the only values that can be observed are values that are previously written to that location. The OCaml memory model guarantees that even for programs with data races, memory safety is preserved.<br>Data races in OCaml can still result in unexpected surprises for the OCaml programmer. A multi-threaded execution may produce behaviours that cannot be explained by the mere interleaving of actions from different threads. The only way such behaviours can be explained is through a reordering of actions in the same thread. Such reasoning is quite unintuitive for a programmer who will be more used to thinking about program behaviour as being an interleaving of actions from different threads.<br>However, if the program is data-race free, then the observed behaviour can be explained by a simple interleaving of operations from different threads (a property known as sequential consistency). Eliminating data races reduces non-determinism in the program and hence it is beneficial to remove data races whenever possible. Note that we do not completely eliminate non-determinism from a parallel program.<br>In essence, because of the differences between the C and OCaml memory models, in order for TSan to detect data races in OCaml the instrumentation of memory accesses must conceptually map OCaml programs to C programs. During development, the team took care to ensure that this mapping preserved the detection of data races (in the OCaml sense) and did not introduce false positives.<br>You can find more details about the inner workings of TSan and its OCaml support in this <a data-tooltip-position="top" aria-label="https://github.com/fabbing/ocaml_tsan_icfp/blob/master/presentation/presentation.pdf" rel="noopener nofollow" class="external-link" href="https://github.com/fabbing/ocaml_tsan_icfp/blob/master/presentation/presentation.pdf" target="_blank">OCaml Workshop 2023 talk</a>.<br><br><br>In terms of the cost of running TSan, currently, it affects memory and performance in the following ways:<br>
<br>Performance cost: about a 2-7x slowdown (compared to 5-15x for C/C++)
<br>Memory consumption: increased by about 4-7x (compared to 5-10x for C++)
<br>As with all tools, TSan has some limitations. These are due to how TSan is built and are unlikely to change. With TSan, data races are only detected on visited code paths. In addition, TSan only remembers a finite amount of memory accesses for space-saving reasons, which can in principle cause TSan to miss some races. TSan also does not currently support Windows.<br>TSan support for OCaml is currently only implemented for x86-64 Linux and macOS, but will hopefully be extended to include more architectures such as arm64.<br><br><br>Knowing the limitations, let us explore TSan's use cases. So far, TSan has helped by unearthing data races in several OCaml libraries:<br>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/saturn" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/saturn" target="_blank">Saturn (formerly known as Lockfree):</a> TSan <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/saturn/issues/39" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/saturn/issues/39" target="_blank">found a benign data race</a>, as well as a data race occuring from the use of <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/saturn/pull/40" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/saturn/pull/40" target="_blank">semaphores</a>.
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/domainslib" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/domainslib" target="_blank">Domainslib:</a> TSan found benign data races in Chan, not just <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/domainslib/issues/72" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/domainslib/issues/72" target="_blank">once</a> but <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/domainslib/pull/103" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/domainslib/pull/103" target="_blank">twice</a>.
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml" target="_blank">The OCaml runtime system</a> itself: TSan warned about a <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/11040" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/issues/11040" target="_blank">number of race conditions</a> in the OCaml runtime.
<br>In addition, TSan has been a great help in transitioning the effects-based I/O library <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/eio" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/eio" target="_blank">Eio</a> and the distributed database <a data-tooltip-position="top" aria-label="https://github.com/mirage/irmin" rel="noopener nofollow" class="external-link" href="https://github.com/mirage/irmin" target="_blank">Irmin</a> to Multicore. It allowed teams to detect potential data races and fix them as required.<br><br><br>We want to hear from you – are you using TSan for your OCaml projects? Please get in touch and let us know about your experience, whether you have encountered any problems, and if you have any suggestions for how it could be improved.<br>You can share your thoughts on the <a data-tooltip-position="top" aria-label="https://discuss.ocaml.org" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org" target="_blank">OCaml Discuss Forum</a> or contact Tarides directly <a data-tooltip-position="top" aria-label="https://tarides.com/contact/" rel="noopener nofollow" class="external-link" href="https://tarides.com/contact/" target="_blank">on our website</a>. Don't forget to check out the <a data-tooltip-position="top" aria-label="https://ocaml.org/docs/multicore-transition" rel="noopener nofollow" class="external-link" href="https://ocaml.org/docs/multicore-transition" target="_blank">TSan tutorial</a> as well. Happy hacking!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/off-to-the-races-using-threadsanitizer-in-ocaml.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Off to the Races Using ThreadSanitizer in OCaml.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:48:10 GMT</pubDate><enclosure url="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/c5bb3/vector-clocks.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/c5bb3/vector-clocks.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Pitfalls of polymorphic ignore]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br>In OCaml, we can ignore the return value of a side-effecting computation by either naming it _ or using Stdlib.ignore. These are commonly used to test functions:<br>let test_eval =
  (* we only care that [eval] doesn't raise an exception *)
  let _ = eval (`Int 5) in
  ignore (eval (`Add (`Int 1, `Int 2)))
<br>There's a bug waiting to happen here. Suppose we later refactor eval to take another variable:<br>val eval : expr -&gt; context -&gt; expr
<br>Suddenly, the test is ignoring partially-applied functions of type context -&gt; expr: it silently became useless! For this reason, one often sees code that explicitly asserts the type of the value being ignored:<br>let ignore_expr (_ : expr) = ()

let test_eval =
  (* require the ignored value to have type [expr] *)
  let (_ : expr) = eval (`Int 5) in
  ignore_expr (eval (`Add (`Int 1, `Int 2)))
<br>The type-checker now catches our partial application bug:<br>File "test_eval.ml", line 9, characters 19-25:
9 |   let (_ : expr) = eval (`Int 5) in
                       ^^^^^^^^^^^^^
Error: This expression has type context -&gt; expr
       but an expression was expected of type expr
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/pitfalls-of-polymorphic-ignore.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Pitfalls of polymorphic ignore.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:48:10 GMT</pubDate></item><item><title><![CDATA[Polymorphic type constraints]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br>
In this post, I explain a common mistake when writing constraints of polymorphic functions in OCaml programs, then show how to correct it.
<br><br>One of the earliest lessons of any functional programming tutorial is how to write polymorphic functions and their signatures:<br>val id  : 'a -&gt; 'a
val fst : ('a * 'b) -&gt; 'a
val map : ('a -&gt; 'b) -&gt; 'a list -&gt; 'b list
<br>A typical explanation of these type signatures goes along the lines of:<br>
Types of the form 'a, 'b, ..., known as type variables, stand for an unknown type. They allow us to describe functions that work uniformly over many possible input types. This is known as "parametric polymorphism".
— Hypothetical education resource<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-1" target="_blank">1</a>
<br>As is often the case with introductory explanations, this is just specific enough to be technically correct without introducing too many new concepts, letting us hurry on to demonstrating useful examples before the student gets bored. Unfortunately, we've laid a trap: when our reader learns about type constraints, they naturally try to combine these two "intuitive" features and get bitten:<br>ᐅ let id1 : 'a -&gt; 'a = (fun x -&gt; x) ;;        (* Accepted. So far so good... *)
  val id1 : 'a -&gt; 'a = &lt;fun&gt;

ᐅ let id2 : 'a -&gt; 'a = (fun x -&gt; x + 1) ;;    (* Also accepted. Uh oh... *)
  val id2 : int -&gt; int = &lt;fun&gt;
<br>In this case, the student finds that 'a -&gt; 'a is a valid constraint for a function of type int -&gt; int, and their mental model is broken almost immediately. It's quite natural to expect id2 to be rejected as a non-polymorphic function, particularly given our vague explanation of what 'a actually means.<br>Our hypothetical student's mistake stems from the fact that type variables in signatures are implicitly universally-quantified – that is, they stand for all types – whereas type variables in constraints are not. To understand what this means, let's try to pin down a more precise idea of what type variables are. If you're already &nbsp;indoctrinated&nbsp; comfortable with type variables, you may wish to <a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#true-polymorphic-constraints" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#true-polymorphic-constraints" target="_blank">cut to the chase</a>.<br><br>Type variables in constraints are referred to as being "unbound" (or "free"), meaning that they stand for some type that is not yet known to the type-checker: they are placeholders that can later be filled by a particular type. Without going into <a data-tooltip-position="top" aria-label="http://dev.stephendiehl.com/fun/006_hindley_milner.html" rel="noopener nofollow" class="external-link" href="http://dev.stephendiehl.com/fun/006_hindley_milner.html" target="_blank">the details</a>, these placeholders are gradually determined as the type-checker resolves constraints. For instance, in our id2 example, the type-checker decides that 'a equals int by first reconciling the user-supplied constraint 'a -&gt; 'a with the constraint int -&gt; int that it inferred from the implementation.<br>To a theorist (or type-system developer), who regularly has to worry about types that are not yet fully known, the notion of a "placeholder" is a sensible default meaning of an unbound type variable. Such people also tend to use explicit syntax to disambiguate the alternative case, type variables that are bound:<br>
∀ a. a -&gt; a (read as: "For all a, a -&gt; a")
<br>We call "∀ a" a universal quantifier because it introduces a variable a, bound inside the quantifier, that can stand for any type in the universe of OCaml types. It's this flavour of type variable that enables parametric polymorphism and – although the OCaml syntax often tries to hide it from you – these quantifiers exist everywhere in your programs. As I already mentioned, all unbound variables in signatures are implicitly quantified in this way:<br>
val length : 'a list -&gt; int
... secretly means ...
val length : ∀ a. a list -&gt; int
<br>On the implementation side of length, the compiler will check to see if there are any placeholder variables left after type-checking the definition and wrap them in universal quantifiers (if it's sure that it's safe to do so<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-2" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-2" target="_blank">2</a>). When this happens, we say that those type variables have been generalised. Once length has been given its polymorphic type, the user gets to pick a specific type a at each call-site by passing it a list of any element type they want. This idea of choosing the instantiation of a at each call-site is what is "parametric" about "parametric polymorphism".<br>Taking a step back, we can now see what went wrong with our hypothetical introduction to type variables above: it led our student to think of all type variables as being implicitly universally-quantified, when this is not true in constraints<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-3" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-3" target="_blank">3</a>. So, given that we can't rely on implicit generalisation in constraints, what can we do to declare that our code is polymorphic within the implementation itself?<br><br>The punchline is that OCaml actually does have syntax for expressing polymorphic constraints – and it even involves an explicit quantifier – but sadly it's not often taught to beginners:<br>let id : 'a. 'a -&gt; 'a = (fun x -&gt; x + 1)
<br>The syntax 'a. 'a -&gt; 'a denotes an <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/types.html#poly-typexpr" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/types.html#poly-typexpr" target="_blank">explicitly-polymorphic type</a>, where 'a. corresponds directly with the ∀ a. quantifier we've been using so far. Applying it here gives us a satisfyingly readable error message:<br>Error: This definition has type int -&gt; int which is less general than
         'a. 'a -&gt; 'a
<br>The caveat of polymorphic constraints is that we can only apply them directly to let-bindings, not to function bodies or other forms of expression:<br>let panic : 'a. unit -&gt; 'a = (fun () -&gt; raise Crisis)  (* Works fine... *)

let panic () : 'a. 'a = raise Crisis                   (* Uh oh... *)
(*               ^
 *  Error: Syntax error  *)
<br>This somewhat unhelpful error message arises because OCaml will never infer a polymorphic type for a value that is not let-bound. Trying to make your type inference algorithm cleverer than this quickly runs into certain <a data-tooltip-position="top" aria-label="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf" target="_blank">undecidable problems</a>; the parser knows that the type-checker is afraid of undecidable problems, and so rejects the program straight away<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-4" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-4" target="_blank">4</a>.<br>In spite of their limitations, explicitly-polymorphic type constraints are a great way to express polymorphic intent in your OCaml programs, either as internal documentation or to have more productive conversations with the type-checker when debugging. I recommend using them frequently and teaching them to beginners as soon as possible.<br>At this point, if you suffered through my explanation of type variables in the previous section, you may be thinking the following:<br>
"If introducing type variables properly requires so many paragraphs of jargon, we shouldn't burden beginners with the details right away."
— Straw-man argument
<br>Personally, I find that introducing these terms early on in the learning process is easily worthwhile in avoiding early roadblocks, but that discussion can wait for another time. In the spirit of functional programming for the masses, let's summarise with a less jargon-heavy attempt at redrafting our hypothetical education resource:<br>
Types of the form 'a, 'b, ..., known as type variables, are placeholders for an undetermined type. When bound by for-all quantifiers (of the form 'a.), they can be used to describe values that can take on many possible types. For instance, we can write the type of (fun x -&gt; x) as 'a. 'a -&gt; 'a, meaning:

"For any type 'a, this function can take on type 'a -&gt; 'a."

The OCaml type-checker will infer polymorphic types wherever it is safe, but we can also explicitly specify a polymorphic type for a let-binding:
ᐅ let fst : 'a 'b. ('a * 'b) -&gt; 'a = (* "For all ['a] and ['b], ..." *)
    fun (x, _) -&gt; x ;;

val fst : 'a * 'b -&gt; 'a = &lt;fun&gt;

Note that all type variables in signatures are implicitly universally-quantified: it's not necessary (or even possible) to write 'a 'b. before the type.
<br>The explanation is undeniably still longer and more technical than the one we started with, but crucially it uses the extra space to give the reader a clue as to how to debug their polymorphic functions.<br>The story doesn't end here. We haven't discussed <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/gadts.html" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/gadts.html" target="_blank">existential quantifiers</a>, the other type of type variable binding; or <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s:polymorphic-recursion" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s:polymorphic-recursion" target="_blank">polymorphic recursion</a>, where polymorphic annotations become compulsory; or <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/locallyabstract.html" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/locallyabstract.html" target="_blank">locally-abstract types</a>, which offer other useful syntaxes for constraining your OCaml programs to be polymorphic. These will all have to wait for future posts. For now, thanks for reading!<br><br>
<br>
Very similar equivalents of this explanation exist in <a data-tooltip-position="top" aria-label="http://dev.realworldocaml.org/guided-tour.html" rel="noopener nofollow" class="external-link" href="http://dev.realworldocaml.org/guided-tour.html" target="_blank">Real World OCaml</a>, <a data-tooltip-position="top" aria-label="https://www.cs.cornell.edu/courses/cs3110/2020sp/textbook/" rel="noopener nofollow" class="external-link" href="https://www.cs.cornell.edu/courses/cs3110/2020sp/textbook/" target="_blank">Cornell's OCaml course</a>, and <a data-tooltip-position="top" aria-label="https://www.cl.cam.ac.uk/teaching/1920/FoundsCS/focs-201920-v1.1.pdf" rel="noopener nofollow" class="external-link" href="https://www.cl.cam.ac.uk/teaching/1920/FoundsCS/focs-201920-v1.1.pdf" target="_blank">Cambridge's OCaml course</a>. Type variables are variously described as representing "any type", "an unknown type" or "a generic type"; explanations that are all as different as they are vague.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-1" target="_blank">↩</a>

<br>
The most famous example of a type variable that is unsafe to generalise is one that has been captured in mutable state:
ᐅ let state = ref [] ;;
ᐅ let sneaky_id x = (state := x :: !state); x ;;

val sneaky_id : '_weak1 -&gt; '_weak1 = &lt;fun&gt;

In this case, it's not possible to give sneaky_id the type ∀ a. a -&gt; a because different choices of the type a are not independent: passing a string to sneaky_id, followed by an integer, would build a list containing both strings and integers, violating type safety. Instead, sneaky_id is given a type containing a "<a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:weak-types" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:weak-types" target="_blank">weak type variable</a>" which represents a single, unknown type. This meaning of type variables should be familiar to you; it's exactly the same as the "unbound" type variables we've been discussing!
In general, it's not easy to decide if it's safe to generalise a particular type variable. OCaml makes a quick under-approximation called the <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:valuerestriction" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:valuerestriction" target="_blank">(relaxed) value restriction</a>.
<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-2" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-2" target="_blank">↩</a>

<br>
As an aside, there's no profound reason why constraints must behave differently with respect to implicit quantification. Both SML and Haskell choose to generalise variables in constraints:
val id1 : 'a -&gt; 'a = (fn x =&gt; x + 1);
(*  Error: pattern and expression in val dec do not agree

pattern:    'a -&gt; 'a
expression: 'Z[INT] -&gt; 'Z[INT] *)


(Note: val f : t = e in SML is analogous to let f : t = e in OCaml.)

I suspect that constraints having the same quantification behaviour as signatures is more intuitive, at least for simple examples. In complex cases, the exact point at which type variables are implicitly quantified can be surprising, and so SML '97 provides an explicit quantification syntax for taking control of this behaviour. See <a data-tooltip-position="top" aria-label="https://www.smlnj.org/doc/Conversion/types.html#Explicit" rel="noopener nofollow" class="external-link" href="https://www.smlnj.org/doc/Conversion/types.html#Explicit" target="_blank">the SML/NJ guide (§ 1.1.3)</a> for much more detail.
The advantage of OCaml's approach is that it enables constraining subcomponents of types without needing to specify the entire thing (as in (1, x) : (int * _)), which can be useful when quickly constraining types as a sanity check or for code clarity. As far as I'm aware, SML has no equivalent feature.
<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-3" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-3" target="_blank">↩</a>

<br>
This limitation of the type-checker can artificially limit the polymorphism that can be extracted from your programs. If you want to take polymorphism to its limits – as God intended – it's sometimes necessary to exploit another point where explicitly-polymorphic types can appear: <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s%3Ahigher-rank-poly" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s%3Ahigher-rank-poly" target="_blank">record and object fields</a>.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-4" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-4" target="_blank">↩</a>

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/polymorphic-type-constraints.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Polymorphic type constraints.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:48:10 GMT</pubDate></item><item><title><![CDATA[Polymorphic Variants]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br><br>This tutorial teaches you how to use polymorphic variants. This includes starting to use them, maintaining a project already using them, deciding when to use them or not, and balancing their unique benefits against their drawbacks.<br>Product types and data types such as option and list are variants and polymorphic. In this tutorial, they are called simple variants to distinguish them from the polymorphic variants presented here. Simple variants and polymorphic variants are close siblings. Their values are both introduced using labels that may carry data. Both can be recursive and have type parameters. By the way, don't trust a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Large_language_model" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">LLM</a> chatbot if it tells you polymorphic variants are dynamically typed; it is a hallucination. Like simple variants, polymorphic variants are type-checked statically.<br>However, they are type-checked using different algorithms, which results in a different programming experience. The relationship between value and type (written with the colon symbol :) is changed with polymorphic variants. Usually, values are thought of as inhabitants of the type, which is regarded as a set-like thing. Rather, polymorphic variant values should be considered as pieces of data that several functions can accept. Polymorphic variants types are a way to express compatibility relationships between those functions. The approach in this tutorial is to build sense from experience using features of polymorphic variants.<br>Prerequisites: This is an intermediate-level tutorial. It is required to have completed tutorials on <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/functions-and-values" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/functions-and-values" target="_blank">Functions and Values</a>, <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/basic-data-types" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/basic-data-types" target="_blank">Basic Data Types</a>, and <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/lists" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/lists" target="_blank">Lists</a> to begin this one.<br><br>Polymorphic variants originate from Jacques Garrigue's work on Objective Label, which was <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/old_caml_site/caml-list-ar/0533.html" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/pub/old_caml_site/caml-list-ar/0533.html" target="_blank">first published in 1996</a>. It became part of standard Objective Caml with <a data-tooltip-position="top" aria-label="https://caml.inria.fr/distrib/ocaml-3.00/" rel="noopener nofollow" class="external-link" href="https://caml.inria.fr/distrib/ocaml-3.00/" target="_blank">release 3.0</a> in 2000, along with labelled and optional function arguments. They were introduced to give more precise types in <a data-tooltip-position="top" aria-label="https://garrigue.github.io/labltk/" rel="noopener nofollow" class="external-link" href="https://garrigue.github.io/labltk/" target="_blank">LablTk</a>.<br>The core type system of OCaml follows a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Nominal_type_system" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Nominal_type_system" target="_blank"><em></em></a>nominal discipline. Variants must be explicitly declared before being used. The typing discipline used for polymorphic variants and classes is different, as it is <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Structural_type_system" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Structural_type_system" target="_blank"><em></em></a>structural.<br>In the nominal approach of typing, types are first defined; later, when the type of an expression is inferred, three outcomes are possible:<br>
<br>If a matching type is found, it becomes the inferred type.
<br>If any type can be applied, a type parameter is created.
<br>If typing inconsistencies are found, an error is raised.
<br>This is very similar to solving an equation in mathematics. Equations accept either zero, exactly one, several, or infinitely many numbers as solutions. Nominal type-checking finds that either zero, exactly one, or any type can be used in an expression.<br>In the structural approach of typing, type definitions are optional, so they can be omitted. Type-checking an expression constructs a data structure that represents the types that are compatible with it. These data structures are displayed as type expressions sharing a resemblance with simple variants.<br><br>The type expression 'a list does not designate a single type; it designates a family of types, basically all the types that can be created by substituting an actual type to type parameter 'a. The type expressions int list, bool option list, or (float -&gt; float) list list are real types. They're actual members of the type family 'a list. Types are intended to have inhabitants, type families don't.<br>The identifiers list, option, and others are type operators. Just like functions, they take parameters. Although these parameters are not values, they are types. Their results aren't values but types, too.<br>Simple variants are polymorphic, but not in the same sense as polymorphic variants.<br>
<br>Simple variants have <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Parametric_polymorphism" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Parametric_polymorphism" target="_blank">parametric polymorphism</a>
<br>Polymorphic variants have a form of <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Structural_type_system" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Structural_type_system" target="_blank">structural polymorphism</a>
<br><br><br>The visual signature of the polymorphic variants is the back quote. Pattern matching on them looks just the same as with simple variants, except for back quotes (and capitals, which are no longer required).<br># let f = function `Broccoli -&gt; "Broccoli" | `Fruit name -&gt; name;;
val f : [&lt; `Broccoli | `Fruit of string ] -&gt; string = &lt;fun&gt;
<br>Here `Broccoli and `Fruit play a role similar to the one played by the constructors Broccoli and Fruit in a variant declared as type t = Broccoli | Fruit of string. Except, and most importantly, that the definition doesn't need to be written. The tokens `Broccoli and `Fruit are called tags instead of constructors. A tag is defined by a name and a list of parameter types.<br>The expression [&lt; `Broccoli | `Fruit of string ] plays the role of a type. However, it does not represent a single type, it represents three different types.<br>
<br>The type that only has `Broccoli as an inhabitant, its translation into a simple variant is type t0 = Broccoli
<br>The type that only has `Fruit as an inhabitant, its translation into a simple variant is type t1 = Fruit of string
<br>Type type that has both `Broccoli and `Fruit inhabitants, its translation into a simple variant is type t2 = Broccoli | Fruit of string
<br>Note each of the above translations into simple variants is correct. However, entering them as-is into the environment would lead to constructor shadowing (unless type annotation is used at pattern or expression level, see section on <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#Shared-Constructors" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#Shared-Constructors" target="_blank">Shared Constructors</a>).<br>This also illustrates the other striking feature of polymorphic variants: values can be attached to several types. For instance, the tag `Broccoli inhabits [ `Broccoli ] and[ `Broccoli | `Fruit of String ], but it also inhabits any type that contains it.<br>What is displayed by the type-checker, for instance [&lt; `Broccoli | `Fruit of string ], isn't a single type. It is a type expression that designates a constrained set of types. For instance, all the types that are defined by a group of tags containing either `Broccoli or `Fruit of string and nothing more. This is the meaning of the &lt; sign in this type expression. This is a bit similar to what happens with 'a list, which isn't a single type either, but a type expression that designates the set of list types of something, i.e. the set of types where 'a has been replaced by some other type. It is also meant to indicate that the exact types are subsets of the indicated ones (this will be explained in the section on <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#Subtyping-of-Polymorphic-Variants" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#Subtyping-of-Polymorphic-Variants" target="_blank">Subtyping</a>).<br>This is the sense of the polymorphism of polymorphic variants. Polymorphic variants types are type expressions. The structural typing algorithm used for polymorphic variants creates type expressions that designate sets of types (here the three types above), which are defined by constraints on sets of tags (the inequality symbols). The polymorphism of polymorphic variants is different.<br>In the rest of this tutorial, the following terminology is used:<br>
<br>“simple variants”: product types and variants such as list, option
<br>polymorphic variant: type expressions displayed by the OCaml type-checker such as [&lt; `Broccoli | `Fruit of string ]
<br><br>Here is another function using pattern matching on a polymorphic variant.<br># let g = function `Edible name -&gt; name | `Broccoli -&gt; "Brassica oleracea";;
val g : [&lt; `Broccoli | `Edible of string ] -&gt; string = &lt;fun&gt;

# f `Broccoli;;
- : string = "Broccoli"

# g `Broccoli;;
- : string = "Brassica oleracea"
<br>Both f and g accept the `Broccoli tag as input because they both have code for it. They do not have the same domain because f also accepts `Fruit of string whilst g also accepts `Edible of string. The domains of f and g express this. The tag `Broccoli satisfies both the constraints of the domain of f: [&lt; `Broccoli | `Fruit of string ] and the constraints of the domain of g: [&lt; `Broccoli | `Fruit of string ]. That type is [ `Broccoli ]. The value defined by a tag belongs to several types, similarly, a tag accepting functions belongs to several types.<br>Polymorphic variant tags are meant to be used as stand-alone values, wherever it makes sense. As long as used consistently, with a single implicit type per tag, the type checker will accept any combination of them in pattern-matching expressions.<br><br>Type-checking of polymorphic variants is static. No information on tag types is available at runtime.<br># [ `Fruit "Banana"; `Fruit true ];;
Error: This expression has type bool but an expression was expected of type
         string
<br>When a tag is used inconsistently, the type-checker raises an error.<br><br>When building an expression from sub-expressions, the type-checker assembles types from sub-expressions to create the type of the whole expression. This is why the type discipline used for polymorphic variants is said to be structural, it follows the structure of the expressions.<br># let brocco = `Broccoli;;

# let pepe = `Peperone;;

# [ brocco; pepe; brocco ];;
- : [&gt; `Broccoli | `Peperone ] list = [`Broccoli; `Peperone; `Broccoli]
<br><br>Polymorphic variant type expressions can have three forms:<br>
<br>Exact: [ `Broccoli | `Gherkin | `Fruit of string ]<br>
This only designates the type inhabited by the values introduced by these tags.
<br>Closed: [&lt; `Broccoli | `Gherkin | `Fruit of string ]<br>
This designates a set of exact types. Each exact type is inhabited by the values introduced by a subset of the tags from 1. For instance, there are 7 exact types as such:
<br>
<br>[ `Broccoli ]
<br>[ `Gherkin ]
<br>[ `Fruit of string ]
<br>[ `Broccoli | `Gherkin ]
<br>[ `Broccoli | `Fruit of string ]
<br>[ `Gherkin | `Fruit of string ]
<br>[ `Broccoli | `Gherkin | `Fruit of string ]
<br>
<br>Open: [&gt; `Broccoli | `Gherkin | `Fruit of string ]<br>
This designates a set of exact types. Each exact type is inhabited by the values introduced by supersets of the tags from 1.
<br>Note: This syntax works like if the set of natural numbers greater or equal than three was written { &gt; 3 } and the set { 0, 1, 2, 3 } was written { &lt; 3 }.<br>Tip: To distinguish closed and open type expressions, you can remember that the less-than sign &lt; is oriented the same way as a capital C letter, as in closed.<br>An exact form is inferred by the type-checker when naming a type defined by a set of tags:<br># type t = [ `Broccoli | `Gherkin | `Fruit of string ]
type t = [ `Broccoli | `Fruit of string | `Gherkin ]
<br>Exact variants correspond closely to simple variants.<br>The closed form is introduced when performing pattern matching over an explicit set of tags<br># let f = function
    | `Broccoli -&gt; "Broccoli"
    | `Gherkin -&gt; "Gherkin"
    | `Fruit Fruit -&gt; Fruit;;
val upcast : [&lt; `Broccoli | `Fruit of string | `Gherkin ] -&gt; string = &lt;fun&gt;
<br>The function f can be used with any exact type that has these three tags or less. When applied to a type with fewer tags, branches associated with removed tags turn safely into dead code. The type is closed because the function can't accept more than what is listed.<br>The open form can be introduced in two different ways.<br>Open polymorphic variants appear when using a catch-all pattern, either the underscore _ symbol or a name:<br># let g = function
    | `Broccoli -&gt; "Broccoli"
    | `Gherkin -&gt; "Gherkin"
    | `Fruit Fruit -&gt; Fruit
    | _ -&gt; "Edible plant";;
val g : [&gt; `Broccoli | `Fruit of string | `Gherkin ] -&gt; string = &lt;fun&gt;
<br>The function g can be used with any exact type that has these three tags or more. Because of the catch-all pattern, if g is passed a value introduced by a tag that is not part of the list, it will be accepted, and "Edible plant" is returned. The type is open because it can accept more than what is listed in its expression.<br>Note: The type of g is also meant to disallow exact types with tags removed or changed in type. OCaml is a statically typed language, which means no type information is available at runtime. As a consequence pattern matching only relies on tag names. If g was assigned to a type with removed tags, such as [&gt; `Broccoli | `Gherkin ], then passing `Fruit to g would be allowed, but since dispatch is based on names, it would execute the `Fruit of string branch and crash because no string is available. Therefore, an open polymorphic variant must include all the tags from pattern matching.<br>Open polymorphic variants also appear when type-checking tags as values.<br># `Gherkin
- : [&gt; `Gherkin ] = `Gherkin

# [ `Broccoli; `Gherkin; `Broccoli ];;
- : [&gt; `Broccoli | `Gherkin ] list = [ `Broccoli; `Gherkin; `Broccoli ]
<br>Setting the type of tag to the open polymorphic variant which only contains it enables:<br>
<br>To use it in all the contexts where applicable code is available.
<br>To avoid using it in contexts that can't deal with it.
<br>The build of sums of polymorphic variants. This is shown in the second example.
<br>This also applies to functions returning polymorphic variant values.<br># let f b = if b then `Up else `Down;;
val f : bool -&gt; [&gt; `Down | `Up ] = &lt;fun&gt;
<br>The codomain of f is the open type [&gt; `Down | `Up ]. This makes sense when having a look at function composition.<br># let g = function
    | `Up -&gt; 1
    | `Down -&gt; 2
    | `Broken -&gt; 3;;

# fun x -&gt; x |&gt; f |&gt; g;;
- : bool -&gt; int = &lt;fun&gt;
<br>Functions g and f can be composed because g accepts more than what f can return. The value `Broken will never pass from f into g, but it is safe to write it as no unexpected value can make its way through.<br>Closed and open cases are polymorphic because they do not designate a single type but families of types. An exact variant type is (structurally) monomorphic, it corresponds to a single variant type, not a set of variant types.<br><br>A closed variant type may also have an additional constraint preventing some tags from being removed.<br># let is_red = function `Clubs -&gt; false | `Diamonds -&gt; true | `Hearts -&gt; true | `Spades -&gt; false;;
val is_red : [&lt; `Clubs | `Diamonds | `Hearts | `Spades ] -&gt; bool = &lt;fun&gt;

# let h = fun u -&gt; List.map is_red (`Hearts :: u);;
val h : [&lt; `Clubs | `Diamonds | `Hearts | `Spades &gt; `Hearts ] list -&gt; bool list = &lt;fun&gt;
<br>Function is_red accepts values from any subtype of [ `Clubs | `Diamonds | `Hearts | `Spades ]. However, function h excludes the exact types [ `Clubs ], [ `Diamonds ] and [ `Spades ]. The list passed to List.map includes a `Hearts tag. Types that do not include it are not allowed.<br>The domain of h is:<br>
<br>closed by [ `Clubs | `Diamonds | `Hearts | `Spades ] and,
<br>open by [ `Hearts ].
<br><br>Simple variants have a form of polymorphism called parametric polymorphism. Together with predefined types, they are type-checked using the nominal typing discipline. In this discipline, a value has a unique type.<br>In the structural type-checking discipline used for polymorphic variants, a value may have several types. Equivalently it can be said to inhabit several types:<br># let gherkin = `Gherkin;;
val Gherkin : [&gt; `Gherkin ] = `Gherkin

# let (gherkin : [ `Gherkin ]) = `Gherkin;;
val Gherkin : [ `Gherkin ] = `Gherkin

# let (gherkin : [ `Gherkin | `Avocado ]) = `Gherkin;;
val Gherkin : [ `Gherkin | `Avocado ] = `Gherkin
<br>
<br>By default, the type assigned to the `Gherkin tag is [&gt; `Gherkin]. It means any variant type that includes that tag.
<br>Using an annotation, the type can be restrained to [ `Gherkin ], the exact variant type only containing `Gherkin
<br>Using another annotation allows assigning the `Gherkin value to a type containing more tags.
<br>This entails a partial ordering relation between exact variant types. The type [ `Gherkin ] is smaller than the type [ `Gherkin | `Avocado ]. The types `[ `Gherkin ] and [ `Avocado ] do not compare. The type `[ `Gherkin ] is the smallest possible type for the tag `Gherkin.<br>The order between the exact variants derives from the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_order" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_order" target="_blank">partial order</a> on <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Subset" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Subset" target="_blank">subsets</a> of tags. The sets considered are the tags, with names and types. This order is said to be partial because some sets can't be compared. This order is called the subtyping order.<br>The OCaml syntax does not allow expression of the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/10687" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/issues/10687" target="_blank">empty</a> polymorphic variant type. If it was, it would be the least element in the subtyping order.<br>OCaml has a cast operator, it allows to raise the type of an expression into any larger type, with respect to the subtyping order. It is written :&gt;<br># (gherkin :&gt; [ `Avocado | `Gherkin | `Tomato ]);;
- : [ `Avocado | `Gherkin | `Tomato ] = `Gherkin
<br>It means the type of gherkin is raised from [ `Gherkin | `Tomato ] into [ `Avocado | `Gherkin | `Tomato ]. It is admissible because [ `Gherkin | `Tomato ] is smaller than [ `Avocado | `Gherkin | `Tomato ] in the subtyping order.<br>When casting, it is also possible to indicate the subtype from which the value is upcast.<br># (gherkin : [ `Gherkin | `Tomato ] :&gt; [ `Avocado | `Gherkin | `Tomato ]);;
- : [ `Avocado | `Gherkin | `Tomato ] = `Gherkin
<br><br><br>Exact polymorphic variant types can be given names.<br># type exotic = [ `Guayaba | `Maracuya | `Papaya ];;
<br>Named polymorphic variants are always exact, thus they are equivalent to simple variants. It is not possible to give names to closed or open polymorphic variants.<br><br>Named polymorphic variants can be used to create extended types.<br># type mexican = [ exotic | `Pitahaya | `Sapodilla ];;
type mexican = [ `Guayaba | `Maracuya | `Papaya | `Pitahaya | `Sapodilla ]
<br><br>Named polymorphic variants can be used as patterns.<br># let f = function
    | #exotic -&gt; "Exotic Fruit"
    | `Mango -&gt; "Mango";;
val f : [&lt; `Guayaba | `Mango | `Maracuya | `Papaya ] -&gt; string = &lt;fun&gt;
<br>This is not a dynamic type check. The #exotic pattern acts like a macro, it is a shortcut to avoid writing all the corresponding patterns.<br><br><br># function `Avocado -&gt; `Cilantro | plant -&gt; plant;;
- : ([&gt; `Cilantro | `Avocado ] as 'a) -&gt; 'a = &lt;fun&gt;
<br>The meaning of the type of this function is twofold.<br>
<br>Any exact variant type which is a super set of [&gt; `Cilantro | `Avocado ] is a domain
<br>The very same type is also a codomain
<br>The meaning of 'a is not the same as with simple variants. It is not a type parameter meant to be replaced by another type. The as 'a part in the type expression [&gt; `Cilantro | `Avocado ] as 'a means [&gt; `Cilantro | `Avocado ] is bound to the local name 'a in the overall expression. It allows expressing that the same variant type is used as both domain and codomain. This is a consequence of the plant -&gt; plant clause that forces domain and codomain types to be the same.<br><br>It is possible to combine polymorphic variants and parametric polymorphism. Here is a duplication of the 'a option type, translated as a polymorphic variant parametrized with a type parameter.<br># let map f = function
    | `Some x -&gt; `Some (f x)
    | `None -&gt; `None;;
val map : ('a -&gt; 'b) -&gt; [&lt; `None | `Some of 'a ] -&gt; [&gt; `None | `Some of 'b ] =
  &lt;fun&gt;
<br>This map function has two type parameters: 'a and 'b. They are used to type its f parameter.<br><br>The inferred type of a polymorphic variant may be recursive.<br># let rec map f = function
    | `Nil -&gt; `Nil
    | `Cons (x, u) -&gt; `Cons (f x, map f u);;
val map :
  ('a -&gt; 'b) -&gt;
  ([&lt; `Cons of 'a * 'c | `Nil ] as 'c) -&gt; ([&gt; `Cons of 'b * 'd | `Nil ] as 'd) =
  &lt;fun&gt;
<br>The aliasing mechanism is used to express type recursion. In the type expression [&lt; `Cons of 'a * 'c | `Nil ] as 'c, the local name 'c occurs inside the defined thing: [&lt; `Cons of 'a * 'c | `Nil ]. Therefore, it is a recursive type definition.<br><br>A tag `Night inhabits any type with additional tags, for instance [ `Night | `Day ] or [`Morning | `Afternoon | `Evening | `Night]. This is summarized by the subtyping order where [ `Night ] is smaller to both [ `Night | `Day ] and [`Morning | `Afternoon | `Evening | `Night ].<br>This can be seen by defining a function upcast in the following way:<br># let upcast (x : [ `Night ]) = (x :&gt; [ `Night | `Day ]);;
val upcast : [ `Night ] -&gt; [ `Night | `Day ] = &lt;fun&gt;
<br>This function is an identity function it returns its parameter unchanged. It illustrates a value of type [ `Night ] can be cast into the type [ `Night | `Day ]. Casting goes from subtype to supertype.<br><br>The subtyping order extends to simple variants. Functions upcast_opt upcast_list and upcast_snd are doing the same thing as upcast except they are taking parameters from variants parametrized by polymorphic variants.<br># let upcast_opt (x : [ `Night ] option) = (x :&gt; [ `Night | `Day ] option);;
val upcast_opt : [ `Night ] option -&gt; [ `Night | `Day ] option = &lt;fun&gt;

# let upcast_list (x : [ `Night ] list) = (x :&gt; [ `Night | `Day ] list);;
val upcast_list : [ `Night ] list -&gt; [ `Night | `Day ] list = &lt;fun&gt;

let upcast_snd (x : [ `Night ] * int) = (x :&gt; [ `Night | `Day ] * int);;
val upcast_snd : [ `Night ] * int -&gt; [ `Night | `Day ] * int = &lt;fun&gt;
<br>The product type and the types option, list are said to be covariant. Subtyping on parametrized variants goes “in the same direction” as on polymorphic variants parameters:<br>
<br>[ `Night ] is a subtype of [ `Night | `Day ]
<br>[ `Night ] list is a subtype of [ `Night | `Day ] list
<br>Note that these types are covariant because of the way their type parameters appear in the type of their constructors. This is detailed in <a data-tooltip-position="top" aria-label="https://v2.ocaml.org/manual/typedecl.html#ss:typedefs" rel="noopener nofollow" class="external-link" href="https://v2.ocaml.org/manual/typedecl.html#ss:typedefs" target="_blank">Chapter 11, Section 8.1</a> of the OCaml Manual. This has nothing to do with being predefined types.<br><br>The function type is covariant on codomains. Casting a function is allowed if the target codomain is larger.<br># let upcast_dom (f : int -&gt; [ `Night ]) = (x :&gt; int -&gt; [ `Night | `Day ]);;
val f : (int -&gt; [ `Night ]) -&gt; int -&gt; [ `Night | `Day ] = &lt;fun&gt;
<br>Covariance means subtyping “goes in the same direction”:<br>
<br>On codomain: [ `Night ] is a subtype of [ `Night | `Day ]
<br>On function type: int -&gt; [ `Night ] is a subtype of int -&gt; [ `Night | `Day ]
<br>Adding tags to a polymorphic variant codomain of a function is harmless. Extending a function's codomain is pretending it can return something that is never returned. It is a false promise, and the precision of the type is reduced, but it is safe, no unexpected data will ever be returned by the function.<br><br>The function type is contravariant on domains. Casting a function is allowed if the target domain is smaller.<br># let upcast_cod (f : [ `Night | `Day ] -&gt; int) = (x :&gt; [ `Night ] -&gt; int);;
val f : ([ `Night | `Day ] -&gt; int) -&gt; [ `Night ] -&gt; int = &lt;fun&gt;
<br>Contravariance means subtyping “is reversed”:<br>
<br>On domain: [ `Night ] is a subtype of [ `Night | `Day ]
<br>On function type: [ `Night | `Day ] -&gt; int is a subtype of [ `Night ] -&gt; int
<br>At first, it may seem counterintuitive. However, removing tags from a polymorphic variant domain is also harmless. The code in charge of the removed tags is turned into dead paths. Implemented generality of the function is lost, but it is safe, no data will be passed that the function can't handle.<br><br># let ingredient = function 0 -&gt; `Flour | _ -&gt; `Masa;;
val ingredient : int -&gt; [&gt; `Masa | `Flour ] = &lt;fun&gt;

# let chef = function
    | `Flour -&gt; `Bread
    | `Egg -&gt; `Tortilla
    | `Masa -&gt; `Tortilla;;
val chef : [&lt; `Masa | `Flour | `Egg ] -&gt; [&gt; `Bread | `Tortilla ] =
  &lt;fun&gt;

# let taste = function `Tortilla | `Bread -&gt; "Nutritious" | `Cake -&gt; "Yummy";;
val taste : [&lt; `Bread | `Tortilla | `Cake ] -&gt; string = &lt;fun&gt;

# fun n -&gt; n |&gt; ingredient |&gt; chef |&gt; taste;;
- : int -&gt; string = &lt;fun&gt;

# let upcasted_chef =
    (chef :&gt; [&lt; `Flour | `Masa ] -&gt; [&gt; `Bread | `Tortilla | `Cake ]);;
val upcasted_chef : [&lt; `Flour | `Masa ] -&gt; [&gt; `Bread | `Tortilla | `Cake ] =
  &lt;fun&gt;

# fun n -&gt; n |&gt; ingredient |&gt; upcasted_chef |&gt; taste;;
- : int -&gt; string = &lt;fun&gt;
<br>The type of chef is a subtype of the type of upcasted_chef. The function upcasted_chef has a reduced domain and enlarged codomain. However, upcasted_chef has a domain that remains larger than the codomain of ingredient and, a codomain that remains smaller than the domain of taste. Therefore, it is safely possible to consider chef as an inhabitant of the type of upcasted_chef in the composition pipe where it is used.<br>It is also possible to refactor chef into a new function that can be used safely at the same place.<br># let refactored_chef = function
    | `Flour -&gt; `Bread
    | `Masa -&gt; (`Tortilla : [&gt; `Bread | `Tortilla | `Cake ]);;

# fun n -&gt; n |&gt; ingredient |&gt; refactored_chef |&gt; taste;;
- : int -&gt; string = &lt;fun&gt;
<br><br><br>Not having to explicitly declare polymorphic variant types is beneficial in several cases.<br>
<br>When few functions use the type
<br>When many types would have to be declared
<br>When reading a pattern-matching expression using polymorphic variant tags, understanding is local. Since polymorphic variant types are anonymous or aliases, there is no need to search for the meaning of the tags somewhere else. The meaning arises from the expression itself.<br><br>When several simple variants use the same constructor name, shadowing takes place. To pattern match over a shadowed variant, type annotation must be added, either to the whole pattern-matching expression or to some patterns.<br># type a = A;;
type a = A
# type b = A;;
type b = A
# function A -&gt; true;;
- : b -&gt; bool = &lt;fun&gt;
# function (A : a) -&gt; true;;
- : a -&gt; bool = &lt;fun&gt;
# (function A -&gt; true : a -&gt; bool);;
- : a -&gt; bool = &lt;fun&gt;
<br>Without this, previously entered ones are no longer reachable. This can be worked around using modules. This is explained in <a data-tooltip-position="top" aria-label="https://dev.realworldocaml.org/records.html#scrollNav-3" rel="noopener nofollow" class="external-link" href="https://dev.realworldocaml.org/records.html#scrollNav-3" target="_blank">Reusing Field Names</a> ) of the Real World OCaml book written by Yaron Minsky and Anil Madhavapeddy.<br>This problem never happens with polymorphic variants. When a tag appears several times in an expression, it must be with the same type, that's the only restriction. This makes polymorphic variants very handy when dealing with multiple sum types sharing constructors.<br><br>Using the same type in two different modules can be done in several ways:<br>
<br>Having a dependency, either direct or shared
<br>Turn the dependent module into a functor and inject the dependence as a parameter
<br>The polymorphic variants provide an additional alternative. This was proposed by Jacques Garrigue in his seminal paper “Programming with Polymorphic Variants” (ACM SIGPLAN Workshop on ML, October 1998):<br>
You [...] define the same [polymorphic variant] type in both [modules], and since these are only type abbreviations, the two definitions are compatible. The type system checks the structural equality when you pass a value from one [module] to the other.
<br><br>The <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/error-handling" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/error-handling" target="_blank">Error Handling</a> guide details possible ways to handle errors in OCaml. Among various mechanisms, the result type, when used as a monad, provides a powerful means to handle errors. Refer to the guide and documentation on this type to learn how to use it. In this section, we discuss why using polymorphic variants to carry error values can be beneficial.<br>Let's consider this exception-raising code:<br># let f_exn m n =
    let open List in
    let u = init m Fun.id |&gt; map (fun n -&gt; n * n) in
    nth u n;;
val f_exn : int -&gt; int -&gt; int = &lt;fun&gt;
<br>The following is an attempt at translating f_exn with result values instead of exceptions. It is using the Result.bind instead of |&gt;:<br># type init_error = Negative_length;;

# let init n f = try Ok (List.init n f) with
    | Invalid_argument _ -&gt; Error Negative_length;;
val init : int -&gt; (int -&gt; 'a) -&gt; ('a list, init_error) result = &lt;fun&gt;

# type nth_error = Too_short of int * int | Negative_index of int;;
type nth_error = Too_short of int * int | Negative_index of int

# let nth u i = try Ok (List.nth u i) with
    | Invalid_argument _ -&gt; Error (Negative_index i)
    | Failure _ -&gt; Error (Too_short (List.length u, i))
val nth : 'a list -&gt; int -&gt; ('a, nth_error) result = &lt;fun&gt;

# let f_res m n =
    let* u = init m Fun.id in
    let u = List.map (fun n -&gt; n * n) u in
    let* x = nth u n in
    Ok x;;
Error: This expression has type (int, nth_error) result
       but an expression was expected of type (int, init_error) result
       Type nth_error is not compatible with type init_error
<br>This does not work because of the type of Result.bind.<br># Result.bind;;
- : ('a, 'e) result -&gt; ('a -&gt; ('b, 'e) result) -&gt; ('b, 'e) result = &lt;fun&gt;
<br>Binding can't change the 'e type, while this code needs it to change throughout the pipe.<br>Here is an equivalent version using polymorphic variants:<br># let ( let* ) = Result.bind;;
val ( let* ) : ('a, 'b) result -&gt; ('a -&gt; ('c, 'b) result) -&gt; ('c, 'b) result =
  &lt;fun&gt;

# let init n f = try Ok (List.init n f) with
    | Invalid_argument _ -&gt; Error `Negative_length;;
val init : int -&gt; (int -&gt; 'a) -&gt; ('a list, [&gt; `Negative_length ]) result =

# let nth u i = try Ok (List.nth u i) with
    | Invalid_argument _ -&gt; Error (`Negative_index i)
    | Failure _ -&gt; Error (`Too_short (List.length u, i));;
val nth :
  'a list -&gt;
  int -&gt; ('a, [&gt; `Negative_index of int | `Too_short of int * int ]) result =
  &lt;fun&gt;

# let f_res m n =
    let* u = init m Fun.id in
    let u = List.map (fun n -&gt; n * n) u in
    let* x = nth u n in
    Ok x;;
val f_res :
  int -&gt;
  int -&gt;
  (int,
   [&gt; `Negative_index of int | `Negative_length | `Too_short of int * int ])
  result = &lt;fun&gt;

<br>Using polymorphic variants, the type-checker generates a unique type for the whole pipe. The constraints coming from calling init and nth_error are merged into a single type.<br><br><br>By default, polymorphic variant types aren't declared with a name before being used. The compiler will generate type expressions corresponding to each function dealing with polymorphic variants. Some of those types are hard to read. When several such functions are composed together, inferred types can become very large and difficult to understand.<br># let rec fold_left f y = function `Nil -&gt; y | `Cons (x, u) -&gt; fold_left f (f x y) u;;
val fold_left :
  ('a -&gt; 'b -&gt; 'b) -&gt; 'b -&gt; ([&lt; `Cons of 'a * 'c | `Nil ] as 'c) -&gt; 'b = &lt;fun&gt;
<br><br>In some circumstances, combining sets of constraints will artificially reduce the domain of a function. This happens when the conjunction of constraints must be taken. Functions f and g used here are those defined in the <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#a-first-example" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#a-first-example" target="_blank">first section</a>.<br># f;;
- : [&lt; `Broccoli | `Fruit of string ] -&gt; string = &lt;fun&gt;

# g;;
- : [&lt; `Broccoli | `Edible of string ] -&gt; string = &lt;fun&gt;

# let u = [f; g];;
u : ([&lt; `Broccoli ] -&gt; string) list = [&lt;fun&gt;; &lt;fun&gt;]

# f (`Fruit "Pitahaya");;
- : string = "Pitahaya"

# (List.hd u) (`Fruit "Pitahaya");;
Error: This expression has type [&gt; `Fruit of string ]
       but an expression was expected of type [&lt; `Broccoli ]
       The second variant type does not allow tag(s) `Fruit
<br>Function f accepts tags `Broccoli and `Fruit whilst g accepts `Broccoli and `Edible. But if f and g are stored in a list, they must have the same type. That forces their domain to be restricted to a common subtype. Although f can handle the tag `Fruit, it no longer accepts that parameter when f is extracted from the list.<br><br>This is adapted from the section <a data-tooltip-position="top" aria-label="https://dev.realworldocaml.org/variants.html#scrollNav-4-2" rel="noopener nofollow" class="external-link" href="https://dev.realworldocaml.org/variants.html#scrollNav-4-2" target="_blank">Example: Terminal Colors Redux</a> from the “Real World OCaml” book.<br>Tags are used to store color representations:<br>
<br>`RGB contains a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/RGB_color_model" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/RGB_color_model" target="_blank">red, green and blue</a> triplet
<br>`Gray contains a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Grayscale" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Grayscale" target="_blank">grayscale</a> value
<br>`RGBA contains a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/RGBA_color_model" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/RGBA_color_model" target="_blank">red, green, blue, alpha</a> quadruplet
<br>The polymorphic variant type color groups the tags `RGB and `Gray whilst the type extended_color has three tags by extending color with `RGBA. Functions are defined to convert colors into integers.<br># type color = [ `Gray of int | `RGB of int * int * int ];;
type color = [ `Gray of int | `RGB of int * int * int ]

# type extended_color = [ color | `RGBA of int * int * int * int ];;
type extended_color =
    [ `Gray of int | `RGB of int * int * int | `RGBA of int * int * int * int ]

# let color_to_int = function
  | `RGB (r, g, b) -&gt; 36 * r + 6 * g + b + 16
  | `Gray i -&gt; i + 232;;
val color_to_int : [&lt; `Gray of int | `RGB of int * int * int ] -&gt; int = &lt;fun&gt;

# let extended_color_to_int = function
  | `RGBA (r, g, b, a) -&gt; 216 * r + 36 * g + 6 * b + a + 16
  | `Grey i -&gt; i + 2000
  | #color as color -&gt; color_to_int color;;
val extended_color_to_int :
  [&lt; `Gray of int
   | `Grey of int
   | `RGB of int * int * int
   | `RGBA of int * int * int * int ] -&gt;
  int = &lt;fun&gt;

<br>The function color_to_int can convert `RGB or `Gray values. The function extended_color_to_int is intended to convert `RGB, `Gray or `RGBA values; but it is supposed to apply a different conversion formula for gray scales. However, a typo was made, it is spelled `Gray. The type-checker accepts this definition of extended_color_to_int as a function accepting four tags.<br><br>The following function was presented in the <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#Inferred-Type-Aliases" rel="noopener nofollow" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#Inferred-Type-Aliases" target="_blank">Inferred Type Aliases</a> section.<br># function `Avocado -&gt; `Cilantro | plant -&gt; plant;;
- : ([&gt; `Cilantro | `Avocado ] as 'a) -&gt; 'a = &lt;fun&gt;
<br>Because of the plant -&gt; plant pattern, the types inferred as domain and codomain are the same. As `Avocado is accepted, it must be part of the domain; and as `Cilantro is returned, it must be part of the codomain. Both end up being part of the common type. However, `Avocado should not be part of the codomain, as this function can't possibly return this value. A finer type-checker would infer more precise types. Type-checking is an approximation and a trade-off, some valid programs are rejected, and some types are too coarse.<br><br>TODO: Expand this section<br>
There is one more downside: the runtime cost. A value Pair (x, y) occupies 3 words in memory, while a value `Pair (x, y) occupies 6 words.
<br>Guillaume Melquiond<br><br>Only using polymorphic variants is over-engineering, it should be avoided. Polymorphic variants aren't an extension of simple variants. From the data perspective, polymorphic variants and simple variants are equivalent. They both are sum types in the algebraic data types sense, both with parametric polymorphism and recursion. Back quote isn't a difference that matters. The only meaningful difference is the algorithm. Therefore, deciding when to use polymorphic variants boils down to another question:<br>
Are nominally or structurally type-checked variants needed?
<br>Answering this isn't significantly easier, but it helps to narrow what to consider. The key difference lies in the way pattern matching is type-checked. Polymorphic variant induces functions that intrinsically accept more data than simple variants. This is a consequence of the subtyping relation between polymorphic variant types.<br>In a precautionary approach (inspired by <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/KISS_principle" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/KISS_principle" target="_blank">KISS</a> or <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it" target="_blank">YAGNI</a> design principles), using simple variants should be the default. However, they can feel too tight. Here are clues indicating this:<br>
<br>Many variant declarations look alike
<br>Equivalent constructor duplicated in several variants
<br>Variant declared for a single-purpose
<br>Variant declaration feeling made up or artificial
<br>Having difficulties finding names for simple variants or constructors may be smell for one of the above. When such discomfort is felt, polymorphic variants may not be the solution, but they can be considered. Keep in mind they may ease some parts but at the expense of understandability, precision and performance. Hopefully, very soon, LLM bots will be able to refactor variants of some sort into the other. This will ease experimentation.<br>To conclude, remember a simple variant that naturally encodes some data should remain a simple variant.<br><br>Although polymorphic variants share a lot with simple variants, they are substantially different. This comes from the structural type-checking algorithm used for polymorphic variants.<br>
<br>Type declaration is optional
<br>A type is a set of constraints over values
<br>A Value satisfies its type rather than inhabits it
<br>Type expression designates sets of types
<br>There is a subtyping relation between types
<br>Polymorphic variants should not be considered as an improvement over simple variants. In some regards, they are more flexible and lightweight, but they also have harder-to-read types and slightly weaker type-checking assurances. Choosing when to prefer polymorphic variants over simple variants is a subtle decision to make. It is safe to prefer simple variants as the default and go for polymorphic variants when there is a solid case for them.<br>It is important to be comfortable with polymorphic variants, many projects are using them. Most often, only a fraction of their expressive strength is used. However, refactoring code using them requires being able to understand more than what's used. Otherwise, one may quickly end up stalled by indecipherable type error messages.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/polymorphic-variants.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Polymorphic Variants.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:48:12 GMT</pubDate></item><item><title><![CDATA[Rethinking OCaml abstract signatures]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br>Abstract module types are one of the less understood features of the OCaml module system. They have been one of the obstacles in the on-going effort to specify, and eventually redesign, the module system.<br>In this blog post, I (Clément Blaudeau) present an explanation of what are those abstract module types, and propose a slightly restricted version that might be easier to understand and specify while remaining pretty expressive.<br>For the past 2 years, I’ve been working on building a new specification for the OCaml module system based on Fω. The goal, besides the theoretical interest, is to eventually redo (the module part of) the typechecker with this approach, which would have several benefits:<br>
<br>fix some soundness issues and edge-cases that have appeared and built up over the years, due to unforeseen interactions between features
<br>simplify the (notoriously hard) code of the typechecker by removing ad-hoc techniques and hacks (such as the strengthening or the treatment of aliases for instance)
<br>provide a clean base to add new and awaited features. Notably, transparent ascription and modular implicits are proposals for OCaml modules stalled by the lack of specification of the module system.
<br>Yet, a key aspect in OCaml development culture is to ensure backward compatibility. Therefore, the new Fω approach I’ve been building should not only subsumes the current typechecker in normal use cases, but actually support all of the features of the module system. For long, abstract signatures (also called abstract module types) were believed to be, at least, problematic for Fω. Hopefully, we found out that a slightly restricted version of the feature was encodable in Fω, and, in passing, made the semantics of abstract signatures much simpler. Thus, only one question remains: does this restricted form actually covers all use cases, i.e., is the restriction backward compatible ?<br>Here, we aim at presenting the current state of abstract signatures and our proposed simplification purely from an OCaml user point of view, not from the theoretical one. We welcome any feedback, specifically, use cases or potential use cases that significantly differ from our examples.<br>We start by introducing abstract signatures through examples. Then, we present the current state of abstract signatures in OCaml: we explain the syntactic approach and the issues associated with it. We argue that it has surprising behaviors and, in its current unrestricted form, it is actually too powerful for its own good. Then, we propose a restriction to make the system predicative which, by decreasing its expressiveness, actually makes it more usable. (Our actual proposal is given in 3.3). We finish by other aspect related to usability (syntax, inference).<br><br>The art of modularity is all about controlling abstraction and interfaces. ML languages offer this control via a module system, which contains a signature language to describe interfaces. Signatures contain declarations (fields): values val x : t, types type t = int, modules module X : S, and module types module type T = S. Type and module type declarations can also be abstract type t, module type T, which serves both to hide implementation details via sealing and to have polymorphic interfaces, using functors.<br>Here, we focus on the construct module type T, called abstract module type or abstract signature. We start with examples adapted from <a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/what-are-abstract-module-types-useful-for/10121/3" rel="noopener nofollow" class="external-link" href="https://discuss.ocaml.org/t/what-are-abstract-module-types-useful-for/10121/3" target="_blank">this forum discussion</a>.<br><br>Let’s consider the following scenario. Two modules providing an implementation of UDP (UDP1 and UDP2) are developed with different design trade-offs. They both implement a signature with basic send and receive operations. Then, functors are added as layers on top: taking a udp library as input, they return another udp library as an output.<br>
<br>Reliable adds sequence numbers to the packets and re-sends missing packets;
<br>CongestionControl tracks the rate of missing packets to adapt the throughput to network congestion situations;
<br>Encryption encrypts the content of all messages.
<br>A project might need different combinations of the basic libraries and functors, while requiring that all combinations use encryption. To enforce this, the solution is to use the module-level sealing of abstract signatures. In practice, the signature of the whole library containing implementations and functors UDPLib (typically, its .mli file) is rewritten to abstract all interfaces except for the output of the Encryption functor.<br>module type UDPLib = sig<br>
module type UNSAFE<br>  module UDP1 : UNSAFE<br>
module UDP2 : UNSAFE<br>  module Reliable : UNSAFE -&gt; UNSAFE<br>
module CongestionControl : UNSAFE -&gt; UNSAFE<br>  module Encryption : UNSAFE -&gt;<br>
sig val send : string -&gt; unit ( ... ) end<br>
end<br>Just as type abstraction, signature abstraction can be used to enforce certain code patterns: users of UDPLib will only be able to use the content of modules after calling the Encryption functor, and yet they have the freedom to choose between different implementations and features:<br>module UDPKeyHandshake = Encryption(Reliable(UDP1))<br>
module UDPVideoStream  = Encryption(CongestionControl(UDP2))<br>
( etc )<br><br>Another use is to introduce polymorphism at the module level. Just as polymorphic functions can be used to factor code, module-level polymorphic functors can be used to factor module expressions. If a code happens to often feature functor applications of the form Hashtbl.Make(F(X)) or Set.Make(F(X)), one can define the MakeApply functor as follows:<br>( Factorizing common expressions )<br>
module type Type = sig module type T end<br>
module MakeApply<br>
(A:Type) (X: A.T)<br>
(B:Type) (F: A.T -&gt; B.T)<br>
(C:Type) (H: sig module Make : B.T -&gt; C.T end) = H.Make(F(X))<br>Downstream the code is rewritten into MakeApply(...)(X)(...)(F)(...)(Set) or MakeApply(...)(X)(...)(F)(...)(Hashtbl) Right now, the verbosity of such example would probably be a deal-breaker. We address this aspect at the end. Ignoring the verbosity, this can be useful for maintenance: by channeling all applications through MakeApply, only one place needs to be updated if the arity or order of arguments is changed. Similarly, if several functors expect a constant argument containing – for instance – global variables, a ApplyGv functor can be defined to always provide the right second argument, which can even latter be hidden away to the user of ApplyGv:<br>( Constant argument )<br>
module Gv : GlobalVars<br>
module ApplyGv (Y : sig module type A module type B end)<br>
(F : Y.A -&gt; GlobalVars -&gt; Y.B)(X : Y.A) = F(X)(Gv)<br>Downstream, code featuring F(X)(GlobalVars) is rewritten into ApplyGv(...)(F)(X) Then, the programmer can hide the GlobalVars module while letting users use ApplyGv, ensuring that global variables are not modified in uncontrolled ways by certain part of the program.<br>Finally, polymorphism can also be used by a developer to prevent unwanted dependencies on implementation details. If the body of a functor uses an argument with a submodule X, but actually does not depend on the content of S, abstracting it is a “good practice”.<br>module F (Arg : sig ... module X : S ... end) =<br>
... ( polymorphism is not enforced )<br>module F' (Y: sig module type S end)<br>
(Arg : sig ... module X : Y.S ... end ) =<br>
... ( polymorphism is enforced )<br><br>Fundamentally, these example are not surprising for developers that are used to rely on abstraction to protect invariants and factor code. Their specificity lies in the fact that there are at the module level, and therefore require projects with a certain size and a strong emphasis on modularity to be justified.<br><br>The challenge for understanding (and implementing) abstract signatures lies more in the meaning of the module-level polymorphism that they offer than the module level sealing, the latter being pretty straightforward. More specifically, the crux lies in the meaning of the instantiation of an abstract signature variable A by some other signature S, that happens when a polymorphic functor is applied. OCaml follows an unrestricted syntactical approach: A can be instantiated by any (well-formed) signature S. During instantiation, all occurrences of A are just replaced by S ; finally, the resulting signature is re-interpreted—as if it were written as is by the user.<br>However, this syntactical rewriting interferes with the variant interpretation of signatures, which can lead to surprising behaviors. We discuss this aspect first. The unrestricted aspect leads to the (infamous) Type : Type issue which has some theoretical consequences. We finish this section by mentioning other—more technical—issues.<br><br>The first key issue of this approach comes from the fact that signatures in OCaml have a variant interpretation: abstract fields (1) have a different meaning (sealing or polymorphism) depending on whether they occur in positive or negative positions, and (2) abstract fields open new scopes, i.e.&nbsp;duplicating an abstract type field introduces two different abstract types. Overall, OCaml signatures can be thought of as having implicit quantifiers: using a signature in positive or negative position changes its implicit quantifiers (from existential to universal) while duplicating a signature duplicates the quantifiers (and therefore introduces new incompatible abstract types).<br>Therefore, when instantiating an abstract signature with a signature that has abstract fields, the user must be aware of this, and mentally infer the meaning of the resulting signature. To illustrate how it can be confusing, let’s revisit the first motivating example and let’s assume that the developer actually want to expose part of the interface of the raw UDP libraries. One might be tempted to instantiate UNSAFE with something along the following lines:<br>module type UDPLib_expose = sig<br>
include UDPLib with module type UNSAFE =<br>
sig<br>
module type CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE ( this part remains abstract )<br>
module Safe : sig ... end ( this part is exposed )<br>
end<br>
end<br>This returns :<br>module type UDPLib_expose =  sig<br>
module type UNSAFE =<br>
sig<br>
module type CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE<br>
module Safe : sig ... end<br>
end<br>
module UDP1 : UNSAFE<br>
module UDP2 : UNSAFE<br>
module Reliable : UNSAFE -&gt; UNSAFE<br>
module CongestionControl : UNSAFE -&gt; UNSAFE<br>
module Encryption : UNSAFE -&gt; sig val send : string -&gt; unit ( ... ) end<br>
end<br>However, the syntactical rewriting and reinterpretation of this signature in the negative positions produces a counter-intuitive result. For instance, if we expand the signature of the argument for the functor Reliable (for instance) we see:<br>module Reliable :<br>
sig<br>
module type CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE<br>
module Safe : sig ... end<br>
end -&gt; UNSAFE<br>This means that the functor actually has to be polymorphic in the underlying implementation of CORE_UNSAFE, rather than using the internal details, which has the opposite meaning as before. If the user wants to hide a shared unsafe core, accessible to the functor when they were defined and then abstracted away, the following pattern may be used instead:<br>module type UDPLib_expose' = sig<br>
module type CORE_UNSAFE<br>
include UDPLib with module type UNSAFE = sig<br>
module type CORE_UNSAFE = CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE<br>
module Safe : sig ... end<br>
end<br>
end<br>Doing so, the instantiated signature does not contain abstract fields and therefore its variant reinterpretation will not introduce unwanted polymorphism. This observation is at the core of the proposal of this post.<br><br>Abstract module types are impredicative: a signature containing an abstract signature can be instantiated by itself. One can trick the subtyping algorithm into an infinite loop of instantiating an abstract signature by itself, as shown by <a data-tooltip-position="top" aria-label="https://sympa.inria.fr/sympa/arc/caml-list/1999-07/msg00027.html" rel="noopener nofollow" class="external-link" href="https://sympa.inria.fr/sympa/arc/caml-list/1999-07/msg00027.html" target="_blank">Andreas Rosseberg</a>, adapting an example from <a data-tooltip-position="top" aria-label="https://doi.org/10.1145/174675.176927" rel="noopener nofollow" class="external-link" href="https://doi.org/10.1145/174675.176927" target="_blank">Harper and Lillibridge (POPL ’94)</a>. This also allows type-checking of (non-terminating) programs with an absurd type, as shown by the encoding of the Girard’s paradox done by <a data-tooltip-position="top" aria-label="https://github.com/lpw25/girards-paradox/tree/master" rel="noopener nofollow" class="external-link" href="https://github.com/lpw25/girards-paradox/tree/master" target="_blank">Leo White</a>.<br><br>The current implementation of the typechecker does not handle abstract signatures correctly in some scenarios. It’s unclear if they are just bugs or pose theoretical challenges.<br><br>Inside a functor, module aliases are disallowed between the parameter and the body (for soundness reasons, due to coercive subtyping). However, this check can be bypassed by using an abstract signature that is then instantiated with an alias. If we try to use it to produce a functor that exports its argument as an alias, the typechecker crashes. This is discussed in <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/11441" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/issues/11441" target="_blank">#11441</a><br>( crashes the typechecker in current OCaml )<br>
module F (Type : sig module type T end)(Y : Type.T) = Y<br>module Crash (Y : sig end) =<br>
F(struct module type T = sig module X = Y end end)<br><br>The use of abstract signatures clashes with applicativity of functors, as discussed <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/12204" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/ocaml/issues/12204" target="_blank">in #12204</a>.<br><br>Another known issue is that the typechecker can abstract a signature when it contains unreachable type fields (types pointing to anonymous modules). This can lead to the production of invalid signatures : signatures that are refused by the typechecker when re-entered back in.<br>module F (Y: sig type t end) =<br>
struct<br>
module type A = sig<br>
type t = Y.t ( this will force the abstraction of all of A )<br>
type u<br>
end<br>
module X : A = struct type t = Y.t type u = int end<br>
type u = X.u<br>
end<br>module Test = F(struct type t end)<br>( returns )<br>
module Test : sig module type A module X : A type u = X.u end<br>Here, the type field type u = X.u is invalid as X has an abstract signature (and therefore, no fields).<br><br>In this section we explore solutions for fixing the issues of the current approach. The core criticism we make of the OCaml approach is that it is actually too expressive for its own good. Abstract signatures are impredicative: they can be instantiated by themselves. Having impredicative instantiation with variant reinterpretation is hard to track for the user and interacts in very subtle ways with other features of the module system, slowing down its development—and breaking its theoretical properties. To address this, we take the opposite stance and propose to make the system actually predicative: we restrict the set of signatures that can be used to instantiate an abstract signature. This also indirectly addresses the complexity of the variant reinterpretation.<br>We start with the simplest solution where instantiation of abstract signatures is restricted to signatures containing no abstract fields. Then, we propose to relax this restriction and allow for signatures that contain abstract type fields (but no abstract module types), which we call simple signatures. This will requires us to briefly discuss the need for module-level sharing.<br>In this section we focus on the theoretical aspects, but present them informally with examples. The practical aspects, notably syntax and inference, are discussed in the next section.<br><br>One might wonder why abstract types and abstract signatures syntactically resembles one another and yet, the latter is much more complex than the former. The key lies in the fact that abstract types can only be instantiated by concrete type expressions, without free variables. Informally, this:<br>sig<br>
type t<br>
val x : t<br>
val f : t -&gt; t<br>
end with type t = (int * 'a)<br>is not allowed, notably because (1) the scope of the abstract type variable 'a is unclear, (2) values of type t, like x, would be ill-typed.<br>Therefore, a first solution is to require abstract signatures to be instantiated only by concrete signatures, i.e.&nbsp;signatures with no abstract fields (neither types nor module types). This circumvents the clash between the rewriting and variant reinterpretation of abstract fields (by disallowing them).<br>This is simple and sound but prevents some valid uses of abstract types: in the first example, UNSAFE could not be instantiated with abstract type fields, forcing UDP1 and UDP2 to have the same type definitions.<br><br>If we want to relax the no-abstraction proposal, some abstract fields will be allowed when instantiating signatures. Then, the question of what sharing (i.e., type equalities) should be kept between different occurrences of the abstract fields arises.<br>In OCaml signatures, sharing between two modules is usually expressed at the core-level by rewriting the fields of the signature of the second module to refer to their counterpart in the first one. This cannot be done with abstract signatures, as they have no fields. Instead, the language needs module-level sharing, which in OCaml is very restricted. Indeed, it provides a form of module aliases (only for submodules, not at the top-level of a signature), but aliasing between a functor body and its parameter is not allowed—while it is typically the use-case for abstract signatures in polymorphic functors. Consider the following code:<br>( Code )<br>
module F1 (Y: sig module type A module X : A end) = Y.X<br>
module F2 (Y: sig module type A module X : A end) = (Y.X : Y.A)<br>Currently, the typechecker cannot distinguish between the two and returns the same signature, while we would expect the first one to keep the sharing between the parameter and the body.<br>( Currently, both are given the same type: )<br>
module F1 (Y: sig module type A module X : A end) : A<br>
module F2 (Y: sig module type A module X : A end) : A<br>As an example, we can consider the argument for the functors:<br>module Y = struct<br>
module type A = sig type t end<br>
module X = struct type t = int end<br>
end<br>module Test1 = F1(Y)<br>
module Test2 = F2(Y)<br>This returns :<br>module Test1 : sig type t end<br>
module Test2 : sig type t end<br>While we would expect :<br>module Test1 : sig type t = int end<br>
module Test2 : sig type t end<br>Two possible extensions would help tackle this issue.<br><br>A recently proposed experimentation, named lazy strengthening, extends the signature language with an operator S with P, where S is a signature and P a module path. It is interpreted as S strengthened by P, i.e.&nbsp;S in which all abstract fields are rewritten to point to their counterpart in P. Initially considered for performance reasons, it would allow for tracking of type equalities when using abstract signatures.<br>( Lazy strengthening would keep type equalities: )<br>
module F1 (Y: sig module type A module X : A end) = Y.A with Y.X<br><br>A more involved solution is the use of an extension of aliasing called transparent ascription, where both the alias and the signature are stored in the signature. The signature language would be extended with an operator (= P &lt; S). The technical implications of this choice are beyond the scope of this discussion.<br>( Transparent ascription would keep module equalities: )<br>
module F1 (Y: sig module type A module X : A end) : (= Y.X &lt; Y.A)<br><br>Maintaining a predicative approach, we propose to restrict instantiation only by simple signatures, i.e., signatures that may contain abstract type fields, but no abstract module types. This reintroduces the need to express module-level sharing and the mental gymnastic of variant re-interpretation of abstract type fields. However, it guarantees that all modules sharing the same abstract signature will also share the same structure (same fields) after instantiation, and can only differ in their type fields. We believe this makes for a good compromise.<br><br>One might wonder how restrictive is this proposal. Specifically, if we consider a simple polymorphic functor as:<br>module Apply (Y : sig module type A end) (F : Y.A -&gt; Y.A)(X : Y.A) = F(X)<br>The following partial application would be rejected:<br>( Rejected as A would be instantiated by sig module type B module X : B -&gt; B end )<br>
module Apply' = Apply(struct module type A = sig module type B module X : B -&gt; B end end)<br>However, this could be circumvented by eta-expanding, thus expliciting module type parameters, and instantiating only a simple signature:<br>( Accepted as A is instantiated by a signature with no abstract fields )<br>
module Apply'' = functor (Y:sig module type B end) -&gt;<br>
Apply(struct module type A = sig module type B = Y.B module X : B -&gt; B end end)<br><br>Concrete and simple signatures can be seen as the first two levels of the predicative approach for types declarations. There are no more levels for type declarations, as types cannot be partially abstract (see 3.1). Could it be useful to add even more expressivity and authorize instantiation by a signature containing again an abstract module type field (which would need to be restricted with a level system like universes)? We have found no example where this was useful. Besides, it would add a great layer of complexity.<br><br><br>A key aspect of abstract module types that reduces their usability is the verbosity of the syntax. Rather than having to pass signature as part of a module argument to a polymorphic functor, using a separate notation for module type parameters could be more concise. In practice, abstract signature arguments could be indicated by using brackets instead of parenthesis, and interleaved with normal module arguments, as in this example:<br>( At definition )<br>
module MakeApply<br>
[A] (X:A)<br>
[B] (F: A -&gt; B)<br>
[C] (H : sig module Make : B -&gt; C end)<br>
= H.Make(F(X))<br>module ApplyGv<br>
[A] [B] (F:A -&gt; GlobalVars -&gt; B) (X:A)<br>
= F(X)(Gv)<br>( At the call site )<br>
module M1 = MakeApply<br>
[T] (X)<br>
[Hashtbl.HashedType] (F)<br>
[Hashtbl.S] (Hashtbl)<br>module M2 = ApplyGv [A] [B] (F) (X)<br>Technically, this is not just syntactic sugar for anonymous parameters due to the fact that OCaml relies on names for applicativity of functors.<br><br>Following up on the previous point, usability of abstract signatures could even be improved with some form of inference at call sites. Further work is needed to understand to what extend this could be done.<br><br>We have presented the feature of abstract signatures in OCaml. After showing use cases via examples, we explained the issues associated with the unrestricted syntactical approach. Then, we propose a new specification: simple abstract signatures. In addition to making the behavior of abstract signatures much more predictable for the user, this approach can be fully formalized by translation into Fω (extended with predicative kinds).<br>As stated above, our goal here was both to sum up the current state and our proposal, but also to gather feedback from users or potential users. In particular, we want to see if it can indeed cover all use cases, and if we missed other usability problems.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/rethinking-ocaml-abstract-signatures.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Rethinking OCaml abstract signatures.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:03 GMT</pubDate></item><item><title><![CDATA[Scopes and effect handlers]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <br>This document was written because I learned something surprisingly obvious about effect handlers. It may help others since I haven't seen this blind spot discussed before.<br>While this begins with a quick introduction to effects, I am going to skip on a lot of details so you should get your hands dirty if you are not already familiar with the topic. It's really fun and if you want to follow along, this local environment works perfectly:<br>$ mkdir effects &amp;&amp; cd effects<br>
effects/ $ opam switch create '.' 'ocaml-variants.4.12.0+domains+effects'<br>
effects/ $ eval $(opam env)<br>The effect system makes good use of advanced OCaml features, so I will try to explain them as we go. You may want to read <a rel="noopener nofollow" class="external-link" href="https://github.com/ocamllabs/ocaml-effects-tutorial" target="_blank">https://github.com/ocamllabs/ocaml-effects-tutorial</a> as a better introduction and <a rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/effects-examples" target="_blank">https://github.com/ocaml-multicore/effects-examples</a> to get hyped. You can also try eio from this local switch.<br>But if you are already familiar with those topics, then the trickery is near the end!<br>Let's begin! When playing with "pure" effects, you will see this weird pattern a lot: A match within parenthesis, where each case produces a lambda. The initial arguments of the lambda are given at the very end after the parenthesis. Each continue expects to receive the arguments of the lambda and is an opportunity to update them.<br>( match ... with<br>
| result -&gt; ( executed last )<br>
(fun _xN _yN -&gt; result)<br>  | effect (E ...) k -&gt; ( in the middle )<br>
(fun x1 y1 -&gt;<br>
...<br>
continue k (response)  x2 y2<br>
...)<br>
)<br>
x0  ( initial state )<br>
y0<br>As you can see, the flow is a bit reversed but you will get used to it very fast in practice.<br>Performing an effect is a lot like raising an exception: The corresponding case in the match will receive the argument of perform and a continuation k.<br>If you ignore k, then the effect behaves exactly like a standard exception.
But you can also call continue k with an argument, in which case the original perform will evaluate to that argument and the program will continue as if the effect never happened.
<br>The only catch is that you can only continue the continuation once.<br>As a simple example, let's implement a "counter" effect. Each time we perform the Incr effect, we will receive the next natural number:<br>effect Incr : int<br>let withcounter fn =<br>
(match fn () with<br>
| result -&gt;<br>
(fun  -&gt; result)<br>   | effect Incr k -&gt;<br>
(fun (counter : int) -&gt; continue k counter (counter + 1))<br>
)<br>
0<br>The with_counter takes a function as argument and calls it within the match ... with. This ensures that effects performed by fn will be caught and handled.<br>Notice how the continue takes both counter and counter + 1? The first one is the response, while the second is updating our counter state for the next time. The initial state of the counter is 0, provided on the very last line.<br>Once this function is defined, you can use it as follow: We first set up the with_counter handler, then each perform Incr triggers this effect handler and receive the new value of the counter.<br>with_counter (fun () -&gt;<br>
Printf.printf "%i\n" (perform Incr) ; ( 0 )<br>
Printf.printf "%i\n" (perform Incr) ; ( 1 )<br>
Printf.printf "%i\n" (perform Incr) ; ( 2 )<br>
Printf.printf "%i\n" (perform Incr) ; ( 3 )<br>
)<br>If we forget to wrap this code within with_counter, then we will get an Unhandled exception!<br>Side note: In OCaml there are two cute syntaxes when you have a function that takes a function as a last argument. The most common is the application (@@) operator:

with_counter @@ fun () -&gt;
Printf.printf "%i\n" (perform Incr) ; (* 0 *)
Printf.printf "%i\n" (perform Incr) ; (* 1 *)

But you can also define the ( let@ ) syntax notation:

let ( let@ ) f x = f x ;;

let@ () = with_counter in
Printf.printf "counter = %i\n" (perform Incr) ; (* 0 *)
Printf.printf "counter = %i\n" (perform Incr) ; (* 1 *)

Both can drastically reduce indentation and help clarify your code!
<br>Back to effects: If we use multiple with_counter in our code, each one will start from 0 and if we nest them, it's always the nearest one that will handle the Incr effect:<br>with_counter (fun () -&gt;<br>
Printf.printf "counter = %i\n" (perform Incr) ;  ( 0 )<br>
Printf.printf "counter = %i\n" (perform Incr) ;  ( 1 )<br>
with_counter (fun () -&gt;<br>
Printf.printf "nested = %i\n" (perform Incr) ; ( 0 )<br>
( we can't ask the outer counter from here )<br>
) ;<br>
Printf.printf "counter = %i\n" (perform Incr) ;  ( 2 )<br>
)<br>How could we reach the outer counter from the nested one? The question is a bit weird because something is missing: We have no way of telling the system that we want to perform this operation. We would need some way of "naming" which counter we want to increment… such that we could write this code:<br>with_counter (fun x -&gt;<br>
Printf.printf "x = %i\n" (x ()) ; ( 0 )<br>
Printf.printf "x = %i\n" (x ()) ; ( 1 )<br>
with_counter (fun y -&gt;<br>
Printf.printf "y = %i\n" (y ()) ; ( 0 )<br>
( we can now ask the outer counter! )<br>
Printf.printf "x = %i\n" (x ()) ; ( 2 )<br>
) ;<br>
Printf.printf "x = %i\n" (x ()) ; ( 3 )<br>
)<br>Let's implement this new with_counter. It's exactly the same code as before, but with a twist: We don't use the global Incr effect anymore, but rather we create a new local effect specific to each counter.<br>let withcounter fn =<br>
let open struct<br>
effect Local_incr : int<br>
end in<br>
(match fn (fun () -&gt; perform Local_incr) with<br>
| result -&gt;<br>
(fun  -&gt; result)<br>   | effect Local_incr k -&gt;<br>
(fun (counter : int) -&gt; continue k counter (counter + 1))<br>
)<br>
0<br>The let open struct ... end in syntax allow us to dynamically create new types, exceptions, and effects, that will only be available locally. We pass the counter as (fun () -&gt; perform Local_incr) to the user function fn and handle its effects as before. When the user calls x (), it performs the corresponding Local_incr for which there is only one handler available… so there's no confusion as to which counter should be incremented.<br>This might not be entirely obvious: when we create multiple counters, we define multiple Local_incr effects that have the same "name" in the code, but at runtime each one will be unique. We setup one effect handler for each of our counter, but since they match on different effects, they can't intercept messages intended to another.<br>If all of this makes sense, let's move on to implementing references. This isn't so different from the counter example, since both allow us to query and update state. The big difficulty is that references are not limited to storing int values.. we want them to be polymorphic.<br>Here is an example usage, showing that we can create references for different types of values:<br>let@ x = with_eref 42 in<br>
let@ y = with_eref "hello" in<br>
Printf.printf "x = %i, y = %S\n" (x.get ()) (y.get ()) ;  (  42, "hello" )<br>
x.put 666 ;<br>
Printf.printf "x = %i, y = %S\n" (x.get ()) (y.get ()) ;  ( 666, "hello" )<br>
y.put "bye" ;<br>
Printf.printf "x = %i, y = %S\n" (x.get ()) (y.get ()) ;  ( 666, "bye" )<br>Once again, we will hide the actual effects from the user: This time there are two operations get and put, so we package them in a record:<br>type 'a eref = ( the type of an effectful reference holding an ['a] value )<br>
{ get : unit -&gt; 'a<br>
; put : 'a -&gt; unit<br>
}<br>The effect handler uses the same trick of locally defined effects as the counter and the code is very similar:<br>let with_eref<br>
: type a.  a -&gt; (a eref -&gt; 'b) -&gt; 'b<br>
= fun initial_value fn -&gt;<br>  ( local effects for this ['a] reference )<br>
let open struct<br>
effect Get : a<br>
effect Put : a -&gt; unit<br>
end in<br>  ( hide the details from the user )<br>
let eref =<br>
{ get = (fun () -&gt; perform Get)<br>
; put = (fun v -&gt; perform (Put v))<br>
}<br>
in<br>  (match fn eref with<br>
| x -&gt;<br>
(fun _ -&gt; x)<br>  | effect Get k -&gt;<br>
(fun (current_value : a) -&gt; continue k current_value current_value)<br>  | effect (Put newvalue) k -&gt;<br>
(fun  -&gt; continue k () new_value)<br>  ) initial_value<br>The last lines are pretty important since they define the semantic of our references, but there is nothing new here.<br>The main difficulty is convincing the typechecker that references can hold different types of values. It happens on the second line where we specify the type of with_eref:<br>let with_eref<br>
: type a.  a -&gt; (a eref -&gt; 'b) -&gt; 'b<br>The type a. notation is explicitly introducing the polymorphic type variable a (when we would normally use 'a). Most importantly, this notation binds the type variable a for the rest of the code, so we are able to define local effects that depend on it:<br>let open struct<br>
effect Get : a<br>
effect Put : a -&gt; unit<br>
end in<br>(You might want to try removing type a and using 'a instead to see the difference!)<br>Side note: There's an alternative notation for binding type variables if we don't want to spell out the full type of the function with_eref, but it comes with a lot of gotchas so I don't recommend it:

let with_eref (type a) fn = ...
<br>It's very impressive how algebraic effects are able to benefit from advanced features of OCaml that were not created for them!<br>While I hope this introduction was clear… I must now reveal that those examples are actually a really bad use of algebraic effects.<br>The counters and references might have given you the wrong idea about algrebraic effects: They are not a real replacement for impure mutable side effects! In fact, they come with a very strong limitation when compared to the standard references of OCaml.<br>Remember the Unhandled exception when we try to perform an effect without first setting up a handler?<br>let escape = with_eref 42 (fun r -&gt; r) in<br>
escape.put 666<br>The effectful reference is created by with_eref… but then it escapes its effect handler. When we then try to update the eref, the Put 666 effect is not handled by anyone and crashes the program.<br>In other words, the "pure" references are only usable from within the scope of their handler. A standard OCaml ref on the other hand will always be available… until it is collected by the GC, but at this point if was already clear that it wasn't going to be used again.<br>The typesystem can't yet protect us from performing effects outside their handler, so you have to be careful. You might think that it's pretty easy to visually check that escape.get () only happens within the with_eref, but that's exactly why I'm writing this document. It's not!<br>In the mean time, the use of algebraic effects to simulate impure references kind of misses the point. Effects are a lot more useful for scripting control flow rather than value flow. Typically, this will involve changing the normal execution of our programs to create enumerators, coroutines, async/await, green thread, etc, as libraries. And it's going to be insanely cool!<br>This article is already too long to present a real scheduler as a more interesting use of algebraic effects. But we can cheat and create a really crappy one!<br>effect Spawn : (unit -&gt; unit) -&gt; unit<br>The Spawn (fun () -&gt; ...) effect is intended to create a new thread of execution that will execute concurrently with the rest of our code:<br>let spawn f = perform (Spawn f) ;;<br>spawn (fun () -&gt; sleep 60 ; print_endline "A") ;<br>
spawn (fun () -&gt; print_endline "B") ;<br>But since we won't do a real scheduler, we'll just evaluate the spawned thread right away before moving on:<br>let with_spawn fn =<br>
match fn () with<br>
| v -&gt; v<br>
| effect (Spawn child) k -&gt;<br>
child () ;<br>
continue k ()<br>(A real implementation would store the spawned child somewhere, to evaluate it later, with an additional Yield effect to cooperatively switch between threads…)<br>It may not be obvious, but even this simple handler has a small bug: A spawned child can't itself perform a Spawn effect! That's easy enough to fix:<br>let rec with_spawn fn =<br>
match fn () with<br>
| v -&gt; v<br>
| effect (Spawn child) k -&gt;<br>
with_spawn child ;<br>
continue k ()<br>Now let's see how this works with the effectful references we defined before. We are not doing anything fancy, so it's easy to see that the following spawn will execute before we try printing the value of the reference:<br>with_spawn @@ fun () -&gt;<br>
with_eref 42 @@ fun x -&gt;<br>
spawn (fun () -&gt; x.put 666) ;<br>
Printf.printf "x = %i\n" (x.get ()) ( 666 or 42? )<br>Clearly, this should give us 666 right? Well… The function (fun () -&gt; x.put 666) will be executed by the with_spawn handler, outside of the with_eref handler! As a result, its Put 666 effect will not be handled by anyone and will crash with an Unhandled exception. The fact that the function (fun () -&gt; x.put 666) is defined inside the with_eref doesn't matter at all since we performed a spawn effect to evaluate it in the scheduler (1).<br>The correct order of effect handlers should be reversed:<br>with_eref 42 @@ fun x -&gt;<br>
with_spawn @@ fun () -&gt;<br>
spawn (fun () -&gt; x.put 666) ;<br>
Printf.printf "x = %i\n" (x.get ())<br>But that's just weird: we would normally setup the scheduler at the beginning of our main function, then setup the local effect handlers for the shorter lived effects like references.<br>And what would be the fix when there's a dynamic number of references? While the following code isn't great with a real scheduler because we don't wait for the threads to terminate before reading their outcome, that's not the reason why it doesn't work here:<br>with_spawn @@ fun () -&gt;<br>  let rec parmap f acc = function<br>
| [] -&gt;<br>
List.map (fun r -&gt; r.get ()) acc<br>| x::xs -&gt;
    let@ r = with_eref None in
    spawn (fun () -&gt; r.put (Some (f x))) ; (* Unhandled! *)
    parmap f (r :: acc) xs
<br>  in<br>
parmap (fun x -&gt; x * x) [1;2;3]<br>I want to state again that this is a self-injured issue, the problem doesn't exist with real references.<br>But it is however a control flow issue! The with_eref are not executed at the right time during the lifetime of the program… And we can solve this with more effects!<br>type 'a cc = { cc : 'b. (('b -&gt; 'a) -&gt; 'a) -&gt; 'b }<br>let with_scope (type a) fn =<br>
let open struct<br>
effect Lift : (('b -&gt; a) -&gt; a) -&gt; 'b<br>
end in<br>
match fn { cc = fun s -&gt; perform (Lift s) } with<br>
| v -&gt; v<br>
| effect (Lift s) k -&gt; s (fun x -&gt; continue k x)<br>This combines a bit of everything we saw before, and even adds an explicit forall with the type cc. The type trickery is required to maximize the usefulness of this handler, as otherwise the type would be too restrictive for some programs (2).<br>This is dark magic from scheme sorcerers, so it's easier to understand what's going on by looking at its usage. The with_scope allow us to add dynamic effect handlers around the scheduler:<br>with_scope @@ fun lift -&gt;<br>
with_spawn @@ fun () -&gt;<br>  let x =<br>
lift.cc @@ fun return -&gt;<br>
with_eref 42 @@ fun r -&gt; return r<br>
in<br>  spawn (fun () -&gt; x.put 666) ; ( works! )<br>  Printf.printf "x = %i\n" (x.get ()) ( 666 )<br>The lift.cc will apply the with_eref effect handler at the position of the with_scope, at which point execution will continue normally. When the scheduler executes the spawned function, it does so at the position of the with_spawn… so our reference effects will work correctly since the with_eref was lifted just above!<br>So even though it looks like we are using the reference x outside the scope of its with_eref, it actually became globally available. This lifting isn't free however: Even if the variable x can be reclaimed at some point, its referenced value and its effect handler will be held until we exit with_scope.<br>In conclusion, you can clearly do some insane stuff with effects!.. But it wasn't obvious to me that it would be so hard to tell when an effect handler is in scope.<br>(1) Here's a minimal example of escaping the scope of an event handler. The continuation of A doesn't capture the B effect handler, so its evaluation from C triggers an Unhandled exception:<br>effect A : unit<br>
effect B : unit<br>
effect C : (unit, unit) continuation -&gt; unit eff<br>let () =<br>
match<br>
match<br>
match perform A ; perform B with<br>
| () -&gt; ()<br>
| effect A k -&gt; perform (C k)<br>
with<br>
| () -&gt; ()<br>
| effect B k -&gt; continue k ()<br>
with<br>
| () -&gt; ()<br>
| effect (C child) k -&gt;<br>
continue child () ; ( Unhandled B! )<br>
continue k ()<br>this is not a bug with effects, but a feature! they are really one-shot delimited continuation, so keeping the b handler in scope would require duplicating the stack.<br>(2) if you add the creation of another reference with a different type, you will quickly discover why we need the forall in cc:<br>  let y =
    lift.cc @@ fun return -&gt;
    with_eref "hello" @@ fun r -&gt; return r
  in
  ...

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/scopes-and-effect-handlers.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Scopes and effect handlers.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 16 Nov 2024 12:37:43 GMT</pubDate></item><item><title><![CDATA[Tail recursion modulo cons]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a><br>If the last action of a function ff is to call another function gg, the language run-time doesn't need to keep ff's stack frame around when calling gg:<br>let f n =
  Printf.printf "Hello, World!";
  g (n + 1)
<br>Instead, the run-time may `re-purpose' ff's stack frame for gg, saving space and time in stack (de)allocations. This optimisation, known as tail call elimination, is useful in many language paradigms. It is useful in functional programming languages for which recursion is the idiomatic way to repeat actions.<br>Unfortunately, many standard uses of recursion are not tail-call optimisable:<br>let rec map f = function
  | [] -&gt; []
  | x :: xs -&gt;
     let y = f x in
     y :: map f xs
<br>The x::xsx::xs case proceeds as follows:<br>
<br>Compute y:=f(x)y:=f(x);
<br>Recursively compute t1:=map&nbsp;f&nbsp;xst1​:=map&nbsp;f&nbsp;xs;
<br>Allocate t2:=y::t1t2​:=y::t1​ on the heap;
<br>Return t2t2​.
<br>Step 3 prevents the tail-call optimisation: the runtime must build the list node after computing the tail with map f xs. Our map function is almost tail-recursive: if not for the data constructor (::), it would be. We call such functions 'tail recursive modulo cons'. There are two ways to make map fully tail-recursive:<br>
<br>We could build the result list in reverse order, then reverse it in one pass at the end. This is not ideal since our intermediate list requires time to build and creates work for the garbage collector.<br>

<br>We could change the list type to allow us to build the list node first, and later fill in the correct tail. In OCaml, this needs a ref indirection.<br>

<br>Let's try the latter approach. We introduce a ref and pass the 'tail to be filled in later' as an explicit argument:<br>type 'a mutable_list = (::) of 'a * 'a mutable_list ref | []

let rec map f xs =
    let rec inner res = function
     | [] -&gt; ()
     | x :: xs -&gt;
        let y = f x in
        (* create an 'incomplete' list node *)
        let tail = ref [] in
        res := y :: tail;
        inner tail !xs (* tail call! *)
    in
    let res = ref [] in
    inner res xs; !res
<br>Putting 'incomplete' values into the heap as a user requires changing the list type to contain references, but the OCaml runtime doesn't have the same restriction! It can choose to modify 'immutable' heap contents if it wants, allowing our original map to be compiled to take O(1)O(1)-space and not generate any garbage!<br>The transformation given above can be applied whenever a function is 'tail recursive modulo cons': whenever the only actions after the last function call are heap allocations. The OCaml compiler doesn't yet make this optimisation, but it could! There are interesting details to be fixed, such as what happens when a garbage collection happens in the middle of the TRMC recursion.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fn-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fn-1" target="_blank">1</a><br>
Many thanks to <a data-tooltip-position="top" aria-label="https://discord.com/users/327286755562618891/" rel="noopener nofollow" class="external-link" href="https://discord.com/users/327286755562618891/" target="_blank">@Splingush</a> for corrections to this post.
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="http://gallium.inria.fr/seminaires/transparents/20141027.Frederic.Bour.pdf" target="_blank">http://gallium.inria.fr/seminaires/transparents/20141027.Frederic.Bour.pdf</a><a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fnref-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fnref-1" target="_blank">↩</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/tail-recursion-modulo-cons.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Tail recursion modulo cons.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:03 GMT</pubDate></item><item><title><![CDATA[Testing manpages]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>Dune supports a <a data-tooltip-position="top" aria-label="https://dune.readthedocs.io/en/stable/concepts.html#diffing-and-promotion" rel="noopener nofollow" class="external-link" href="https://dune.readthedocs.io/en/stable/concepts.html#diffing-and-promotion" target="_blank"><code></code> action</a>diff that compares two files aa and bb and fails if aa ≠ bb. The magic of this action is that it allows the user to set a:=ba:=b if aa is a source file and bb is a generated file. This is used under the hood for code formatting with dune build @fmt:<br>
<br>for each file aa, generate a formatted file bb;
<br>assert that a=ba=b;
<br>if not, the user may run dune promote to set a:=ba:=b.
<br>The diff action can also be used to write 'self-correcting' tests: write a series of tests with some expected output; run each test and diff the output with an expected output; if any of the errors are expected, run dune promote to auto-correct the test.<br>One particularly useful type of 'self-correcting' test is an assertion of the output of the --help option to a binary. Snapshot the --help output in a help.txt file and diff it against the true --help output each time you run your tests. This has two advantages:<br>
<br>you are certain of how any PR will change your program's CLI;<br>

<br>the help.txt file serves as documentation that is guaranteed to be up-to-date.<br>

<br>For example, here's how it's being done in <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/oskel" rel="noopener nofollow" class="external-link" href="https://github.com/CraigFe/oskel" target="_blank">CraigFe/oskel</a>:<br>(rule
 (with-stdout-to
  oskel-help.txt.gen
  (run oskel --help=plain)))

(rule
 (alias runtest)
 (action
  (diff oskel-help.txt oskel-help.txt.gen)))
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/testing-manpages.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Testing manpages.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:03 GMT</pubDate></item><item><title><![CDATA[The _intf trick]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>
In this post, I explain a trick for avoiding duplication of types between .ml and .mli files that will be familiar to anyone who's worked with Jane Street codebases.
<br><br>OCaml compilation units live a double life: one as source code (foo.ml) and one as header information (foo.mli).<br><img src="https://www.craigfe.io/posts/the-intf-trick/two_files.svg" referrerpolicy="no-referrer"><br>This works well in encouraging abstraction, so you'll often see less type information in the .mli than in the .ml, but any types that are not abstracted are duplicated. This is a big deal for functor-heavy projects, since large module types will end up being duplicated across the two files.<br>There's a couple of standard mitigations for this:<br>
<br>move all of your types into a single file with no corresponding .mli.
  Each foo.{ml,mli} file can now alias the types and module types from a central s.ml or types.ml file. Unfortunately, all those types are now defined separately from their point-of-use, making your codebase harder to understand and less scalable.<br>

<br>minimise the number of module types being defined.
  Since we're paying twice for each module type we define, it's natural to want to define as few of them as possible. For instance, we might avoid defining a MAKER type for our Make functor and just keep the constraints in the .mli file instead. Unfortunately, this hides the constraints from <a data-tooltip-position="top" aria-label="https://github.com/ocaml/merlin" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/merlin" target="_blank">Merlin</a>, so you won't discover any discrepancies until compile time:<br>

<br>(* --- foo.mli -------------------------------------------------------------- *)

module Make (A: Arg.S) : S with type arg = A.t

(* --- foo.ml --------------------------------------------------------------- *)

module Make (A : Arg.S) = struct

  (* We must define [type arg = A.t], but Merlin doesn't know this *)
  type arg = string

end
<br>Both of these mitigations have their drawbacks. If only our foo.ml could refer to the module types defined in foo.mli. Hmm...<br><br>As with <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" target="_blank">most problems</a>, we can solve this with another layer of indirection. We add a third file, named foo_intf.ml. This file holds types and signatures, so is like our old foo.mli file, but has the distinct advantage that foo.ml can pull types from it:<br><img src="https://www.craigfe.io/posts/the-intf-trick/three_files.svg" referrerpolicy="no-referrer"><br>Now our types are defined in exactly one place, with no unnecessary duplication. The foo_intf.ml file contains all of the types required by foo.ml and also defines a special module type Intf to act as the public interface.<br>(* --- foo_intf.ml ---------------------------------------------------------- *)

(* Type definitions go here: *)

module type S = sig ... end
module type MAKER = functor (A: Arg.S) -&gt; S with type arg = A.t
type error = [ `Msg of string | `Code of int ]

(* The interface of [foo.ml]: *)

module type Intf = sig
  type error
  (** [error] is the type of errors returned by {!S}. *)

  module type S = S
  module type MAKER = MAKER

  module Make : MAKER
end

(* --- foo.ml --------------------------------------------------------------- *)

(* Fetch module types and type definitions from the [_intf] file *)
include Foo_intf

(* Implementation here as normal *)
module Make : MAKER = functor (A : Arg.S) -&gt; struct ... end

(* --- foo.mli -------------------------------------------------------------- *)

include Foo_intf.Intf (** @inline *)
<br>There are some nice advantages to this approach:<br>
<br>
We've avoided duplicate definitions of foo's module types and kept them in the foo* namespace in our source tree. The code is now easier to change<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/the-intf-trick#fn-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/the-intf-trick#fn-1" target="_blank">1</a> and easier to understand.

<br>
Since we no longer have to minimise our use of module types, we can give the types of functors at the point of definition (module Make : MAKER = ...). This style works better with Merlin.

<br>The _intf style is commonly used in Jane Street packages (c.f. <a data-tooltip-position="top" aria-label="https://github.com/janestreet/higher_kinded/tree/master/src" rel="noopener nofollow" class="external-link" href="https://github.com/janestreet/higher_kinded/tree/master/src" target="_blank"><code></code></a>higher_kinded, <a data-tooltip-position="top" aria-label="https://github.com/janestreet/base/tree/master/src" rel="noopener nofollow" class="external-link" href="https://github.com/janestreet/base/tree/master/src" target="_blank"><code></code></a>base, <a data-tooltip-position="top" aria-label="https://github.com/janestreet/core/tree/master/src" rel="noopener nofollow" class="external-link" href="https://github.com/janestreet/core/tree/master/src" target="_blank"><code></code></a>core). Note that it's typically only used for files that export module types, for which this trick is most effective.<br>I hope you find this technique useful in making your OCaml code more concise and less frustrating to work with.<br><br><br>Use of the _intf trick is an implementation detail that (ideally) shouldn't be exposed in your documentation. At time of writing, Odoc renders all includes and type aliases with links to the source definition. In the case of included module types, you can use the @inline annotation to prevent Odoc from displaying the indirection:<br>include Foo_intf.Intf (** @inline *)
<br>Unfortunately: (a) there's no equivalent trick for plain type definitions, and (b) any cross-references between module types will link to the true definition. This leaves you with rendered output like the following:<br>module Make : functor (Input : Foo__.Foo_intf.INPUT) -&gt; S
<br>where INPUT is defined in the Foo_intf file but accessible to the user as Foo.INPUT (via an alias).<br>Fortunately, the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/odoc/pull/439" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml/odoc/pull/439" target="_blank">new Odoc model</a> solves this problem by generating links to "canonical" definitions of types, which are never taken from hidden modules (those with double underscores like Foo__). At time of writing, this new model hasn't yet been released.<br><br>An interesting side-effect of being able to reference interfaces from implementations is that you can use them to kick-start initial development on a file. If your development process begins by defining signatures, the .ml + .mli workflow requires a secondary step of "add stub implementations of everything" to sneak past the type-checker:<br>(* --- stack.mli ------------------------------------------------------------ *)

type 'a t
val empty : 'a t
val push : 'a t -&gt; 'a -&gt; 'a t
val pop : 'a t -&gt; ('a t * 'a) option

(* --- stack.ml ------------------------------------------------------------- *)

type 'a t
let empty = failwith "TODO"
let push = failwith "TODO"
let pop = failwith "TODO"
<br>With an _intf file, we can provide all of these stubs in one go:<br>(* --- stack.ml ------------------------------------------------------------- *)

include (val (failwith "TODO") : Stack_intf.Intf)
<br>(I learned about this trick from a <a data-tooltip-position="top" aria-label="https://blog.janestreet.com/simple-top-down-development-in-ocaml/" rel="noopener nofollow" class="external-link" href="https://blog.janestreet.com/simple-top-down-development-in-ocaml/" target="_blank">blog post</a> by Carl Eastlund.)<br><br>Emacs users with tuareg-mode can use tuareg-find-alternate-file to quickly jump between corresponding .ml and .mli files. If you use this feature (as I do), you'll want it to be aware of _intf files. This can be done by customising the tuareg-find-alternate-file variable to include the correspondence &lt;foo&gt;.ml ↔ &lt;foo&gt;_intf.ml:<br>;; Add support for `foo_intf.ml' ↔ `foo.ml' in tuareg-find-alternate-file
(custom-set-variables
 '(tuareg-other-file-alist
   (quote
    (("\\.mli\\'" (".ml" ".mll" ".mly"))
     ("_intf.ml\\'" (".ml"))
     ("\\.ml\\'" (".mli" "_intf.ml"))
     ("\\.mll\\'" (".mli"))
     ("\\.mly\\'" (".mli"))
     ("\\.eliomi\\'" (".eliom"))
     ("\\.eliom\\'" (".eliomi"))))))
<br>If you're currently looking at some file foo.ml, tuareg-find-alternate-file will try to open foo_intf.ml and then foo.mli in that order. (If one of the two already has an open buffer, that will take priority.)<br><br>
<br>2020-06-10: changed the recommended name of the interface module type from Foo_intf.Foo to Foo_intf.Intf. In the time since I originally wrote this post, I've come to dislike the duplication of the module name using the Jane Street convention: in practice, Foo is often quite long and subjected to later renaming.
<br><br>
<br>via reducing the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Cognitive_dimensions_of_notations" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Cognitive_dimensions_of_notations" target="_blank">repetition viscosity</a> of our notation for types.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/the-intf-trick#fnref-1" rel="noopener nofollow" class="external-link" href="https://www.craigfe.io/posts/the-intf-trick#fnref-1" target="_blank">↩</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/the-_intf-trick.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/The _intf trick.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:05 GMT</pubDate><enclosure url="https://www.craigfe.io/posts/the-intf-trick/two_files.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://www.craigfe.io/posts/the-intf-trick/two_files.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Transitioning to Multicore with ThreadSanitizer]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a><br>The 5.0 release brought Multicore, Domain-based parallelism to the OCaml language. Parallel Domains performing uncoordinated operations on shared mutable memory locations may however cause data races. Such issues will unfortunately not <a data-tooltip-position="top" aria-label="https://blog.janestreet.com/oxidizing-ocaml-parallelism/" rel="noopener nofollow" class="external-link" href="https://blog.janestreet.com/oxidizing-ocaml-parallelism/" target="_blank">(yet)</a> be caught by OCaml's strong type system, meaning they may go unnoticed when introducing parallelism into an existing OCaml code base. In this guide, we will therefore study a step-wise workflow that utilises the <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/ocaml-tsan" rel="noopener nofollow" class="external-link" href="https://github.com/ocaml-multicore/ocaml-tsan" target="_blank">ThreadSanitizer (TSan)</a> tool to help make your OCaml code 5.x ready.<br>Note: TSan is currently only supported under Linux with AMD/Intel cpus. It furthermore requires at least GCC 11 or Clang 11 and the libunwind library.<br><br>Consider a little bank library with the following signature in bank.mli:<br>type t
(** a collective type representing a bank *)

val init : num_accounts:int -&gt; init_balance:int -&gt; t
(** [init ~num_accounts ~init_balance] creates a bank with [num_accounts] each
    containing [init_balance]. *)

val transfer : t -&gt; src_acc:int -&gt; dst_acc:int -&gt; amount:int -&gt; unit
(** [transfer t ~src_acc ~dst_acc ~amount] moves [amount] from account
    [src_acc] to account [dst_acc].
    @raise Invalid_argument if amount is not positive,
    if [src_acc] and [dst_acc] are the same, or if [src_acc] contains
    insufficient funds. *)

val iter_accounts : t -&gt; (account:int -&gt; balance:int -&gt; unit) -&gt; unit
(** [iter_accounts t f] applies [f] to each account from [t]
    one after another. *)
<br>Underneath the hood, the library may have been implemented in various ways. Consider the following thread-unsafe implementation in bank.ml:<br>type t = int array

let init ~num_accounts ~init_balance =
  Array.make num_accounts init_balance

let transfer t ~src_acc ~dst_acc ~amount =
  begin
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    if t.(src_acc) &lt; amount then raise (Invalid_argument "Not enough money on account");
    t.(src_acc) &lt;- t.(src_acc) - amount;
    t.(dst_acc) &lt;- t.(dst_acc) + amount;
  end

let iter_accounts t f = (* inspect the bank accounts *)
  Array.iteri (fun account balance -&gt; f ~account ~balance) t;
<br><br>Now if we want to see if this code is Multicore ready for OCaml 5.x, we can utilise the following workflow:<br>
<br>Install TSan
<br>Write a parallel test runner
<br>Run tests under TSan
<br>If TSan complains about data races, address the reported issue and go to step 2.
<br><br>We will now go through the proposed workflow for our example application.<br><br>For now, convenient 5.1.0+tsan and 5.0.0+tsan opam switches are available until TSan is officially included with the forthcoming 5.2.0 OCaml release. You can install such a TSan switch as follows:<br>opam switch create 5.1.0+tsan
<br><br>For a start, we can test our library under parallel usage by running two Domains in parallel. Here's a quick little test runner in bank_test.ml utilising this idea:<br>let num_accounts = 7

let money_shuffle t = (* simulate an economy *)
  for i = 1 to 10 do
    Unix.sleepf 0.1 ; (* wait for a network request *)
    let src_acc = i mod num_accounts in
    let dst_acc = (i*3+1) mod num_accounts in
    try Bank.transfer t ~src_acc ~dst_acc ~amount:1 (* transfer $1 *)
    with Invalid_argument _ -&gt; ()
  done

let print_balances t = (* inspect the bank accounts *)
  for _ = 1 to 12 do
    let sum = ref 0 in
    Bank.iter_accounts t
      (fun ~account ~balance -&gt; Format.printf "%i %3i " account balance; sum := !sum + balance);
    Format.printf "  total = %i @." !sum;
    Unix.sleepf 0.1;
  done

let _ =
  let t = Bank.init ~num_accounts ~init_balance:100 in
  (* run the simulation and the debug view in parallel *)
  [| Domain.spawn (fun () -&gt; money_shuffle t);
     Domain.spawn (fun () -&gt; print_balances t);
  |]
  |&gt; Array.iter Domain.join
<br>The runner creates a bank with 7 accounts containing $100 each and then runs two loops in parallel with:<br>
<br>One transfering money with money_shuffle
<br>Another one repeatedly printing the account balances with print_balances:
<br>$ opam switch 5.1.0
$ dune runtest
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 100 5 100 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5  99 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
<br>From the above run under a regular 5.1.0 compiler, one may get the impression that everything is OK, as the balances sum to a total of $700 as expected, indicating that no money is lost.<br><br>Let us now perform the same test run under TSan. Doing so is as simple as follows and immediately complains about races:<br>$ opam switch 5.1.0+tsan
$ dune runtest
File "test/dune", line 2, characters 7-16:
2 |  (name bank_test)
           ^^^^^^^^^
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 100 5 100 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5  99 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
==================
WARNING: ThreadSanitizer: data race (pid=26148)
  Write of size 8 at 0x7f5b0c0fd6d8 by thread T4 (mutexes: write M85):
    #0 camlBank.transfer_322 lib/bank.ml:11 (bank_test.exe+0x6de4d)
    #1 camlDune__exe__Bank_test.money_shuffle_270 test/bank_test.ml:8 (bank_test.exe+0x6d7c5)
    #2 camlStdlib__Domain.body_703 /home/opam/.opam/5.1.0+tsan/.opam-switch/build/ocaml-variants.5.1.0+tsan/stdlib/domain.ml:202 (bank_test.exe+0xb06b0)
    #3 caml_start_program &lt;null&gt; (bank_test.exe+0x13fdfb)
    #4 caml_callback_exn runtime/callback.c:197 (bank_test.exe+0x106053)
    #5 caml_callback runtime/callback.c:293 (bank_test.exe+0x106b70)
    #6 domain_thread_func runtime/domain.c:1102 (bank_test.exe+0x10a2b1)

  Previous read of size 8 at 0x7f5b0c0fd6d8 by thread T1 (mutexes: write M81):
    #0 camlStdlib__Array.iteri_367 /home/opam/.opam/5.1.0+tsan/.opam-switch/build/ocaml-variants.5.1.0+tsan/stdlib/array.ml:136 (bank_test.exe+0xa0f36)
    #1 camlDune__exe__Bank_test.print_balances_496 test/bank_test.ml:15 (bank_test.exe+0x6d8f4)
    #2 camlStdlib__Domain.body_703 /home/opam/.opam/5.1.0+tsan/.opam-switch/build/ocaml-variants.5.1.0+tsan/stdlib/domain.ml:202 (bank_test.exe+0xb06b0)
    #3 caml_start_program &lt;null&gt; (bank_test.exe+0x13fdfb)
    #4 caml_callback_exn runtime/callback.c:197 (bank_test.exe+0x106053)
    #5 caml_callback runtime/callback.c:293 (bank_test.exe+0x106b70)
    #6 domain_thread_func runtime/domain.c:1102 (bank_test.exe+0x10a2b1)

  [...]
<br>Notice we obtain a back trace of the two racing accesses, with<br>
<br>A write in one Domain coming from the array assignment in Bank.transfer
<br>A read in another Domain coming from a call to Stdlib.Array.iteri to read and print the array entries in print_balances.
<br><br>One way to address the reported races is to add a Mutex, ensuring exclusive access to the underlying array. A first attempt could be to wrap transfer and iter_accounts with lock-unlock calls as follows:<br>let lock = Mutex.create () (* addition *)

let transfer t ~src_acc ~dst_acc ~amount =
  begin
    Mutex.lock lock; (* addition *)
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    if t.(src_acc) &lt; amount then raise (Invalid_argument "Not enough money on account");
    t.(src_acc) &lt;- t.(src_acc) - amount;
    t.(dst_acc) &lt;- t.(dst_acc) + amount;
    Mutex.unlock lock; (* addition *)
  end

let iter_accounts t f = (* inspect the bank accounts *)
  Mutex.lock lock; (* addition *)
  Array.iteri (fun account balance -&gt; f ~account ~balance) t;
  Mutex.unlock lock (* addition *)
<br>Rerunning our tests, we obtain:<br>$ dune runtest
File "test/dune", line 2, characters 7-16:
2 |  (name bank_test)
           ^^^^^^^^^
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
Fatal error: exception Sys_error("Mutex.lock: Resource deadlock avoided")
<br>How come we may hit a resource deadlock error when adding just two pairs of Mutex.lock and Mutex.unlock calls?<br><br>Oh, wait! When raising an exception in transfer, we forgot to unlock the Mutex again. Let's adapt the function to do so:<br>let transfer t ~src_acc ~dst_acc ~amount =
  begin
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    Mutex.lock lock; (* addition *)
    if t.(src_acc) &lt; amount
    then (Mutex.unlock lock; (* addition *)
          raise (Invalid_argument "Not enough money on account"));
    t.(src_acc) &lt;- t.(src_acc) - amount;
    t.(dst_acc) &lt;- t.(dst_acc) + amount;
    Mutex.unlock lock; (* addition *)
  end
<br>We can now rerun our tests under TSan to confirm the fix:<br>$ dune runtest
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2 100 3 100 4 100 5  99 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
<br>This works well and TSan no longer complains, so our little library is ready for OCaml 5.x parallelism, hurrah!<br><br>The programming pattern of 'always-having-to-do-something-at-the-end' that we encountered with the missing Mutex.unlock is a recurring one for which OCaml offers a dedicate function:<br> Fun.protect : finally:(unit -&gt; unit) -&gt; (unit -&gt; 'a) -&gt; 'a
<br>Using Fun.protect, we could have written our final fix as follows:<br>let transfer t ~src_acc ~dst_acc ~amount =
  begin
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    Mutex.lock lock; (* addition *)
    Fun.protect ~finally:(fun () -&gt; Mutex.unlock lock) (* addition *)
      (fun () -&gt;
         begin
           if t.(src_acc) &lt; amount
           then raise (Invalid_argument "Not enough money on account");
           t.(src_acc) &lt;- t.(src_acc) - amount;
           t.(dst_acc) &lt;- t.(dst_acc) + amount;
         end)
  end
<br>Admittedly, using a Mutex to ensure exclusive access may be a bit heavy if performance is a concern. If this is the case, one option is to replace the underlying array with a lock-free data structure, such as the <a data-tooltip-position="top" aria-label="https://ocaml-multicore.github.io/kcas/doc/kcas_data/Kcas_data/Hashtbl/index.html" rel="noopener nofollow" class="external-link" href="https://ocaml-multicore.github.io/kcas/doc/kcas_data/Kcas_data/Hashtbl/index.html" target="_blank"><code></code> from<code></code></a>HashtblKcas_data.<br>As a final word of warning, Domains are so fast that in a too simple test runner, one Domain may complete before the second has even started up yet! This is problematic, as there will be no apparent parallelism for TSan to observe and check. In the above example, the calls to Unix.sleepf help ensure that the test runner is indeed parallel. A useful alternative trick is to coordinate on an Atomic to make sure both Domains are up and running before the parallel test code proceeds. To do so, we can adapt our parallel test runner as follows:<br>let _ =
  let wait = Atomic.make 2 in
  let t = Bank.init ~num_accounts ~init_balance:100 in
  (* run the simulation and the debug view in parallel *)
  [| Domain.spawn (fun () -&gt;
         Atomic.decr wait; while Atomic.get wait &gt; 0 do () done; money_shuffle t);
     Domain.spawn (fun () -&gt;
         Atomic.decr wait; while Atomic.get wait &gt; 0 do () done; print_balances t);
  |]
  |&gt; Array.iter Domain.join
<br>With that warning in mind and TSan in hand, you should now be equipped to hunt for data races.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/transitioning-to-multicore-with-threadsanitizer.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Transitioning to Multicore with ThreadSanitizer.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:03 GMT</pubDate></item><item><title><![CDATA[Typed Design Patterns for the Functional Era]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typesystem" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typesystem</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typesystem" class="tag" target="_blank" rel="noopener nofollow">#typesystem</a> <br>作者是Will Crichton，来自布朗大学。这篇论文探讨了在主流的函数式编程语言时代如何重新审视设计模式。作者讨论了应该以函数式设计模式的形式表示哪些知识：这些知识是相对独立的架构概念，但其整体不能作为语言级抽象来表示。作者提出了四个具体的例子来体现这个观点：Witness（证人模式）、状态机、并行列表和注册表。每个模式都在Rust语言中实现，展示了如何通过精心使用复杂的类型系统更好地建模每个领域构造，并在编译时捕获用户错误。<br>本文探索了在主流函数式编程语言时代如何重新审视设计模式。作者讨论了应该以函数式设计模式的形式表示哪些知识：这些是相对独立的架构概念，但其整体不能作为语言级抽象来表示。作者通过四个具体的例子来体现这个观点：Witness、状态机、并行列表和注册表。每个模式都在Rust中实现，展示了如何通过精心使用复杂的类型系统来更好地建模每个领域构造，并在编译时捕获用户错误。<br>
<br>引言： 函数式编程设计模式都在哪里？十多年来，人们一直在寻找这个问题的答案。随着函数式编程概念逐渐成为软件工程的主流，这种呼声也越来越大。然而，并没有出现一个清晰的函数式继承者目录，继承了著名的《设计模式》[Gamma等人，1994年]。本文探讨了函数式设计模式目录可能是什么样子的问题。
<br>函数式设计模式的一个样本： 从领域驱动设计[Evans 2003; Wlaschin 2017]中汲取灵感，每个设计模式都围绕一个“领域模式”，即在许多软件应用领域中出现的独立语言概念。领域模式以心理模式理论[Rumelhart 1980]中使用的“模式”来描述。

<br>Witness 模式：只有在满足特定条件后才能执行的动作。 案例研究：考虑一个网站，它有普通用户和管理员用户，并且有一个只有登录为管理员的用户才能访问的管理面板。
<br>状态机模式：对象存在于多个状态之一，可以响应事件转换到其他状态。 案例研究：考虑一个文件模型，有三个状态：读取、文件结束和关闭。
<br>并行列表模式：两个具有并行关系的异构元素列表。 案例研究：考虑一个printf风格的字符串格式化程序，它采用模板列表（字符串文字和占位符）和参数列表（要转换为字符串的值）。
<br>注册表模式：对象将键映射到异构值，用户可以为值注册键请求。 案例研究：考虑一个事件系统，其中包含事件（值）和名称（键），例如OnClick代表鼠标点击。


<br>相关工作： 本文展示了四种模式的详细情况后，我们应该退一步考虑这种展示风格与以前关于教授函数式系统设计的工作的关系。
<br>讨论： 本文的一个目标是推进作者对函数式设计模式的特定愿景。但另一个目标是在相关的学术和工业界内促进讨论，讨论的关键问题是：如果有的话，应该将哪些知识表述为类型化函数设计模式？或者换句话说，开发人员需要知道什么，目前还没有关于如何在函数式语言中设计有效系统的相关文档？
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/typed-design-patterns-for-the-functional-era.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Typed Design Patterns for the Functional Era.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:45:20 GMT</pubDate></item><item><title><![CDATA[Typing the Y combinator in OCaml.]]></title><description><![CDATA[ 
 <br>(* Typing the Y combinator in OCaml.
 * $ ocaml y.ml
   720
   Succ(Succ(Succ(Succ(Succ(Succ(Zero))))))
 *)

type 'a fix = Fix of ('a fix -&gt; 'a)

let fix x = Fix x
let unfix (Fix x) = x

let y : 'a 'b. (('a -&gt; 'b) -&gt; 'a -&gt; 'b) -&gt; 'a -&gt; 'b
  = fun f -&gt;
  let g x a =
    f ((unfix x) x) a
  in
  g (fix g)

(* The factorial function. *)
let fact self n = if n = 0 then 1 else n * self (n - 1)

(* A representation of natural numbers. *)
type nat = Zero | Succ of nat

let int2nat self n = if n = 0 then Zero else Succ (self (n - 1))
let rec string_of_nat = function
  | Zero -&gt; "Zero"
  | Succ n -&gt; Printf.sprintf "Succ(%s)" (string_of_nat n)

let _ =
  let result = y fact 6 in
  Printf.printf "%d\n%!" result;
  let result = y int2nat 6 in
  Printf.printf "%s\n%!" (string_of_nat result)
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/typing-the-y-combinator-in-ocaml..html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Typing the Y combinator in OCaml..md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:45:36 GMT</pubDate></item><item><title><![CDATA[Rescript @genType 生成的 TypeScript 代码中的 import 的问题]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rescript" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rescript</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typescript" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typescript</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rescript" class="tag" target="_blank" rel="noopener nofollow">#rescript</a> <a href="https://muqiuhan.github.io/wiki?query=tag:fp" class="tag" target="_blank" rel="noopener nofollow">#fp</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typescript" class="tag" target="_blank" rel="noopener nofollow">#typescript</a><br>Rescript 11 之后，@genType 被合并进编译器，无需任何依赖就能使用，当在 Rescript 中 @genType 了使用某些 Rescript built-in 的基本类型时，可能会生成有问题的 import 相关代码，例如：<br>@genType
module LoginResponse = {
  let status = response =&gt; {
    response
    -&gt;Js.Json.decodeObject
    -&gt;Option.flatMap(response =&gt; {
      response-&gt;Js_dict.get("status")
    })
  }
}
<br>status 函数具有 Js.Json.t =&gt; option&lt;Js.Json.t&gt; 类型，那么在生成的 TypeScript 文件中，会出现这样的 import:<br>import type { Json_t as Js_Json_t } from "./Js.gen.tsx"
<br>而 Js.gen.tsx 这个文件是不存在的，解决方案是使用 @genType 的 shim：<br>
<br>在 rescript.json 中的 gentypeconfig 中添加 shim 配置:
<br>...
"gentypeconfig": {
...
  "shims": {
    "Js": "Js"
  },
...
}
...
<br>
<br>然后新建 Js.shim.ts:
<br>export type Json_t = unknown;

export type t = unknown;

export type Exn_t = Error;
<br>
<br>删除原来由 @genType 生成的 TypeScript 文件并重新生成
<br>现在生成的 TypeScript 文件将会从 Js.shim.ts import 类型:<br>import type {Json_t as Js_Json_t} from '../../src/model/Js.shim.ts';
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rescript/rescript-@gentype-生成的-typescript-代码中的-import-的问题.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rescript/Rescript @genType 生成的 TypeScript 代码中的 import 的问题.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:03 GMT</pubDate></item><item><title><![CDATA[How to build a plugin system in Rust]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <br>
<a rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems" target="_blank">https://www.arroyo.dev/blog/rust-plugin-systems</a>
<br>Software used by businesses often needs to be extensible. For Arroyo, a real-time SQL engine, that means supporting user-defined functions (UDFs). But how can we support dynamic, user-written code in a static language like Rust? This post dives deep into the technical details of building a dynamically-linked, FFI-based plugin system in Rust.<br>Arroyo is a stream processing engine; users write SQL to build real-time data pipelines. Now SQL is a great language for data processing, supports <a data-tooltip-position="top" aria-label="https://doc.arroyo.dev/sql/scalar-functions" rel="noopener nofollow" class="external-link" href="https://doc.arroyo.dev/sql/scalar-functions" target="_blank">a huge variety of functions</a>, and is adaptable to a wide variety of use cases including <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/what-is-streaming-sql" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/what-is-streaming-sql" target="_blank">streaming data</a>.<br>But as powerful as SQL is, sometimes there's a computation that just isn't easy (or possible) to express as a SQL expression. Maybe you need to parse a custom binary data format, implement a complex aggregation strategy, or call some pre-existing business logic.<br>For these cases many SQL engines support user-defined functions (UDFs), which come in several forms: scalar UDFs, UDAFs (aggregate functions, which operate over multiple rows), and UDWFs (window function, which can reference other rows).<br>UDFs make SQL engines extensible. They let users customize the system to their needs. This is particularly important for us, as a small startup that can't build all of the functionality every user might want ourselves. But Arroyo is built in Rust—which likes to build static binaries. How can we support dynamic, user-defined behavior?<br>Historically, Arroyo supported UDFs via static, Ahead-of-Time (AoT) compilation, but with <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/why-arrow-and-datafusion" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/why-arrow-and-datafusion" target="_blank">Arroyo 0.10 dropping AoT in favor of interpreted SQL runtime</a> built around <a data-tooltip-position="top" aria-label="https://arrow.apache.org" rel="noopener nofollow" class="external-link" href="https://arrow.apache.org" target="_blank">Apache Arrow</a> and <a data-tooltip-position="top" aria-label="https://datafusion.apache.org/" rel="noopener nofollow" class="external-link" href="https://datafusion.apache.org/" target="_blank">DataFusion</a>, we needed a new strategy. The result is a dynamically-linked, FFI-based plugin system with support for sync and async functions.<br>You can see how this looks from a user perspective in our <a data-tooltip-position="top" aria-label="https://doc.arroyo.dev/sql/udfs" rel="noopener nofollow" class="external-link" href="https://doc.arroyo.dev/sql/udfs" target="_blank">docs</a>, or jump <a data-tooltip-position="top" aria-label="https://github.com/ArroyoSystems/arroyo/tree/master/crates/arroyo-udf" rel="noopener nofollow" class="external-link" href="https://github.com/ArroyoSystems/arroyo/tree/master/crates/arroyo-udf" target="_blank">into the code</a>. The rest of this post will dive deep into how we ended up with this design, the technical details behind it, and what you need to know to build your own FFI-based plugin interface in Rust.<br><br>Arroyo has supported UDFs since <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/arroyo-0-3-0" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/arroyo-0-3-0" target="_blank">0.3</a> and UDAFs since <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/arroyo-0-6-0" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/arroyo-0-6-0" target="_blank">0.6</a>, and it's become one of our most widely used features. We supported UDFs very early on in part because they were so easy to build into our architecture. Until our recent <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/arroyo-0-10-0" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/arroyo-0-10-0" target="_blank">0.10</a> release, we relied on ahead-of-time compilation for our pipelines. In short, we would generate Rust code with the data types, expressions, and graph structure of the pipeline, then compile that into a binary which would execute it against the data streams.<br>In this paradigm, supporting UDFs was straightforward. For example, a SQL expression like<br>LENGTH(CAST((counter + 5) as TEXT))
<br>would be compiled into something like the following Rust code<a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-1" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-1" target="_blank">1</a>:<br>(counter + 5).to_string().len()
<br>Since we're just generating Rust code, we can slot in a user-provided Rust function easily.<br><img alt="Pasted image 20241220105055.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/how-to-build-a-plugin-system-in-rust/attachments/pasted-image-20241220105055.png"><br>The way it worked was this: the Arroyo UI provided an editor where users could write UDF functions as Rust code. A UDF function looked like this:<br>pub fn square(x: i64) -&gt; i64 {
    x * x
}
<br>UDFs could also include a special comment at the top to add additional dependencies, in Cargo.toml format:<br>/*
[dependencies]
regex = "1"
*/
<br>When a user runs a pipeline with this UDF, the system would parse the function definition, pull out the arguments and return types, map them to the corresponding <a data-tooltip-position="top" aria-label="https://doc.arroyo.dev/sql/data-types" rel="noopener nofollow" class="external-link" href="https://doc.arroyo.dev/sql/data-types" target="_blank">SQL data type</a>, and register it with the SQL planner.<br>After planning comes codegen. UDFs source code would be written directly to a <a data-tooltip-position="top" aria-label="http://lib.rs" rel="noopener nofollow" class="external-link" href="http://lib.rs" target="_blank">lib.rs</a> file in a crate, alongside another crate with generated wrapper code, which would then be added a dependency to the pipeline crate. At this point, it's a normal, statically linked Rust function that could be called like any other within the generated expression code.<br><br>For Arroyo 0.10, we knew we would need a different strategy, since we were no longer building a static pipeline binary. Instead, expressions would be executed at runtime by a tree-walking interpreter, which would need some way to call out to the UDF code without having it available at compile time.<br>At this point, we started looking at prior art in the Rust world, and came across <a data-tooltip-position="top" aria-label="https://nullderef.com/series/rust-plugins/" rel="noopener nofollow" class="external-link" href="https://nullderef.com/series/rust-plugins/" target="_blank">this great blog series</a> on Rust plugins from Mario Ortiz Manero, which we're very indebted to. Unfortunately, it quickly became clear that this was not a solved problem in Rust.<br>There are a number of options for how one can invoke user code at runtime:<br><br>languages like Python and JavaScript can be run dynamically without any pre-compilation, and ir interpreter could be hosted within the engine. Arroyo uses the cross-language-compatible <a data-tooltip-position="top" aria-label="https://arrow.apache.org/" rel="noopener nofollow" class="external-link" href="https://arrow.apache.org/" target="_blank">Apache Arrow</a> data format, so there needn't be any serialization across the language barrier. However, there would still be a significant performance cost compared to our native Rust functions, and this would break backwards-compatibility with existing Rust UDFs.<br>This approach is common in applications where plugin performance isn't critical, for example GUI plugins in creative applications where the plugin code is orchestrating rather than processing itself.<br><br>Each UDF could be compiled as a separate service (or all UDFs for a pipeline could be compiled together into a single service) that runs as a sidecar to the main worker process. When the UDF is invoked during expression evaluation the engine would make an RPC call with the input data and get the return value back.<br>There are several advantages to this approach, which isolates user-written code from the engine process. It's able to crash independently, and have its own resource limits for CPU and memory. And by running it in a separate, locked-down container or VM, we could even run potentially malicious UDFs in a cloud environment without concern that they could compromise shared infrastructure.<br>The downside again is performance. Even with batching, the overhead of an RPC call is much higher than a pure function call, and requires serialization and deserialization of data. It also adds to the deployment complexity, as this would be other services that need to be managed.<br>The RPC approach to plugins is often used when the host seeks to isolate itself from potentially buggy plugin code and to easily support multiple languages; for example text editors today use RPCs via the <a data-tooltip-position="top" aria-label="https://microsoft.github.io/language-server-protocol/" rel="noopener nofollow" class="external-link" href="https://microsoft.github.io/language-server-protocol/" target="_blank">Language Server Protocol</a> and in the data space <a data-tooltip-position="top" aria-label="https://beam.apache.org/" rel="noopener nofollow" class="external-link" href="https://beam.apache.org/" target="_blank">Apache Beam</a> uses RPCs to support cross-language UDFs.<br><br>Now we're cooking with gas. Web assembly—Wasm—is the hottest thing in cross-language execution. UDF code written in a language like Rust can be compiled into a Wasm binary, which can then be dynamically executed by a Wasm runtime like <a data-tooltip-position="top" aria-label="https://wasmtime.dev/" rel="noopener nofollow" class="external-link" href="https://wasmtime.dev/" target="_blank">Wasmtime</a> or <a data-tooltip-position="top" aria-label="https://www.wasmer.io" rel="noopener nofollow" class="external-link" href="https://www.wasmer.io" target="_blank">Wasmer</a>.<br>Wasm is a genuinely exciting technology; it's an assembly-like language that is portable across languages, operating systems, and CPU architectures. In theory this means that a host doesn't need to know or care what the original language of the plugin. In addition to the portability, Wasm runtimes are also designed to be sandboxes, in theory allowing untrusted user code to run without compromising their host. Finally, Wasm supports fine-grained resource management; a host can limit how much CPU and memory a plugin can use.<br>All of these are great properties for a UDF system, and I have no doubt that Wasm will be part of how engines solve this problem in the future. Today, though, there are some limitations.<br>First, performance. Wasm code is still slower than native (anywhere from 1.5x-3x depending on the task). Beyond the runtime cost, sending data to a Wasm function generally requires copying it into the Wasm memory, which can be substantial overhead for simpler operations.<br>Second, compatibility. Not all Rust code can easily be compiled to Wasm. For example, anything that requires linking to a C library will not work out of the box. Many other features of Rust (threads, syscalls, networking, etc.) are not directly supported. The situation for other languages is much worse. For dynamic, GC'd languages like Python the best option today is to <a data-tooltip-position="top" aria-label="https://github.com/pyodide/pyodide" rel="noopener nofollow" class="external-link" href="https://github.com/pyodide/pyodide" target="_blank">compile the interpreter itself to Wasm</a>, but this means that most libraries that rely on C extensions (like numpy, scipy, pandas) won't work without special support.<br>In short, today plugin authors need to be very familiar with Wasm and its limitations in order to successfully build more complex UDFs.<br><br>Shared objects (.dlls on Windows, .so files on Linux, .dylibs on MacOS) are a common way to distribute library code. They can be dynamically linked, meaning that the application code needs only to know the interface at compile time, but not the actual code. They can even be dynamically loaded after program startup.<br>Dynamic linking is also a traditional way to implement plugin systems. For example, Digital Audio Workstations like Logic Pro and Ableton support shared-object plugins to implement effects and instruments, with standards like <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Virtual_Studio_Technology" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Virtual_Studio_Technology" target="_blank">VST</a> and <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Audio_Units" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Audio_Units" target="_blank">AU</a>. Other creative applications like photo editors and 3d graphics tools have similarly offered plugin interfaces via shared libraries.<br>In many ways, this is an obvious choice: functions from shared libraries have almost the same performance as native functions<a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-2" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-2" target="_blank">2</a>, and we would have complete compatibility for the wide universe of crates.<br>Unfortunately, dynamic linking is quite challenging in Rust. The language <a data-tooltip-position="top" aria-label="https://github.com/rust-lang/rfcs/issues/600" rel="noopener nofollow" class="external-link" href="https://github.com/rust-lang/rfcs/issues/600" target="_blank">lacks a stable ABI</a> (Application Binary Interface), which is the contract between function caller and callees about how variables will be laid out in memory, how structures are laid out, and other low-level details needed to correctly call out to foreign code. This means that shared libraries need to have been compiled with the exactly same compiler version (and possibly compiler options) as the host binary in order to be loadable.<br>But there is a workaround: use the C ABI. Unlike Rust, C does have a stable ABI on every major OS and processor architecture. So if we can constrain our plugin interface to only use C-compatible data structures and functions we can safely link against plugins compiled by any Rust compiler. Even better: as the C ABI is the lingua franca in the systems world, many other languages are able to emit it, opening the door to supporting UDFs in a variety of compiled languages.<br>This is the path we took for UDFs in Arroyo.<br><img alt="Pasted image 20241220105410.png" src="https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/how-to-build-a-plugin-system-in-rust/attachments/pasted-image-20241220105410.png"><br><br>So how do you go about building a C ABI in Rust? There are a number of limitations and rules to follow to safely call functions over a C FFI boundary.<br><br>The first thing we need to think about is data—the stuff getting passed between the host and plugin code. Rust gives us many powerful tools for modeling data, including structs, enums, tuples, various data structures… and we're going to have to give nearly all of them up. To correctly and reliably pass data over a C FFI boundary, we have to follow some very limiting rules.<br>Rule 1: repr(C)<br>Our first issue is that Rust's data layout is ABI dependent, and can (and does) change with different versions of the compiler. So we get to the first rule of building a C interface: all data needs to be #[repr(C)].<br>At this point I'd like to introduce an extremely helpful resource for anyone straying from the cosy, warm cottage of safe Rust and out into the deep dark night of unsafe: the <a data-tooltip-position="top" aria-label="https://doc.rust-lang.org/nomicon/" rel="noopener nofollow" class="external-link" href="https://doc.rust-lang.org/nomicon/" target="_blank">Rustonomicon</a>. It helpfully disclaims responsibility for potentially “UNLEASHING INDESCRIBABLE HORRORS THAT SHATTER YOUR PSYCHE AND SET YOUR MIND ADRIFT IN THE UNKNOWABLY INFINITE COSMOS.”<br>With that warning in mind, its section on data layout and repr can be found <a data-tooltip-position="top" aria-label="https://doc.rust-lang.org/nomicon/data.html" rel="noopener nofollow" class="external-link" href="https://doc.rust-lang.org/nomicon/data.html" target="_blank">here</a>.<br>Repr annotations allow developers to specify specific data layouts for structs and enums, where the default is “whatever the Rust compiler wants to do and thinks is efficient.” There are several support layouts, but the rules for repr(C) is pretty simple: just do whatever C does.<br>To use alternative representations, we can create a data type (struct or enum) and annotate it like this:<br>#[repr(C)]
struct MyData {
  a: f32,
  b: i64,
  c: u8
}
<br>This struct demonstrates why #[repr(C)] is important. Compiling this code using the nightly-only rustc option -Zprint-type-sizes we can see that we end up with completely different layouts for #[repr(Rust)] and #[repr(C)]:<br>// #[repr(Rust)]
print-type-size type: `MyData`: 16 bytes, alignment: 8 bytes
print-type-size     field `.b`: 8 bytes
print-type-size     field `.a`: 4 bytes
print-type-size     field `.c`: 1 bytes
 
// #[repr(C)]
print-type-size type: `MyData`: 24 bytes, alignment: 8 bytes
print-type-size     field `.a`: 4 bytes
print-type-size     padding: 4 bytes
print-type-size     field `.b`: 8 bytes, alignment: 8 bytes
print-type-size     field `.c`: 1 bytes
print-type-size     end padding: 7 bytes
<br>In fact, the Rust representation is much more efficient, taking up only 16 bytes instead of 24 bytes for the C representation. This is because Rust is free to reorder the fields to reduce the number of padding bytes needed to align to 8 bytes. C on the other hand will always lay out fields in order with predictable padding rules, which we see in the second version.<br>That example struct stuck to simple, primitive data types. And that leads into rule number 2:<br>Rule 2: all data must be FFI safe<br>While #[repr(C)] allows us to create structs and enums that can be passed across an FFI boundary, that property is not recursive—that is, it controls the layout of fields within the struct, but doesn't affect the representation of the fields themselves.<br>In fact, we are quite limited in what data types are FFI safe. This is not a well documented area of Rust, but an incomplete list of FFI safe types includes:<br>
<br>Primitive Types: u8, u16, u32, u64, i8, i16, i32, i64, usize, isize, f32, f64, and bool.
<br>Pointers: Raw pointers const T and mut T; safe wrappers for nullable pointers like Option&lt;NonNull&lt;T&gt;&gt;
<br>C-compatible Enums: Enums with explicitly defined repr(C).
<br>C-compatible Structs: Structs with repr(C) and containing only FFI-safe types.
<br>Slices: [T], const [T], and mut [T] when length is provided separately.
<br>So no passing String, Vec, HashMap, or random data types from your favorite Rust crate. However, we do have tools for passing some useful types of data with a little bit of transformation. The <a data-tooltip-position="top" aria-label="https://doc.rust-lang.org/std/ffi/index.html" rel="noopener nofollow" class="external-link" href="https://doc.rust-lang.org/std/ffi/index.html" target="_blank">std::ffi</a> package includes CString and CStr which are owned and borrowed null-terminated C-style strings. Similarly, we can pass Vec by converting it into a raw pointer + length + capacity and then back again.<br><br>Once we've decided on our data types, we're ready to design our actual API, by exporting C-compatible functions. A C FFI function is a bare Rust function with<br>
<br>A #[no_mangle] annotation, which tells rustc to name the symbol as exactly the function name, instead of rewriting it (or “mangling”) to ensure uniqueness and include useful metadata
<br>The extern "C" keyword, which tells rustc to export the function for external use using the C ABI
<br>All arguments and a return type that are FFI safe<a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-3" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-3" target="_blank">3</a>, as described above
<br>So putting that together, we can export a C FFI function with the following definition<br>#[no_mangle]
extern "C" fn add(a: u32, b: u32) -&gt; u32 {
  a + b
}
<br>Within the plugin code, we can use almost any Rust construct or features, so long as nothing leaks into the type signature.<br>The biggest exception is panic. Rust's default panicking behavior is unwinding, which means we travel up the call stack until we hit either a catch_unwind call (which stops the unwinding) or the top stack frame for the thread, in which case the thread exits. But this is a Rust-specific feature, part of the unstable Rust ABI. Unwinding can't cross a C FFI boundary without risking undefined behavior.<br>There are two ways to handle this: either we need to compile our plugin code with panic = 'abort' (which causes the process to terminate on panic) or we need to ensure that the plugin cannot panic. But even if we can ensure our own code is panic-free<a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-4" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-4" target="_blank">4</a>, how can we make sure our plugin-writing-users do the same?<br>One answer is to use a plugin interface with a top-level catch_unwind call that converts panics into an Error enum across the FFI boundary.<br><br>Our plugin will be a library crate that's built as a shared object, in a binary format that depends on our operating system (.so on Linux, .dll on Windows, .dylib on MacOS).<br>By default, Rust compiles libraries as an rlib artifact, which is a Rust-specific static library format. To tell it to instead build a dynamic system library that can be linked by other languages, we'll use the cdylib type. This can be specified in a Cargo.toml by setting the lib.crate-type option, like this:<br>[package]
name = "my-plugin"
version = "1.0.0"
edition = "2021"
 
[lib]
crate-type = ["cdylib"]
<br><br>There are two different ways we might link and call plugin code across an FFI: at program start time, or dynamically as the program is executing. In either case, we'll use the extern keyword again but without a function body in order to tell the host side what the function signature is.<br>If we know the name of the library at compile time, Rust provides built-in support for loading system libraries using the [link] annotation. It looks like this:<br>#[link(name = "my_plugin")]
extern "C" {
    fn add(a: u32, b: u32) -&gt; u32;
}
 
fn main() {
    unsafe { add(1, 5) };
}
<br>Rust will look for a shared library with the name my_plugin (for example, on Linux it will look for /usr/lib/my_plugin.so, /usr/local/lib/my_plugin.so, etc.) and attempt to link it at program start time, and will fail if it can't find the library. The function can be called like any other (unsafe) Rust function<a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-5" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-5" target="_blank">5</a>.<br>But for a plugin system, having to know the name of the library at compile time (and ensuring that it's installed in a system location) is somewhat limiting. Instead, we can turn to dynamic loading.<br>The interfaces for dynamic library loading are OS-specific, but there are several crates that can handle the cross-platform boilerplate for us. The two most popular are <a data-tooltip-position="top" aria-label="https://crates.io/crates/libloading" rel="noopener nofollow" class="external-link" href="https://crates.io/crates/libloading" target="_blank">libloading</a> and <a data-tooltip-position="top" aria-label="https://crates.io/crates/dlopen2" rel="noopener nofollow" class="external-link" href="https://crates.io/crates/dlopen2" target="_blank">dlopen2</a>. For Arroyo we decided to use dlopen2 which has a nicer interface and stronger guarantees around thread safety.<br>In dlopen2, we can define structs for each of our plugin interfaces. They look like this:<br>#[derive(WrapperApi)]
pub struct PluginInterface {
    add: extern "C" fn(a: u32, b: u32) -&gt; u32,
}
<br>A plugin can be loaded and called like this:<br>let container: Container&lt;PluginInterface&gt; = unsafe {
	Container::load(dylib_path).unwrap()
};
 
unsafe { container.add(1, 3) }
<br><br>So that was a lot of theory. Let's put it into practice with a complete example! We'll be working off a (very simplified) example plugin system found in <a data-tooltip-position="top" aria-label="https://github.com/mwylde/rust-plugin-tutorial" rel="noopener nofollow" class="external-link" href="https://github.com/mwylde/rust-plugin-tutorial" target="_blank">this repo</a>. Clone it locally to try this out yourself.<br>The code is divided into two parts: the plugin, which compiles to a shared object, and the host, which loads the plugin. (In a real system, you'd likely want a few more components, including a common library to share definitions between the plugin and host, and a macro to do code generation, but we're keeping this relatively simple.)<br><br>Before we can start writing code, we need to decide on the contract between the plugin and the host. For this example, we'll adopt a flexible contract that supports a variable number of arguments of various common data types, as would be needed for a UDF system.<br>The plugin interface has two methods:<br>extern "C" fn plugin_metadata() -&gt; PluginMetadata,
 
extern "C" fn plugin_entrypoint(
    args: *const PluginValue, args_len: usize) -&gt; PluginResult
<br>The metadata function is called by the host to determine the number and types of the arguments the plugin expects, and the type of the data it returns, while the entrypoint function is called to actually execute the plugin's logic.<br>As discussed in detail above, all of our data types need to be FFI-safe. For example, the PluginMetadata type looks like this:<br>#[repr(C)]
#[derive(Copy, Clone)]
pub enum PluginType {
    Bool,
    Int,
    UInt,
    Double,
    String,
}
 
#[repr(C)]
pub struct PluginMetadata {
    // should have a static lifetime
    pub name: *const i8,
    pub arg_types: *const PluginType,
    pub arg_types_len: usize,
    pub return_type: PluginType,
}
<br>Note that instead of of passing a String for the name, we pass a *const i8, which represents a null-terminated C-style string. Instead of a Vec&lt;PluginType&gt; for the args, we pass a pointer to some memory and a length.<br>For the entrypoint function, we'll need to pass our actual data to the plugin. That relies on an array of values of the type PluginValue:<br>#[repr(C)]
pub enum PluginValue {
    Bool(bool),
    Int(i64),
    UInt(u64),
    Double(f64),
    // All strings are owned by the host
    String(*const i8),
}
<br>For primitives, we can use them as-is as all Rust primitives are FFI-safe. However, strings again need special attention. We have two typical options for passing strings: we can pass Rust-style strings (with an array of chars and a length) or C-styles strings (whose end is determined by a null byte). While the former are much safer and generally preferred in modern APIs, for C interfaces the latter is more common as it's more easily supported by languages with a C FFI.<br>Beyond the format of the data, we also need to consider ownership<a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-6" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fn-6" target="_blank">6</a>. Once memory is allocated, exactly one part of our code (in this case, one side of the host/plugin divide) needs to own that memory. PluginValues are both created by the host (to provide data) and by the plugin (to return its result), but to simplify the memory management we have documented that in both cases the host owns the memory and is responsible for freeing it. This does mean the plugin code needs to be careful never to create an owned-object from the memory (in this case, a CString) which would free it on drop.<br>Finally, we have our return type, which is just an FFI-safe Result type with a CString error message:<br>#[repr(C)]
pub enum PluginResult {
    Ok(PluginValue),
    // Null-terminated c-string; he host is responsible for
    // freeing this value
    Err(*mut i8),
}
<br>Now that we have our common interface, let's see how they're used. We'll start with the plugin.<br>We're using two different types of raw pointers here: *mut and *const. Why is that? What's the difference? In the context of FFI, the answer is: not much. The choice of mut vs const doesn't affect the generated code, and you can freely cast between them.<br>However, they are useful for documenting intent and ownership across an FFI boundary. Using *const tells the calling code that they shouldn't modify the data behind the pointer, and probably shouldn't free it, while *mut indicates that it's ok to modify the data and can also communicate ownership.<br>We're playing a bit fast-and-loose here, because we're using a single data type (PluginValue) for both our arguments—which the host creates and owns—and our return value, which the plugin creates but transfers to the host, so we opt for *const to tell the plugin not to modify or free its arguments. However, on the host side we then have to cast it to *mut so that we can take ownership.<br><br>We're going to implement a simple plugin that takes in two arguments, a string and a number, and will return the string repeated that number of times: f("cool", 3) ⇒ "coolcoolcool" .<br>The plugin is responsible for building a shared library, so we need to tell Cargo that's what we want. We do that by specifying the crate type as cdylib, a C-compatible dynamic library. Our Cargo.toml looks like this:<br>[package]
name = "plugin"
version = "0.1.0"
edition = "2021"
 
[lib]
crate-type = ["cdylib"]
<br>Next is our src/lib.rs file. This will contain implementations of the plugin interface documented above.<br>The first function we need to implement is plugin_metadata(), which is pretty straightforward, telling the host about our arguments and our return type:<br>#[no_mangle]
pub extern "C" fn plugin_metadata() -&gt; PluginMetadata {
    PluginMetadata {
        name: "repeat\0".as_ptr() as *const i8,
        arg_types: [PluginType::String, PluginType::UInt].as_ptr(),
        arg_types_len: 2,
        return_type: PluginType::String,
    }
}
<br>Next we'll implement our plugin's unique logic, in this case repeating a string N times. I find it easiest to separate this out from the boilerplate that's involved in converting to and from FFI types, so that when we're developing the logic we can stay in safe, normal Rust land.<br>fn repeat_impl(input: &amp;str, count: u64) -&gt; String {
    input.repeat(count as usize)
}
<br>Nice and simple. Unfortunately, we still need the complex code to bridge the FFI and Rust worlds. For this example, it looks like this:<br>#[no_mangle]
pub extern "C" fn plugin_entrypoint(args: *const PluginValue,
                                    args_len: usize) -&gt; PluginResult {
    // first we need to check if the arguments are valid
    if args_len != 2 {
        return plugin_error("args_len should be 2");
    }
 
    let PluginValue::String(string) = (unsafe { &amp;*args.offset(0) }) else {
        return plugin_error("arg0 is invalid; expected String");
    };
 
    let PluginValue::UInt(count) = (unsafe { &amp;*args.offset(1) }) else {
        return plugin_error("arg1 is invalid; expected UInt");
    };
 
    let string = match unsafe { CStr::from_ptr(*string) }.to_str() {
        Ok(value) =&gt; value,
        Err(_) =&gt; {
            return plugin_error("arg0 is invalid; expected valid UTF-8 string");
        }
    };
 
    // then we can call our logic with the converted arguments and re-wrap them
    // in our Result type, catching any panics that might occur so that they
    // don't cross the FFI boundary
    match catch_unwind(|| repeat_impl(string, *count)) {
        Ok(value) =&gt; PluginResult::Ok(
          PluginValue::String(CString::new(value).unwrap().into_raw())),
        Err(_) =&gt; plugin_error("function panicked"),
    }
}
<br>Making your users write all of this unsafe boilerplate for every plugin isn't great UX, so you may want to use a macro or just wrapper code (if you don't need to support multiple types). You can see the macro for the Arroyo plugin system <a data-tooltip-position="top" aria-label="https://github.com/ArroyoSystems/arroyo/blob/master/crates/arroyo-udf/arroyo-udf-macros/src/lib.rs" rel="noopener nofollow" class="external-link" href="https://github.com/ArroyoSystems/arroyo/blob/master/crates/arroyo-udf/arroyo-udf-macros/src/lib.rs" target="_blank">here</a>.<br><br>The host is a normal Rust application, created with cargo new. It has one dependency, dlopen2, which we'll use to dynamically load our plugin:<br>[dependencies]
dlopen2 = { version = "0.7.0", features = ["derive"] }
<br>The meat is in src/main.rs, which builds our binary. We need to repeat the definitions (or include them from a common library), but we'll also include one more, an owned version of PluginValue:<br>pub enum OwnedPluginValue {
    Bool(bool),
    Int(i64),
    UInt(u64),
    Double(f64),
    String(CString),
}
 
impl PluginValue {
    pub fn to_owned(self) -&gt; OwnedPluginValue {
        match self {
            PluginValue::Bool(b) =&gt; OwnedPluginValue::Bool(b),
            PluginValue::Int(i) =&gt; OwnedPluginValue::Int(i),
            PluginValue::UInt(u) =&gt; OwnedPluginValue::UInt(u),
            PluginValue::Double(d) =&gt; OwnedPluginValue::Double(d),
            PluginValue::String(s) =&gt; {
                OwnedPluginValue::String(
                    unsafe { CString::from_raw(s as *mut i8) })
            }
        }
    }
}
<br>This owned struct will allow us to ensure that values returned from the plugin (and the arguments we send it) are eventually freed.<br>Next, we'll define the plugin interface using dlopen2's WrapperApi macro:<br>#[derive(WrapperApi)]
struct PluginApi {
    plugin_metadata: unsafe extern "C" fn() -&gt; PluginMetadata,
    plugin_entrypoint:
        unsafe extern "C" fn(args: *const PluginValue, args_len: usize)
            -&gt; PluginResult,
}
<br>This let's us conveniently bundle up all of the plugin functions into a struct which we can store and pass around our application.<br>Now we're ready to load the plugin and call it. I'll spare you the details of CLI argument processing (which you can see in the <a data-tooltip-position="top" aria-label="https://github.com/mwylde/rust-plugin-tutorial/blob/main/host/src/main.rs" rel="noopener nofollow" class="external-link" href="https://github.com/mwylde/rust-plugin-tutorial/blob/main/host/src/main.rs" target="_blank">full example file</a>). Here's the meat of it:<br>// load the plugin via the dlopen2's Container API
let container: Container&lt;PluginApi&gt; =
    unsafe { Container::load(&amp;args[1]) }.expect("Could not load plugin");
 
// get the metadata, which will tell us which arguments to expect
let metadata: PluginMetadata = unsafe { container.plugin_metadata() };
 
// read the arguments from the command line
let mut call_args: Vec&lt;PluginValue&gt; = vec![];
for (i, arg) in args[2..].iter().enumerate() {
    match unsafe { *metadata.arg_types.add(i) } {
        PluginType::Bool =&gt; {
            call_args.push(PluginValue::Bool(
                arg.parse().expect("Invalid bool")))
        }
        ...
    }
}
 
// call the plugin function
let result = unsafe {
    container.plugin_entrypoint(call_args.as_ptr(), call_args.len())
};
 
// take ownership and drop the arguments to free their memory
drop(call_args.into_iter().map(|t| t.to_owned()));
 
// print out the result or error to the user
match result {
    PluginResult::Ok(value) =&gt; {
        println!("Plugin returned: {}", value.to_owned());
    }
    PluginResult::Err(err) =&gt; {
        eprintln!("{}", unsafe { CString::from_raw(err) }.to_string_lossy());
        std::process::exit(1);
    }
}
<br><br>Here we are. After more than 5000 words, we're going to actually dynamically load some Rust code.<br>If you want to follow along, check out the example repo<br>$ git clone https://github.com/mwylde/rust-plugin-tutorial.git
<br>Then we're going to build both the plugin and host<br>$ cd rust-plugin-tutorial/plugin &amp;&amp; cargo build
$ cd ../host &amp;&amp; cargo build
<br>Now we should have a dynamic library in plugin/target/debug and a host binary in host/target/debug. The dynamic library will be named something like “libplugin.dylib,” “libplugin.so,” or “libplugin.dll” depending on your operating system. Note which it is, then invoke the host like this:<br>$ host/target/debug/host plugin/target/debug/libplugin.dylib cool 3
Loaded plugin repeat
Plugin returned: coolcoolcool
<br>If all went well, you should see the output from your plugin code (and no pesky segfaults).<br><br>So that's the background for how we built our plugin system, and how you can build your own.<br>Recapping a bit:<br>
<br>We defined our data types as enums and structs of FFI-safe types
<br>We defined a plugin interface, as #[no_mangle] extern "C" functions consuming and returning those data types
<br>We used dlopen2 to load and call our plugin interface from the host
<br>In part 2 of this series, we'll cover how this works in a real, production plugin system, including support for async functions. (If you're impatient, all of the code can be found <a data-tooltip-position="top" aria-label="https://github.com/ArroyoSystems/arroyo/tree/6bbc5484e2d9f515ca1c067d23ad0e8ff25f9882/crates/arroyo-udf" rel="noopener nofollow" class="external-link" href="https://github.com/ArroyoSystems/arroyo/tree/6bbc5484e2d9f515ca1c067d23ad0e8ff25f9882/crates/arroyo-udf" target="_blank">here</a>.)<br>Questions? Concerns? Issues? Abuse? You can reach me on the <a data-tooltip-position="top" aria-label="https://discord.gg/cjCr5rVmyR" rel="noopener nofollow" class="external-link" href="https://discord.gg/cjCr5rVmyR" target="_blank">Arroyo Discord</a> or at <a data-tooltip-position="top" aria-label="mailto:micah@arroyo.systems" rel="noopener nofollow" class="external-link" href="https://muqiuhan.github.io/wiki/mailto:micah@arroyo.systems" target="_blank">micah@arroyo.systems</a>.<br><br>
<br>This is a bit simplified; the actual compilation has to take into account SQL's complex nullability rules. See a more complete example <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/why-arrow-and-datafusion#implementing-expressions" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/why-arrow-and-datafusion#implementing-expressions" target="_blank">here</a>. <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-1" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-1" target="_blank">↩</a>
<br>In practice dynamically-linked functions can run a bit slower as they can't be inlined or take advantage of link-time or profile-guided optimizations. <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-2" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-2" target="_blank">↩</a>
<br>The Rust compiler will helpfully warn you if you use a non-FFI-safe type in an extern function <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-3" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-3" target="_blank">↩</a>
<br>Which… of course we all do <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-4" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-4" target="_blank">↩</a>
<br>All FFI functions are inherently unsafe, as Rust can't guarantee anything on the other side of the FFI boundary. For that reason, It's typical to write safe wrappers around FFI functions that ensure the invariants expected by the library are upheld. <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-5" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-5" target="_blank">↩</a>
<br>Rust-haters complain about the lifetime and borrowchecker systems but those issues don't go away just because your compiler doesn't yell at you about it. In C APIs you always need to know who owns a piece of memory and who's responsible for freeing it. But without language support you just have to document it and hope for the best. <a data-tooltip-position="top" aria-label="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-6" rel="noopener nofollow" class="external-link" href="https://www.arroyo.dev/blog/rust-plugin-systems#user-content-fnref-6" target="_blank">↩</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/how-to-build-a-plugin-system-in-rust/how-to-build-a-plugin-system-in-rust.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/How to build a plugin system in Rust/How to build a plugin system in Rust.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:44:48 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/how-to-build-a-plugin-system-in-rust/attachments/pasted-image-20241220105055.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/how-to-build-a-plugin-system-in-rust/attachments/pasted-image-20241220105055.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Rust NewType 模式]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a><br>New Type模式是一种软件设计模式，用于在已有类型的基础上创建一个新的类型。在Rust中，这通常是通过定义一个结构体，其中只包含一个单一成员。这个结构体（New Type）对外提供了一个新的、独立的类型，用于对原始类型增加额外的语义或限制。<br><br>struct Meters(f64);
struct Feet(f64);

let length_in_meters = Meters(100.0);
let length_in_feet = Feet(328.084);

// 编译器会防止以下代码执行，因为类型不匹配
// let wrong_length = Meters(length_in_feet); // 编译错误

// 正确的构造
fn add_lengths(length1: Meters, length2: Meters) -&gt; Meters {
    Meters(length1.0 + length2.0)
}
<br>这个例子使用 newtype 模式避免将原始类型f64用于不同的量度，从而增强了类型的安全性。<br><br>struct Kilometers(f64);

impl Kilometers {
    fn to_miles(&amp;self) -&gt; f64 {
        self.0 * 0.621371
    }
}

let distance = Kilometers(10.0);
println!("The distance in miles is {}", distance.to_miles());
<br>这里，Kilometers有一个方法to_miles，该方法是不会影响其他f64数据的。如果我们有另一个表示温度的f64类型，就不会意外调用到与距离相关的方法。<br>New Type模式同样适用于对Box&lt;dyn SomeTrait&gt;类型的包装，这可以在需要动态分派（动态调用实现了某个接口的不同类型的对象的方法）的时候提供便利。通过创建一个New Type来包装这样的Box&lt;dyn SomeTrait&gt;类型，可以提供自定义的方法或实现更多的trait，同时也可以让API更加清晰和易于使用。<br><br>在Rust中，New Type模式不仅是类型安全的，还是一种零成本抽象。这是因为Rust编译器在编译时期会进行足够的优化，以确保New Type的使用没有运行时开销。 Rust的零成本抽象原则确保了抽象不会引入额外的运行时成本。例如，当你使用Meters这样的New Type时，Rust确保：<br>
<br>无额外内存开销：Meters只包含一个f64，在内存中的表现和单独的f64是一样的。
<br>无额外运行时开销：使用Meters时，性能和直接使用f64完全相同。编译器会移除任何关于New Type的包装和解包的代码。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-newtype-模式.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust NewType 模式.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:05 GMT</pubDate></item><item><title><![CDATA[Rust Partial 语义]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a><br>
在Rust中，PartialEq和PartialOrd trait处理了不是所有值都可以相互比较的情况。
<br><br>PartialEq trait用于定义值相等性的比较。它的设计允许类型的值之间进行相等（==）和不等（!=）的比较。与其对应的 Eq trait 确保一个类型的所有值都是可以可靠比较的，即满足等价关系的特性，如自反性、对称性和传递性。<br>fn eq(&amp;self, other: &amp;Self) -&gt; bool;
fn ne(&amp;self, other: &amp;Self) -&gt; bool;
<br>在大多数情况下，类型的值都能够完全比较相等性，这时可以实现Eq。然而，对于一些特殊类型的值，如浮点数，由于存在无穷大的正负值和NaN值，导致它们的比较更加复杂。例如，根据IEEE浮点数的标准，NaN与任何值（包括它自己）比较都不相等。<br><br>PartialOrd trait用于定义值之间的大小比较。类似于PartialEq，它允许部分比较大小，返回一个Option，表示比较结果可能存在，也可能不存在（即比较无法进行时返回None）:<br>fn partial_cmp(&amp;self, other: &amp;Self) -&gt; Option&lt;Ordering&gt;;
<br>在全部比较可能的场景，我们会使用Ord trait，它要求实现cmp方法，总是返回一个Ordering，表示两个值之间的确切比较关系。Ord是在所有值都能够比较时使用的，例如整数和字符串。<br><br>Rust 设计 PartialEq 和 PartialOrd trait 主要出于以下几个理由：<br>
<br>非总序理念：并不是所有类型都有一个全局的排序方法。例如，复数之间就没有一个自然的大小顺序。为了避免为这些类型人为地赋予一个排序方法，Rust 提供了一个只需部分实现序列操作的选择。
<br>IEEE 浮点数标准：由于浮点数标准定义了特殊值（NaN, 正负无穷），以及NaN不等于自身的规则，浮点数在一些情况下不能进行相等性或大小比较。
<br>提升错误处理能力和安全性：通过返回 Option&lt;Ordering&gt;，partial_cmp 方法明确指出了失败的可能性，从而迫使程序员在使用时考虑并处理这种情况，增加了代码的正确性和稳健性。
<br>表达性和灵活性：这些 trait 允许开发者为自定义类型定义适当的相等性和排序行为，从而加强了 Rust 类型系统的表达性和灵活性。
<br>PartialEq 和 PartialOrd trait 的设计允许程序员选择精准的相等性和排序语义，同时明确了对于某些类型相等性比较和大小排序并不总是可能的事实。通过引入适度的复杂性，让 Rust 的类型系统更加安全。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-partial-语义.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust Partial 语义.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:49:05 GMT</pubDate></item><item><title><![CDATA[Rust trait 实现函数重载]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typescript" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typescript</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typescript" class="tag" target="_blank" rel="noopener nofollow">#typescript</a><br>函数重载和 Rust 的 trait 本质上都是 Ad-hoc polymorphism 的一种表现形式，两者表达能力基本相同，比如如下 C++ 中的函数重载<br>int foo(int);
int foo(int, double);
<br>在 Rust 中可以用 Trait + Tuple + Generic 模拟<br>trait FooImpl {
    fn foo_impl(self) -&gt; i32;
}

impl FooImpl for (i32,) {
    fn foo_impl(self) -&gt; i32 { .. }
}

impl FooImpl for (i32, f64) {
    fn foo_impl(self) -&gt; i32 { .. }
}

fn foo&lt;F: FooImpl&gt;(f: F) -&gt; i32 {
    f.foo_impl()
}
foo((1,));
foo((1,2.0));
<br>只不过多了一层括号来表示这是个 Tuple。如果使用fn_trait 和 unboxed_closures，就能把这层括号给去了<br>struct foo;

impl FnOnce&lt;(i32,)&gt; for foo {
    type Output = i32;

    extern "rust-call" fn call_once(self, (a,): (i32,)) -&gt; i32 {
        ...
    }
}

impl FnOnce&lt;(i32,)&gt; for foo {
    type Output = i32;
    
    extern "rust-call" fn call_once(self, (a, b): (i32, f64)) -&gt; i32 {
        ...
    }
}

add(1);
add(1, 2.0);
<br>C++ 中的函数重载中只能对参数类型重载，返回值类型是不能重载的。如果在上述的FooImpl trait 中把返回值类型作为 Generic 参数，就能实现对返回值类型的重载：<br>trait FooImpl&lt;R&gt; {
    fn foo_impl(self) -&gt; R;
}

impl FooImpl&lt;i32&gt; for (i32,) {
    fn foo_impl(self) -&gt; i32 { .. }
}

impl FooImpl&lt;f64&gt; for (i32,) {
    fn foo_impl(self) -&gt; f64 { .. }
}

fn foo&lt;R, F: FooImpl&lt;R&gt;&gt;(f: F) -&gt; R {
    f.foo_impl()
}

let i: i32 = foo((1,));
let j: f64 = foo((1,));
<br>通过上述方式还可以实现更丧心病狂的东西：Variadic Function。因为 Rust 目前不支持 Variadic Generic，所以无法为任意长的 tuple 实现 trait，但我们可以通过把(A, B, C, ...) 转换为 (A, (B, (C, ...)))的形式来绕过这个限制，<a data-tooltip-position="top" aria-label="" rel="noopener nofollow" class="external-link" href="" target="_blank">tuple_list</a> 这个库已经为我们做好了封装<br>例如，使用它我们可以实现一个接受任意多参数的concat<br>use tuple_list::{Tuple, TupleList};

trait ConcatImpl: TupleList {
    fn concat_impl(self) -&gt; String;
}

impl ConcatImpl for () {
    fn concat_impl(self) -&gt; String {
        "".into()
    }
}
impl&lt;Head, Tail&gt; ConcatImpl for (Head, Tail)
where
    Self: TupleList,
    Head: ToString,
    Tail: ConcatImpl,
{
    fn concat_impl(self) -&gt; String {
        self.0.to_string() + &amp;self.1.concat_impl()
    }
}

struct concat;
impl&lt;'a, T&gt; FnOnce&lt;T&gt; for concat
where
    T: tuple_list::Tuple + std::marker::Tuple,
    &lt;T as Tuple&gt;::TupleList: ConcatImpl,
{
    type Output = String;
    extern "rust-call" fn call_once(self, args: T) -&gt; Self::Output {
        ConcatImpl::concat_impl(Tuple::into_tuple_list(args))
    }
}
<br>然后<br>// (0.1 + 0.2 != 0.3) == true
concat("(", 0.1, " + 0.2 != ", 0.3, ") == ", true);
// 0.11false
concat(0.1, 1, false);
<br>这就相当于cpp中的<br>auto concat(auto &amp;&amp;fst, auto &amp;&amp;...res) {
    auto head = [&amp;] {
        if constexpr (requires { std::string{fst}; })
            return std::string{fst};
        else
            return std::to_string(fst);
    }();
    if constexpr (sizeof...(res) == 0)
        return head;
    else
        return head + concat(std::forward&lt;decltype(res)&gt;(res)...);
}
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-trait-实现函数重载.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust trait 实现函数重载.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:50:04 GMT</pubDate></item><item><title><![CDATA[Rust 无锁线程池异步运行时的实现]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:os" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#os</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:multi-thread" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#multi-thread</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a> <a href="https://muqiuhan.github.io/wiki?query=tag:os" class="tag" target="_blank" rel="noopener nofollow">#os</a> <a href="https://muqiuhan.github.io/wiki?query=tag:multi-thread" class="tag" target="_blank" rel="noopener nofollow">#multi-thread</a> <br>用 epoll（只支持linux）实现一个最小功能的线程池异步运行时，效果为：<br>
<br>实现一个 TcpStream，具体 api 仿照 std 里的 TcpStream，但是是异步版本
<br>实现一个在 main 上的 attribute proc macro，类似于 tokio::main 和 tokio::test，使得可以将 main 或单元测试变成异步函数，并且不破坏 rust analyzer 对于函数内容的类型标注、鼠标悬停提示等。
<br>要求：不使用任何形式的锁，包含 std 给的锁、parking_lot 的锁或变相的自旋锁逻辑。<br>环境：<br>
<br>最新版 rustc，可使用所有的非 incomplete 的 unstable 功能。
<br>可使用 std 和所有跟线程同步、异步基础设施无关的 crate。
<br>代码中可有 unsafe，但不可存在 ub。
<br>需要以下检查通过，并且没有任何错误/警告：<br>
cargo miri test<br>
cargo clippy<br>
cargo fmt --check
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-无锁线程池异步运行时的实现.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust 无锁线程池异步运行时的实现.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 15 Feb 2025 05:22:19 GMT</pubDate></item><item><title><![CDATA[Rust 虚表布局规则介绍]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a><br>在 Rust 中，一个指向未知大小对象（!Sized）的引用或指针被实现为一个由两个 usize 大小的域构成的胖指针。这两个域中，其中一个域保存了被引用或被指向的对象的地址，另一个域保存了一个名为 metadata 的数据。对于 slice 的引用或指针来说，其 metadata 为 slice 的长度。对于 trait object 的引用或指针来说，其 metadata 为虚表（vtable）地址。与 C++ 虚表类似，Rust 虚表的存在使得诸多动态语言特性得以实现，例如动态派发（dynamic dispatch）、向上转换（upcasting）、向下转换（downcast）等。本文将对 Rust 中虚表的布局规则进行简要介绍，并在此过程中对 Rust 中若干动态特性的实现方法进行简要介绍。<br>
注意：Rust 虚表及其结构属于 Rust 语言的内部实现细节，不保证稳定性。本文所介绍的虚表布局仅反映本文创作时最新的 Rust 虚表结构[1]，在将来 Rust 虚表结构可能会发生变化。一个 Rust 程序的正确性不应该以任何方式依赖于 Rust 虚表的结构。
<br><br>Rust 程序中的所有虚表均以一个固定结构的 header 开头。Header 中按顺序包含三个usize 大小的字段：drop_in_place ，size 和 align 。在 header 之后是一系列的 usize 大小字段，其数量以及含义在每个虚表中可能都不同。<br>+---------------+
| drop_in_place |
+---------------+
| size          |
+---------------+
| align         |
+---------------+
| entry1        |
+---------------+
| entry2        |
+---------------+
| entry3        |
+---------------+
<br>虚表 header 中的drop_in_place 是一个函数指针，其指向的函数能够原地 drop 当前胖指针所引用的对象。size 和 align 两个域分别给出对象的大小和内存对齐，这两个域共同构成一个 std::alloc::Layout 结构，可用于释放当前胖指针所引用的对象所占据的内存。虚表 header 的存在使得 trait object 总是能被销毁和释放。例如当销毁一个 Box&lt;dyn Trait&gt; 时，Box::&lt;dyn Trait&gt;::drop 会首先调用虚表中的 drop_in_place 函数原地销毁 Box 所引用的对象，然后再调用 dealloc 函数并传递虚表中的 size 和 align 释放堆空间。<br>在虚表 header 之后是一系列的字段。在最普遍的情况下，每个字段代表一个指向 trait 所定义的函数的指针。例如，对于下列 object safe 的 trait:<br>pub trait Trait {
    fn fun1(&amp;self);
    fn fun2(&amp;self);
    fn fun3(&amp;self);
}
<br>如果类型T 实现了 Trait，那么为 T 生成的 Trait 虚表的结构为：<br>+--------------------------+
| fn drop_in_place(*mut T) |
+--------------------------+
| size of T                |
+--------------------------+
| align of T               |
+--------------------------+
| fn &lt;T as Trait&gt;::fun1    |
+--------------------------+
| fn &lt;T as Trait&gt;::fun2    |
+--------------------------+
| fn &lt;T as Trait&gt;::fun3    |
+--------------------------+
<br>Trait 中的函数按照声明顺序依次排列在虚表 header 之后。当通过一个指向 T 对象的 &amp;dyn Trait 调用 fun2 函数时，程序会先从虚表的第 5 个域中得到为 T 实现的 Trait::fun2 函数的地址，然后再调用之。<br><br>Object safe 的 trait 可以有 super trait。例如：<br>pub trait Grand {
    fn grand_fun1(&amp;self);
    fn grand_fun2(&amp;self);
}

pub trait Parent : Grand {
    fn parent_fun1(&amp;self);
    fn parent_fun2(&amp;self);
}

pub trait Trait : Parent {
    fn fun(&amp;self);
}
<br>如果类型T 实现了 Trait，那么此时为 T 生成的 Trait 虚表的结构为：<br>+-------------------------------+
| fn drop_in_place(*mut T)      |
+-------------------------------+
| size of T                     |
+-------------------------------+
| align of T                    |
+-------------------------------+
| fn &lt;T as Grand&gt;::grand_fun1   |
+-------------------------------+
| fn &lt;T as Grand&gt;::grand_fun2   |
+-------------------------------+
| fn &lt;T as Parent&gt;::parent_fun1 |
+-------------------------------+
| fn &lt;T as Parent&gt;::parent_fun2 |
+-------------------------------+
| fn &lt;T as Trait&gt;::fun          |
+-------------------------------+
<br>可以看到，此时Trait 以及 Trait 的所有直接或间接父 trait 所定义的所有函数均包含在虚表 header 之后，且顺序为后序（即先排布 Trait 的父 trait 所定义的所有函数，最后再排布 Trait 所定义的所有函数）。这样的排布方式使得在得到 T 类型的 Trait 虚表的同时也同时得到了 T 类型的 Parent 虚表和 Grand 虚表。T 类型的 Grand 虚表恰好由 Trait 虚表的前五个域构成，T 类型的 Parent 虚表恰好由 Trait 虚表的前七项构成。这使得向上转换变得非常简单。<br>所谓向上转换，即 Rust 允许将&amp;dyn Trait 转换为 &amp;dyn Parent 或 &amp;dyn Grand 。在向上转换的过程中，胖指针的对象地址域保持不变，但 metadata 域可能需要进行调整，因为不同的 trait 可能具有不同的虚表地址。但在当前示例中，向上转换不需要调整 metadata 域，因为一个指向 Trait 虚表的指针同时也指向 Parent 虚表和 Grand 虚表。在后文中我们会进一步介绍需要调整 metadata 域的向上转换的情况。<br>
注意：目前 stable Rust 暂不支持向上转换。要使用向上转换特性，必须使用 nightly 工具链，并向源文件中添加 #![feature(trait_upcasting)] 特性开关。
<br><br>Trait 可以有多个 super trait。例如：<br>pub trait Base {
    fn base_fun1(&amp;self);
    fn base_fun2(&amp;self);
}

pub trait Left : Base {
    fn left_fun1(&amp;self);
    fn left_fun2(&amp;self);
}

pub trait Right : Base {
    fn right_fun1(&amp;self);
    fn right_fun2(&amp;self);
}

pub trait Trait : Left + Right {
    fn fun(&amp;self);
}
<br>如果类型T 实现了 Trait，那么此时为 T 生成的 Trait 虚表的结构为：<br>+-----------------------------+
| fn drop_in_place(*mut T)    |
+-----------------------------+
| size of T                   |
+-----------------------------+
| align of T                  |
+-----------------------------+
| fn &lt;T as Base&gt;::base_fun1   |
+-----------------------------+
| fn &lt;T as Base&gt;::base_fun2   |
+-----------------------------+
| fn &lt;T as Left&gt;::left_fun1   |
+-----------------------------+
| fn &lt;T as Left&gt;::left_fun2   |
+-----------------------------+
| fn &lt;T as Right&gt;::right_fun1 |
+-----------------------------+
| fn &lt;T as Right&gt;::right_fun2 |
+-----------------------------+
| ptr to &lt;T as Right&gt;::vtable |
+-----------------------------+
| fn &lt;T as Trait&gt;::fun        |
+-----------------------------+
<br>可以看到，此时Trait 及其所有直接或间接父 trait 所定义的所有函数仍然包含在虚表内，因此通过 &amp;dyn Trait 调用的函数仍然可以直接从虚表内得到其实际目标函数的地址。另外，Trait 虚表内仍然包含有效的 Base 虚表和 Left 虚表。因此，将 &amp;dyn Trait 向上转换为 &amp;dyn Left 或 &amp;dyn Base 仍然是极其简单的，不需要调整胖指针的 metadata 域。但是，将 &amp;dyn Trait 向上转换为 &amp;dyn Right 就需要调整 metadata 域了，因为 Trait 虚表内并不包含一个有效的 Right 虚表。这也是 Trait 虚表中 ptr to &lt;T as Right&gt;::vtable 域的作用：在执行向上转换时，程序会读取 Trait 虚表的这个域作为得到的 &amp;dyn Right 胖指针的 metadata 。这也是 Rust 向上转换与 C++ 向上转换的一个很大不同：在 C++ 中的向上转换通常并不需要访问虚表（除非需要执行跨虚继承边界的转换），但在 Rust 中向上转换可能需要访问虚表。<br>更加一般地，对于一个 object safe 的 traitTr，将其第一个父 trait、第一个父 trait 的第一个父 trait、…… 这一系列直接或间接父 trait 记为这个 trait 的 PrefixTrait 集合。在将 &amp;dyn Tr 向上转换时，如果转换到的目标 trait 包含在 PrefixTrait 集合内，那么这个向上转换是平凡的：不需要调整胖指针的 metadata 域。否则，这个向上转换需要在 Tr 的虚表内读取目标 trait 的虚表指针作为转换结果的 metadata 。在 Tr 的虚表结构中，位于 PrefixTrait 集合中的父 trait 只需要排布他们所定义的函数即可；对于其他父 trait 还需要额外在虚表内排布一个指向其虚表的指针用于向上转换。<br><br>Rust 提供了一个特殊的 trait：std::any::Any 。该 trait 支持向下转换，即可以将 &amp;dyn Any 转换为 T 。转换过程中会对胖指针所指向的对象的实际类型进行检查，确认其确实是一个 T 类型的对象。Any trait 的虚表结构有一些特殊；在虚表 header 之后，Any 虚表仅包含一个域，这个域直接给出胖指针指向的对象的类型标识（由一个 std::any::TypeId 类型的值表示）。例如，对于任意的 T: 'static，编译器为其生成的 Any 虚表为：<br>+--------------------------+
| fn drop_in_place(*mut T) |
+--------------------------+
| size of T                |
+--------------------------+
| align of T               |
+--------------------------+
| TypeId of T              |
+--------------------------+
<br>在执行向下转换时，程序首先检查转换到的类型是否与虚表中给出的TypeId 所标识的类型一致。若类型检查通过，向下转换操作可以直接返回胖指针中的指针域作为转换结果。<br><br>
<br>Vtable format to support dyn upcasting coercion <a rel="noopener nofollow" class="external-link" href="https://rust-lang.github.io/dyn-upcasting-coercion-initiative/design-discussions/vtable-layout.*html" target="_blank">https://rust-lang.github.io/dyn-upcasting-coercion-initiative/design-discussions/vtable-layout.*html</a>*
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-虚表布局规则介绍.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust 虚表布局规则介绍.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:50:04 GMT</pubDate></item><item><title><![CDATA[Rust 闭包 lifetime may not live long enough 问题]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a><br>代码：<br>...
    fn handlers(self) -&gt; crate::server::request::Handlers {
        vec![(
            "/tree",
            routing::get(move || async {
                (
                    StatusCode::OK,
                    Json(json!(self.clone().tree(self.clone().root))),
                )
            }),
        )]
    }
...
<br>编译错误：<br>error: lifetime may not live long enough
  --&gt; src/storage/filesystem/mod.rs:45:34
   |
45 |               routing::get(move || async {
   |  __________________________-------_^
   | |                          |     |
   | |                          |     return type of closure `{async block@src/storage/filesystem/mod.rs:45:34: 50:14}` contains a lifetime `'2`
   | |                          lifetime `'1` represents this closure's body
46 | |                 (
47 | |                     StatusCode::OK,
48 | |                     Json(json!(self.clone().tree(self.clone().root))),
49 | |                 )
50 | |             }),
   | |_____________^ returning this value requires that `'1` must outlive `'2`
   |
   = note: closure implements `Fn`, so references to captured variables can't escape the closure
<br>这是因为 handlers 里面的闭包捕获了一个引用，并且尝试返回一个包含该引用的值导致的。<br>细说就是：闭包内部使用了 self.clone() 来获取一个新的实例，然后在异步块中返回一个 JSON 对象，这个 JSON 对象依赖于 self.tree() 的结果。因为闭包捕获了 self 的引用，所以它必须保证 self 在闭包执行完毕后仍然有效。<br>解决这个问题的思路是：确保闭包中的所有引用都在闭包执行完毕之前就不再被使用。<br>
就是说，要将闭包的作用域限制在一个更短的生命周期内，或者使用其他方式来避免闭包捕获长期存在的引用:<br>
...
    fn handlers(self) -&gt; crate::server::request::Handlers {
        let tree = json!(self.clone().tree(self.clone().root));
        vec![(
            "/tree",
            routing::get(move || async { (StatusCode::OK, Json(tree)) }),
        )]
    }
...
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-闭包-lifetime-may-not-live-long-enough-问题.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust 闭包 lifetime may not live long enough 问题.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:50:00 GMT</pubDate></item><item><title><![CDATA[有关 Axum 中 WebSocket 的使用]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:rust" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#rust</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:rust" class="tag" target="_blank" rel="noopener nofollow">#rust</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a><br><br>创建一个项目，并将 axum 添加到依赖中：<br>cargo new axum-ws-test
cd axum-ws-test
cargo add tokio -F full
cargo add serde_json
cargo add axum -F ws
cargo add rand
<br>然后用自己喜欢的编辑器/IDE 打开整个项目，找到 Cargo.toml，可以看到 Cargo.toml 如下：<br>[dependencies]
axum = { version = "0.6.20", features = ["ws"] }
rand = "0.8.5"
serde_json = "1.0.107"
tokio = { version = "1.33.0", features = ["full"] }
<br>
以上依赖版本为本文编写时的最新稳定版，需要注意和自己的版本区别，axum 的功能基本都有解释，可以查看 <a data-tooltip-position="top" aria-label="https://docs.rs/axum/latest/axum/#feature-flags" rel="noopener nofollow" class="external-link" href="https://docs.rs/axum/latest/axum/#feature-flags" target="_blank"><code></code> 文档</a>axum
<br><br>从官方文档可以看到，一个 axum 程序，包含了程序入口、路由、路由服务和 axum 服务端（即 axum::Server），我们先将其基本结构写入到 main.rs 的文件中（代码来自官方文档）：<br>use axum::{routing::get, Router};

// 主函数入口
#[tokio::main]
async fn main() {
    // 路由
    let app = Router::new().route("/", get(|| async { "Hello, World!" }));

    // axum 的 Server
    axum::Server::bind(&amp;"0.0.0.0:8081".parse().unwrap())
        .serve(app.into_make_service())
        .await
        .unwrap();
}
<br>运行程序后，打开 <a rel="noopener nofollow" class="external-link" href="https://muqiuhan.github.io/wiki/localhost:8081" target="_blank">localhost:8081</a>，应该可以看见网页上有 Hello, World!。<br><br>前端使用 Vue，建议选择另一个文件夹来创建前端项目。输入下面的指令来创建前端项目，项目名称命名为 axum-test-front：<br>npm create vue@latest
cd axum-test-front
npm install
npm run dev
<br><br>我们模拟的情况试试，前端每次点击按钮都会获取后端的一个随机数。一开始我们先不使用 WebSocket，来测试一下效果。<br><br>添加一个函数，用于前端获取随机数：<br>use axum::{response::Json, routing::get, Router};
use rand::Rng;
use serde_json::{json, Value};

#[tokio::main]
async fn main() {
    let app = Router::new()
        .route("/", get(|| async { "Hello, World!" }))
        // add here
        .route("/random", get(get_rand));

    // Server
}
// handler
async fn get_rand() -&gt; Json&lt;Value&gt; {
    let mut rng = rand::thread_rng();
    Json(json! ({"num": rng.gen_range(1..=100)}))
}
<br>可以在浏览器中输入 <a rel="noopener nofollow" class="external-link" href="https://muqiuhan.github.io/wiki/localhost:8081/random" target="_blank">localhost:8081/random</a>来测试，每个刷新应该都可以得到一个新的 num 值。<br><br>前端在 App.vue 中添加一个按钮和一个用于显示获取到的随机数的节点，代码如下：<br>&lt;script&gt;
  export default {
    data() {
      return {
        num: null,
      };
    },
    methods: {
      get_random() {
        // 从后端的对应地址获取随机数
        fetch("http://localhost:8081/random", {
          mode: "cors",
          headers: {
            accpet: "application/json",
          },
        })
          .then((response) =&gt; response.json())
          .then((data) =&gt; {
            this.num = data.num;
          });
      },
    },
  };
&lt;/script&gt;

&lt;template&gt;
  &lt;main&gt;
    &lt;button @click="get_random()"&gt;click to get num&lt;/button&gt;
    &lt;div&gt;num is {{ num }}&lt;/div&gt;
  &lt;/main&gt;
&lt;/template&gt;
<br>执行的时候会发现无法从后端拿到数据，这是因为后端没有配置跨域请求。<br><br>首先需要添加一个依赖，输入下面的指令添加：<br>cargo add tower-http -F cors
<br>然后在创建路由之前，先新建一个跨域的许可：<br>use axum::{http::HeaderValue, response::Json, routing::get, Router};
use rand::Rng;
use serde_json::{json, Value};
use tower_http::cors::{Any, CorsLayer};

#[tokio::main]
async fn main() {
    // 跨域配置
    let cors = CorsLayer::new()
        .allow_methods(Any)
        .allow_headers(Any)
        .allow_origin("http://localhost:5173".parse::&lt;HeaderValue&gt;().unwrap());

    let app = Router::new()
        .route("/", get(|| async { "Hello, World!" }))
        // 为路由方法处理添加跨域许可
        .route("/random", get(get_rand).layer(cors));

    // Server
}
<br>这时再运行后端和前端，打开前端的网页，点击按钮应该可以每次获取到不同的数字。打开开发者控制台，并选择网络（没有的话点加号或者 》 可以找到），再多次点击按钮，可以看到每次点击按钮都发送了一次 Http 请求。<br>
如果前端开发者控制台报 Uncaught (in promise) ReferenceError: num is not defined 这种错误，应该是在给 Vue 中 data 里的字段赋值的时候没有加 this 关键字，把 num 改为 this.num 即可
<br><br><br>添加一个新的函数，函数名为 handle_random，再添加一个函数名为 handle_random_socket：<br>async fn handle_random(ws_upgrade: WebSocketUpgrade) -&gt; Response {
    ws_upgrade.on_upgrade(handle_random_socket)
}

async fn handle_random_socket(mut socket: WebSocket) {
    while let Some(msg) = socket.recv().await {
        let msg = if let Ok(msg) = msg {
            msg
        } else {
            println!("Web Socket Closed");
            return;
        };

    }
}
<br>在 handle_random 中，调用 ws_upgrade 的 on_upgrade 函数可以建立 Web Socket 连接。handle_random_socket 就是用于处理连接时的函数，此处先使用循环来接收来自连接另一端的消息，如果接收发生错误或者无法接收到，则视为连接关闭。WebSocket::recv() 函数在连接关闭后，才会返回 None。<br>设想建立连接后，前端发送一个 get 字符串，后端收到这个字符串，如果收到的确实是 get，则返回给前端一个随机数。那么要做的事情就很简单了，首先要匹配发送过来的消息是否是字符串且内容是否为 get。<br>async fn handle_random_socket(mut socket: WebSocket) {
    while let Some(msg) = socket.recv().await {
        let msg = if let Ok(msg) = msg {...};
        // 匹配字符串
        if let Message::Text(text) = msg {
            if text.eq("get") {
                todo!()
            }
        }
    }
}
<br>在匹配成功后，将生成一个随机数，并返回给前端。这里会用到 WebSocket 的 Send 函数来返回响应，获取随机数可以用到之前写的函数。前端对数据的宽容性较大，所以可以考虑直接返回 JSON 格式的文本：<br>async fn handle_random_socket(mut socket: WebSocket) {
    while let Some(msg) = socket.recv().await {
        let msg = if let Ok(msg) = msg {...};

        if let Message::Text(text) = msg {
            if text.eq("get") {
                if socket
                    // 返回随机数
                    .send(Message::Text(get_rand().await.to_string()))
                    .await
                    .is_err()
                {
                    // 如果出错了就关闭连接
                    println!("Web Socket Closed");
                    return;
                }
            }
        }
    }
    println!("Web Socket Closed");
}
<br>编写完成，最后将 handle_random 添加到路由中。<br>#[tokio::main]
async fn main() {
    // 跨域配置
    // ...
    // 路由
    let app = Router::new()
        .route("/", get(|| async { "Hello, World!" }))
        .route("/random", get(get_rand).layer(cors));
        .route("/ws/random", get(handle_random));

    // axum 的 Server
    // ...
}
<br><br>在组件挂载时，会尝试创建一个 WebSocket 的连接，并且绑定一个函数，用于在收到消息后，设置 num 的值：<br>&lt;script&gt;
export default {
  data() {
    return {
      num: null,
      ws: null,
    };
  },
  methods: {
    build_connect() {
      // 防止子域的 this 与 vue 的 this 冲突
      var that = this;
      that.ws = new WebSocket("ws://localhost:8081/ws/random");
      // 收到消息后，设置 num 的值
      that.ws.addEventListener("message", function (event) {
        that.num = JSON.parse(event.data).num;
      });
    },
  },
  mounted() {
    this.build_connect();
    window.onclose = () =&gt; {
      this.ws.close();
    };
  },
};
<br>之后的每次点击都会变为通过连接来向后端发送消息。按照逻辑修改后的代码如下：<br>get_random() {
  // 通过连接向后端发送信息
  this.ws.send("get");
},
<br><br>首先启动后端：<br>cargo run
<br>然后启动前端：<br>npm run dev
<br>打开前端页面，点击按钮就可以看到每次都能够从后端获取到不同的随机数的效果了。<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.rs/axum" rel="noopener nofollow" class="external-link" href="https://docs.rs/axum" target="_blank">axum</a>

<br><a data-tooltip-position="top" aria-label="https://docs.rs/axum/latest/axum/extract/ws/index.html" rel="noopener nofollow" class="external-link" href="https://docs.rs/axum/latest/axum/extract/ws/index.html" target="_blank">axum::extract::ws - Rust</a>


<br><a data-tooltip-position="top" aria-label="https://cn.vuejs.org/" rel="noopener nofollow" class="external-link" href="https://cn.vuejs.org/" target="_blank">vue</a>
<br><a data-tooltip-position="top" aria-label="https://developer.mozilla.org/zh-CN/docs/Web/API/WebSocket" rel="noopener nofollow" class="external-link" href="https://developer.mozilla.org/zh-CN/docs/Web/API/WebSocket" target="_blank">WebSocket - Web API 接口参考 | MDN</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/有关-axum-中-websocket-的使用.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/有关 Axum 中 WebSocket 的使用.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:50:01 GMT</pubDate></item><item><title><![CDATA[50 TypeScript Fck Ups 50 Subtle Mistakes to Screw Your Code and How to Avoid (Azat Mardan)]]></title><description><![CDATA[ 
 <br>这本书描述了 100 个最常见和最关键的 TypeScript 使用错误的场景，但目前还没写完，像是一个经验性的小册子。<br>当前有的章节主要内容有:<br>
<br>常见的基础错误

<br>过度使用 any
<br>不使用 strict mode，不正确使用变量以及滥用 <a data-tooltip-position="top" aria-label="https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-7.html" rel="noopener nofollow" class="external-link" href="https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-7.html" target="_blank">optional chaining</a>
<br>过度使用 <a data-tooltip-position="top" aria-label="https://mariusschulz.com/blog/nullish-coalescing-the-operator-in-typescript" rel="noopener nofollow" class="external-link" href="https://mariusschulz.com/blog/nullish-coalescing-the-operator-in-typescript" target="_blank">nullish</a>
<br>滥用或不恰当得使用 module export
<br>混淆 == 和 ===
<br>忽略类型推导


<br>错误使用类型，别名和接口

<br>混淆类型别名和接口
<br>错误理解或管理 <a data-tooltip-position="top" aria-label="https://www.typescriptlang.org/play/?#example/type-widening-and-narrowing" rel="noopener nofollow" class="external-link" href="https://www.typescriptlang.org/play/?#example/type-widening-and-narrowing" target="_blank">Type widening</a>
<br>如何适当的使用 <a data-tooltip-position="top" aria-label="https://app.immersivetranslate.com/pdf/" rel="noopener nofollow" class="external-link" href="https://app.immersivetranslate.com/pdf/" target="_blank">Type guards</a>
<br>如何正确使用 readonly 修饰
<br>如何正确使用 keyof 和 <a data-tooltip-position="top" aria-label="https://www.typescriptlang.org/docs/handbook/utility-types.html#extracttype-union" rel="noopener nofollow" class="external-link" href="https://www.typescriptlang.org/docs/handbook/utility-types.html#extracttype-union" target="_blank">Extract</a> 等实用类型


<br>规范地编写函数和方法

<br>如何正确编写重载函数的签名来增强类型安全
<br>明确函数的返回类型
<br>在函数中正确使用 <a data-tooltip-position="top" aria-label="https://www.tutorialsteacher.com/typescript/rest-parameters" rel="noopener nofollow" class="external-link" href="https://www.tutorialsteacher.com/typescript/rest-parameters" target="_blank">Rest 参数</a>
<br>正确使用 this 和 globalThis 以及 bind, apply, call, StrictBindingCallApply 
<br>适当的为函数使用 ReturnType, Paramaters, Partial, ThisParamaterType, OmitThisParamater 等实用类型。


<br>规范地编写类和构造函数

<br>实现接口和抽象类
<br>管理静态成员和访问修饰符
<br>合理地初始化类中的属性
<br>组织类的 getter setter
<br>在类中使用装饰器
<br>确保安全的 overrides


]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/typescript/50-typescript-fck-ups-50-subtle-mistakes-to-screw-your-code-and-how-to-avoid-(azat-mardan).html</link><guid isPermaLink="false">Computer Science/Programming Language/Typescript/50 TypeScript Fck Ups 50 Subtle Mistakes to Screw Your Code and How to Avoid (Azat Mardan).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 07 Oct 2024 06:51:23 GMT</pubDate></item><item><title><![CDATA[What is Reflection API in programming?]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:javascript" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#javascript</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typescript" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typescript</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:javascript" class="tag" target="_blank" rel="noopener nofollow">#javascript</a> <a href="https://muqiuhan.github.io/wiki?query=tag:typescript" class="tag" target="_blank" rel="noopener nofollow">#typescript</a><br>Before talking about Reflection API in JavaScript, let’s review some general concepts to understand better.<br><br>The Reflection API in programming refers to a set of built-in tools and capabilities within a programming language that allows a program to inspect and manipulate its own structure, behavior, and metadata during runtime. Reflection enables the examination of classes, interfaces, methods, and fields at runtime without knowing their names at compile time.<br>Key functionalities provided by Reflection API typically include:<br>
<br>Inspecting Classes: Reflection allows you to examine the structure of classes, including fields, methods, constructors, and annotations associated with them.
<br>Instantiating Classes Dynamically: With reflection, you can create instances of classes dynamically at runtime, even if you don’t know the class name at compile time.
<br>Invoking Methods Dynamically: You can invoke methods on objects dynamically without knowing their names at compile time.
<br>Accessing and Modifying Fields: Reflection provides the ability to access and modify fields of classes, including private fields, which are otherwise not accessible.
<br>Examining Annotations: You can inspect annotations associated with classes, methods, fields, etc., and take appropriate actions based on their presence or values. And even more!
<br><br>Now let’s see some examples in JavaScript:<br>
<br>Inspecting Object Properties: You can use for...in loop or Object.keys() to iterate over the properties of an object dynamically:
<br>const obj = {  
    name: 'John',  
    age: 30,  
    city: 'New York'  
};  
  
for (const key in obj) {  
    console.log(`${key}: ${obj[key]}`);  
}
<br>
<br>Accessing and Modifying Object Properties Dynamically: You can access and modify object properties dynamically using square bracket notation:
<br>const obj = { name: 'John' };  
  
// Accessing property dynamically  
const propertyName = 'name';  
console.log(obj[propertyName]); // Output: John  
  
// Modifying property dynamically  
const newValue = 'Jane';  
obj[propertyName] = newValue;  
console.log(obj.name); // Output: Jane

3. Invoking Methods Dynamically: JavaScript allows you to invoke object methods dynamically using square bracket notation:

const obj = {  
    greet: function() {  
        console.log('Hello!');  
    }  
};  
  
const methodName = 'greet';  
obj[methodName](); // Output: Hello!
<br><br>In JavaScript, compared to languages like Java or C#, there are certain reflection capabilities that are not directly available or are more limited:<br>
<br>Strict Typing: JavaScript is dynamically typed, which means types are determined at runtime. In Java and C#, reflection can be used to inspect and manipulate types at compile time due to their static typing nature. This means that in JavaScript, you may not have as much compile-time safety when using reflection-like features.
<br>Type Metadata: Java and C# have extensive metadata available at runtime due to their static typing nature and rich type systems. In JavaScript, since types are determined dynamically, there is less type metadata available at runtime.
<br>Reflection Emit: In languages like C#, you can use reflection emit to generate IL (Intermediate Language) code dynamically at runtime and execute it. JavaScript does not have a direct equivalent to IL code or the ability to emit and execute code dynamically at runtime in the same manner.
<br>Annotations and Attributes: Java and C# support annotations and attributes, respectively, which can be used to add metadata to types, methods, and fields. While JavaScript has something similar in the form of decorators (with TypeScript or Babel), they are not as deeply integrated into the language and runtime as annotations in Java or attributes in C#.
<br>Overall, while JavaScript provides some level of reflection-like capabilities, its dynamic nature and design differences mean that certain features available in statically-typed languages like Java or C# are not directly possible or are more limited in JavaScript.<br><br>There are some packages that solves some of the limitations. Reflect-Metadata is one of them.<br>Before seeing what Reflect-Metadata is, let’s see what the problem is!<br><br>Let’s say we want to disregard users who don’t have permission to a certain API. We can implement in this way:<br>function adminOnlyMethod() {  
  if (currentUser.role === roles.ADMIN) {  
    console.log("This method is only for admins.");  
    // Perform admin-specific actions  
  } else {  
    console.log("You don't have permission to perform this action.");  
    // Optionally, handle lack of permissions  
  }  
}

But what if we have one hundred APIs? Do we want to implement this logic each time? The answer is no. But still, we can accomplish acceptable ways without the need for Reflection API. To achieve a more declarative approach and avoid checking roles in every function, you can use a higher-order function or a middleware-like pattern to wrap your methods with role-based access control logic. Here’s an example using a higher-order function:

// Higher-order function to create role-restricted methods  
function restrictToRole(role, method) {  
  return function (...args) {  
    if (currentUser.role === role) {  
      return method.apply(this, args); // Execute the original method  
    } else {  
      console.log("You don't have permission to perform this action.");  
      // Optionally, handle lack of permissions  
    }  
  };  
}  
  
// Example method  
function adminOnlyMethod() {  
  console.log("This method is only for admins.");  
  // Perform admin-specific actions  
}  
  
// Wrap the method with role-based access control  
const restrictedAdminMethod = restrictToRole(roles.ADMIN, adminOnlyMethod);
<br>This is not too bad! But still not the best option! Here is where Reflect-Metadata comes into place.<br><br>reflect-metadata is a library in JavaScript and TypeScript that provides reflection metadata API for runtime reflection capabilities. It enables developers to add and read metadata to/from class declarations and members at runtime.<br>Here’s a brief overview of how reflect-metadata works:<br>
<br>Adding Metadata: Developers can use Reflect.metadata() to add metadata to class declarations, methods, or properties. Metadata is added using a metadata key and a metadata value.
<br>Accessing Metadata: The library provides methods such as Reflect.getMetadata() to retrieve metadata associated with class declarations, methods, or properties at runtime.
<br>Usage in TypeScript: reflect-metadata is often used in conjunction with TypeScript decorators. Decorators allow developers to add metadata to classes and members in a more intuitive and declarative way.
<br>Runtime Reflection: With the help of reflect-metadata, developers can perform runtime reflection tasks such as dependency injection, serialization, validation, and more.
<br>Here’s a simple example of using reflect-metadata with TypeScript decorators:<br>import 'reflect-metadata';  
  
// Define a decorator function  
function MyDecorator(target: any, key: string) {  
  // Add metadata to the target (class) with the given key  
  Reflect.defineMetadata('customMetadataKey', 'someValue', target, key);  
}  
  
class MyClass {  
  @MyDecorator  
  myMethod() {}  
}  
  
// Retrieve metadata at runtime  
const metadataValue = Reflect.getMetadata('customMetadataKey', MyClass.prototype, 'myMethod');  
console.log(metadataValue); // Output: 'someValue'
<br><br>What if you solve the problem with a decorator?<br>@Post('login')  
@Public()  
async login(@Request() req: UserRequest): Promise&lt;LoginResponseDto&gt; {  
  return this.authService.login(req.user);  
}
<br>We define a @Public decorator as follows (I did it in NestJS which abstracts the same lib):<br>import { SetMetadata } from '@nestjs/common';  
  
export const IS_PUBLIC_KEY = 'isPublic';  
export const Public = () =&gt; SetMetadata(IS_PUBLIC_KEY, true);

Now in my Guard, I can check if the current API is decorated with the given metadata key or not:

@Injectable()  
export class JwtAuthGuard extends AuthGuard('jwt') {  
  constructor(private reflector: Reflector) {  
    super();  
  }  
  
  canActivate(context: ExecutionContext) {  
    // we pass IS_PUBLIC_KEY as metadata key to see  
    // if the method is decorated with that  
    const isPublic = this.reflector.getAllAndOverride&lt;boolean&gt;(IS_PUBLIC_KEY, [  
      context.getHandler(),  
      context.getClass(),  
    ]);  
  
    if (isPublic) {  
      return true;  
    }  
  
    return super.canActivate(context);  
  }  
}
<br>This is a more declarative way and reduces code duplication!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/typescript/a-deep-dive-into-the-reflection-api-in-javascript.html</link><guid isPermaLink="false">Computer Science/Programming Language/Typescript/A Deep Dive into the Reflection API in Javascript.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:29:12 GMT</pubDate></item><item><title><![CDATA[What is a DI Container?]]></title><description><![CDATA[ 
 <br>#typescript 
<br>In this post, we are going to implement a Dependency Injection Container also known as Inversion of Control Container (IoC container) from scratch with typescript.<br><br>Before going through the code, let’s see what exactly a DI container (or IoC container) is! Let’s say we have a user class that needs to work with the database class. The first idea could be:<br>class Database {  
  constructor(user: string, pass: string) { /* connect */ }  
}  
  
class User {  
  constructor() {  
    const database = new Database('test_user', 123456);  
    database.query();  
  }  
}  
  
const user = new User();
<br>In this case, we are violating the Inversion of Control principle. This has two main issues:<br>
<br>Any change in the shape of the Database class will affect the User class (User is highly coupled to the Database class)
<br>It’s hard to write a unit test for the User class.
<br><br>To fix this issue, we need to pass the Database class from outside. Let’s see how it works in the code:<br>class Database {  
  constructor(user: string, pass: string) { /* connect */ }  
}  
  
class User {  
  constructor(private database: Database) {}  
}  
  
const database = new Database('test_user', 123456);  
const user = new User(database);  // ✅
<br>Now, not only User class won’t change in the case of the Database changes, but also you can write unit tests easily by providing a fake database instance.<br><br>Now we fixed our issue, but we still want to manage our classes in a better way so that we don’t need to manage them directly. Let’s say we have such a dependency:<br>User -&gt; Database -&gt; ORM -&gt; EnvVariable<br>Then, whenever we want to instantiate a User class it would be really frustrating to start from SomeOtherClass and create an instance for each class manually, something like this:<br>new EnvVariable() -&gt; new ORM(env) -&gt; new Database(orm) -&gt; new User(database)<br><br>Now it’s time to create our DI container from scratch.<br>Let’s first start with an API of the code. What should the result look like? For the sake of simplicity, I added some simple classes instead of our previous example ( User -&gt; Order-&gt; Product)<br>@Injectable()  
class UserService {  
  constructor(private orderService: OrderService) {}  
  
  getUsers() {  
    console.log('getUsers runs!');  
    this.orderService.getOrders();  
  }  
}  
  
@Injectable()  
class OrderService {  
  constructor(private productService: ProductService) {}  
  
  getOrders() {  
    console.log('getting orders..!! 📦📦📦');  
    this.productService.getProducts();  
  }  
}  
  
@Injectable()  
class ProductService {  
  constructor() {}  
  
  getProducts() {  
    console.log('getting products..!! 🍊🍊🍊');  
  }  
}

And our container result would look like this.

const app = new Container().init([UserService]);  
const userService = app.get(UserService);  
  
userService.getUsers();
<br>I am highly inspired by NestJS. This is what we have in the main.ts of a NestJS project:<br>const app = await NestFactory.create(AppModule);  
const appConfigService = app.get&lt;AppConfigService&gt;(AppConfigService);
<br>As you see, we don’t need to manually instantiate the classes and pass one to another in order to get the user object. Now let’s implement Container class. Before reading make sure you check my previous article here: <a data-href="A Deep Dive into the Reflection API in Javascript" href="https://muqiuhan.github.io/wiki/computer-science/programming-language/typescript/a-deep-dive-into-the-reflection-api-in-javascript.html" class="internal-link" target="_self" rel="noopener nofollow">A Deep Dive into the Reflection API in Javascript</a><br>Because I’ve used Reflection API in the implementation.<br>The next part is @Injectable decorator. We just define metadata on top of each class and set it true. (As you see in the class definition in the previous part)<br>function Injectable() {  
  return function (target: any) {  
    Reflect.defineMetadata('injectable', true, target);  
  };  
}

And finally our `Container` :

class Container {  
  dependencies = [];  
  
  init(deps: any[]) {  
    deps.map((target) =&gt; {  
      const isInjectable = Reflect.getMetadata('injectable', target);  
      if (!isInjectable) return;  
  
      // get the typeof parameters of constructor  
      const paramTypes = Reflect.getMetadata('design:paramtypes', target) || [];  
  
      // resolve dependecies of current dependency  
      const childrenDep = paramTypes.map((paramType) =&gt; {  
        // recursively resolve all child dependencies:  
        this.init([paramType]);  
  
        if (!this.dependencies[paramType.name]) {  
          this.dependencies[paramType.name] = new paramType();  
          return this.dependencies[paramType.name];  
        }  
        return this.dependencies[paramType.name];  
      });  
  
      // resolve dependency by injection child classes that already resolved  
      if (!this.dependencies[target.name]) {  
        this.dependencies[target.name] = new target(...childrenDep);  
      }  
    });  
  
    return this;  
  }  
  
  public get&lt;T extends new (...args: any[]) =&gt; any&gt;(  
    serviceClass: T,  
  ): InstanceType&lt;T&gt; {  
    return this.dependencies[serviceClass.name];  
  }  
}

<br>Let’s analyze it. As you see init method gets an array of classes. So we passed User class and we want to get an instance of that. We iterate over the input and make sure we resolve all the dependencies of the input class and also its dependencies recursively.<br><br>Because we don’t know about the depth of dependencies. What if the product class has another dependency, And so on? That’s why we are calling this.init() inside the init function. This is how all the dependencies are being resolved in our example:<br>... -&gt; new Product -&gt; new Order -&gt; new User
<br><br>Last but not least, make sure you have these configurations in the tsconfig file:<br>"emitDecoratorMetadata": true,  
"experimentalDecorators": true,
<br>You can find the <a data-tooltip-position="top" aria-label="https://github.com/vahidvdn/realworld-design-patterns/tree/master/app/dependency-injection" rel="noopener nofollow" class="external-link" href="https://github.com/vahidvdn/realworld-design-patterns/tree/master/app/dependency-injection" target="_blank">source code here</a>. If you liked it feel free to give it a star 🌟. I hope you enjoyed it!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/typescript/implement-a-dependency-injection-container-in-typescript-from-scratch.html</link><guid isPermaLink="false">Computer Science/Programming Language/Typescript/Implement a Dependency Injection Container in TypeScript from Scratch.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 02 Oct 2024 13:59:52 GMT</pubDate></item><item><title><![CDATA[TypeScript With Rust Errors, No Try Catch, Heresy.]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:typescript" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#typescript</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:typescript" class="tag" target="_blank" rel="noopener nofollow">#typescript</a><br>
It’s hard to miss things when you don’t know different things exist
<br>The first problem is, and personally, I believe it’s the biggest JavaScript problem ever: we don’t know what can throw an error. From a JavaScript error perspective, it’s the same as the following:<br>try {
  let data = “Hello”;
} catch (err) {
  console.error(err);
}
<br>JavaScript doesn’t know; JavaScript doesn’t care. You should know.<br>Second thing, this is perfectly viable code:<br>const request = { name: “test”, value: 2n };
const body = JSON.stringify(request);
const response = await fetch("https://example.com", {
  method: “POST”,
  body,
});
if (!response.ok) {
  return;
}
<br>No errors, no linters, even though this can break your app.<br>Right now, in my head, I can hear, “What’s the problem, just use try/catch everywhere.” Here comes the third problem: we don’t know which one is thrown. Of course, we can somehow guess by the error message, but what about bigger services/functions with many places where errors can happen? Are you sure you are handling all of them properly with one try/catch?<br><br>let greeting_file_result = File::open(“hello.txt”);  
let greeting_file = match greeting_file_result {  
  Ok(file) =&gt; file,  
  Err(error) =&gt; panic!("Problem opening the file: {:?}", error),  
};
<br>The most verbose of the three shown here and, ironically, the best one. So, first of all, Rust handles the errors using its amazing enums (they are not the same as TypeScript enums!). Without going into detail, what is important here is that it uses an enum called Result with two variants: Ok and Err. As you might guess, Ok holds a value and Err holds…surprise, an error :D.<br>The summary here is that Rust always know where there might be an error. And it force you to deal with it right where it appears (mostly). No hidden ones, no guessing, no breaking app with a surprise face.<br>And this approach is just better. By A MILE.<br>We cannot make TypeScript errors work like the Rust. The limiting factor here is the language itself; it doesn’t have the proper tools to do that.<br>But what we can do is try to make it similar. And make it simple:<br>export type Safe&lt;T&gt; =  
  | {  
    success: true;  
    data: T;  
  }  
  | {  
    success: false;  
    error: string;  
  };
<br>we do need a few try/catches. The good thing is we only need about two, not 100,000:<br>export function safe&lt;T&gt;(promise: Promise&lt;T&gt;, err?: string): Promise&lt;Safe&lt;T&gt;&gt;;
export function safe&lt;T&gt;(func: () =&gt; T, err?: string): Safe&lt;T&gt;;
export function safe&lt;T&gt;(
  promiseOrFunc: Promise&lt;T&gt; | (() =&gt; T),
  err?: string,
): Promise&lt;Safe&lt;T&gt;&gt; | Safe&lt;T&gt; {
  if (promiseOrFunc instanceof Promise) {
    return safeAsync(promiseOrFunc, err);
  }
  return safeSync(promiseOrFunc, err);
}

async function safeAsync&lt;T&gt;(
  promise: Promise&lt;T&gt;, 
  err?: string
): Promise&lt;Safe&lt;T&gt;&gt; {
  try {
    const data = await promise;
    return { data, success: true };
  } catch (e) {
    console.error(e);
    if (err !== undefined) {
      return { success: false, error: err };
    }
    if (e instanceof Error) {
      return { success: false, error: e.message };
    }
    return { success: false, error: "Something went wrong" };
  }
}

function safeSync&lt;T&gt;(
  func: () =&gt; T, 
  err?: string
): Safe&lt;T&gt; {
  try {
    const data = func();
    return { data, success: true };
  } catch (e) {
    console.error(e);
    if (err !== undefined) {
      return { success: false, error: err };
    }
    if (e instanceof Error) {
      return { success: false, error: e.message };
    }
    return { success: false, error: "Something went wrong" };
  }
}
<br>This is just a wrapper with our Safe type as the return one. But sometimes simple things are all you need. Let’s combine them with the example from above.<br>const request = { name: “test”, value: 2n };  
const body = safe(  
  () =&gt; JSON.stringify(request),  
  “Failed to serialize request”,  
);  
if (!body.success) {  
  // handle error (body.error)  
  return;  
}  
const response = await safe(  
  fetch("https://example.com", {  
    method: “POST”,  
    body: body.data,  
  }),  
);  
if (!response.success) {  
  // handle error (response.error)  
  return;  
}  
if (!response.data.ok) {  
  // handle network error  
  return;  
}  
// handle response (body.data)
<br>New solution is longer, but it performs better because of the following reasons:<br>
<br>no try/catch
<br>we handle each error where it occurs
<br>we can specify an error message for a specific function
<br>we have a nice top-to-bottom logic, all errors on top, then only the response at the bottom
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/typescript/typescript-with-rust-errors,-no-try-catch,-heresy..html</link><guid isPermaLink="false">Computer Science/Programming Language/Typescript/TypeScript With Rust Errors, No Try Catch, Heresy..md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:51:07 GMT</pubDate></item><item><title><![CDATA[You don’t Need a Book to Know DDD(Domain-Driven Design)]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:domain-modeling-driven" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#domain-modeling-driven</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <a href="https://muqiuhan.github.io/wiki?query=tag:domain-modeling-driven" class="tag" target="_blank" rel="noopener nofollow">#domain-modeling-driven</a> <br><img alt="Pasted image 20241115123905.png" src="https://muqiuhan.github.io/wiki/computer-science/software-engineering/you-don’t-need-a-book-to-know-ddd(domain-driven-design)/pasted-image-20241115123905.png"><br>It took me a while to figure out the patterns behind DDD, though the most important thing are ubiquitous language and bounded context. In this article, I don’t want to address the philosophical part of DDD, instead I want to dive into the practical implementation of some parts to connect with the philosophy behind it. If you are not familiar with the two most important concepts behind DDD, there are countless articles out there explaining them, just go and search them.<br>Somewhere I read this conclusion of DDD patterns and I found it quite good:<br>
You model your business using Entities (the ID matters) and Value Objects (the values matter). You use Repositories to retrieve and store them. You create them with the help of Factories. If an object is too complex for a single class, you’ll create Aggregates that will bind Entities &amp; Value Objects under the same root. If a business logic doesn’t belong to a given object, you’ll define Services that will manipulate the involved elements. Eventually, when the state of the business changes (a change that matters to business experts), you’ll publish Domain Events to communicate the change.
<br>Breaking down each concept will help piece up the whole picture of DDD.<br>I just use some Rust example, they are very readable I promise!<br><br>Entities are objects that have a distinct identity that runs through time and different states. The identity is usually represented by an ID.<br>// src/entities/user.rs
pub struct User {
    pub id: u32,
    pub username: String,
    pub email: String,
    pub age: u8,
}

impl User {
    pub fn new(id: u32, username: String, email: String, age: u8) -&gt; Self {
        User { id, username, email, age }
    }
}
<br><br>Value objects are objects that are defined by their attributes. They do not have a distinct identity.<br>// src/value_objects/address.rs
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Address {
    pub street: String,
    pub city: String,
    pub zip_code: String,
}

impl Address {
    pub fn new(street: String, city: String, zip_code: String) -&gt; Self {
        Address { street, city, zip_code }
    }
}
<br><br>Repositories are used to retrieve and store entities. They act as a collection of entities.<br>// src/repositories/user_repository.rs
use crate::entities::user::User;

pub struct UserRepository {
    users: Vec&lt;User&gt;,
}

impl UserRepository {
    pub fn new() -&gt; Self {
        UserRepository { users: Vec::new() }
    }

    pub fn add(&amp;mut self, user: User) {
        self.users.push(user);
    }

    pub fn find_by_id(&amp;self, id: u32) -&gt; Option&lt;&amp;User&gt; {
        self.users.iter().find(|&amp;user| user.id == id)
    }
}

<br><br>Factories are used to create complex objects and aggregates.<br>// src/factories/user_factory.rs
use crate::entities::user::User;

pub struct UserFactory;

impl UserFactory {
    pub fn create_user(id: u32, username: String, email: String, age: u8) -&gt; User {
        User::new(id, username, email, age)
    }
}

<br><br>Aggregates are clusters of entities and value objects that are treated as a single unit.<br>// src/aggregates/order.rs
use crate::entities::user::User;
use crate::value_objects::address::Address;

pub struct Order {
    pub id: u32,
    pub user: User,
    pub shipping_address: Address,
}

impl Order {
    pub fn new(id: u32, user: User, shipping_address: Address) -&gt; Self {
        Order { id, user, shipping_address }
    }
}
<br><br>Services contain business logic that doesn’t naturally fit within an entity or value object.<br>// src/services/order_service.rs
use crate::aggregates::order::Order;
use crate::entities::user::User;
use crate::value_objects::address::Address;

pub struct OrderService;

impl OrderService {
    pub fn create_order(user: User, shipping_address: Address) -&gt; Order {
        let order_id = 1; // In a real application, this would be generated
        Order::new(order_id, user, shipping_address)
    }
}

<br>Key Points about Services are:<br>1.Stateless: Services are typically stateless. They do not hold any state themselves but operate on the state of entities and value objects.<br>2.Encapsulation of Business Logic: They encapsulate business logic that spans multiple entities or value objects or that doesn’t fit neatly within a single entity or value object.<br>3.Coordination: They often coordinate interactions between multiple entities and value objects.<br><br>Domain events are used to communicate changes in the state of the business.<br>// src/events/user_registered.rs
pub struct UserRegistered {
    pub user_id: u32,
    pub username: String,
}

impl UserRegistered {
    pub fn new(user_id: u32, username: String) -&gt; Self {
        UserRegistered { user_id, username }
    }
}
<br>They are typically published to an event bus or event store, which can then be used to notify other parts of the system or external systems about these events.<br>Where Domain Events Are Published:<br>1.Event Bus: Domain events are often published to an in-memory event bus within the application. This allows other parts of the application to subscribe to and handle these events. Since it is in-memory, the events are ephemeral and will be lost if the application restarts or crashes. It is suitable for scenarios where events need to be processed quickly and do not require persistence, such as inter-component communication within a single application instance.<br>2.Event Store: For more complex scenarios, domain events can be published to an event store, such as AWS EventBridge, Kafka, or a custom event store. This allows for durable storage and replay of events. Think of it as just:<br>pub struct EventStore {
    file_path: String,
}

<br>An event store can serve as the single source of truth in an event-driven architecture, particularly in the context of event sourcing.<br>In event sourcing, every change to the state of an application is captured as an event and stored in the event store. The current state of the application can be reconstructed by replaying these events.<br>A quick take-away is events are immutable and represent facts that have occurred. Once an event is stored, it is never changed or deleted.<br><img alt="Pasted image 20241115124216.png" src="https://muqiuhan.github.io/wiki/computer-science/software-engineering/you-don’t-need-a-book-to-know-ddd(domain-driven-design)/pasted-image-20241115124216.png"><br>3.Message Brokers: Domain events can also be published to message brokers like RabbitMQ or AWS SNS/SQS for asynchronous processing and integration with other systems.<br><br>mod entities {
    pub mod user;
}

mod value_objects {
    pub mod address;
}

mod repositories {
    pub mod user_repository;
}

mod factories {
    pub mod user_factory;
}

mod aggregates {
    pub mod order;
}

mod services {
    pub mod order_service;
}

mod events {
    pub mod user_registered;
}

use entities::user::User;
use value_objects::address::Address;
use repositories::user_repository::UserRepository;
use factories::user_factory::UserFactory;
use services::order_service::OrderService;
use events::user_registered::UserRegistered;

fn main() {
    // Create a user using the factory
    let user = UserFactory::create_user(1, String::from("Alice"), String::from("alice@example.com"), 30);

    // Create a repository and add the user
    let mut user_repo = UserRepository::new();
    user_repo.add(user);

    // Find the user by ID
    if let Some(user) = user_repo.find_by_id(1) {
        println!("User found: {}", user.username);
    }

    // Create an address
    let address = Address::new(String::from("123 Main St"), String::from("Hometown"), String::from("12345"));

    // Create an order using the service
    let order = OrderService::create_order(user.clone(), address);

    // Publish a domain event, typically published to an event bus or event store, 
   // which can then be used to notify other parts of the system or external systems about these events.
    let event = UserRegistered::new(user.id, user.username.clone());
    println!("Domain event: UserRegistered for user {}", event.username);
}
<br>A key point take-away is that factories focus on object creation, while services focus on business logic and operations.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/software-engineering/you-don’t-need-a-book-to-know-ddd(domain-driven-design)/you-don’t-need-a-book-to-know-ddd(domain-driven-design).html</link><guid isPermaLink="false">Computer Science/Software Engineering/You don’t Need a Book to Know DDD(Domain-Driven Design)/You don’t Need a Book to Know DDD(Domain-Driven Design).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:44:22 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/software-engineering/you-don’t-need-a-book-to-know-ddd(domain-driven-design)/pasted-image-20241115123905.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/software-engineering/you-don’t-need-a-book-to-know-ddd(domain-driven-design)/pasted-image-20241115123905.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Exploring Event Sourcing and Related Patterns in OCaml]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ocaml" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ocaml</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:domain-modeling-driven" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#domain-modeling-driven</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:ocaml" class="tag" target="_blank" rel="noopener nofollow">#ocaml</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <a href="https://muqiuhan.github.io/wiki?query=tag:domain-modeling-driven" class="tag" target="_blank" rel="noopener nofollow">#domain-modeling-driven</a> <br>
<a rel="noopener nofollow" class="external-link" href="https://garba.org/posts/2016/event-sourcing/" target="_blank">https://garba.org/posts/2016/event-sourcing/</a>
<br><br>The Event Sourcing Pattern (Young 2010, 17; Fowler 2005a) is typically used in conjunction with the CQRS Pattern (Homer et al. 2014, 45) to form the “CQRS/ES” superpattern . Event Sourcing is about storing an application’s state as a sequence of events—that reproduce the state when replayed in the right order—as opposed to simply storing the “current state”. The reason as to why CQRS and ES go “hand in hand” is that CQRS suggests that updates should be modelled as commands, which are similar to events although they depict an intention rather than an actual outcome.<br>Introducing Event Sourcing creates new problems for which new solutions are required. For instance, events may be missing or wrong. In this case, the Retroactive Event Pattern is proposed by Fowler (2005c). Likewise, Event Sourcing also lends itself to new interesting features such as Parallel Models, also proposed by Fowler (2005b).<br><br>Jane is enrolled on the Super Miles programme operated by Van Damme Airlines, headquartered in Brussels. The Super Miles programme allows passengers to upgrade to business class in exchange of 10,000 miles.<br>Van Damme Airlines is currently running a promotion which doubles the miles of every flight following a London (LHR) -&gt; Brussels (BRU) connection in order to prompt passengers to take long haul flights from Brussels rather than from London.<br>Jane has just arrived to Bangkok (BKK) from London (LHR) via Brussels (BRU) for business—really, she is not headed to Khao San Road.<br>Knowing that the distance between Brussels and Bangkok is about 5,000 miles, Jane expects to have accumulated over 10,000 miles (with the Super Miles promotion) so that she can return to London in business class after so many days of partying business meetings.<br>Jane approaches the Van Damme Airline’s desk to request the upgrade to business class for her return leg back to London but she is told that she hasn’t got enough miles for that:<br>“Madam, your Super Miles account has only got 5730 miles.”<br>“That’s not possible”, Jane complains and then explains the conditions under which the Super Miles promotion should have given her about double the quoted mileage.<br>In the end, it turns out that the LHR -&gt; BRU leg was not entered onto the system, so the Van Damme’s clerk proceeds to enter this leg which awards Jane an extra 217 miles and says:<br>“Apologies for the mistake Madam. However, you only have 5947 miles after including the missing leg so you still haven’t got the necessary 10,000 miles required for an upgrade to business class”.<br>The clerk cannot enter miles directly onto the system for security reasons and he is unable to trigger the promotion retroactively. Jane is told that she would need to contact the Customer Care to look into the issue. After giving up, Jane asks one more question:<br>“I need to come back to Bangkok in two months time again, if I fly directly from London, would I have enough miles to fly both in and out in business class without the need to come via Brussels in order to double my miles?”<br>The clerk replies: “That is looking too much ahead into the future. If you don’t get the necessary miles you can always top them up by taking one more flight”. It is a calculation way too complex for the Clerk to perform and the system does not allow to enter hypothetical routes either.<br>Jane decides to never fly with Van Damme Airlines again.<br><br>These are a few minor imports that are required by the examples to follow:<br><br>We first define a few types to describe our problem domain which includes airports, miles, routes and a special type of route called “last route” which may double the points of a following route:<br>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/software-engineering/exploring-event-sourcing-and-related-patterns-in-ocaml.html</link><guid isPermaLink="false">Computer Science/Software Engineering/Exploring Event Sourcing and Related Patterns in OCaml.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Tue, 14 Jan 2025 06:22:36 GMT</pubDate></item><item><title><![CDATA[从事件风暴看领域驱动设计]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:domain-modeling-driven" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#domain-modeling-driven</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <a href="https://muqiuhan.github.io/wiki?query=tag:domain-modeling-driven" class="tag" target="_blank" rel="noopener nofollow">#domain-modeling-driven</a><br>事件风暴（Event Storming）是一种领域驱动设计（DDD）的工作坊实践方法，由Alberto Brandolini提出，旨在通过团队协作的方式快速理解和建模业务领域。以下是事件风暴及相关领域驱动设计中的一些核心概念和知识点：<br>1.&nbsp;领域（Domain）：指的是业务相关知识的集合，可以进一步划分为子域。<br>
2.&nbsp;子域（Subdomain）：是领域的一部分，可以是核心域、支撑域或通用域。<br>
3.&nbsp;核心域（Core Domain）：指领域中最核心的部分，通常对应企业的核心业务。<br>
4.&nbsp;通用语言（Ubiquitous Language）：团队所有成员使用的一种语言，用于确保业务和软件之间的沟通一致性。<br>
5.&nbsp;限界上下文（Bounded Context）：定义了一组规则和协议，用于明确领域模型的适用范围。<br>
6.&nbsp;实体（Entity）：具有唯一标识和生命周期的领域对象。<br>
7.&nbsp;值对象（Value Object）：描述了某种特性或属性的对象，没有概念标识。<br>
8.&nbsp;聚合（Aggregate）：一组相关对象的集合，由一个聚合根（Aggregate Root）统一管理。<br>
9.&nbsp;领域事件（Domain Event）：领域中发生的重要事件，可以用于通知其他领域对象或跨限界上下文进行解耦和协作。<br>
10.&nbsp;命令（Command）：表示要执行的操作，通常与事件一一对应。<br>
11.&nbsp;读模型（Read Model）：为了优化读取操作而设计的模型，可能与写模型不同。<br>
12.&nbsp;决策命令（Decision Command）：在事件风暴中，直接导致事件发生的命令。<br>
13.&nbsp;战略设计（Strategic Design）：高层次的抽象和归类，包括理清上下文和子域的划分。<br>
14.&nbsp;战术设计（Tactical Design）：对特定上下文下的模型进行详细设计，包括聚合、实体和值对象。<br>
15.&nbsp;贫血模型（Anemic Domain Model）：领域对象只有属性及其getter/setter方法的纯数据类，业务逻辑通过服务实现。<br>
16.&nbsp;充血模型（Rich Domain Model）：领域对象包含业务逻辑，每个对象都是活跃的。<br>
17.&nbsp;资源库（Repository）：用于检索和持久化领域对象的机制。<br>
18.&nbsp;服务（Service）：在模型中独立的操作，可以是领域服务或应用服务。<br>
19.&nbsp;固定规则（Invariant）：为设计元素做出的断言，必须一直保持为真。<br>事件风暴通常包括以下步骤：<br>
<br>识别领域事件
<br>确定事件顺序
<br>识别命令和触发器
<br>识别聚合和实体
<br>划分限界上下文
<br>识别领域服务和资源库
<br>通过这些步骤，团队可以共同创建出反映业务领域的领域模型，为软件设计和开发提供指导。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/software-engineering/从事件风暴看领域驱动设计.html</link><guid isPermaLink="false">Computer Science/Software Engineering/从事件风暴看领域驱动设计.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:10 GMT</pubDate></item><item><title><![CDATA[如何管理用户界面中的危险操作]]></title><description><![CDATA[ 
 <br>
“任何可能出错的事情都会出错。”我们的目标是防止出现问题，并在出现问题时减轻后果。
<br>界面是用户与系统通信的中介层，界面中的交互通常需要用户执行某些操作。不同的操作可能会导致不同的结果，其中可能有一些对于双方来说都非常重要甚至危险的操作。<br>所以经常需要提供额外的保护措施来“保护”用户执行一些危险甚至无法恢复的操作。<br>
这里的“保护”并不是完全的阻止，否则这个操作也没有存在的必要。<br>
“良好的错误消息很重要，但最好的设计首先会小心地防止问题发生。要么消除容易出错的情况，要么检查它们并在用户承诺操作之前向他们提供确认选项。”
<br><br>危险行为并不意味着要删除某些内容，具体的危险行为应该由系统的“领域”来定义，例如:<br>
<br>进行金融交易
<br>签署法律文件
<br>拉黑用户
<br>授予某些用户权限
<br>...
<br><br>最常用的一种方法是要求用户明确确认他们的操作，这种方法也有很多实现上的细节，但无论从什么角度实现都有优劣：<br><br>首先需要明确 Modal Dialog 和 Non-modal Dialog 的区别。<br>
“Modal 是一种设计技术，它以一个独立的模式呈现内容，阻止用户与父视图交互，并需要明确的操作来退出。”
<br>所以 Modal Dialog 需要用户立即操作它。换句话说，除非以某种方式做出响应，否则用户无法继续使用系统。<br>而 Non-modal Dialog 允许用户不间断地继续使用系统。Non-modal Dialog 一个常见用法就是出现在屏幕一角的 Toast 消息。<br>所以如果使用得当，Modal Dialog 是防止意外点击危险操作的有效方法。这也是目前最流行的方法，它还可以和其他方法结合使用。<br>但首先，需要明确一个保护用户进行危险操作的 Modal Dialog 需要有哪些元素：<br>
<br>不要含糊其辞, 如果单纯的问用户：“你确定吗？”，用户很有可能不会有任何疑问直接点确定。
<br>在标题中，指定具体会发生什么或哪个实体将受到影响（例如，项目名称、用户名、金额）。
<br>提供一个图标来指示该操作是危险的，这既降低了用户盲目确认它的概率，也有利于页面的无障碍性（色盲的人可以通过图标来判断）。
<br>在描述中，要具体并突出显示必要的信息。
<br>按钮还应包含反映操作的词语，不要用“是”或“确认”之类的词，应该用更具描述性的“支付 10 元”，“删除用户韩暮秋”之类的词语。
<br>除此之外，还有更加严格的保护，例如在某些情况下，可以让用户输入某些内容来“解锁”操作，例如 Github 在删除仓库的时候需要输入仓库名称才能删除，这能让用户明确的知道自己在做什么，在删除哪个仓库。<br>最后，根据 <a data-tooltip-position="top" aria-label="https://lawsofux.com/law-of-proximity/" rel="noopener nofollow" class="external-link" href="https://lawsofux.com/law-of-proximity/" target="_blank">临近法则</a> ，确认操作的按钮最好放在左侧。<br><br>对于关键的操作，可以使用 Danger Zone，常见的实现方式是将这一类关键的操作单独放在某一个页面的同一处，例如页面的底部。如果操作比较多，可以考虑使用一个单独的页面存放。<br>使用 Danger Zone 存放关键操作组件也有一些基本要素：<br>
<br>使用红色或其他富含警示性颜色的警告图标或边框，在视觉上将 Danger Zone 与页面的其他部分区分开来。
<br>Danger Zone 中的每个操作都应该清楚地描述如果用户继续进行将会发生什么，以便用户了解潜在的后果。
<br>对于一些非常关键且无法恢复的行为，可以要求用户进行额外的操作。例如要求用户重复输入密码或使用 2FA。
<br>只存放真正关键的操作。避免为了拥有一个 Danger Zone 而搞出一个 Danger Zone。
<br><br>这个方法解释起来挺简单，可以用在一些零碎的页面元素中，例如有一个删除一条消息的按钮，可以在用户单击这个按钮之后将其变为红色背景并修改按钮字体为 “确认删除”，用户再次点击就确认删除了。这用来防止误点击非常有用。<br>但是也有一些细节需要注意：<br>
<br>这种方法对于不那么危险的动作来说很方便，注意是“不那么危险”
<br>一个“不那么危险”的操作应该可以恢复，所以应该提供一个选项来撤消操作或将已删除的项目放到回收站之类的地方，这是确保用户安全操作的良好组合。
<br>Inline Guard 不能滥用，当用户非常频繁的遇到时会烦死，得权衡一下。<br><br>还有一些方案这里简单说一下，一是 2FA，基本不用过多解释，二是双人验证甚至多人验证，就是用户发起的一个操作需要两个以上的人来验证才可以执行，例如 Github 的 Merge PR。<br>当系统要求用户进行一些额外的操作时，应该明确其最初的目的，因为：<br>
<br>认知惯性：一个人倾向于近乎惯性地决定，即使这些决定不适合当前情况。例如，绝大多数人不阅读用户协议。他们只是同意冗长的文本，因为从法律角度来看这是必要的。（是的我就是这样）
<br>人们经常根据他们容易获得或熟悉的信息做出决策，而不是动脑子想。当用户看到相同的弹出窗口时，他们可能会根据之前的经验看都不看一眼直接接受它们。
<br>人类倾向于以更简单、更省力的方式思考和解决问题，而不是更复杂、更省力的方式，无论智力如何。所以许多用户只是点击“是”或“同意”而没有仔细阅读文本。
<br>所以在某些情况下，我们可以用一些更加优雅的方法：<br><br>上面 Inline Guard 用在删除消息的场景下，也可以点击删除按钮直接删除，但同时显示一个倒计时的 Toast 并附带一个“撤销”的按钮来提醒用户。<br><br>允许用户撤消刚刚执行的操作，从而提供一个安全网来减少因犯错误而产生的焦虑。<br>
与 Modal Dialog 这种中断系统并要求用户确认的模式不同，撤消允许完成操作后在需要时选择撤消操作，从而提供更流畅的体验。<br>它非常适合非破坏性、不可恢复的操作以及不会产生重大和直接后果的操作。<br>撤消选项与“软删除”的概念密切相关，“软删除” 的意思是：当用户通过 UI 删除某些内容时，看起来它已被删除，但在数据库中，我们保留数据但将其标记为已删除。数据不会丢失，这就是为什么可以使用撤消选项，因为我们实际上并没有删除任何内容，而是将其标记为已删除。<br>晚安。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/ui-ux/如何管理用户界面中的危险操作.html</link><guid isPermaLink="false">Computer Science/UI UX/如何管理用户界面中的危险操作.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 Jan 2025 10:46:23 GMT</pubDate></item><item><title><![CDATA[Svelte 的编译器和运行时]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:svelte" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#svelte</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:compiler" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#compiler</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:svelte" class="tag" target="_blank" rel="noopener nofollow">#svelte</a> <a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a> <a href="https://muqiuhan.github.io/wiki?query=tag:compiler" class="tag" target="_blank" rel="noopener nofollow">#compiler</a><br>在 svelte 源码里，使用了 <a data-tooltip-position="top" aria-label="https://github.com/acornjs/acorn" rel="noopener nofollow" class="external-link" href="https://github.com/acornjs/acorn" target="_blank"><code></code></a>acorn parser 将 javascript 编译成 ast 树，然后对 Javascript 的语义解释过程做了额外的工作：<br>
<br>编译赋值语句时，除了生成对应的赋值逻辑，额外生成数据更新逻辑代码<br>

<br>编译变量声明时，变量被编译成上下文数组<br>

<br>编译模板时，标记依赖，并对每个变量引用生成更新逻辑<br>

<br>这就是编译型框架，与传统前端框架的区别：把运行时的逻辑提前在编译期就完成。所以自然而然的，运行时逻辑很轻量级，很显然是有利于页面的首屏和渲染性能的。<br><br>每一个 .svelte 文件代表一个 svelte 的组件。<br>通过 svelte 的编译，最终会转换为下图所示的组件的结构<br><img alt="Pasted image 20241115111951.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115111951.png"><br>每一个 svelte 的组件类，都继承了 SvelteComponent。<br>svelte 组件使用 create, mount, patch, destroy 这四个方法实现对 DOM 视图的操作。<br>
<br>create 负责组件 dom 的创建<br>

<br>mount 负责将 dom 挂载到对应的父节点上<br>

<br>patch 负责根据数据的变化更新 dom<br>

<br>destroy 负责销毁对应的 dom<br>

<br>svelte 的组件实例化，是通过 instance 方法和组件上下文构成的。<br>
<br>instance 方法是 svelte 组件的构造器。写在 script 里的代码，会被生成在 instance 方法里。每个组件实例都会调用一次形成自己的闭包，从而隔离各自的数据，通过 instance 方法返回的数组就是上下文。代码中的赋值语句，会被生成为数据更新逻辑。变量定义会被收集生成上下文数组。
<br>每个 svelte 组件都会有自己的上下文，上下文存储的就是 script 标签内定义的变量的值。svelte 会为每个组件实例内定义的数据生成上下文，按照变量的声明顺序保存在一个名为 ctx 数组内。
<br><img alt="Pasted image 20241115112012.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112012.png"><br><br>例如有如下 Svelte 代码:<br>&lt;h1&gt;Hello world!&lt;/h1&gt;
<br>编译后:<br>/* App.svelte generated by Svelte v3.59.1 */
import {
  SvelteComponent,
  detach,
  element,
  init,
  insert,
  noop,
  safe_not_equal
} from "svelte/internal";

function create_fragment(ctx) {
  let h1;

  return {
    c() {
      h1 = element("h1");
      h1.textContent = "Hello world!";
    },
    m(target, anchor) {
      insert(target, h1, anchor);
    },
    p: noop,
    i: noop,
    o: noop,
    d(detaching) {
      if (detaching) detach(h1);
    }
  };
}

class App extends SvelteComponent {
  constructor(options) {
    super();  
    init(this, options, null, create_fragment, safe_not_equal, {});
  }
}

export default App;
<br>很明显，组件编译之后，会返回一个继承了SvelteComponent的类，并且在构造函数中执行了init方法，它的其中一个参数是组件中定义的create_fragment函数。<br>这个函数会返回一个对象，包含组件对应的的create mount update delete操作。由于上面的代码中是个静态的字符串，所以p对应的值为noop即no operate没有操作。<br>接下来加点料:<br>&lt;script&gt;
let count = 0
&lt;/script&gt;
&lt;h1 on:click={() =&gt; count++}&gt;Hello world!&lt;/h1&gt;
<br>编译结果:<br>function create_fragment(ctx) {
  let h1;
  let mounted;
  let dispose;
  
  return {
    c() {
      h1 = element("h1");
      h1.textContent = "Hello world!";
    },
    m(target, anchor) {
      insert(target, h1, anchor);
      if (!mounted) {
        dispose = listen(h1, "click", /*click_handler*/ ctx[1]);
        mounted = true;
      }
    },
    // ...
    d(detaching) {
      if (detaching) detach(h1);
      mounted = false;
      dispose();
    }
  };
}

function instance($$self, $$props, $$invalidate) {
  let count = 0;
  const click_handler = () =&gt; $$invalidate(0, count++, count);
  return [count, click_handler];
}
<br>我们可以看到在mounted之后使用listen方法新增了一个针对h1的click方法的监听事件，并且在delete阶段移除监听事件。<br>同时多了个实例方法instance，它的返回值是count的实际值，以及修改count的处理函数。<br>再改改:<br>&lt;script&gt;
let count = 0
&lt;/script&gt;
&lt;h1 on:click={() =&gt; count++}&gt;Hello world!{count}&lt;/h1&gt;
<br>编译结果:<br>function create_fragment(ctx) {
  let h1;
  let t0;
  let t1;
  let mounted;
  let dispose;
  
  return {
    c() {
      h1 = element("h1");
      h1.textContent = "Hello world!";
      t0 = text("Hello world!");
      t1 = text(/*count*/ ctx[0]);
    },
    m(target, anchor) {
      insert(target, h1, anchor);
      append(h1, t0);
      append(h1, t1);
      if (!mounted) {
        dispose = listen(h1, "click", /*click_handler*/ ctx[1]);
        mounted = true;
      }
    },
    // ...
    p(ctx, [dirty]) {
      if (dirty &amp; /*count*/ 1) set_data(t1, /*count*/ ctx[0]);
    },
    d(detaching) {
      if (detaching) detach(h1);
      mounted = false;
      dispose();
    }
  };
}
<br><br>svelte 通过位运算(bitmask)对变量的改变进行脏标记<br>每个变量都被分配一个位值，可以用于在 ctx 上下文数据里取得变量对应的值，也可以通过位运算对变量改动进行标记和检查。<br>比如 name 的位值是 1，那 name 的值可以通过 ctx[1] 取得。<br>通过 dirty |= 1 设置 name 已经改动的状态，再通过 dirty &amp; 1 判断 name 是否改动。<br>Javascript 的位运算可以有 32 位。 svelte 支持每个组件里对 32 个变量标记改动。<br>一般一个组件不应该定义过多的变量。当然如果定义变量多于 32 个，无非就是拿两个位标记变量，凑成 64 位，以此类推。<br><img alt="Pasted image 20241115112612.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112612.png"><br>设置位：bitmask |= 1 &lt;&lt; (n-1)<br>检测位：if (bitmask &amp; (1 &lt;&lt; (n-1)))<br><br><br>前端框架创建视图的方式有几种，比如虚拟 dom，字符串模板，过程式创建。<br>svelte 采用的是过程式创建。<br>举个例子，假设我想要通过纯 js 的方式创建一个如下的 web ui：<br><img alt="Pasted image 20241115112039.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112039.png"><br>我们可能会写下这样的代码：<br>const todoListNode = document.createElement('ul');
const todos = [1,2,3];
for (const todo of todos) {
    const itemNode = document.createElement('li');
    itemNode.textContent = `item ${todo}`;
    todoListNode.appendChild(itemNode);
}
<br>而 svelte 生成的视图代码就很类似我们手动编写的 js 代码。<br>这部分创建 dom 的代码，会生成为组件内部的 create 函数， mount 函数，patch 函数。<br>下面我们来看一下模板编译过程。<br>
<br>首先解析 svelte 模板并生成模板 AST<br>

<br>然后遍历模板 AST<br>

<br>如果碰到普通的 html tag 或者文本，输出 dom 创建语句（dom.createElement)

<br>如果碰到变量
<br>转换为上下文引用方式并输出取值语句（如： name 被生成为 ctx[/** name */0])

<br>在 patch 函数中生成对应的更新语句


<br>如果碰到 if 模板
<br>获取 condition 语句，输出选择函数 select_block （子模板选择器）

<br>获取 condition 为 true 的模板片段，输出 if_block 子模板构建函数
<br>获取 condition 为 false 的模板片段，输出 else_block 子模板构建函数


<br>如果碰到 each 模板
<br>获取循环模板片段，生成块构建函数 create_each_block

<br>根据循环内变量引用，生成循环实例上下文获取 get_each_block_context




<br>生成 key获取函数 get_key<br>

<br>生成基于key更新列表的 patch 逻辑函数 update_keyed_each<br>

<br><img alt="Pasted image 20241115112127.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112127.png"><br><br>svelte 会把 if 模板， each 模板中的逻辑分支，抽取成子模板，并为其生成独立的模板实例（包含创建，挂载，更新，销毁等生命周期）<br><br>视图更新时通过 patch 函数来完成的。<br>下图是模板解析过程中 patch 函数的逻辑：<br>function patch(ctx, [dirty]) {
  if (dirty &amp; /*name*/ 1) set_data(t1, /*name*/ ctx[0]);
  if (dirty &amp; /*age*/ 2) set_data(t4, /*age*/ ctx[1]);
  if (dirty &amp; /*school*/ 4) set_data(t6, /*school*/ ctx[2]);
}
<br>通过 dirty 位检查变量是否发生更新，如果发生更新调用 dom 操作函数对 dom 进行局部更新。上面例子的 set_data 函数作用是给 dom 设置 innerText。根据数据更新的视图位置的不同，还会有 set_props 之类的更新 dom 属性的函数等。<br><br>条件分支例子：<br>&lt;script&gt;
   let isLogin = false;
	const login = () =&gt; {
		isLogin = true;
	}
	const logout = () =&gt; {
		isLogin = false;
	}
&lt;/script&gt;

{#if !isLogin}
&lt;button on:click={login}&gt;
	login
&lt;/button&gt;
{:else}
&lt;div&gt;
	hello, xxx
	&lt;button on:click={logout}&gt;logout&lt;/button&gt;
&lt;/div&gt;
{/if}
<br><img alt="Pasted image 20241115112203.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112203.png"><br>
<br>条件分支的判断语句会生成 select_block 函数，用于判断条件，并根据条件返回条件判断为真的子模板 (if_block) 或者条件判断为假的子模板 (else_block)
<br>// 根据条件返回对应的block构造函数
function select_block(ctx, dirty) {
  if (!/*isLogin*/ ctx[0]) return if_block;
  return else_block;
}
// 选择block构造函数
let current_block = select_block(ctx, -1);
// 返回子模板实例，跟组件类似，提供create，mount，patch等生命周期
let block = current_block(ctx);
<br>
<br>条件逻辑分支会生成独立的子模板构造函数
<br>if block示例<br>// 子模板构造函数
function if_block(ctx) {
  let button;
  let mounted;
  let dispose;

  return {
    // 创建block
    create() {
      button = element("button");
      button.textContent = "login";
    },
    // 挂载block
    mount(target, anchor) {
      insert(target, button, anchor);
      if (!mounted) {
        mounted = true;
      }
    },
    // 销毁block
    destroy(detaching) {
      if (detaching) detach(button);
      mounted = false;
      dispose();
    }
  };
}
<br>
<br>if分支如何挂载及更新
<br>if 分支的创建：<br><img alt="Pasted image 20241115112230.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112230.png"><br>if 分支的更新：<br><img alt="Pasted image 20241115112241.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112241.png"><br><br>svelte 的循环模板跟条件分支模板一样，也会生成迭代逻辑的子模板，每一个循环迭代都是子模板的实例，并且拥有独立的上下文。<br>主要由4部分组成：<br>
<br>循环迭代构建函数 create_each_block
<br>循环迭代实例上下文获取函数 get_each_block_context
<br>循环迭代 key 获取函数 get_key
<br>基于 key 更新列表的 patch 逻辑函数 update_keyed_each
<br><br>
<br>svelte 调用 acorn 生成 JS AST 树<br>

<br>遍历 AST 找到赋值语句<br>

<br>为赋值语句生成数据响应式代码<br>

<br><img alt="Pasted image 20241115112450.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112450.png"><br>例如：<br>&lt;script&gt;
    let name = 'world';
    const changeName = () =&gt; {
        name = 'yyb';
    }
&lt;/script&gt;
<br>编译结果：<br>function instance($$self, $$props, $$invalidate) {
	let name = 'world';

	const changeName = () =&gt; {
		$$invalidate(0, name = 'yyb');
	};

	return [name];
}
<br><br>每个数据的赋值语句，svelte 都会生成对 invalidate 的调用, invalidate 的调用主要做的是对某个改动的变量进行标记，然后在微任务中调用patch函数，根据变量改动的脏标记进行局部更新<br>数据赋值触发视图更新：<br><img alt="Pasted image 20241115112544.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115112544.png"><br><br><br>在进入运行时，首先执行init方法，该方法大致流程如下：<br>
<br>初始化状态
<br>初始化周期函数
<br>执行instance方法，在回调函数中标记 脏组件 (dirty_components)
<br>执行所有beforeUpdate生命周期的函数
<br>执行创建片段create_fragment函数
<br>挂载当前组件并执行create_fragement返回的m（mounted）方法
<br>执行flush方法
<br>export function init(
  component,
  options,
  instance,
  create_fragment,
  not_equal,
  props,
  append_styles,
  dirty = [-1]
) {
  const parent_component = current_component;
  set_current_component(component);

  const $$: T$$ = component.$$ = {
    fragment: null,
    ctx: [],

    // state
    props,
    update: noop,
    not_equal,
    bound: blank_object(),

    // lifecycle
    on_mount: [],
    on_destroy: [],
    on_disconnect: [],
    before_update: [],
    after_update: [],
    context: new Map(options.context || (parent_component ? parent_component.$$.context : [])),

    // everything else
    callbacks: blank_object(),
    dirty,
    skip_bound: false,
    root: options.target || parent_component.$$.root
  };

  append_styles &amp;&amp; append_styles($$.root);

  let ready = false;

  $$.ctx = instance
    ? instance(component, options.props || {}, (i, ret, ...rest) =&gt; {
        const value = rest.length ? rest[0] : ret;
        if ($$.ctx &amp;&amp; not_equal($$.ctx[i], $$.ctx[i] = value)) {
            if (!$$.skip_bound &amp;&amp; $$.bound[i]) $$.bound[i](value);
            if (ready) make_dirty(component, i);
        }
        return ret;
    })
    : [];

  $$.update();
  ready = true;
  run_all($$.before_update);

  // `false` as a special case of no DOM component
  $$.fragment = create_fragment ? create_fragment($$.ctx) : false;

  if (options.target) {
    if (options.hydrate) {
        start_hydrating();
        const nodes = children(options.target);
        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
        $$.fragment &amp;&amp; $$.fragment!.l(nodes);
        nodes.forEach(detach);
    } else {
        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
        $$.fragment &amp;&amp; $$.fragment!.c();
    }

    if (options.intro) transition_in(component.$$.fragment);
    mount_component(component, options.target, options.anchor, options.customElement);
    end_hydrating();
    flush();
  }

  set_current_component(parent_component);
}
<br><br>flush 的方法主要做了一件事：<br>
遍历需要更新的组件（dirty_components），然后更新它，并且调用afterUpdate方法:<br>export function flush() {
  // Do not reenter flush while dirty components are updated, as this can
  // result in an infinite loop. Instead, let the inner flush handle it.
  // Reentrancy is ok afterwards for bindings etc.
  if (flushidx !== 0) {
    return;
  }

  const saved_component = current_component;

  do {
    // first, call beforeUpdate functions
    // and update components
    try {
        while (flushidx &lt; dirty_components.length) {
            const component = dirty_components[flushidx];
            flushidx++;
            set_current_component(component);
            update(component.$$);
        }
    } catch (e) {
        // reset dirty state to not end up in a deadlocked state and then rethrow
        dirty_components.length = 0;
        flushidx = 0;
        throw e;
    }

    set_current_component(null);

    dirty_components.length = 0;
    flushidx = 0;

    // then, once components are updated, call
    // afterUpdate functions. This may cause
    // subsequent updates...
    for (let i = 0; i &lt; render_callbacks.length; i += 1) {
        const callback = render_callbacks[i];

        if (!seen_callbacks.has(callback)) {
            // ...so guard against infinite loops
            seen_callbacks.add(callback);

            callback();
        }
    }

    render_callbacks.length = 0;
  } while (dirty_components.length);

  while (flush_callbacks.length) {
    flush_callbacks.pop()();
  }

  update_scheduled = false;
  seen_callbacks.clear();
  set_current_component(saved_component);
}
<br>再来看看具体的更新操作update函数做了啥<br>
<br>首先执行所有的before_update方法
<br>然后执行create_fragment返回的p（update）方法
<br>function update($$) {
    if ($$.fragment !== null) {
        $$.update();
        run_all($$.before_update);
        const dirty = $$.dirty;
        $$.dirty = [-1];
        $$.fragment &amp;&amp; $$.fragment.p($$.ctx, dirty);

        $$.after_update.forEach(add_render_callback);
    }
}
<br>总结,在运行时:<br>
<br>初始化状态、初始化周期函数
<br>执行instance方法，在回调函数中标记脏组件
<br>执行所有beforeUpdate生命周期的函数
<br>执行创建片段create_fragment函数
<br>挂载当前组件并执行create_fragement返回的m（mounted）方法
<br>执行flush方法
<br>执行所有的before_update方法
<br>执行create_fragment返回的p（update）方法
<br>最后，执行afterUpdate方法
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/index.js" target="_blank">https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/index.js</a>
<br>/**
 * `compile` converts your `.svelte` source code into a JavaScript module that exports a component
 *
 * @param {string} source The component source code
 * @param {CompileOptions} options The compiler options
 * @returns {CompileResult}
 */
export function compile(source, options) {
	source = remove_bom(source);
	state.reset_warning_filter(options.warningFilter);
	const validated = validate_component_options(options, '');
	state.reset(source, validated);

	let parsed = _parse(source);

	const { customElement: customElementOptions, ...parsed_options } = parsed.options || {};

	/** @type {ValidatedCompileOptions} */
	const combined_options = {
		...validated,
		...parsed_options,
		customElementOptions
	};

	if (parsed.metadata.ts) {
		parsed = {
			...parsed,
			fragment: parsed.fragment &amp;&amp; remove_typescript_nodes(parsed.fragment),
			instance: parsed.instance &amp;&amp; remove_typescript_nodes(parsed.instance),
			module: parsed.module &amp;&amp; remove_typescript_nodes(parsed.module)
		};
	}

	const analysis = analyze_component(parsed, source, combined_options);
	const result = transform_component(analysis, source, combined_options);
	result.ast = to_public_ast(source, parsed, options.modernAst);
	return result;
}
<br>从这里看, Svelte的编译过程可以分为三个主要阶段：<br>
<br>Parse&nbsp;解析
<br>Analyze&nbsp;分析
<br>Transform&nbsp;转换
<br> velte 文件的 Script、HTML 和 CSS 代码被分开以形成单个组件对象:<br>
<img alt="Pasted image 20241115115009.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115115009.png"><br>
在Parser构造函数内部， <a data-tooltip-position="top" aria-label="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/1-parse/state/fragment.js" rel="noopener nofollow" class="external-link" href="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/1-parse/state/fragment.js" target="_blank"><code></code></a>fragment 函数用于递归地解析自身:<br>/** @param {Parser} parser */
export default function fragment(parser) {
	if (parser.match('&lt;')) {
		return element;
	}

	if (parser.match('{')) {
		return tag;
	}

	return text;
}
<br>fragment 根据条件返回 element tag text 等函数, 这个返回值会被调用以从源代码中提取信息。<br>包含在script标签中的区域通过<a data-tooltip-position="top" aria-label="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/1-parse/read/script.js" rel="noopener nofollow" class="external-link" href="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/1-parse/read/script.js" target="_blank"><code></code></a>read_script进行解析:<br>/**
 * @param {Parser} parser
 * @param {number} start
 * @param {Array&lt;AST.Attribute | AST.SpreadAttribute | Directive&gt;} attributes
 * @returns {AST.Script}
 */
export function read_script(parser, start, attributes) {
...
	const source =
		parser.template.slice(0, script_start).replace(regex_not_newline_characters, ' ') + data;
	parser.read(regex_starts_with_closing_script_tag);

	/** @type {Program} */
	let ast;

	try {
		ast = acorn.parse(source, parser.ts);
	} catch (err) {
		parser.acorn_error(err);
	}
...
}
<br>其他区域使用 Svelte Compiler 自己的解析逻辑。当遇到style标签时，会使用<a data-tooltip-position="top" aria-label="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/1-parse/read/style.js" rel="noopener nofollow" class="external-link" href="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/1-parse/read/style.js" target="_blank"><code></code></a>read_style:<br>/**
 * @param {Parser} parser
 * @param {number} start
 * @param {Array&lt;AST.Attribute | AST.SpreadAttribute | Directive&gt;} attributes
 * @returns {Css.StyleSheet}
 */
export default function read_style(parser, start, attributes) {
	const content_start = parser.index;
	const children = read_body(parser, '&lt;/style');
	const content_end = parser.index;

	parser.read(/^&lt;\/style\s*&gt;/);

	return {
		type: 'StyleSheet',
		start,
		end: parser.index,
		attributes,
		children,
		content: {
			start: content_start,
			end: content_end,
			styles: parser.template.slice(content_start, content_end),
			comment: null
		}
	};
}
<br>其他类似的也都和 read_style 放在一起.<br>最终会生成一个这样的 JSON:<br>{
  html: { type, start, end, children }
  css: { type, start, end, attributes, children, content }
  instance: { type, start, end, context, content }
  module: { type, start, end, context, content }
}
<br><br>从创建的 AST 中提取用于执行组件的各种操作的信息:<br>
<img alt="Pasted image 20241115115845.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115115845.png"><br>参考<a data-tooltip-position="top" aria-label="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/2-analyze/index.js#L398" rel="noopener nofollow" class="external-link" href="https://github.com/sveltejs/svelte/blob/main/packages/svelte/src/compiler/phases/2-analyze/index.js#L398" target="_blank">源码</a>，组件的主要属性可以总结如下：<br>const analysis = {
root: scope_root,
module,
instance,
template,
stylesheet: new Stylesheet({...}),
// Various compile options
runes,
warnings,
reactive_statements: new Map(),
binding_groups: new Map(),
slot_names: new Set(),
...
};
<br>ScopeRoot是一个充当组件最高<a data-tooltip-position="top" aria-label="https://developer.mozilla.org/ko/docs/Glossary/Scope" rel="noopener nofollow" class="external-link" href="https://developer.mozilla.org/ko/docs/Glossary/Scope" target="_blank">作用域</a>的对象。在内部，它使用Set数据结构来确保变量和函数等标识符的唯一性。<br>遍历 instance script 和 module script 的 AST 来识别变量被引用的所有区域，从而了解变量可能发生变化的所有情况。在此过程中，将创建 script 的较低作用域，引用 scope_root 为变量分配唯一标识符.<br>instance script 和 module script 的区别是:<br>
<br>module 定义组件之间共享的状态和逻辑, 可以使用<a data-tooltip-position="top" aria-label="https://learn.svelte.dev/tutorial/sharing-code" rel="noopener nofollow" class="external-link" href="https://learn.svelte.dev/tutorial/sharing-code" target="_blank"><code></code></a>&lt;script context="module"&gt;声明, 但不能包含 reactive 的代码
<br>instance 定义组件的唯一状态和逻辑.
<br>其他的基本没啥讲的, stylesheet 会根据收集到的analysis信息执行一些优化任务:<br>
<br>重复的全局 CSS 选择器和组件范围内未使用的选择器将被删除。
<br>组件范围内使用的 CSS 选择器被哈希为.svelte-xxx格式，防止与同名选择器发生冲突。
<br><br>最后会根据获得的analysis信息，再次遍历 AST 来优化组件状态:<br>walk(
  /** @type {import('#compiler').SvelteNode} \*/ (ast),
  /** @type {import('./types').AnalysisState} \*/ (state),
  merge(
    set_scope(scopes),
    validation_runes,
    runes_scope_tweaker,
    common_visitors,
  ),
);
<br>
<br>validation_runes 检查不正确的赋值或更新表达式, 验证变量的声明和导出并检查新的reactive语法runes的有效性。
<br>runes_scope_tweaker 调整作用域, 设置使用特定模式的变量声明的作用域和绑定类型, 并将变量或函数移动到实例之外而不改变状态。
<br>common_visitors 处理directive和binding相关指令和一般 HTML 或 Svelte 特定元素, 并处理与事件相关的属性并确定是否需要事件委托或提升。
<br><br>最后，Svelte编译器经过一个转换过程来生成渲染代码。<br><img alt="Pasted image 20241115120819.png" src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115120819.png"><br>在这个过程中，SSR（服务器端渲染）和 CSR（客户端渲染）的代码生成逻辑是不同的。编译器使用两个函数server_component和client_component来创建针对每种场景优化的代码, 就像这样:<br>const program =
  options.generate === 'server'
    ? server_component(analysis, options)
    : client_component(source, analysis, options);

<br>在 SSR 中，组件仅渲染一次，并且组件没有生命周期。<br>
因此， server_component专注于创建<a data-tooltip-position="top" aria-label="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals" rel="noopener nofollow" class="external-link" href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals" target="_blank"><code></code></a>template literals 。它使用javascript_visitors和template_visitors等访问者添加代码块。<br>在 CSR 中，需要与 DOM 不断交互，并且组件需要有生命周期。<br>
因此， client_component由更加多样化和复杂的访问者组成。即使相同的javascript_visitors也包含遍历过程中函数的附加处理逻辑。<br><br>
<br><a data-tooltip-position="top" aria-label="https://svelte.dev/docs/svelte-compiler" rel="noopener nofollow" class="external-link" href="https://svelte.dev/docs/svelte-compiler" target="_blank">svelte/compiler</a>
<br><a data-tooltip-position="top" aria-label="https://blog.kalan.dev/en/series/svelte-series/1" rel="noopener nofollow" class="external-link" href="https://blog.kalan.dev/en/series/svelte-series/1" target="_blank">Svelte — What made me meet you like this</a>
<br><a data-tooltip-position="top" aria-label="https://juejin.cn/post/7235628080219078693" rel="noopener nofollow" class="external-link" href="https://juejin.cn/post/7235628080219078693" target="_blank">🚀Svelte原理和进阶看这篇就够了🚀</a>
<br><a data-tooltip-position="top" aria-label="https://bepyan.me/en/post/svelte-compiler-operation" rel="noopener nofollow" class="external-link" href="https://bepyan.me/en/post/svelte-compiler-operation" target="_blank">How Does the Svelte Compiler Work?</a>
<br><a data-tooltip-position="top" aria-label="https://juejin.cn/post/7185433911437033531" rel="noopener nofollow" class="external-link" href="https://juejin.cn/post/7185433911437033531" target="_blank">一文讲透前端新秀 svelte</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/svelte-的编译器和运行时.html</link><guid isPermaLink="false">Computer Science/Web/Svelte/Svelte 核心原理/Svelte 的编译器和运行时.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 18 Dec 2024 07:15:59 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115111951.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://muqiuhan.github.io/wiki/computer-science/web/svelte/svelte-核心原理/pasted-image-20241115111951.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Turborepo 简介]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a><br><br>Monorepos 有很多优势，但它们难以扩展。每个工作区都有自己的测试套件、自己的 linting 和构建过程。单个 monorepo 可能有数千个任务要执行。<br>Turborepo 是一个专为 JavaScript 和 TypeScript 代码库设计的构建系统，旨在优化 monorepos 和 single-package workspace 中的任务。它通过远程缓存（remote caching）和高效的任务调度（task scheduling）来解决 monorepos 中的扩展问题。Turborepo 也可以增量部署（adopted incrementally），并与各种包管理器配合使用。<br>
<br>远程缓存: 存储所有任务的结果，CI 就不需要重复执行相同的工作了。
<br>任务调度：利用所有核心的性能并行处理任务，尽可能的加速。
<br>turbo 基于 workspace 构建，workspaces 是 JavaScript 生态系统中包管理器的一项功能，允许将多个包分组到一个存储库中:<br><br>在 JavaScript 中，Workspace 是指仓库中的特定实体，可以是<a data-tooltip-position="top" aria-label="https://vercel.com/docs/vercel-platform/glossary#single-package-workspace" rel="noopener nofollow" class="external-link" href="https://vercel.com/docs/vercel-platform/glossary#single-package-workspace" target="_blank">单个包</a>或<a data-tooltip-position="top" aria-label="https://vercel.com/docs/vercel-platform/glossary#multi-package-workspace" rel="noopener nofollow" class="external-link" href="https://vercel.com/docs/vercel-platform/glossary#multi-package-workspace" target="_blank">包的集合</a>。<br>
包管理器的 root lock 文件（例如 pnpm-lock.yaml）以及任何其他配置都位于 Wrokspace 的根目录。在 Monorepo 中可以有多个工作区，每个工作区位于存储库的子目录中。<br><br>只有一个独立包的工作区，在工作区根目录下有一个 package.json 文件。<br><br>包含多个包的工作区，包含多个 package.json 文件，其中一个位于工作区根目录中用于全局配置，其他位于每个包目录中。<br>
这种类型的工作区通常称为 monorepo
<br><br>以 npm 为例，turbo 会初始化一个这样的目录结构使其成为有效的 workspace:<br>|- package.json
|- package-lock.json
|- turbo.json
|- apps
|-- docs
|--- package.json
|-- web
|--- package.json
|- packages
|--- ui
<br>一个 “有效的” turbo 项目至少要有：<br>
<br>包管理器描述的包
<br>包管理器的 lock 文件
<br>根目录下的 package.json
<br>根目录下的 turbo.json
<br>每个包中的 package.json
<br>例如，在根目录的 package.json 中配置:<br>{
  "workspaces": [
    "apps/*",
    "packages/*"
  ]
}
<br>那么 apps 或 packages 目录中有 package.json 的每个目录都将被视为一个包。<br>
注意：Turborepo 不支持嵌套包，例如 apps/** 或 packages/** 这种，将一个包放在apps/a 并将另一个包放在 apps/a/b 的结构将导致错误。
如果想按目录对包进行分组，可以使用 packages/* 和 packages/group/* 等 glob 来完成此操作，而不是创建 packages/group/package.json 文件。
<br>根目录的 package.json 是 workspace 的基础，常见的配置：<br>{
  "private": true,
  "scripts": {
    "build": "turbo run build",
    "dev": "turbo run dev",
    "lint": "turbo run lint"
  },
  "devDependencies": {
    "turbo": "latest"
  },
  "packageManager": "npm@10.0.0"
}
<br>而根目录的 turbo.json 用于配置 turbo 的行为。那些 lock 文件是包管理器和 turbo 用于 reproducible 的关键。此外，Turborepo 还利用它们分析工作区中<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/internal-packages" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/internal-packages" target="_blank">内部包</a>之间的依赖关系。<br><br><br><a data-tooltip-position="top" aria-label="https://nodejs.org/api/packages.html#name" rel="noopener nofollow" class="external-link" href="https://nodejs.org/api/packages.html#name" target="_blank"><code></code> 字段</a>name用于标识包。它在 workspace 中应该是唯一的。<br>
最佳做法是为<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/internal-packages" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/internal-packages" target="_blank">内部包</a>使用命名空间前缀，以避免与 npm 注册表上的其他包发生冲突。例如，如果组织名为 clin，则可以将包命名为 @clin/package-name。
<br>scripts 字段用于定义可在包的上下文中运行的脚本。Turborepo 将使用这些脚本的名称来确定要在包中运行的脚本（如果有）。<br><a data-tooltip-position="top" aria-label="https://nodejs.org/api/packages.html#exports" rel="noopener nofollow" class="external-link" href="https://nodejs.org/api/packages.html#exports" target="_blank"><code></code> 字段</a>exports用于指定要使用该包的其他包的入口点。如果要在另一个包中使用一个包中的代码，将从该入口点导入。<br>例如，如果有一个 @repo/math 包，则可以这么写 exports 字段:<br>{
  "exports": {
    ".": "./dist/constants.ts",
    "./add": "./dist/add.ts",
    "./subtract": "./dist/subtract.ts"
  }
}
<br>然后就可以从 @repo/math 包中导入 add 和 subtract 函数了:<br>import { GRAVITATIONAL_CONSTANT, SPEED_OF_LIGHT } from '@repo/math';
import { add } from '@repo/math/add';
import { subtract } from '@repo/math/subtract';
<br>以这种方式使用导出有三个主要好处：<br>
<br>避免 barrel 文件：barrel 文件是重新导出同一包中其他文件的文件，从而为整个包创建一个入口点。虽然它们可能看起来很方便，但编译器<a data-tooltip-position="top" aria-label="https://vercel.com/blog/how-we-optimized-package-imports-in-next-js#what's-the-problem-with-barrel-files" rel="noopener nofollow" class="external-link" href="https://vercel.com/blog/how-we-optimized-package-imports-in-next-js#what's-the-problem-with-barrel-files" target="_blank">和捆绑程序很难处理</a>它们，并且可能很快导致性能问题。
<br>更强大的功能：与<a data-tooltip-position="top" aria-label="https://nodejs.org/api/packages.html#main" rel="noopener nofollow" class="external-link" href="https://nodejs.org/api/packages.html#main" target="_blank"><code></code>字段</a>主（如 <a data-tooltip-position="top" aria-label="https://nodejs.org/api/packages.html#conditional-exports" rel="noopener nofollow" class="external-link" href="https://nodejs.org/api/packages.html#conditional-exports" target="_blank">Conditional Exports</a>）相比，exports 还具有其他强大的功能。一般来说，尽可能使用 exports 而不是 main 就行了。
<br>IDE 友好：通过使用 export 指定包的入口点，代码编辑器可以为包的导出提供自动完成。
<br><br>除此之外还有 import 字段，也就是一种创建包中其他模块的子路径的方法。可以简单地视为 “快捷方式” ，用于编写更简单的导入路径，这些路径对日后文件被移动后的重构更具弹性。<br>
其他：包通常使用 src 目录来存储其源代码并编译到 dist 目录（也应位于包中）。
<br><br>{
  "dependencies": {
    "next": "latest", // 外部依赖
    "@repo/ui": "*" // 内部依赖
  }
}
<br>在存储库中安装依赖项时，应将其直接安装在使用它的软件包中。包的 package.json 将包含所需的每个依赖项。外部和内部依赖项都是如此。<br>要在多个包中快速安装依赖项，可以：<br>npm install jest --workspace=web --workspace=@repo/ui --save-dev
<br>这种做法有几个好处：<br>
<br>更清晰：当软件包的依赖项列在其 package.json 中时，更容易理解软件包所依赖的内容。在存储库中工作的开发人员可以一目了然地看到包中使用了哪些依赖项。
<br>更具灵活性：在大规模的 monorepo 中，想让每个包都使用相同版本的外部依赖项可能是不现实的。当有许多团队在同一个代码库中工作时，优先级、时间表和需求会有所不同。通过在 “使用它们的包” 中安装依赖项，可以让 UI 团队能够升级到最新版本的 TypeScript，而 Web 团队可以优先发布新功能并在以后使用 TypeScript。
<br>更好的缓存能力：如果在存储库的根目录中安装了太多依赖项，则每当添加、更新或删除依赖项时，都会更改工作区根目录，从而导致不必要的缓存未命中。
<br>修剪未使用的依赖项：对于 Docker 用户，<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/prune" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/prune" target="_blank">Turborepo 的修剪功能</a>可以从 Docker 镜像中删除未使用的依赖项，以创建更轻量级的镜像。当依赖项安装在它们所适用的包中时，Turborepo 可以读取锁文件并删除所需的包中未使用的依赖项。
<br>
属于工作区根目录的唯一依赖项是用于管理存储库的工具，而用于构建应用程序和库的依赖项安装在各自的包中。一些适合安装在根中的依赖项示例包括 <a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/turbo" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/turbo" target="_blank"><code></code></a>turbo、<a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/husky" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/husky" target="_blank"><code></code></a>husky 或 <a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/lint-staged" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/lint-staged" target="_blank"><code></code></a>lint-staged。
<br><br>一些 monorepo 维护者更喜欢按照规则在所有软件包中保持对相同版本的依赖关系。有几种方法可以实现此目的：<br>
<br>使用专用的工具，比如 <a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/syncpack" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/syncpack" target="_blank"><code></code></a>syncpack、<a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/@manypkg/cli" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/@manypkg/cli" target="_blank"><code></code></a>manypkg 和 <a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/sherif" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/sherif" target="_blank"><code></code></a>sherif 等工具可用于此特定目的。
<br>或者单纯的使用包管理器，可以使用软件包管理器通过一个命令更新依赖项版本:

<br>npm install typescript@latest --workspaces


<br>或者最粗暴的用编辑器一次查找并替换存储库中所有 package.json 文件的依赖项版本。用 “next”： “.*” 之类的正则表达式来查找并替换为所需的版本。完成后再运行包管理器的 install 命令来更新 lock 文件.
<br><br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/internal-packages" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/internal-packages" target="_blank">内部包</a>是工作区的构建块（building blocks），是一种在存储库中共享代码的强大方式。Turborepo 读取 package.json 中的依赖项来分析内部包之间的关系，并在后台创建 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" target="_blank">Package Graph</a> 以优化存储库的工作流程。<br>在创建内部包时，建议创建具有单一 “用途” 的包。这是最佳实践，具体取决于存储库的规模、组织、团队需求等。此策略具有以下几个优点：<br>
<br>更易于理解：随着存储库的扩展，在存储库中工作的开发人员将能够更轻松地找到他们需要的代码。
<br>减少每个包的依赖项：每个包使用更少的依赖项，以便 Turborepo 可以更有效地<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/prune" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/prune" target="_blank">修剪包图的依赖项</a>。
<br>在创建<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/package-types#application-packages" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/package-types#application-packages" target="_blank">应用程序包</a>时，最好避免将共享代码放在这些包中。相反，应该为共享代码创建一个单独的包，并让应用程序包依赖于该包。<br>
此外，应用程序包不应安装到其他包中。相反，应将它们视为 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" target="_blank">Package Graph</a> 的入口点。<br><br>Turborepo 将始终按照 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/configuration" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/configuration" target="_blank"><code></code> 配置</a>turbo.json和 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" target="_blank">Package Graph</a> 中描述的顺序运行任务，并尽可能并行化工作以确保一切尽可能快地运行。<br>根目录的 turbo.json 文件是注册 Turborepo 将运行的任务的位置。定义任务后，将能够使用 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/run" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/run" target="_blank"><code></code></a>turbo run 运行一个或多个任务。<br>tasks 对象中的每个 key 都是一个可以通过 turbo run 执行的任务。Turborepo 将在 package.json 中搜索与任务同名的软件包:<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/configuration#dependson" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/configuration#dependson" target="_blank"><code></code> 键</a>dependsOn用于指定在其他任务开始运行之前必须完成的任务。在大多数情况下，库的build脚本在应用程序的build脚本运行之前完成，所以可以这么写:<br>{
  "tasks": {
    "build": {
      "dependsOn": ["^build"] 
    }
  }
}
<br>
^ 这个语法告诉 Turborepo 从依赖关系图的底部开始运行任务。如果应用程序依赖于名为 ui 的库，并且该库具有build任务，则 ui 中的build脚本将首先运行。成功完成后，才会运行应用程序中的build任务。<br>
这是一个重要的形式，因为它可以确保应用程序的build任务具有编译所需的所有必要依赖项。当依赖关系图发展到具有多个级别的任务依赖关系的更复杂的结构时，此概念也适用。
<br>有时可能需要确保同一包中的两个任务按特定顺序运行。例如需要先在库中运行build任务，然后再在同一库中运行test任务。这种情况删掉 ^ 就行了:<br>{
  "tasks": {
    "test": {
      "dependsOn": ["build"] 
    }
  }
}
<br>还可以在特定包中指定要依赖的单个任务。例如在任何 lint 任务之前运行 utils 中的build任务:<br>{
  "tasks": {
    "lint": {
      "dependsOn": ["utils#build"] 
    }
  }
}
<br>或者更加细致的限定 lint:<br>{
  "tasks": {
    "web#lint": {
      "dependsOn": ["utils#build"] 
    }
  }
}
<br>即 Web 包中的 lint 任务只能在 utils 包中的build任务完成后运行。<br>某些任务可能没有任何依赖项。例如用于在 Markdown 文件中查找拼写错误的任务可能不需要关心其他任务的状态。在这种情况下，省略 dependsOn 键或给个空数组就行了:<br>{
  "tasks": {
    "spell-check": {
      "dependsOn": [] 
    }
  }
}
<br><br>outputs 键告诉 Turborepo 文件和目录在任务成功完成时应该缓存在哪。如果未定义此 key，Turborepo 将不会缓存任何文件。<br>例如缓存 vite 的输出一般可以这么写:<br>{
  "tasks": {
    "build": {
      "outputs": ["dist/**"] 
    }
  }
}
<br>inputs 键用于指定要包含在任务哈希中以进行<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/caching" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/caching" target="_blank">缓存</a>的文件。默认情况下，Turborepo 将包含包中由 Git 跟踪的所有文件。但是也可以使用 inputs 键更具体地说明哈希中包含哪些文件, 例如，在 Markdown 文件中查找拼写错误的任务可以定义如下：:<br>{
  "tasks": {
    "spell-check": {
      "inputs": ["**/*.md", "**/*.mdx"] 
    }
  }
}
<br>可以通过微调 input 以忽略对已知不会影响任务输出的文件的更改来提高某些任务的缓存命中率, 可以使用 $TURBO_DEFAULT$ 微语法来微调默认 input 行为：<br>{
  "tasks": {
    "build": {
      "inputs": ["$TURBO_DEFAULT$", "!README.md"] 
    }
  }
}
<br>这里 Turborepo 使用build任务的默认input，但会忽略对 README.md 文件的更改。如果 README.md 文件发生更改，任务仍将用上缓存。<br><br>还可以使用 turbo 在 Workspace 根的 package.json中运行脚本。例如，除了每个软件包中的 lint 任务外，可能还需要对 Workspace 根目录中的文件运行 lint：root 任务：<br>{
  "tasks": {
    "lint": {
      "dependsOn": ["^lint"]
    },
    "//#lint:root": {} 
  }
}
<br><br>
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/package-configurations" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/package-configurations" target="_blank">包配置</a>是直接放入包中的turbo.json文件。这允许软件包为其自己的任务定义特定行为，而不会影响存储库的其余部分。
<br>有一些始终需要运行的任务，例如缓存生成后的部署脚本。对于这些任务，用 “cache”： false :
<br>{
  "tasks": {
    "deploy": {
      "dependsOn": ["^build"],
      "cache": false
    },
    "build": {
      "outputs": ["dist/**"]
    }
  }
}
<br>
<br>某些任务可以并行运行，例如 Linter 不需要等待依赖项中的输出成功才能运行：
<br>{
  "tasks": {
    "transit": {
      "dependsOn": ["^transit"]
    },
    "check-types": {
      "dependsOn": ["transit"]
    },
  },
}
<br>
这里用到了 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#transit-nodes" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#transit-nodes" target="_blank">Transit Nodes</a> （就是名为 transit 的任务），这些 Transit Node 使用不执行任何操作的任务在软件包依赖项之间创建关系，这里用了名称 transit，但可以将任务命名为 Workspace 中尚未包含脚本的任何名称。
<br><br>当在软件包的目录中时，turbo 会自动将命令范围限定为该软件包的 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#package-graph" target="_blank">Package Graph</a>:<br>cd apps/docs
turbo build
<br>将使用 turbo.json 中注册的build任务运行 docs 包的build任务。<br>
但也可以<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/running-tasks#using-filters" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/running-tasks#using-filters" target="_blank">使用过滤器</a>覆盖 Automatic Package Scoping。
<br><br>Turbo 能够运行多个任务，并尽可能并行化:<br>turbo run build test lint check-types
<br><br>Turborepo 的缓存在本地工作时可以节省大量时间 - 启用<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/remote-caching" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/remote-caching" target="_blank">远程缓存</a>时，它的功能更加强大，可在整个团队和 CI 之间共享缓存。<br><br>
<br>在 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/configuration#outputs" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/configuration#outputs" target="_blank"><code></code> 键</a>turbo.json 的 outputs中定义的任务的文件输出。
<br>任务的终端输出，从任务第一次运行时开始将这些日志恢复到终端。
<br>对输入进行哈希处理，为任务运行创建一个 “fingerprints”。当 “fingerprints” 匹配时，运行任务将命中缓存。
<br><br>
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/run#--dry----dry-run" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/run#--dry----dry-run" target="_blank"><code></code> 标志</a>--dry，可用于查看如果在没有实际运行任务的情况下运行任务会发生什么。当不确定正在运行的任务时，这对于调试缓存问题非常有用。
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/run#--summarize" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/run#--summarize" target="_blank"><code></code> 标志</a>--summarize，可用于获取任务的所有输入、输出等的概览。比较两个摘要将揭示两个任务的哈希值不同的原因。
<br>强制 turbo 重新执行已缓存的任务，请使用 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/run#--force" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/run#--force" target="_blank"><code></code> 标志</a>--force。请注意，这将禁用读取缓存，而不是写入。
<br><br>在 turbo.json 中定义开发任务 (development task) 会告诉 Turborepo 将运行一个长期任务。这对于运行开发服务器、运行测试或构建应用程序等操作非常有用:<br>{
  "tasks": {
    "dev": {
      "cache": false,
      "persistent": true
    }
  }
}
<br>
<br>"cache"： false：告诉 Turborepo 不要尝试缓存任务的结果。由于这是一项开发任务，可能会频繁更改代码，因此缓存结果没有用。
<br>"persistent": true：告诉 Turborepo 保持任务运行，直到停止它。此键用作终端 UI 的信号，用于将任务视为长时间运行和交互式任务。此外，它还可以防止意外依赖不会退出的任务。
<br>一些脚本允许使用 stdin 在其中键入以进行交互式输入。使用<a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/configuration#ui" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/configuration#ui" target="_blank">终端 UI</a>，可以选择一个任务，输入它，然后像往常一样使用 stdin。<br>需要运行用于设置开发环境或预构建包的脚本。可以使用 dependsOn 确保这些任务在 dev 任务之前运行：<br>{
  "tasks": {
    "dev": {
      "cache": false,
      "persistent": true,
      "dependsOn": ["//#dev:setup"]
    },
    "//#dev:setup": {
      "outputs": [".codegen/**"]
    }
  }
}
<br>这里用的是 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/configuring-tasks#registering-root-tasks" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/configuring-tasks#registering-root-tasks" target="_blank">Root Task</a>，但可以对 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/configuring-tasks#depending-on-a-specific-task-in-a-specific-package" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/configuring-tasks#depending-on-a-specific-task-in-a-specific-package" target="_blank">packages 中的任意任务</a>使用相同的思路。<br><br>许多工具都有一个内置的 watcher，比如 <a data-tooltip-position="top" aria-label="https://www.typescriptlang.org/docs/handbook/compiler-options.html#compiler-options" rel="noopener nofollow" class="external-link" href="https://www.typescriptlang.org/docs/handbook/compiler-options.html#compiler-options" target="_blank"><code></code></a>tsc --watch，它会响应源代码中的更改。有些则没有，Turbo Watch 为任何工具添加了依赖项感知的 Watcher。对源代码的更改将遵循在 turbo.json 中描述的 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#task-graph" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts/package-and-task-graph#task-graph" target="_blank">Task Graph （任务图</a>），例如:<br>{
  "tasks": {
    "dev": {
      "persistent": true,
      "cache": false
    },
    "lint": {
      "dependsOn": ["^lint"]
    }
  }
}
<br>当运行 turbo watch dev lint 时，会看到每当更改源代码时，lint 脚本都会重新运行，尽管 ESLint 没有内置的 watcher。Turbo Watch 还知道内部依赖关系，因此 @repo/UI 中的代码更改将在 @repo/UI 和 Web 中重新运行任务。<br><br>Turborepo 需要根据环境变量来决定是否改变应用程序的行为。在 turbo.json 文件中使用 env 和 globalEnv 键:<br>{
  "globalEnv": ["IMPORTANT_GLOBAL_VARIABLE"],
  "tasks": {
    "build": {
      "env": ["MY_API_URL", "MY_API_KEY"]
    }
  }
}
<br>
<br>globalEnv：更改此列表中任何环境变量的值都将更改所有任务的哈希值。
<br>env：包括对影响任务的环境变量值的更改，从而实现更好的粒度。例如，当 API_KEY 的值发生变化时，lint 任务可以继续用缓存，但build任务应该不用。
<br>
Turborepo 会自动将前缀通配符添加到常见框架的 <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/configuration#env" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/configuration#env" target="_blank"><code></code></a>env 键中 （### <a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/using-environment-variables#framework-inference" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/using-environment-variables#framework-inference" target="_blank">Framework Inference</a>）
<br><br>Turborepo 的 Environment Mode 允许控制哪些环境变量在运行时可用于任务：<br>
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/using-environment-variables#strict-mode" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/using-environment-variables#strict-mode" target="_blank">严格模式</a>（默认）：将环境变量过滤为仅在 turbo.json 的 env 和 globalEnv 键中指定的环境变量。
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/using-environment-variables#loose-mode" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/using-environment-variables#loose-mode" target="_blank">宽松模式</a>：允许进程的所有环境变量可用。
<br><br>
<br>.env 文件非常适合在本地处理应用程序。Turborepo 不会将 .env 文件加载到任务的运行时中，而是让它们由框架或 <a data-tooltip-position="top" aria-label="https://www.npmjs.com/package/dotenv" rel="noopener nofollow" class="external-link" href="https://www.npmjs.com/package/dotenv" target="_blank"><code></code></a>dotenv 等工具处理。但是，turbo 必须知道 .env 文件中值的更改，以便它可以将它们用于哈希。如果在两次构建之间更改 .env 文件中的变量，则 build 任务应该不会用上缓存。所以可以将其添加到 input 键中:
<br>{
  "globalDependencies": [".env"], // All task hashes
  "tasks": {
    "build": {
      "inputs": ["$TURBO_DEFAULT$", ".env", ".env.local"] // Only the `build` task hash
    }
  }
}
<br>
<br>不建议在存储库的根目录中使用 .env 文件。相反，建议将 .env 文件放入使用它们的包中。
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/reference/eslint-config-turbo" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/reference/eslint-config-turbo" target="_blank"><code></code> 软件包</a>eslint-config-turbo可帮助查找代码中使用但未在 turbo.json中列出的环境变量。这有助于确保在配置中考虑所有环境变量。
<br>Turborepo 在任务开始时对任务的环境变量进行哈希处理。如果在任务期间创建或更改环境变量，Turborepo 将不知道这些更改，也不会在任务哈希中考虑这些更改。
<br><br><br>
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/crafting-your-repository/constructing-ci" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/crafting-your-repository/constructing-ci" target="_blank">配置 CI</a>
<br><a data-tooltip-position="top" aria-label="https://turbo.build/repo/docs/core-concepts" rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs/core-concepts" target="_blank">核心概念</a>
<br>
本快速入门文档参照 Turborepo 2.x 官方文档: <a rel="noopener nofollow" class="external-link" href="https://turbo.build/repo/docs" target="_blank">https://turbo.build/repo/docs</a><br>
最后一次编辑：二〇二四年九月二十七日下午六点〇七分
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/web/turborepo-简述.html</link><guid isPermaLink="false">Computer Science/Web/Turborepo 简述.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 30 Sep 2024 02:51:07 GMT</pubDate></item><item><title><![CDATA[前端设计架构概述]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:web" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#web</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:software-engineering" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#software-engineering</a> 
 <br><a href="https://muqiuhan.github.io/wiki?query=tag:web" class="tag" target="_blank" rel="noopener nofollow">#web</a> <a href="https://muqiuhan.github.io/wiki?query=tag:software-engineering" class="tag" target="_blank" rel="noopener nofollow">#software-engineering</a> <br><br>
<br><a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Service-oriented_architecture" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Service-oriented_architecture" target="_blank">Service-oriented architecture</a>
<br><a data-tooltip-position="top" aria-label="https://www.maguangguang.xyz/bff-governance" rel="noopener nofollow" class="external-link" href="https://www.maguangguang.xyz/bff-governance" target="_blank">BFF治理与优化实践</a>
<br><a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Enterprise_service_bus" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Enterprise_service_bus" target="_blank">Enterprise service bus</a>
<br><a data-tooltip-position="top" aria-label="https://www.phodal.com/blog/architecture-101-bff-for-legacy-system-migrate/" rel="noopener nofollow" class="external-link" href="https://www.phodal.com/blog/architecture-101-bff-for-legacy-system-migrate/" target="_blank">【架构拾集】前后端分离演进：不能微服务，那就 BFF 隔离</a>
<br><a data-tooltip-position="top" aria-label="https://www.maguangguang.xyz/backend-for-frontend" rel="noopener nofollow" class="external-link" href="https://www.maguangguang.xyz/backend-for-frontend" target="_blank">BFF（Backends for frontends）避坑指南</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/web/前端设计架构概述.html</link><guid isPermaLink="false">Computer Science/Web/前端设计架构概述.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 24 Jan 2025 01:15:53 GMT</pubDate></item></channel></rss>