<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[韩暮秋的个人维基]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://muqiuhan.github.io/wiki/</link><image><url>https://muqiuhan.github.io/wiki/lib/media/favicon.ico</url><title>韩暮秋的个人维基</title><link>https://muqiuhan.github.io/wiki/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 25 Jul 2024 10:02:26 GMT</lastBuildDate><atom:link href="https://muqiuhan.github.io/wiki/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 25 Jul 2024 10:02:04 GMT</pubDate><copyright><![CDATA[韩暮秋]]></copyright><ttl>60</ttl><dc:creator>韩暮秋</dc:creator><item><title><![CDATA[Intro to Databases (for people who don’t know a whole lot about them)]]></title><description><![CDATA[ 
 <br>I was a CS major from an Ivy League university who’s now a software developer at <a data-tooltip-position="top" aria-label="https://www.brandverity.com/" rel="noopener" class="external-link" href="https://www.brandverity.com/" target="_blank">an awesome company</a> — and I don’t know much about databases.<br>I’m guessing I’m not the only one. It didn’t seem to be a beginner-friendly subject. The specific class wasn’t required. Any time storage systems were mentioned, they were on a higher level, very theoretical. There seems to be a common issue around CS graduates knowing very little about real-world software development (source control? deployment? huh?), and it’s up to us to figure most of this stuff out after the fact.<br>But I’m embracing my lack of knowledge on the subject, and blogging is a cool thing. Here’s hoping for some clarity of thought via writing.<br><br>Besides the fact that it wasn’t super accessible or approachable in college, it’s probably also taken me so long to do any self-learning here because it just seemed so intimidating and mysterious (but this might just be me). I switched from design to CS two years into college with no prior experience, and I was grasping at things that were readily available and that piqued my interest. For whatever reason, databases seemed like a thing that more experienced or “smarter” people were into. “Backend work” was something I veered away from, and the word “database” conjured images of highly technical, complicated systems with jargon I just wouldn’t understand. It was much easier to pretend it was ~magic~ and leave it for others to figure out (for now — it was always my intention to pick it all up eventually).<br><img src="https://miro.medium.com/v2/resize:fit:998/1*66lYXVFX3rJxRu-8Ze3meg.gif" referrerpolicy="no-referrer"><br>Basically how I viewed data storage and retrieval.<br><br>Google defines database as “a structured set of data held in a computer, especially one that is accessible in various ways.” At its most basic, a database is just a way of storing and organizing information. Ideally it is organized in such a way that it can be easily accessed, managed, and updated.<br>I like metaphors, so this simple definition of a database for me is like a toolbox. You’ve got lots of screws, nails, bits, a couple different hammers… A toolbox is a storage system that allows you to easily organize and access all of these things. Whenever you need a tool, you go to the toolbox. Maybe you have labels on the drawers — those will help you find, say, a cordless power drill. But now you need the right battery for the drill. You look in your “battery” drawer, but how do you find the one that fits this particular drill? You can run through all of your batteries using trial and error, but that seems inefficient. You think, ‘Maybe I should store my batteries with their respective drills, link them in some way.’ That might be a viable solution. But if you need all of your batteries (because you’re setting up a nice new charging station maybe?), will you have to access each of your drills to get them? Maybe one battery fits multiple drills? Also, toolboxes are great for storing disjointed tools and pieces, but you wouldn’t want to have to take your car apart and store every piece separately whenever you park it in the garage. In that case, you would want to store your car as a single entry in the database (ahem garage), and access its pieces through it.<br><img src="https://miro.medium.com/v2/resize:fit:1180/1*Xfxl8HoQqqg_KtpEsEcskw.jpeg" referrerpolicy="no-referrer"><br>At least I can easily find the alternator this way, right?<br>This example is contrived, but reveals some issues you’ll have to consider when choosing a database or how to store your data within it.<br><br>If you start directionlessly googling “databases” (like I did), you’ll soon realize there are several different types and lots of terminology surrounding them. So let’s try and clear up any potential language barriers.<br>While I’m sure someone has written books on each of these (some of which I should probably read), I’ll try to keep my definitions relatively simple. These were all terms that I came across while doing this research that I thought could use some quick explanation.<br>
<br>Query  

<br>A query is a single action taken on a database, a request presented in a predefined format. This is typically one of SELECT, INSERT, UPDATE, or DELETE.  
<br>We also use ‘query’ to describe a request from a user for information from a database. “Hey toolbox, could you get me the names of all the tools in the ‘wrenches’ drawer?” might look something like SELECT ToolName FROM Wrenches.


<br>Transaction<br>
A transaction is a sequence of operations (queries) that make up a single unit of work performed against a database. For example, Rob paying George $20 is a transaction that consists of two UPDATE operations; reducing Rob’s balance by $20 and increasing George’s.
<br>ACID: Atomicity, Consistency, Isolation, Durability<br>
In most popular databases, a transaction is only qualified as a transaction if it exhibits the four “ACID” properties:<br>
- Atomicity: Each transaction is a unique, atomic unit of work. If one operation fails, data remains unchanged. It’s all or nothing. Rob will never lose $20 without George being paid.  

<br>Consistency: All data written to the database is subject to any rules defined. When completed, a transaction must leave all data in a consistent state.  
<br>Isolation: Changes made in a transaction are not visible to other transactions until they are complete.  
<br>Durability: Changes completed by a transaction are stored and available in the database, even in the event of a system failure.


<br>Schema<br>
- A database schema is the skeleton or structure of a database; a logical blueprint of how the database is constructed and how things relate to each other (with tables/relations, indices, etc).  

<br>Some schemas are static (defined before a program is written), and some are dynamic (defined by the program or data itself).


<br>DBMS: database management system<br>
<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Database" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Database" target="_blank">Wikipedia</a> has a great summary: “A database management system is a software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases.” MySQL, PostgreSQL, Oracle — these are database management systems.
<br>Middleware<br>
Database-oriented middleware is “all the software that connects some application to some database.” Some definitions include the DBMS under this category. Middleware might also facilitate access to a DBMS via a web server for example, without having to worry about database-specific characteristics.
<br>Distributed vs Centralized Databases  

<br>As their names imply, a centralized database has only one database file, kept at a single location on a given network; a distributed database is composed of multiple database files stored in multiple physical locations, all controlled by a central DBMS.  
<br>Distributed databases are more complex, and require additional work to keep the data stored up-to-date and to avoid redundancy. However, they provide parallelization (which balances the load between several servers), preventing bottlenecking when a large number of requests come through.  
<br>Centralized databases make data integrity easier to maintain; once data is stored, outdated or inaccurate data (stale data) is no longer available in other places. However, it may be more difficult to retrieve lost or overwritten data in a centralized database, since it lacks easily accessible copies by nature.


<br>Scalability<br>
Scalability is the capability of a database to handle a growing amount of data. There are two types of scalability:  

<br>Vertical scalability is simply adding more capacity to a single machine. Virtually every database is vertically scalable.  
<br>Horizontal scalability refers to adding capacity by adding more machines. The DBMS needs to be able to partition, manage, and maintain data across all machines.


<br><img src="https://miro.medium.com/v2/resize:fit:1400/1*pWp5uSIjn0TgU9pnJe9MzA.png" referrerpolicy="no-referrer"><br>Vertical vs Horizontal Scaling. Little buckets don’t need as much brawn to carry, but they do require better coordination.<br><br>
<br>“relational”  

<br>I highly recommend <a data-tooltip-position="top" aria-label="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" rel="noopener" class="external-link" href="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" target="_blank">this article</a>, which explains, “The word ‘relational’ in a ‘relational database’ has nothing to do with relationships. It’s about relations from relational algebra.”  
<br>In a relational database, each relation is a set of tuples. Each tuple is a list of attributes, which represents a single item in the database. Each tuple (“row”) in a relation (“table”) shares the same attributes (“columns”). Each attribute has a well-defined data type (int, string, etc), defined ahead of time — schema in a relational database is static.  
<br>Examples include: Oracle, MySQL, SQLite, PostgreSQL


<br><img src="https://miro.medium.com/v2/resize:fit:700/1*ko1siDrIKwAdrEA1P5o--g.png" referrerpolicy="no-referrer"><br>Thanks, <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Relational_database#Terminology" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Relational_database#Terminology" target="_blank">Wikipedia</a>.<br>
<br>SQL: Structured Query Language<br>
SQL is a programming language based on relational algebra used to manipulate and retrieve data in a relational database. note: In the bullet above, I’m intentionally separating the relational database terminology (relation, tuple, attribute) from the SQL terminology (table, row, column) in order to provide some clarity and accuracy. Again, see <a data-tooltip-position="top" aria-label="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" rel="noopener" class="external-link" href="https://medium.com/@pocztarski/what-if-i-told-you-there-are-no-tables-in-relational-databases-13d31a2f9677#.gtwav0tad" target="_blank">this post</a> for more details on this.
<br>Illustrative Example:<br>
We could store all the data behind a blog in a relational database. One relation will represent our blog posts, each of which will have ‘post_title’ and ‘post_content’ attributes as well as a unique ‘post_id’ (a primary key). Another relation might store all of the comments on a blog. Each item here will also have attributes like ‘comment_author’, ‘comment_content’, and ‘comment_id’ (again, a primary key), as well as its own ‘post_id.’ This attribute is a foreign key, and tells us which blog post each comment “relates” to. When we want to open a webpage for post #2 for example, we might say to the database: “select everything from the ‘posts’ table where the ID of the post is 2,” and then “select everything from the comments table where the ‘post_id’ is 2.”
<br>JOIN operations<br>
- A JOIN operation combines rows from multiple tables in one query. There are a few different types of joins and reasons for using them, but t<a data-tooltip-position="top" aria-label="http://www.sql-join.com/" rel="noopener" class="external-link" href="http://www.sql-join.com/" target="_blank">his page</a> provides good explanations and examples.<br>
- These operations are typically only used with relational databases and so are mentioned often when characterizing “relational” functionality.
<br>**Normalization and Denormalization  

<br>Normalization** is the process of organizing the relations and attributes of a relational database in a way that reduces redundancy and improves data integrity (accurate, consistent, up-to-date data). Data might be arranged based on dependencies between attributes, for example — we might prevent repeating information by using JOIN operations.  
<br>Denormalization then, is the process of adding redundant data in order to speed up complex queries. We might include the data from one table in another to eliminate the second table and reduce the number of JOIN operations.


<br>ORM: Object-Relational Mapping<br>
ORM is a technique for translating the logical representation of objects (as in object-oriented programming) into a more atomized form that is capable of being stored in a relational database (and back again when they are retrieved). I won’t go into more detail here, but it’s good to know it exists.
<br><br>
<br>“non-relational”<br>
At it’s simplest, a non-relational database is one that doesn’t use the relational model; no relations (tables) with tuples (rows) and attributes (columns). This title covers a pretty wide range of models, typically grouped into four categories: key-value stores, graph stores, column stores, and document stores.
<br><img src="https://miro.medium.com/v2/resize:fit:1280/1*Pq8DSf6o1z2N-Qc30yAoPA.jpeg" referrerpolicy="no-referrer"><br>No tables.<br>
<br>Illustrative Example:  

<br>When we set up our blog posts and comments in a relational database, it worked in the same way as the drawers of our toolbox. But, much like our drill and battery example, does it make sense to always store our blog posts in one place, and comments in another? They’re clearly related, and it’s probably rare that we’d want to look at a post’s comments without also wanting the post itself. If we used a non-relational database, opening a webpage for post #2 might look something like this: “select post #2 and everything related to it.” In this case, that would mean a ‘title’ and ‘content’, as well as a list of comments. And since we’re no longer constrained by rows always sharing the same columns, we can associate any arbitrary data with any blog posts as well — maybe some have tags, others images, or as your blog grows, you’d like some of your new posts to link to live Twitter streams. With the non-relational model, we don’t need to know ahead of time that all of our blog posts have the same attributes, and as we add attributes to newer items, we are not required to also add that “column” to all previous items as well.  
<br>This model also works well for the car example from earlier in this post. If you have three cars in your garage, it doesn’t make sense to store all of their tires together, seats together, radiators together… Instead, you store an entire car and everything related to it in its own “document.”  
<br>However, there may be a downside to this. If you wanted to know how many seats (or comments, or batteries) you have total, you may have to go through every car and count each seat individually. There are ways around this of course, but it’s less trivial than just opening up the “seats” drawer and checking your total, especially on much larger scales.


<br>NoSQL<br>
“NoSQL” originally referred to “non-SQL” or “non-relational” when describing a database. Sometimes “NoSQL” is also meant to mean “Not only SQL”, to emphasize that they don’t prohibit SQL or SQL-like query languages; they just avoid functionality like relation/table schemas and JOIN operations.
<br>Key-Value Store  

<br>Key-value stores don’t use the pre-defined structure of relational databases, but instead treat all of their data as a single collection of items. For example, a screwdriver in our toolbox might have attributes like “drive_type”, “length”, and “size”, but a hammer may only have one attribute: “size”. Instead of storing (often empty) “drive_type” and “length” fields for every item in your toolbox, a “hammer_01” key will return only the information relevant to it.  
<br>Success with this model lies in its simplicity. Like a map or a dictionary, each key-value pair defines a link between some unique “key” (like a name, ID, or URL) and its “value” (an image, a file, a string, int, list, etc). There are no fields, so the entire value must be updated if changes are made. Key-value stores are generally fast, scalable, and flexible.  
<br>Examples include: Dynamo, MemcacheDB, Redis


<br>Graph Store  

<br>Graph stores are a little more complicated.Using graph structures, this type of database is made for dealing with interconnected data — think social media connections, a family tree, or a food chain. Items in the database are represented by “nodes”, and “edges” directly represent the relationships between them. Both nodes and edges can store additional “properties”: id, name, type, etc.


<br><img src="https://miro.medium.com/v2/resize:fit:1232/1*pK3tfUnVRfdCeUtS76ixAQ.png" referrerpolicy="no-referrer"><br>Something <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Graph_database#Description" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Graph_database#Description" target="_blank">like this</a>.<br>
<br>
The strength of a graph database is in traversing the connections between items, but their scalability is limited.  

<br>
Examples include: Allegro, OrientDB, Virtuoso

<br>
Column Store  

<br>Row-oriented databases describe single items as rows, and store all the data in a particular table’s rows together: ‘hammer_01’, ‘medium’, ‘blue’; ‘hammer_02’, ‘large’, ‘yellow’. A column store, on the other hand, generally stores all the values of a particular column together: ‘hammer_01’, ‘hammer_02’; ‘medium’, ‘large’; ‘blue’, ‘yellow’.  
<br>This can definitely get confusing, but the two map data very differently. In a row-oriented system, the primary key is the row ID, mapped to its data. In the column-oriented system, the primary key is the data, mapping back to row IDs. This allows for some very quick aggregations like totals and averages.  
<br>Examples include: Accumulo, Cassandra, HBase


<br>
Document Store  

<br>Document stores treat all information for a given item in the database as a single instance in the database (each of which can have its own structure and attributes, like other non-relational databases). These “documents” can generally be thought of as sets of key-value pairs: {ToolName: “hammer_01”, Size: “medium”, Color: “blue”}  
<br>Documents can be independent units, which makes performance and horizontal scalability better, and unstructured data can be stored easily.  
<br>Examples include: Apache CouchDB, MongoDB, Azure DocumentDB.


<br>
Object or Object-Oriented Database<br>
Not as common as other non-relational databases, an object or object-oriented database is ones in which data is represented in the form of “objects” (with attributes and methods) as used in object-oriented programming. This type might be used in place of a relational database and ORM, and may make sense when the data is complex or there are complex many-to-many relationships involved. Beware its language dependence and difficulty with ad-hoc queries though.

<br><br>Now that we know some stuff about databases, how can we apply that knowledge? How do you compare/test/benchmark different databases? What does it look like when they’re actually implemented, or when you have many working together?<br>All of this and more coming soon in blog post dos.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/database/intro-to-databases-(for-people-who-don’t-know-a-whole-lot-about-them).html</link><guid isPermaLink="false">Computer Science/Database/Intro to Databases (for people who don’t know a whole lot about them).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 10 May 2024 09:55:39 GMT</pubDate><enclosure url="https://miro.medium.com/v2/resize:fit:998/1*66lYXVFX3rJxRu-8Ze3meg.gif" length="0" type="image/gif"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:998/1*66lYXVFX3rJxRu-8Ze3meg.gif&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Introduction to Actors]]></title><description><![CDATA[ 
 <br>Actor 模型为编写并发和分布式系统提供了更高级别的抽象。它使开发人员不必显式管理线程（例如加锁），从而更容易编写正确的并发和并行系统。<br>首先从一个简单的例子开始:<br>```scala3
Copy<br>object HelloWorld:<br>
final case class Greet(whom: String, replyTo: ActorRef[Greeted])<br>
final case class Greeted(whom: String, from: ActorRef[Greet])<br>def apply(): Behavior[Greet] =
    Behaviors.receive { (context, message) =&gt;
        context.log.info("Hello {}!", message.whom)
        message.replyTo ! Greeted(message.whom, context.self)
        Behaviors.same
    }
end apply
Copy<br>end HelloWorld<br>

Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/akka/introduction-to-actors/introduction-to-actors.html</link><guid isPermaLink="false">Computer Science/Distributed System/akka/Introduction to Actors/Introduction to Actors.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Tue, 14 May 2024 14:35:01 GMT</pubDate></item><item><title><![CDATA[In Search of an Understandable Consensus Algorithm(Extended Version)]]></title><description><![CDATA[ 
 <br>寻找一种可理解的一致性算法(拓展版)<br><br><br>Raft is a consensus algorithm for managing a replicated log.<br>
It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos;<br>
this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems.<br>
In order to enhance understandability, Raft separates the key elements of consensus,<br>
such as leader election, log replication, and safety,<br>
and it enforces a stronger degree of coherency to reduce the number of states that must be considered.<br>
Results from a user study demonstrate that Raft is easier for students to learn than Paxos.<br>
Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.<br>Raft是一种用于管理复制日志的一致性算法。<br>
其和(multi-)Paxos算法作用相同，并且和Paxos一样高效，但其结构与Paxos不同；这使得Raft比起Paxos更容易理解同时也为构建实际可行的系统提供了一个更好的基础。<br>
为了让Raft更容易理解，Raft拆分了有关一致性的关键元素，例如leader选举，日志复制以及安全性等，并通过增强一致性的程度以减少必须被考虑的状态数量。<br>
用户的研究成果表示Raft比起Paxos要更容易让学生进行学习。<br>
Raft还包含了一个改变集群成员的新机制，其使用重叠的大多数(overlapping majorities)来保证安全。<br><br>Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.<br>
Because of this, they play a key role in building reliable large-scale software systems.<br>
Paxos has dominated the discussion of consensus algorithms over the last decade:<br>
most implementations of consensus are based on Paxos or influenced by it,<br>
and Paxos has become the primary vehicle used to teach students about consensus.<br>一致性算法允许一个机器的集群作为一个具有一致性的组来进行工作，使得在一些成员出现故障时集群依然能正常工作。<br>
正因为如此，在构建可靠的大规模软件系统时其起到了关键的作用。<br>
Paxos主导了过去十年中关于一致性算法的讨论：<br>
大多数的一致性的实现都给予Paxos或者受其影响，并且Paxos成为了教导学生一致性相关知识的主要工具。<br>Unfortunately, Paxos is quite difficult to understand, in spite of numerous attempts to make it more approachable.<br>
Furthermore, its architecture requires complex changes to support practical systems.<br>
As a result, both system builders and students struggle with Paxos.<br>不幸的是，Paxos相当难理解，尽管很多人试图让其变得更易理解。<br>
此外，为了支持实际的系统其架构需要进行复杂的改变。<br>
因此，所有的系统构建者和学生都在于Paxos进行斗争。<br>After struggling with Paxos ourselves,<br>
we set out to find a new consensus algorithm that could provide a better foundation for system building and education.<br>
Our approach was unusual in that our primary goal was understandability:<br>
could we define a consensus algorithm for practical systems and describe it in a way that is significantly easier to learn than Paxos?<br>
Furthermore, we wanted the algorithm to facilitate the development of intuitions that are essential for system builders.<br>
It was important not just for the algorithm to work, but for it to be obvious why it works.<br>在与Paxos斗争后，我们开始着手去寻找一种新的一致性算法，其能够为构建系统和教育提供更好的支持。<br>
我们的方法是不同寻常的，因为我们的主要目标是(增进)可理解性：我们可以为实际的系统定义一个一致性算法并以比Paxos更容易学习的方式去描述它吗？<br>
此外，我们希望该算法能够促进直觉的发展，这对系统构建者来说是必要的。<br>
重要的不仅仅是算法是如何工作的，理解算法为什么能工作也很重要。<br>The result of this work is a consensus algorithm called Raft.<br>
In designing Raft we applied specific techniques to improve understandability,<br>
including decomposition (Raft separates leader election, log replication, and safety)<br>
and state space reduction (relative to Paxos, Raft reduces the degree of nondeterminism and the ways servers can be inconsistent with each other).<br>
A user study with 43 students at two universities shows that Raft is significantly easier to understand than Paxos:<br>
after learning both algorithms, 33 of these students were able to answer questions about Raft better than questions about Paxos.<br>这项工作的成果是一个名为Raft的一致性算法。<br>
在设计Raft时，我们应用了特别的技术来改善可理解性，包括分解(Raft将leader选举，日志复制和安全性进行了分解)<br>
以及状态空间的缩减(相对于Paxos，Raft缩减了不确定性的程度以及服务器之间彼此不一致的方式)。<br>
一项对两所大学中的43名学生的调查显示Raft比Paxos容易理解的多：在学习了两种算法后，相比回答Paxos相关问题，其中33名学生能更好的回答关于Raft的问题。<br>Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov’s Viewstamped Replication),<br>
but it has several novel features:<br>
<br>Strong leader: Raft uses a stronger form of leadership than other consensus algorithms.<br>
For example, log entries only flow from the leader to other servers.<br>
This simplifies the management of the replicated log and makes Raft easier to understand.
<br>Leader election: Raft uses randomized timers to elect leaders.<br>
This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm,<br>
while resolving conflicts simply and rapidly.
<br>Membership changes: Raft’s mechanism for changing the set of servers in the cluster uses a new joint consensus approach<br>
where the majorities of two different configurations overlap during transitions.<br>
This allows the cluster to continue operating normally during configuration changes.
<br>Raft与已有的一致性算法在很多方面都很相似(尤其是Oki和Liskov的Viewstamped Replication算法)，但Raft有几个新颖的功能：<br>
<br>Strong leader: Raft使用比其它一致性算法更强力的leader。<br>
举个例子，日志条目仅从leader流向其它服务器。这简化了被复制日志的管理并且使得Raft更加容易被理解。
<br>Leader election: Raft使用随机计时器来选举leader。<br>
这只在任何一致性算法都需要的心跳检测中增加了少量机制，同时简单且快速的解决冲突。
<br>Membership changes: Raft用于改变集群中服务器集合的机制使用了一种新的联合的一致性方法，其中两个不同配置的多数在过渡期间是重叠的。<br>
这允许集群在配置改变时继续正常工作。
<br>We believe that Raft is superior to Paxos and other consensus algorithms, both for educational purposes and as a foundation for implementation.<br>
It is simpler and more understandable than other algorithms;<br>
it is described completely enough to meet the needs of a practical system;<br>
it has several open-source implementations and is used by several companies;<br>
its safety properties have been formally specified and proven; and its efficiency is comparable to other algorithms.<br>我们认为，无论是处于教育的目的还是作为实际(系统)的实现，Raft都是胜过Paxos和其它一致性算法的。<br>
它比其它算法更加简单和容易理解；<br>
它被详细的描述使得其足以满足实际系统的需要；<br>
它有着几个开源的实现并且被几家公司所使用；<br>
它的安全性已经被正式的认定和证明；并且它的效率与其它算法相当。<br>The remainder of the paper introduces the replicated state machine problem (Section 2),<br>
discusses the strengths and weaknesses of Paxos (Section 3),<br>
describes our general approach to understandability (Section 4),<br>
presents the Raft consensus algorithm (Sections 5–8),<br>
evaluates Raft (Section 9), and discusses related work (Section 10).<br>本文的剩余部分介绍了复制状态机问题(第2节)，<br>
讨论了Paxos的优缺点(第3节)，<br>
描述了我们使算法易于理解的一般性方法(第4节)，<br>
提出了Raft一致性算法(第5-8节)，<br>
评估了Raft(第9节)，并且讨论了相关的工作(第10节)。<br><br>Consensus algorithms typically arise in the context of replicated state machines.<br>
In this approach, state machines on a collection of servers compute identical copies of the same state<br>
and can continue operating even if some of the servers are down.<br>
Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems.<br>
For example, large-scale systems that have a single cluster leader, such as GFS, HDFS, and RAMCloud,<br>
typically use a separate replicated state machine to manage leader election and store configuration information<br>
that must survive leader crashes.<br>
Examples of replicated state machines include Chubby and ZooKeeper.<br>一致性算法是在复制状态机的背景下产生的。<br>
在这个方法中，服务器集合中的状态机在具有相同状态的完全一致的副本上进行计算，并且即使一些服务器已经宕机也能够持续的工作。<br>
复制状态机被用于在分布式系统中解决一系列的容错问题。<br>
举个例子，有着一个单独集群leader的大规模系统，例如GFS，HDFS以及RAMCloud，通常使用一个单独的复制状态机来管理leader选举和存储在leader崩溃后所必须的配置信息。<br>
复制状态机的例子包括Chubby和ZooKeeper。<br><img alt="Pasted image 20240725172940.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172940.png"><br>Figure 1: Replicated state machine architecture.<br>
The consensus algorithm manages a replicated log containing state machine commands from clients.<br>
The state machines process identical sequences of commands from the logs, so they produce the same outputs.<br>图1：复制状态机的架构。<br>
一致性算法管理包含了来自客户端的状态机指令的复制日志。<br>
状态机以完全相同的顺序处理来自日志的指令，因此它们会产生相同的输出。<br>Replicated state machines are typically implemented using a replicated log, as shown in Figure 1.<br>
Each server stores a log containing a series of commands, which its state machine executes in order.<br>
Each log contains the same commands in the same order, so each state machine processes the same sequence of commands.<br>
Since the state machines are deterministic, each computes the same state and the same sequence of outputs.<br>复制状态机通常使用复制log(replicated log)来实现，如图1所示。<br>
每个服务器存储着一个包含一系列指令的日志，这些指令在状态机上被顺序执行。<br>
每个日志中包含了以相同顺序排布的相同的指令，因此每个状态机都处理相同的指令序列。<br>
因为状态机是确定性的，每一个状态机都计算出相同的状态以及有着相同的输出序列。<br>Keeping the replicated log consistent is the job of the consensus algorithm.<br>
The consensus module on a server receives commands from clients and adds them to its log.<br>
It communicates with the consensus modules on other servers to ensure that every log eventually contains<br>
the same requests in the same order, even if some servers fail.<br>
Once commands are properly replicated, each server’s state machine processes them in log order,<br>
and the outputs are returned to clients.<br>
As a result, the servers appear to form a single, highly reliable state machine.<br>保持复制日志的一致性是一致性算法的工作。<br>
服务器中的一致性模块接受来自客户端的指令并且将其加入日志。<br>
它与其它服务器的一致性模块进行通信以确保每一个日志最终以同样的顺序包含同样的请求，即使其中一些服务器故障了。<br>
一旦指令被正确的复制，每一个服务器的状态机都按照日志中的顺序处理这些指令，并将输出返回给客户端。<br>
因此，服务器的集合似乎形成了一个单独的，高度可靠的状态机。<br>Consensus algorithms for practical systems typically have the following properties:<br>
<br>They ensure safety (never returning an incorrect result) under all non-Byzantine conditions,<br>
including network delays, partitions, and packet loss, duplication, and reordering.
<br>They are fully functional (available) as long as any majority of the servers are operational<br>
and can communicate with each other and with clients.<br>
Thus, a typical cluster of five servers can tolerate the failure of any two servers.<br>
Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster.
<br>They do not depend on timing to ensure the consistency of the logs:<br>
faulty clocks and extreme message delays can, at worst, cause availability problems.
<br>In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls;<br>
a minority of slow servers need not impact overall system performance.
<br>实际系统中的一致性算法通常具有以下属性：<br>
<br>它们确保在所有非拜占庭条件下的安全性(永远不返回错误结果)，(非拜占庭条件)包括网络延迟，分区，和丢包，重复以及重新排序。
<br>只要大多数服务器能够正常工作并且能够与其它服务器以及客户端互相通信，一致性算法就能发挥其全部的功能(可用性)。<br>
因此，一个典型的有着5台服务器组成的集群能够容忍任意两台服务器出现故障。<br>
假设服务器因为故障而停机；他们可以稍后从稳定的存储状态中恢复并重新加入集群。
<br>他们不依赖时间来确保日志的一致性：错误的时钟和极端的消息延迟在最坏的情况下会造成可用性问题。
<br>通常情况下，只要集群中的大多数对单轮的远过程调用做出了响应，命令就可以完成。占少数的慢速服务器不会对系统整体性能造成影响。
<br><br>Over the last ten years, Leslie Lamport’s Paxos protocol has become almost synonymous with consensus:<br>
it is the protocol most commonly taught in courses, and most implementations of consensus use it as a starting point.<br>
Paxos first defines a protocol capable of reaching agreement on a single decision, such as a single replicated log entry.<br>
We refer to this subset as single-decree Paxos.<br>
Paxos then combines multiple instances of this protocol to facilitate a series of decisions such as a log (multi-Paxos).<br>
Paxos ensures both safety and liveness, and it supports changes in cluster membership.<br>
Its correctness has been proven, and it is efficient in the normal case.<br>在过去的十年中，Leslie Lamport的Paxos协议几乎已经成为了一致性算法的代名词：<br>
它是课堂教学中最常用的协议，大多数的一致性算法也将其作为起点。<br>
Paxos首先定义了一个协议，其能够就单个决定达成一致，例如单个日志条目的复制。<br>
我们将这一自己称为single-decree Paxos。<br>
然后Paxos将该协议的多个实例组合起来以达成一系列的决定，例如日志(multi-Paxos)。<br>
Paxos同时保证了安全性和活性，并且支持集群成员的变更。<br>
其正确性已经得到证明，并且在通常情况下是高效的。<br>Unfortunately, Paxos has two significant drawbacks.<br>
The first drawback is that Paxos is exceptionally difficult to understand.<br>
The full explanation is notoriously opaque; few people succeed in understanding it, and only with great effort.<br>
As a result, there have been several attempts to explain Paxos in simpler terms.<br>
These explanations focus on the single-decree subset, yet they are still challenging.<br>
In an informal survey of attendees at NSDI 2012, we found few people who were comfortable with Paxos, even among seasoned researchers.<br>
We struggled with Paxos ourselves;<br>
we were not able to understand the complete protocol until after reading several simplified explanations<br>
and designing our own alternative protocol, a process that took almost a year.<br>不幸的是，Paxos有着两个明显的缺点。<br>
第一个缺点是Paxos异乎寻常的难理解。<br>
Paxos出了名的难理解，即使在付出了巨大努力的情况下，也很少有人能成功的理解它。<br>
因此，有一些人尝试着用更简单的方式来理解Paxos。<br>
这些解释聚焦于single-decree这一子集，但这仍具有挑战性。<br>
在一项针对NSDI 2012与会者的非正式调查中，我们发现很少有人对Paxos感到满意，即使对于经验丰富的研究员来说也是如此。<br>
我们也与Paxos进行了艰难的斗争；直到阅读了几个简化的解释并设计了我们自己的替代方案后我们才能够理解完整的协议，而这个过程花费了将近一年的时间。<br>We hypothesize that Paxos’ opaqueness derives from its choice of the single-decree subset as its foundation.<br>
Single-decree Paxos is dense and subtle:<br>
it is divided into two stages that do not have simple intuitive explanations and cannot be understood independently.<br>
Because of this, it is difficult to develop intuitions about why the single-decree protocol works.<br>
The composition rules for multi-Paxos add significant additional complexity and subtlety.<br>
We believe that the overall problem of reaching consensus on multiple decisions (i.e., a log instead of a single entry)<br>
can be decomposed in other ways that are more direct and obvious.<br>我们猜定Paxos晦涩难懂的原因在于作者选择以single-decree这一子集作为Paxos的基础。<br>
Single-decree Paxos是难理解和精巧的：<br>
它被分为了两个阶段，并且没有简单直接的说明，每一阶段也无法单独的理解。<br>
正因如此，很难凭借直觉的理解single-decree协议为什么能够工作。<br>
multi-Paxos的组合规则也显著的增加了复杂性和微妙之处。<br>
我们认为，就多个决定达成一致的总体问题(例如，使用日志而不是单个的entry)能够被分解为其它更直接和更容易理解的方式。<br>The second problem with Paxos is that it does not provide a good foundation for building practical implementations.<br>
One reason is that there is no widely agreed-upon algorithm for multi-Paxos.<br>
Lamport’s descriptions are mostly about single-decree Paxos;<br>
he sketched possible approaches to multi-Paxos, but many details are missing.<br>
There have been several attempts to flesh out and optimize Paxos, such as [26], [39], and [13],<br>
but these differ from each other and from Lamport’s sketches.<br>
Systems such as Chubby [4] have implemented Paxos-like algorithms, but in most cases their details have not been published.<br>Paxos的第二个问题是它没有为构建实际可行的实现提供一个好的基础。<br>
其中一个原因是对于multi-Paxos没有一个被广泛认同的算法。<br>
Lamport的描述大多数都是关于single-decree Paxos的；他简要的概述了实现multi-Paxos的可行的方法，但缺失了很多的细节。<br>
已经有几个(团队)试图去具体化和优化Paxos，例如[26],[39]和[13],但这些尝试彼此间不同且也不同于Lamport的概述。<br>
像Chubby系统已经实现了类似Paxos的算法，但大多数情况下的细节并没有被公开。<br>Furthermore, the Paxos architecture is a poor one for building practical systems;<br>
this is another consequence of the single-decree decomposition.<br>
For example, there is little benefit to choosing a collection of log entries independently and then melding them into a sequential log;<br>
this just adds complexity.<br>
It is simpler and more efficient to design a system around a log,<br>
where new entries are appended sequentially in a constrained order.<br>
Another problem is that Paxos uses a symmetric peer-to-peer approach at its core<br>
(though it eventually suggests a weak form of leadership as a performance optimization).<br>
This makes sense in a simplified world where only one decision will be made, but few practical systems use this approach.<br>
If a series of decisions must be made, it is simpler and faster to first elect a leader, then have the leader coordinate the decisions.<br>此外，Paxos的架构在构建实际的系统时表现不佳；这是对single-decree进行分解的另一个结果。<br>
例如，选择一组独立的日志集合并将其合并到一个顺序日志中几乎没有带来什么好处；这只会增加复杂性。<br>
围绕日志来设计系统会更简单和更高效，其中新的日志条目以受约束的顺序追加。<br>
另一个问题是，Paxos使用了一种对称的点对点(P2P)方法作为其核心(尽管最后提出了一种更弱形式的leadership作为性能优化)。<br>
在一个只需要做一次决定的，被简化的世界中这样是行得通的，但很少有实际的系统使用这个方式。<br>
如果有一系列的决定必须要做，首先选举出一个leader，然后leader来协调决策会更简单和更快速。<br>As a result, practical systems bear little resemblance to Paxos.<br>
Each implementation begins with Paxos, discovers the difficulties in implementing it,<br>
and then develops a significantly different architecture.<br>
This is time-consuming and error-prone, and the difficulties of understanding Paxos exacerbate the problem.<br>
Paxos’ formulation may be a good one for proving theorems about its correctness,<br>
but real implementations are so different from Paxos that the proofs have little value.<br>The following comment from the Chubby implementers is typical:<br>
There are significant gaps between the description of the Paxos algorithm<br>
and the needs of a real-world system. . . . the final system will be based on an unproven protocol [4].<br>因此，实际的系统与Paxos几乎没有相似之处。<br>
每一个实现都从Paxos出发，发现实现Paxos的困难之处，然后开发出一个与之截然不同的架构。<br>
这既耗费时间又容易出错，并且Paxos的晦涩难懂加剧了这一问题。<br>
Paxos的公式可能可以很好的证明其正确性，但是实际的实现与Paxos是如此的不同，以至于这些证明几乎毫无价值。<br>以下Chubby实现者的评论是具有代表性的：<br>
Paxos算法的描述与现实世界系统的需求之间有着巨大的鸿沟....最终的系统将建立在一个未被证明的协议之上。<br>Because of these problems, we concluded that Paxos does not provide a good foundation either for system building or for education.<br>
Given the importance of consensus in large-scale software systems,<br>
we decided to see if we could design an alternative consensus algorithm with better properties than Paxos.<br>
Raft is the result of that experiment.<br>由于这些问题，我们的结论是Paxos并没有为构建系统或是进行教育提供一个好的基础。<br>
考虑到一致性在大规模软件系统中的重要性，我们决定看看我们是否可以设计出一个比起Paxos有着更好特性的一致性算法。<br>
Raft正是这一实验的成果。<br><br>We had several goals in designing Raft: it must provide a complete and practical foundation for system building,<br>
so that it significantly reduces the amount of design work required of developers;<br>
it must be safe under all conditions and available under typical operating conditions; and it must be efficient for common operations.<br>
But our most important goal—and most difficult challenge—was understandability.<br>
It must be possible for a large audience to understand the algorithm comfortably.<br>
In addition, it must be possible to develop intuitions about the algorithm,<br>
so that system builders can make the extensions that are inevitable in real-world implementations.<br>我们在设计Raft时有几个目标：它必须为构建系统提供一个完整的和实际的基础，从而显著的减少开发者设计时所需的工作；<br>
它必须在任何条件下都是安全的并且在典型的工作状态下是可用的；同时它必须在通常工作状态下是高效的。<br>
但我们最重要的目标也是最困难的挑战是使得Raft通俗易懂。<br>
必须尽可能的使大多数人能够轻松的理解该算法。<br>
这样系统构建者才能够在现实世界的实现中进行不可避免的拓展。<br>There were numerous points in the design of Raft where we had to choose among alternative approaches.<br>
In these situations we evaluated the alternatives based on understandability:<br>
how hard is it to explain each alternative (for example, how complex is its state space,<br>
and does it have subtle implications?), and how easy will it be for a reader to completely understand the approach and its implications?<br>在设计Raft时有很多要点都必须在多个可选方案中抉择。<br>
在这些情况下，我们基于易懂性来评估这些可选方案：<br>
对于每一个可选方案解释起来有多困难(例如，状态空间有多复杂以及是否有微妙的含义？)，以及对于一个读者来说完全理解这个方法和其含义有多容易？<br>We recognize that there is a high degree of subjectivity in such analysis; nonetheless, we used two techniques that are generally applicable.<br>
The first technique is the well-known approach of problem decomposition:<br>
wherever possible, we divided problems into separate pieces that could be solved, explained, and understood relatively independently.<br>
For example, in Raft we separated leader election, log replication, safety, and membership changes.<br>我们意识到这一分析方式具有高度的主观性；尽管如此，但我们还是使用了两种可行的通用技术。<br>
第一个技术是众所周知的问题分解方法：<br>
在可能的情况下，我们将问题分解为几个部分，使得每一部分都可以被相对独立的解决，解释和理解。<br>
例如，我们将Raft分解为leader选举，日志复制，安全性和成员变更这几个部分。<br>Our second approach was to simplify the state space by reducing the number of states to consider,<br>
making the system more coherent and eliminating nondeterminism where possible.<br>
Specifically, logs are not allowed to have holes, and Raft limits the ways in which logs can become inconsistent with each other.<br>
Although in most cases we tried to eliminate nondeterminism, there are some situations where nondeterminism actually improves understandability.<br>
In particular, randomized approaches introduce nondeterminism,<br>
but they tend to reduce the state space by handling all possible choices in a similar fashion(“choose any; it doesn’t matter”).<br>
We used randomization to simplify the Raft leader election algorithm.<br>我们的第二种方法是通过减少需要考虑的状态数量以简化状态空间，使系统变得更加连贯并尽可能的消除不确定性。<br>
特别的，日志是不允许存在空洞的，并且Raft限制了使得日志间变得彼此不一致的方式。<br>
尽管在大多数情况下我们试图消除不确定性，但在一些条件下不确定性实际上能提高可理解性。<br>
特别的，随机化方法引入了不确定性，但它们倾向于通过用相似的方式来处理所有可能的选择以减少状态空间("选择任意一个;具体是哪一个则无关紧要")。<br>
我们使用随机化来简化Raft中的leader选举算法。<br><br>Raft is an algorithm for managing a replicated log of the form described in Section 2.<br>
Figure 2 summarizes the algorithm in condensed form for reference, and Figure 3 lists key properties of the algorithm;<br>
the elements of these figures are discussed piecewise over the rest of this section.<br>Raft是一种管理如第二节所述的复制日志的算法。<br>
图2以简明扼要的总结了算法以供参考，图3列举出了算法的关键特性；这些图中的元素将在本节剩余的部分中进行讨论。<br>Raft implements consensus by first electing a distinguished leader,<br>
then giving the leader complete responsibility for managing the replicated log.<br>
The leader accepts log entries from clients,<br>
replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines.<br>
Having a leader simplifies the management of the replicated log.<br>
For example, the leader can decide where to place new entries in the log without consulting other servers,<br>
and data flows in a simple fashion from the leader to other servers.<br>
A leader can fail or become disconnected from the other servers, in which case a new leader is elected.<br>Raft通过受限选举出一位distinguished leader，然后让它全权的管理复制日志以实现一致性。<br>
这个leader接受来自客户端的日志条目，将其复制到其它服务器中，并且在日志条目可以被安全的应用在它们的状态机上时通知这些服务器。<br>
拥有一个leader可以简化对复制日志的管理。<br>
例如，leader可以决定新日志条目的位置而无需咨询其它服务器，并且数据流以一种简单的形式由leader流向其它服务器。<br>
leader可能会故障或者与其它服务器失联，这种情况下一位新的leader将会被选举出来。<br>Given the leader approach, Raft decomposes the consensus problem into three relatively independent sub-problems,<br>
which are discussed in the subsections that follow:<br>
<br>Leader election: a new leader must be chosen when an existing leader fails (Section 5.2).
<br>Log replication: the leader must accept log entries from clients and replicate them across the cluster,<br>
forcing the other logs to agree with its own (Section 5.3).
<br>Safety: the key safety property for Raft is the State Machine Safety Property in Figure 3:<br>
if any server has applied a particular log entry to its state machine,<br>
then no other server may apply a different command for the same log index.<br>
Section 5.4 describes how Raft ensures this property;<br>
the solution involves an additional restriction on the election mechanism described in Section 5.2.
<br>After presenting the consensus algorithm, this section discusses the issue of availability and the role of timing in the system.<br>通过引入leader的方法，Raft将一致性问题分解为3个相对独立的子问题，这些子问题将在以下子章节中被讨论：<br>
<br>leader选举： 当一位现存的leader故障时必须选出一位新的leader(5.2节)。
<br>日志复制： leader必须从客户端接收日志条目并且在集群中复制它们，并且强制其它节点的日志与leader保持一致(5.3节)。
<br>安全性： Raft的关键安全特性就是图3中的状态机的安全特性：如果任一服务器已经将一个特定的日志条目作用于它的状态机，则没有任何服务器可以对相同的日志索引应用不同的指令。<br>
5.4节描述了Raft是如何确保这一特性的；这一解决方案涉及到对5.2节中所描述的选举机制的额外限制。
<br>在展示了一致性算法后，本章节还将讨论可用性问题以及时序在系统中起到的作用。<br><img alt="Pasted image 20240725172955.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172955.png"><br>A condensed summary of the Raft consensus algorithm (excluding membership changes and log compaction).<br>
The server behavior in the upper-left box is described as a set of rules that trigger independently and repeatedly.<br>
Section numbers such as §5.2 indicate where particular features are discussed.<br>
A formal specification [31] describes the algorithm more precisely.<br>关于Raft一致性算法的精简摘要(不包括成员变更和日志压缩)。<br>
左上方框内所描述的服务器行为被描述为一系列独立和重复触发的规则。<br>
章节编号例如§5.2标识了具体讨论该特定功能的章节。<br>
形式化规约以更精确的方式描述该算法。<br><img alt="Pasted image 20240725173009.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173009.png"><br>Figure 3: Raft guarantees that each of these properties is true at all times.<br>
The section numbers indicate where each property is discussed.<br>图3：Raft保证每一个特性在任何时候都是成立的、名副其实的。<br>
章节号标识着每一个特性被讨论的具体章节。<br><br>A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures.<br>
At any given time each server is in one of three states: leader, follower, or candidate.<br>
In normal operation there is exactly one leader and all of the other servers are followers.<br>
Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates.<br>
The leader handles all client requests (if a client contacts a follower, the follower redirects it to the leader).<br>
The third state, candidate, is used to elect a new leader as described in Section 5.2.<br>
Figure 4 shows the states and their transitions; the transitions are discussed below.<br>一个Raft的集群包含几个服务器;通常是5个节点，这样的系统能容忍系统中的2个节点出现故障。<br>
在任一给定的时间内，每个服务器只会处于3种状态中的一种：领导者(leader),追随者(follower)，或者候选者(candidate)。<br>
在通常情况下，只会有1个leader并且其它的服务器都是follower。<br>
Follower都是被动的: 它们自己不会提出请求而只会简单的响应来自leader和candidate的请求。<br>
leader处理所有来自客户端的请求(如果一个客户端与follower进行联络，follower会将其重定向到leader)。<br>
第三种状态，candidate，用于选举出一个如5.2章节所描述的新leader。<br>
图4展示了状态以及状态间的转换关系；转换关系将在下文被讨论。<br>Raft divides time into terms of arbitrary length, as shown in Figure 5.<br>
Terms are numbered with consecutive integers.<br>
Each term begins with an election, in which one or more candidates attempt to become leader as described in Section 5.2.<br>
If a candidate wins the election, then it serves as leader for the rest of the term.<br>
In some situations an election will result in a split vote.<br>
In this case the term will end with no leader; a new term (with a new election) will begin shortly.<br>
Raft ensures that there is at most one leader in a given term.<br>Raft将时间分割为任意长度的任期(term)，如图5所示。<br>
任期由连续的整数进行编号。<br>
每一个任期都以一次选举开始，其中一个或更多的candidate试图成为leader(如5.2节中所描述的)。<br>
如果一个candidate赢得了选举，然后它将在余下的任期中作为leader。<br>
在一些情况下一次选举可能会导致分裂的投票结果。<br>
在这种情况下，任期将在没有leader的情况下结束; 一个新的任期(伴随者一个新的选举)将很快开始。<br>
Raft保证了在一个给定的任期内最多只会有一个leader。<br><img src="https://img2023.cnblogs.com/blog/1506329/202307/1506329-20230713200905466-706345890.png" referrerpolicy="no-referrer"><br>Figure 4: Server states. Followers only respond to requests from other servers.<br>
If a follower receives no communication, it becomes a candidate and initiates an election.<br>
A candidate that receives votes from a majority of the full cluster becomes the new leader.<br>
Leaders typically operate until they fail.<br>图4：服务器状态。<br>
follower只能响应来自其它服务器的请求。<br>
如果follower没有收到通信，它将成为一名candidate并且初始化一场选举。<br>
一位candidate收到了来自整个集群中的大多数投票则成为新的leader。<br>
leader通常持续工作直到它们发生故障。<br><img alt="Pasted image 20240725173021.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173021.png"><br>Figure 5: Time is divided into terms, and each term begins with an election.<br>
After a successful election, a single leader manages the cluster until the end of the term.<br>
Some elections fail, in which case the term ends without choosing a leader.<br>
The transitions between terms may be observed at different times on different servers.<br>图5：时间以任期进行划分，每一个任期都以一次选举开始。<br>
在成功的选举之后，一个leader管理集群直到任期结束。<br>
有些选举失败了，在这种情况下任期结束时并没有选出一个leader。<br>
可以在不同服务器的不同时间上观察到任期的转换。<br>Different servers may observe the transitions between terms at different times,<br>
and in some situations a server may not observe an election or even entire terms.<br>
Terms act as a logical clock [14] in Raft, and they allow servers to detect obsolete information such as stale leaders.<br>
Each server stores a current term number, which increases monotonically over time.<br>
Current terms are exchanged whenever servers communicate;<br>
if one server’s current term is smaller than the other’s, then it updates its current term to the larger value.<br>
If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state.<br>
If a server receives a request with a stale term number, it rejects the request.<br>不同服务器可能会在不同的时间上观察到任期之间的状态转换，并且在一些情况下一个服务器可能不会观察到一次选举甚至整个任期。<br>
任期在Raft中充当逻辑时钟，并且它们允许服务器检测到过时的信息比如之前的、老leader。<br>
每一个服务器存储了一个当前任期的编号，其随着时间单调增加。<br>
每当服务器之间互相通信时，它们都会互相交换当前的任期(编号);如果一个服务器的当前任期(编号)小于其它的服务器，则其将会将当前的任期(编号)更新为那个更大的值。<br>
如果一个candidate或者leader发现它们的任期(编号)已经过时，它将立即将自己恢复为follower的状态。<br>
如果一个服务器接受到一个带有过时任期编号的请求，它将拒绝这一请求。<br>Raft servers communicate using remote procedure calls(RPCs), and the basic consensus algorithm requires only two types of RPCs.<br>
RequestVote RPCs are initiated by candidates during elections (Section 5.2),<br>
and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat (Section 5.3).<br>
Section 7 adds a third RPC for transferring snapshots between servers.<br>
Servers retry RPCs if they do not receive a response in a timely manner, and they issue RPCs in parallel for best performance.<br>Raft服务器使用远过程调用(RPC)进行通信，并且基本的一致性算法只需要两种类型的RPC。<br>
请求投票的RPC由candidate在选举期间发起(第5.2节)，拓展条目的RPC由leader发起，用于日志条目的复制以及提供心跳机制(第5.3节)。<br>
第7节加入了第三种RPC用于在服务器间传输快照。<br>
如果服务器在给定的时间内没有收到响应，则会对RPC进行重试，并且它们会发起并行的rpc以获得最好的性能。<br><br>Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers.<br>
A server remains in follower state as long as it receives valid RPCs from a leader or candidate.<br>
Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority.<br>
If a follower receives no communication over a period of time called the election timeout,<br>
then it assumes there is no viable leader and begins an election to choose a new leader.<br>Raft使用心跳机制来触发leader选举。当服务器启动时，它们会成为follower。<br>
只要服务器能从leader或者candidate处接收到有效的RPC请求，它们就将保持follower状态。<br>
leader向所有follower发送周期性的心跳(不携带日志条目的AppendEntries RPC)来维持它的权威性。<br>
如果一个follower在一段被成为选举超时的时间段内未接收到任何通信，则它假设当前没有可用的leader并且发起选举来选择一个新的leader。<br>To begin an election, a follower increments its current term and transitions to candidate state.<br>
It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster.<br>
A candidate continues in this state until one of three things happens:<br>
(a) it wins the election,<br>
(b) another server establishes itself as leader, or<br>
(c) a period of time goes by with no winner.<br>
These outcomes are discussed separately in the paragraphs below.<br>为了开始一轮选举，follower增加它当前的任期值并且转换为candidate状态。<br>
然后它将选票投给它自己并且向集群中的其它服务器并行的发起请求投票的RPC(RequestVote RPCs)。<br>
一个candidate会一直保持这种状态直到以下三种情况之一发生：<br>
(a) 它赢得此次选举 (b) 另一个服务器将自己确认为leader，或者 (c) 一段时间后没有产生胜利者。<br>
下文中的各个段落将分别讨论这些结果。<br>A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term.<br>
Each server will vote for at most one candidate in a given term, on a first-come-first-served basis<br>
(note: Section 5.4 adds an additional restriction on votes).<br>
The majority rule ensures that at most one candidate can win the election for a particular term (the Election Safety Property in Figure 3).<br>
Once a candidate wins an election, it becomes leader.<br>
It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.<br>如果一个candidate在同一个任期内接收到了整个集群中大多数服务器的投票，其将赢得这次选举。<br>
每个服务器在给定的某一任期内将会基于先来先服务的原则(first-come-first-served)投票给至多一位candidate(第5.4节对投票增加了额外的限制)。<br>
多数规则确保了对于一个特定的任期，最多只会有一名candidate能够赢得选举(图3中选举的安全特性)。<br>
一旦一个candidate赢得了一次选举，它将成为leader。<br>
然后它向其它服务器发送心跳信息以建立权威并且阻止新的选举。<br>While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader.<br>
If the leader’s term (included in its RPC) is at least as large as the candidate’s current term,<br>
then the candidate recognizes the leader as legitimate and returns to follower state.<br>
If the term in the RPC is smaller than the candidate’s current term, then the candidate rejects the RPC and continues in candidate state.<br>在等待投票时，一个candidate可能会接受到来自自称是leader的其它服务器的AppendEntries RPC。<br>
如果leader的任期(包含在它的RPC中)大于或等于candidate的当前任期，那么candidate承认该leader是合法的并且返回到follower状态。<br>
如果RPC中的任期小于candidate的当前任期，candidate将会拒绝这一RPC并且继续保持candidate的状态。<br>The third possible outcome is that a candidate neither wins nor loses the election:<br>
if many followers become candidates at the same time, votes could be split so that no candidate obtains a majority.<br>
When this happens, each candidate will time out and start a new election by incrementing its term<br>
and initiating another round of RequestVote RPCs.<br>
However, without extra measures split votes could repeat indefinitely.<br>第三种可能的结果是一个candidate既没有赢得选举也没有输掉选举：<br>
如果许多follower都在同一时间成为了candidate，投票可能会被瓜分导致没有candidate获得大多数的选票。<br>
当这种情况发生时，每一个candidate都将会超时并且通过增加它的任期值并且初始化另一轮的RequestVote RPCs以开始一轮新的选举。<br>
然而，如果不采取额外的措施，分裂的投票可能会无限的重复。<br>Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly.<br>
To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms).<br>
This spreads out the servers so that in most cases only a single server will time out;<br>
it wins the election and sends heartbeats before any other servers time out.<br>
The same mechanism is used to handle split votes.<br>
Each candidate restarts its randomized election timeout at the start of an election, and it waits for that timeout to elapse before<br>
starting the next election; this reduces the likelihood of another split vote in the new election.<br>
Section 9.3 shows that this approach elects a leader rapidly.<br>Raft使用随机化的选举超时时间来确保分裂的投票很少会发生并使得它们能够被迅速的解决。<br>
为了防止一开始就出现分裂的投票，选举的超时时间是从一个固定的间隔中被随机选取的(例如150-300ms)。<br>
这打散了服务器使得在大多数情况下只有单独一个服务器将会超时；它赢得选举并且在其它服务器超时之前发送心跳(译者注：超时后自己就会在别的服务器没反应过来前发起新一轮任期更大的投票，让别人都投给它来赢得选举)。<br>
同样的机制也被用于解决分裂的投票。<br>
每个candidate在一轮选举开始时会重新随机的设置其选举超时时间，并且在下一轮选举前等待直到超时；这减少了在新的选举中再一次出现分裂投票的可能性。<br>
第9.3节展示了该方法能迅速的选举出一个leader。<br>Elections are an example of how understandability guided our choice between design alternatives.<br>
Initially we planned to use a ranking system: each candidate was assigned a unique rank, which was used to select between competing candidates.<br>
If a candidate discovered another candidate with higher rank,<br>
it would return to follower state so that the higher ranking candidate could more easily win the next election.<br>
We found that this approach created subtle issues around availability<br>
(a lower-ranked server might need to time out and become a candidate again if a higher-ranked server fails,<br>
but if it does so too soon, it can reset progress towards electing a leader).<br>
We made adjustments to the algorithm several times, but after each adjustment new corner cases appeared.<br>
Eventually we concluded that the randomized retry approach is more obvious and understandable.<br>选举是一个可理解性如何指导我们在可选设计间进行选择的例子。<br>
最初，我们计划使用等级系统(ranking system)：每一个candidate都被分配一个唯一的等级，其用于在彼此竞争的candidate做选择。<br>
如果一个candidate发现了一个具有更高等级的candidate，它将返回到follower状态因此更好等级的candidate将更容易赢得下一次选举。<br>
但我们发现这个方法在可用性方面存在微妙的问题(如果一个高等级的服务器故障了，则一个低等级的服务器可能需要超时并再次成为candidate，但如果这样做的太早，它将会重置选举leader的进度)。<br>
我们对算法进行了数次调整，但每次调整后都出现了新的困境。<br>
最终我们得出结论，随机化重试的方法更直观且更容易被理解。<br><br>Once a leader has been elected, it begins servicing client requests.<br>
Each client request contains a command to be executed by the replicated state machines.<br>
The leader appends the command to its log as a new entry,<br>
then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry.<br>
When the entry has been safely replicated (as described below),<br>
the leader applies the entry to its state machine and returns the result of that execution to the client.<br>
If followers crash or run slowly, or if network packets are lost,<br>
the leader retries AppendEntries RPCs indefinitely (even after it has responded to the client)<br>
until all followers eventually store all log entries.<br>一旦一个leader被选举出来，它将开始服务于客户端的请求。<br>
每一个客户端的请求都包含了一个被用于在复制状态机上执行的指令。<br>
leader将指令作为一个新的条目追加到其日志中，然后向其它的每个服务器发起并行的AppendEntries RPC令它们复制这一条目。<br>
当条目已被安全的被复制(如下所述)，leader在它的状态机上应用这一条目并且将执行的结果返回给客户端。<br>
如果follower崩溃了或者运行的很慢，或者网络丢包，leader会无限的重试AppendEntries RPC(即使在响应了客户端的请求之后)，<br>
直到所有的follower最终都存储了所有的日志条目。<br><img alt="Pasted image 20240725173038.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173038.png"><br>Figure 6: Logs are composed of entries, which are numbered sequentially.<br>
Each entry contains the term in which it was created (the number in each box) and a command for the state machine.<br>
An entry is considered committed if it is safe for that entry to be applied to state machines.<br>图6：日志由按照顺序编号的条目组成。<br>
每一个条目都包含它被创建时的任期(框中的数字)以及用于状态机的指令。<br>
如果条目已经安全的被作用于状态机，则该条目被视为已提交。<br>Logs are organized as shown in Figure 6.<br>
Each log entry stores a state machine command along with the term number when the entry was received by the leader.<br>
The term numbers in log entries are used to detect inconsistencies between logs and to ensure some of the properties in Figure 3.<br>
Each log entry also has an integer index identifying its position in the log.<br>日志如图6所示的方式被组织。<br>
每一个日志条目存储了一个状态机的指令，以及从leader处接受条目时的任期编号。<br>
日志条目中的任期编号被用于检测日志间的不一致，并且用于保证图3中的一些特性。<br>
每个日志条目也有一个整数的索引标识其在日志中的位置。<br>The leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed.<br>
Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines.<br>
A log entry is committed once the leader that created the entry has replicated it on a majority of the servers (e.g., entry 7 in Figure 6).<br>
This also commits all preceding entries in the leader’s log, including entries created by previous leaders.<br>
Section 5.4 discusses some subtleties when applying this rule after leader changes,<br>
and it also shows that this definition of commitment is safe.<br>
The leader keeps track of the highest index it knows to be committed,<br>
and it includes that index in future AppendEntries RPCs (including heartbeats) so that the other servers eventually find out.<br>
Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order).<br>leader决定何时能安全的在状态机上应用日志条目；这样的条目被称作已提交的日志。<br>
Raft保证已提交的条目都会被持久化并且最终将会在所有可用的状态机上被执行。<br>
一旦被创建的条目被大多数服务器所复制，leader就会将其提交(例如，图6中的条目7)。<br>
同时也会提交leader日志中更早之前的所有条目，其中包括被前任leader们所创建的条目。<br>
第5.4节讨论了在leader变更时应用这一规则的微妙之处，同时它也证明了所承诺的定义是安全的。<br>
leader持续的跟踪它已知的被提交日志的最大索引值，并且将索引值包含在未来的AppendEntries RPC中(包括心跳)，以便其它的服务器最终能知道(最大编号的已提交索引)。<br>
一旦一个follower知道一个日志条目已被提交，它便将这一条目应用于本地的状态机(基于日志的顺序)。<br>We designed the Raft log mechanism to maintain a high level of coherency between the logs on different servers.<br>
Not only does this simplify the system’s behavior and make it more predictable, but it is an important component of ensuring safety.<br>
Raft maintains the following properties, which together constitute the Log Matching Property in Figure 3:<br>
<br>If two entries in different logs have the same index and term, then they store the same command.
<br>If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.
<br>我们设计了Raft日志机制，其用于维持不同服务器之间日志的高度一致。<br>
其不仅仅简化了系统的行为，还使得它更加的可预测，同时这也是确保安全性的重要部分。<br>
Raft维护着以下特性，这些特性一并组成了图3中的日志匹配特性(Log Matching Property)：<br>
<br>如果不同日志中的两个条目有着相同的索引值和任期，则它们存储着相同的指令。
<br>如果不同日志中的两个条目有着相同的索引值和任期，则该日志之前的所有条目也都是完全相同的。
<br>The first property follows from the fact that a leader creates at most one entry with a given log index in a given term,<br>
and log entries never change their position in the log.<br>
The second property is guaranteed by a simple consistency check performed by AppendEntries.<br>
When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries.<br>
If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.<br>
The consistency check acts as an induction step: the initial empty state of the logs satisfies the Log Matching Property,<br>
and the consistency check preserves the Log Matching Property whenever logs are extended.<br>
As a result, whenever AppendEntries returns successfully,<br>
the leader knows that the follower’s log is identical to its own log up through the new entries.<br>第一个特性源自这样一个事实，即一个leader只会在特定任期内的某一索引值下最多只会创建一个条目，并且日志条目在日志中的位置是永远不会改变的。<br>
第二个特性则由AppendEntries执行一个简单的一致性检查来保证。<br>
当发送AppendEntries RPC时，leader将前一个条目的索引和任期包含在新条目中。<br>
如果follower没有找到一个具有相同索引值和任期的日志条目，则它将拒绝这一新条目。<br>
一致性检查就像一个归纳的步骤:初始化时的空状态满足日志匹配的特性(Log Matching Property)，并且每当扩展日志时，一致性检查都会维持日志匹配的特性。<br>
因此，每当AppendEntries返回成功时，通过新的条目leader就知道follower的日志与leader自己的是完全一致的，<br>During normal operation, the logs of the leader and followers stay consistent,<br>
so the AppendEntries consistency check never fails.<br>
However, leader crashes can leave the logs inconsistent (the old leader may not have fully replicated all of the entries in its log).<br>
These inconsistencies can compound over a series of leader and follower crashes.<br>
Figure 7 illustrates the ways in which followers’ logs may differ from that of a new leader.<br>
A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader, or both.<br>
Missing and extraneous entries in a log may span multiple terms.<br>在正常操作期间，leader和follower的日志始终保持一致，因此AppendEntries的一致性检查从来不会失败。<br>
然而，leader奔溃会导致日志的不一致(老的leader可能没有将它所有的日志条目完全复制完成)。<br>
这些不一致可能会随着一系列的leader和follower的崩溃而加剧。<br>
图7说明了follower日志可能与新leader不同的方式。<br>
一个Follower可能缺少了之前leader中才有的条目，也可能拥有一些在新leader中不存在的额外的条目，或者这两种方式皆有。<br>
缺失的或者额外多出的条目可能涉及到多个任期。<br><img alt="Pasted image 20240725173049.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173049.png"><br>Figure 7: When the leader at the top comes to power, it is possible that any of scenarios (a–f) could occur in follower logs.<br>
Each box represents one log entry; the number in the box is its term.<br>
A follower may be missing entries (a–b), may have extra uncommitted entries (c–d), or both (e–f).<br>
For example, scenario (f) could occur if that server was the leader for term 2, added several entries to its log,<br>
then crashed before committing any of them; it restarted quickly, became leader for term 3, and added a few more entries to its log;<br>
before any of the entries in either term 2 or term 3 were committed, the server crashed again and remained down for several terms.<br>图7：当leader获得最高权力上台时，以下任何一种情况(a-f)都可能出现在follower的日志中。<br>
每一个框表示一个日志条目；框中的数字是它的任期。<br>
follower可能会缺少一些条目(a-b)，可能有一些额外的未提交的条目(c-d),或者两种情况皆有(e-f)。<br>
例如，如果一个服务器是任期2的leader，其增加了一些条目到它们的日志中，然后在提交这些日志条目之前崩溃了;<br>
它很快重新启动，成为了任期3的leader，并且增加了几个条目到它的日志中，在提交任期2或者任期3中的任何一个条目之前，这个服务器再次崩溃并且在后几个任期内一直处于停机状态，<br>
则会发生情况(f);<br>In Raft, the leader handles inconsistencies by forcing the followers’ logs to duplicate its own.<br>
This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log.<br>
Section 5.4 will show that this is safe when coupled with one more restriction.<br>在Raft中，leader通过强制follower复制它的日志来处理不一致问题。<br>
这意味着follower中存在冲突的日志条目将会被来自leader的日志给覆盖。<br>
第5.4节将展示在加上一个限制时，这将会是安全的。<br>To bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree,<br>
delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point.<br>
All of these actions happen in response to the consistency check performed by AppendEntries RPCs.<br>
The leader maintains a nextIndex for each follower, which is the index of the next log entry the leader will send to that follower.<br>
When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log (11 in Figure 7).<br>
If a follower’s log is inconsistent with the leader’s,<br>
the AppendEntries consistency check will fail in the next AppendEntries RPC.<br>
After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC.<br>
Eventually nextIndex will reach a point where the leader and follower logs match.<br>
When this happens, AppendEntries will succeed,<br>
which removes any conflicting entries in the follower’s log and appends entries from the leader’s log (if any).<br>
Once AppendEntries succeeds, the follower’s log is consistent with the leader’s, and it will remain that way for the rest of the term.<br>为了使得follower的日志与自己的保持一致，leader必须找到两个日志中一致的条目中最新的那个，<br>
删除follower日志中位于该点位之后的所有条目，并且将leader在该点位后的所有条目发送给follower。<br>
所有的这些动作都发生在对AppendEntries RPC的一致性检查工作的响应中。<br>
leader为每一个follower维护了一个nextIndex,这是leader将发送给follower的下一个日志条目的索引编号。<br>
当leader第一次掌权时，其将所有的nextIndex的值初始化为其最后一个日志索引值再加1(图7中的11)。<br>
如果follower的日志与leader的不一致，AppendEntries的一致性检查将会在下一次AppendEntries RPC中失败。<br>
在一次拒绝后，leader将会递减nextIndex并且重试AppendEntries RPC。<br>
最终nextIndex将会到达一个leader与follower的日志想匹配的点位。<br>
当这一情况发生时，AppendEntries将会成功，其将删除follower日志中的所有冲突的条目并且追加来自leader日志中的条目(如果需要的话)。<br>
一旦AppendEntries成功，follower的日志将会与leader一致，并且在本任期内接下来的时间内保持一致。<br>If desired, the protocol can be optimized to reduce the number of rejected AppendEntries RPCs.<br>
For example, when rejecting an AppendEntries request,<br>
the follower can include the term of the conflicting entry and the first index it stores for that term.<br>
With this information, the leader can decrement nextIndex to bypass all of the conflicting entries in that term;<br>
one AppendEntries RPC will be required for each term with conflicting entries, rather than one RPC per entry.<br>
In practice, we doubt this optimization is necessary,<br>
since failures happen infrequently and it is unlikely that there will be many inconsistent entries.<br>如果有需要的话，协议可以通过减少被拒绝的AppendEntries RPCs数量来进行优化。<br>
例如，当一次AppendEntries请求被拒绝时，follower可以将包含对应任期的冲突条目和存储了对应任期的第一个索引值返回给leader。<br>
有了这些信息，leader递减nextIndex来避开对应任期内的所有冲突的条目;对于每一个任期的冲突条目，将只需要一次AppendEntries RPC，而不是一次RPC(处理)一个条目。<br>
在实践中，我们怀疑这一优化是否是必要的，因为很少发生故障并且不太可能有很多不一致的条目。<br>With this mechanism, a leader does not need to take any special actions to restore log consistency when it comes to power.<br>
It just begins normal operation, and the logs automatically converge in response to failures of the AppendEntries consistency check.<br>
A leader never overwrites or deletes entries in its own log (the Leader Append-Only Property in Figure 3).<br>有了这一机制，leader将不需要在掌权时使用任何特别的方法来恢复日志的一致性。<br>
它只是开始进行正常的操作，日志便会在响应AppendEntries的一致性检查时自动的趋于一致。<br>
leader从来不会覆盖或者删除它自己的日志(图3中leader的Append-Only特性)。<br>This log replication mechanism exhibits the desirable consensus properties described in Section 2:<br>
Raft can accept, replicate, and apply new log entries as long as a majority of the servers are up;<br>
in the normal case a new entry can be replicated with a single round of RPCs to a majority of the cluster;<br>
and a single slow follower will not impact performance.<br>这一日志复制机制展示了第2节中所描述的理想的一致性特性。<br>
只要大多数服务器是在线的，Raft便能接收，复制并且应用新的日志条目；<br>
在正常情况下一个新的条目可以通过单轮的RPC复制到集群中的大多数服务器上;并且单独的慢速的follower将不会影响性能。<br><br>The previous sections described how Raft elects leaders and replicates log entries.<br>
However, the mechanisms described so far are not quite sufficient to ensure<br>
that each state machine executes exactly the same commands in the same order.<br>
For example, a follower might be unavailable while the leader commits several log entries,<br>
then it could be elected leader and overwrite these entries with new ones;<br>
as a result, different state machines might execute different command sequences.<br>前面的章节描述了Raft是如何选举leader和复制日志条目的。<br>
然而，目前为止已描述的机制还不足以确保每一个状态机以相同的顺序准确地执行相同的指令。<br>
例如，当leader提交了几个日志条目后一个follower可能会变得不可用，随后follower可以被选举为leader并且用新的条目覆盖这些条目；<br>
因此，不同的状态机可能会执行不同的指令序列。<br>This section completes the Raft algorithm by adding a restriction on which servers may be elected leader.<br>
The restriction ensures that the leader for any given term contains all of the entries committed in previous terms<br>
(the Leader Completeness Property from Figure 3).<br>
Given the election restriction, we then make the rules for commitment more precise.<br>
Finally, we present a proof sketch for the Leader Completeness Property<br>
and show how it leads to correct behavior of the replicated state machine.<br>这一节通过增加一个对哪些服务器可以被选举为leader的限制来完善Raft算法。<br>
该限制确保leader对于给定的任期，其包含了所有之前任期的已提交条目（图3中的leader Completeness特性）。<br>
有了选举的限制，我们也使得关于提交的规则变得更加清晰。<br>
最后，我们给出了关于Leader Completeness的简要证明，并且展示了它是如何让复制状态机执行正确行为的。<br><br>In any leader-based consensus algorithm, the leader must eventually store all of the committed log entries.<br>
In some consensus algorithms, such as Viewstamped Replication [22],<br>
a leader can be elected even if it doesn’t initially contain all of the committed entries.<br>
These algorithms contain additional mechanisms to identify the missing entries and transmit them to the new leader,<br>
either during the election process or shortly afterwards.<br>
Unfortunately, this results in considerable additional mechanism and complexity.<br>
Raft uses a simpler approach where it guarantees<br>
that all the committed entries from previous terms are present on each new leader from the moment of its election,<br>
without the need to transfer those entries to the leader.<br>
This means that log entries only flow in one direction, from leaders to followers,<br>
and leaders never overwrite existing entries in their logs.<br>在任何基于leader的一致性算法中，leader必须最终存储所有已提交的日志条目。<br>
在一些一致性算法中，例如Viewstamped Replication，一个leader即使最初不包含所有已提交的条目也能被选举为leader。<br>
这些算法包含了额外的机制来识别缺失的条目并在选举过程中或选举后不久将其传输给新的leader。<br>
不幸的是，这带来了非常多的额外机制和复杂性。<br>
Raft使用了一种更简单的方法来确保每一个新的leader当选时都拥有之前任期的所有已提交的条目，而无需传输这些条目给leader。<br>
这意味着日志条目只会单方向的从leader向follower流动，并且leader从不覆盖它们已存在的条目。<br>Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries.<br>
A candidate must contact a majority of the cluster in order to be elected,<br>
which means that every committed entry must be present in at least one of those servers.<br>
If the candidate’s log is at least as up-to-date as any other log in that majority<br>
(where “up-to-date” is defined precisely below), then it will hold all the committed entries.<br>
The RequestVote RPC implements this restriction: the RPC includes information about the candidate’s log,<br>
and the voter denies its vote if its own log is more up-to-date than that of the candidate.<br>Raft使用投票机制来防止不包含所有已提交条目的candidate赢得选举。<br>
一个candidate必须与集群中的大多数成员联系后才能当选，这意味着每个提交的条目必须至少存在于其中的至少一个服务器中。<br>
如果candidate的日志至少和其它大多数的日志一样新(何为"最新"(up-to-date)将在下面被定义)，则它将持有所有已提交的条目。<br>
RequestVote RPC中实现了之一限制：RPC包括了candidate的日志信息，并且如果candidate的日志不如投票人(voter)的日志新，则voter将拒绝投票给该candidate。<br>Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs.<br>
If the logs have last entries with different terms, then the log with the later term is more up-to-date.<br>
If the logs end with the same term, then whichever log is longer is more up-to-date.<br>Raft通过比较两个日志中最后一个条目的索引和任期来决定谁是最新的。<br>
如果两个日志中最后的条目有着不同的任期，则任期较后的日志是更新的。<br>
如果两个日志中最后的条目有着相同的任期，则较长的(注：索引值更大的)那个日志是更新的。<br><br>As described in Section 5.3, a leader knows that an entry from its current term is committed once<br>
that entry is stored on a majority of the servers.<br>
If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry.<br>
However, a leader cannot immediately conclude that an entry from a previous term is committed once it is stored on a majority of servers.<br>
Figure 8 illustrates a situation where an old log entry is stored on a majority of servers,<br>
yet can still be overwritten by a future leader.<br>如5.3节所描述的那样，leader一旦知道当前任期内的一个条目被存储在了大多数的服务器中，就会将其提交。<br>
如果leader在提交一个条目前崩溃了，未来的leader将试图去完成该条目的复制。<br>
然而，leader无法立即得出结论，即一个来自之前任期的条目一旦被大多数服务器所存储就是已被提交的。<br>
图8展示了这样一种情况，一个老的日志条目被存储在了大多数的服务器上，但任然被未来的leader覆盖掉了。<br><img alt="Pasted image 20240725173101.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173101.png"><br>A time sequence showing why a leader cannot determine commitment using log entries from older terms.<br>
In(a) S1 is leader and partially replicates the log entry at index2.<br>
In (b) S1 crashes; S5 is elected leader for term 3 with votes from S3, S4, and itself, and accepts a different entry at log index 2.<br>
In (c) S5 crashes; S1 restarts, is elected leader, and continues replication.<br>
At this point, the log entry from term 2 has been replicated on a majority of the servers, but it is not committed.<br>
If S1 crashes as in (d), S5 could be elected leader (with votes from S2, S3, and S4) and overwrite the entry with its own entry from term 3.<br>
However, if S1 replicates an entry from its current term on a majority of the servers before crashing, as in (e),<br>
then this entry is committed (S5 cannot win an election).<br>
At this point all preceding entries in the log are committed as well.<br>一个时间序列，展示了为什么leader不能使用来自旧任期的日志条目来决定是否已提交。(注：S1-S5是集群中的5台服务器，a-e是时间序列)<br>
在(a)中S1是leader并且部分的复制了位于index2的日志条目。<br>
在(b)中S1崩溃了;S5通过任期3中来自S3，S4和它自己的投票而被选举为leader，并且接受了一个不同的条目在日志index2。<br>
在(c)中S5崩溃了;S1重新启动，被选举为了leader，并且继续复制。<br>
在这个时间点，来自任期2的日志条目已经被复制到了大多数服务器中，但还没有被提交。<br>
如果S1像(d)中那样崩溃了，S5可以被选举为leader(通过来自S2，S3,和S4的投票)并且用它自己的来自任期3的条目进行覆盖。<br>
然而，如果S1在崩溃前复制了来自它当前任期的条目在大多数服务器中，就像(e),则这一条目是已提交的(S5不能赢得选举)。<br>
此时日志中所有之前的条目都已经被提交。<br><img alt="Pasted image 20240725173109.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173109.png"><br>Figure 9: If S1 (leader for term T) commits a new log entry from its term, and S5 is elected leader for a later term U,<br>
then there must be at least one server (S3) that accepted the log entry and also voted for S5.<br>图9：如果S1(任期T的leader)提交了已给来自它任期的新日志条目，并且S5在后面的任期U被选举为leader,<br>
则至少有一个服务器(S3)能够接收该日志条目并且也投票给S5。<br>To eliminate problems like the one in Figure 8, Raft never commits log entries from previous terms by counting replicas.<br>
Only log entries from the leader’s current term are committed by counting replicas;<br>
once an entry from the current term has been committed in this way,<br>
then all prior entries are committed indirectly because of the Log Matching Property.<br>
There are some situations where a leader could safely conclude that an older log entry is committed<br>
(for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity.<br>为了消除像图8中那样的问题，Raft从来不基于副本数量来提交来自之前任期的日志条目。<br>
只有来自leader当前任期的日志条目才基于副本数量被提交，一旦一个来自当前任期的条目以这种方式被提交，则所有之前的条目都将由于Log Matching特性而间接的被提交。<br>
在一些情况下，leader可以安全的断定一个之前的log已经被提交(比如，如果一个entry已经被存储在每一个服务器上了)，但为了简单起见，Raft采取了一种更保守的方法。<br>Raft incurs this extra complexity in the commitment rules because log entries retain their original term numbers<br>
when a leader replicates entries from previous terms.<br>
In other consensus algorithms, if a new leader re-replicates entries from prior “terms,” it must do so with its new “term number.”<br>
Raft’s approach makes it easier to reason about log entries, since they maintain the same term number over time and across logs.<br>
In addition, new leaders in Raft send fewer log entries from previous terms<br>
than in other algorithms (other algorithms must send redundant log entries to renumber them before they can be committed).<br>Raft向提交规则中引入了额外的复杂性，因为当leader复制来自之前任期的条目时，这些日志条目会保留它原始的任期编号。<br>
在其它一致性算法中，如果一个新的leader要重新复制来自之前任期的条目，它必须使用新的任期编号。<br>
Raft的方法使得更容易理解日志条目，因为它们在不同服务器的日志中自始至终保留了相同的任期编号。<br>
额外的，相比其它算法，raft中的新leader会发送更少的来自之前任期的日志条目(其它算法必须发送冗余的日志条目以对让对应的日志条目在提交前重新进行编号)。<br><br>Given the complete Raft algorithm,<br>
we can now argue more precisely that the Leader Completeness Property holds (this argument is based on the safety proof; see Section 9.2).<br>
We assume that the Leader Completeness Property does not hold, then we prove a contradiction.<br>
Suppose the leader for term T (leaderT) commits a log entry from its term, but that log entry is not stored by the leader of some future term.<br>
Consider the smallest term U &gt; T whose leader (leaderU) does not store the entry.<br>在给出了完整的Raft算法后，我们可以更加准确的讨论leader的完整性(Completeness)特性是否成立了(这一讨论基于9.2节的安全性证明)。<br>
我们假设leader的Completeness特性不成立，则我们可以推到出矛盾来。<br>
假设任期T的leader(leaderT)提交了一个来自当前任期的日志条目，但该日志条目没有被未来某些任期的leader所存储。<br>
考虑一个大于T的最小任期U，其leader(leaderU)没有存储这个条目。<br>
<br>
The committed entry must have been absent from leaderU’s log at the time of its election (leaders never delete or overwrite entries).

<br>
leaderT replicated the entry on a majority of the cluster, and leaderU received votes from a majority of the cluster.<br>
Thus, at least one server (“the voter”) both accepted the entry from leaderT and voted for leaderU, as shown in Figure 9.<br>
The voter is key to reaching a contradiction.

<br>
The voter must have accepted the committed entry from leaderT before voting for leaderU;<br>
otherwise it would have rejected the AppendEntries request from leaderT (its current term would have been higher than T).

<br>
The voter still stored the entry when it voted for leaderU, since every intervening leader contained the entry (by assumption),<br>
leaders never remove entries, and followers only remove entries if they conflict with the leader.

<br>
The voter granted its vote to leaderU, so leaderU’s log must have been as up-to-date as the voter’s.<br>
This leads to one of two contradictions.

<br>
First, if the voter and leaderU shared the same last log term,<br>
then leaderU’s log must have been at least as long as the voter’s, so its log contained every entry in the voter’s log.<br>
This is a contradiction, since the voter contained the committed entry and leaderU was assumed not to.

<br>
Otherwise, leaderU’s last log term must have been larger than the voter’s.<br>
Moreover, it was larger than T, since the voter’s last log term was at least T (it contains the committed entry from term T).<br>
The earlier leader that created leaderU’s last log entry must have contained the committed entry in its log (by assumption).<br>
Then, by the Log Matching Property, leaderU’s log must also contain the committed entry, which is a contradiction.

<br>
This completes the contradiction. Thus, the leaders of all terms greater than T must contain all entries from term T<br>
that are committed in term T.

<br>
The Log Matching Property guarantees that future leaders will also contain entries that are committed indirectly,<br>
such as index 2 in Figure 8(d).

<br>
已提交的条目在leaderU当选时，必须不在leaderU的日志中(leader从来不会删除或者覆盖条目)。

<br>
leaderT将对应条目复制到了集群中的大多数(服务器)中,并且leaderU获得了来自集群中的大多的选票。<br>
因此，至少有一个服务器(作为voter)同时接收到了来自leaderT的条目并且投票给了leaderU.如图9所示。该voter是达成矛盾的关键所在。

<br>
voter必须在投票给leaderU之前接受来自leaderT的已提交的条目;<br>
否则其将拒绝来自leaderT的AppendEntries request(它当前的任期将已经高于T)。

<br>
voter在投票给leaderU时依然存储了该条目，因为每一个介于其中的leader(任期位于T和U之间)都包含了该条目，<br>
leader从不移除条目，并且follower只移除与leader相冲突的条目。

<br>
voter同意投票给leaderU,因此leaderU的日志必须至少与voter是一样新的。这带来了以下两个矛盾中的一个。

<br>
首先，如果voter和leaderU的最后一个日志有着相同的任期，则leaderU的日志必须至少与voter一样长，<br>
因此leaderU的日志包含了voter日志中的每一个条目。<br>
这是矛盾的，因为voter包含了已提交的条目而leaderU被假设为没有包含。

<br>
否则leaderU的最后一个日志的任期就必须比voter要大了。<br>
此外，任期的值也大于T，因为voter的最后一个日志的任期至少是T(其包含了来自任期T的所有已提交条目)。<br>
创建leaderU最后一个日志条目的更早的leader也必须包含这个日志(假设)。<br>
然后，基于Log Matching特性，leaderU的日志必须也包含已提交的条目，这是一个矛盾。

<br>
这就终结了矛盾。因此，所有任期大于T的leader必须包含所有的任期T内的已提交条目。

<br>
Log Matching特性保证了未来的leader也包含间接提交的日志，就像图8中的索引2。

<br>Given the Leader Completeness Property, we can prove the State Machine Safety Property from Figure 3,<br>
which states that if a server has applied a log entry at a given index to its state machine,<br>
no other server will ever apply a different log entry for the same index.<br>
At the time a server applies a log entry to its state machine,<br>
its log must be identical to the leader’s log up through that entry and the entry must be committed.<br>
Now consider the lowest term in which any server applies a given log index;<br>
the Log Completeness Property guarantees that the leaders for all higher terms will store that same log entry,<br>
so servers that apply the index in later terms will apply the same value.<br>
Thus, the State Machine Safety Property holds.<br>通过Leader Completeness特性，我们可以证明来自图3的State Machine Safety(安全状态机)特性，<br>
如果服务器将给定索引日志条目作用于状态机，其它的服务器将不能在相同的索引处应用不同的日志条目。<br>
一旦服务器应用了一个日志条目到其状态机上，其日志必须与传递该条目的leader的日志完全一样，并且这个条目必须被提交。<br>
现在考虑任一服务器应用给定日志索引的最小任期，Log Completeness特性保证了所有更高任期的leader将存储相同的日志条目，所以服务器在最晚任期所应用的索引将作用于相同的值。<br>
因此，State Machine Safety特性是成立的。<br>Finally, Raft requires servers to apply entries in log index order.<br>
Combined with the State Machine Safety Property,<br>
this means that all servers will apply exactly the same set of log entries to their state machines, in the same order.<br>最后，Raft要求服务器按照日志索引的顺序应用日志条目。<br>
结合State Machine Safety特性，这意味着所有的服务器将精确的以相同的顺序为它们的状态机应用一个相同的日志条目集合。<br><br>Until this point we have focused on leader failures.<br>
Follower and candidate crashes are much simpler to handle than leader crashes, and they are both handled in the same way.<br>
If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail.<br>
Raft handles these failures by retrying indefinitely; if the crashed server restarts, then the RPC will complete successfully.<br>
If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts.<br>
Raft RPCs are idempotent, so this causes no harm.<br>
For example, if a follower receives an AppendEntries request that includes log entries already present in its log,<br>
it ignores those entries in the new request.<br>在此之前我们一直聚焦于leader出故障的情况。<br>
follower和candidate的崩溃比起leader的崩溃会更加容易处理，并且它们都以相同的方式被处理。<br>
如果一个follower或者candidate崩溃了，则未来发送给它的投票请求(RequestVote)和AppendEntries RPC的发送将会失败。<br>
Raft通过无限的重试来处理这些失败，如果已崩溃的服务器重启了，则RPC将会成功的完成。<br>
如果服务器在完成了一个RPC但是在进行响应之前崩溃了，则它将会在重启后再一次接受到相同的RPC。<br>
Raft的RPC是幂等的，所以这不会有问题。<br>
例如，如果一个follower接受到的一个AppendEntries请求中包含的日志条目已经在它自己的日志中了，该follower就会在这次新的请求中忽略掉这些条目。<br><br>One of our requirements for Raft is that safety must not depend on timing:<br>
the system must not produce incorrect results just because some event happens more quickly or slowly than expected.<br>
However, availability (the ability of the system to respond to clients in a timely manner) must inevitably depend on timing.<br>
For example, if message exchanges take longer than the typical time between server crashes,<br>
candidates will not stay up long enough to win an election; without a steady leader, Raft cannot make progress.<br>我们对Raft的要求之一是安全性不得依赖时间：系统不能因为一些事件比所期望的更快或更慢发生而产生不正确的结果。<br>
然而，可用性(系统及时响应客户端的能力)一定不可避免的依赖于时间。<br>
例如，如果消息交换所花费的时间比服务器崩溃时所花费的时间还长，candidates将无法一直等待以赢得一场选举；没有一个稳定的leader，Raft就无法工作。<br>Leader election is the aspect of Raft where timing is most critical.<br>
Raft will be able to elect and maintain a steady leader as long as the system satisfies the following timing requirement:<br>
broadcastTime ≪ electionTimeout ≪ MTBF<br>leader选举是Raft关于时间的最关键的方面。<br>
只要系统能满足以下时间的需求，Raft将能够选出并且维持一个稳定的leader：<br>
广播时间(broadcastTime) ≪ 选举超时时间(electionTimeout) ≪ 平均故障间隔时间(MTBF: Mean Time between Failures)<br>In this inequality broadcastTime is the average time it takes a server to send RPCs in parallel to every server<br>
in the cluster and receive their responses;<br>
electionTimeout is the election timeout described in Section 5.2;<br>
and MTBF is the average time between failures for a single server.<br>
The broadcast time should be an order of magnitude less than the election timeout so<br>
that leaders can reliably send the heartbeat messages required to keep followers from starting elections;<br>
given the randomized approach used for election timeouts, this inequality also makes split votes unlikely.<br>
The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.<br>
When the leader crashes, the system will be unavailable for roughly the election timeout;<br>
we would like this to represent only a small fraction of overall time.<br>在这个不等式中，广播时间是服务器并行发送RPC给集群中的每一个服务器并且接受到它们的响应所花费的时间；<br>
选举超时时间是在5.2节中所描述的选举超时时间；同时MTBF是对于单一服务器在两次故障间隔的平均时间。<br>
广播时间应该比选举超时时间小一个数量级因此leader可以可靠的发送所需的心跳信息来阻止follower开始选举；<br>
考虑到用于选举超时的随机化方法，这个不等式也使得不太可能出现投票分裂。<br>
选举超时时间必须比MTBF低几个数量级才能使得系统能稳定的运行。<br>
当leader崩溃时，系统将有大致等于选举超时时间左右的不可用时间，我们希望这只占用整个(工作)时间的一小部分。<br>The broadcast time and MTBF are properties of the underlying system, while the election timeout is something we must choose.<br>
Raft’s RPCs typically require the recipient to persist information to stable storage,<br>
so the broadcast time may range from 0.5ms to 20ms, depending on storage technology.<br>
As a result, the election timeout is likely to be somewhere between 10ms and 500ms.<br>
Typical server MTBFs are several months or more, which easily satisfies the timing requirement.<br>广播时间和平均故障间隔时间是底层系统的特性，只有选举超时时间是我们必须选择的。<br>
Raft的RPC通常需要接收方将信息持久化到稳定的存储介质中，所以广播时间可能在0.5ms到20ms之间，这取决于存储技术。<br>
因此，选举时间可能在10ms到500ms之间。<br>
典型的服务器平均故障间隔时间是几个月或者更多，因此对这一时间(的要求)很容易满足。<br><img alt="Pasted image 20240725173120.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173120.png"><br>Figure 10: Switching directly from one configuration to another is unsafe because different servers will switch at different times.<br>
In this example, the cluster grows from three servers to five.<br>
Unfortunately, there is a point in time where two different leaders can be elected for the same term,<br>
one with a majority of the old configuration (Cold) and another with a majority of the new configuration (Cnew).<br>图10：直接将一种配置切换到另一种配置是不安全的因为不同的服务器将会在不同的时间点进行切换。<br>
在这个例子中，集群从3台服务器增长到5台。<br>
不幸的是，这个时间点将会在相同的任期内选举出两个不同的leader，其中之一获得了旧配置中的大多数(Cold)同时另一个获得了新配置中的大多数(Cnew)。<br><br>Up until now we have assumed that the cluster configuration (the set of servers participating in the consensus algorithm) is fixed.<br>
In practice, it will occasionally be necessary to change the configuration,<br>
for example to replace servers when they fail or to change the degree of replication.<br>
Although this can be done by taking the entire cluster off-line, updating configuration files,<br>
and then restarting the cluster, this would leave the cluster unavailable during the changeover.<br>
In addition, if there are any manual steps, they risk operator error.<br>
In order to avoid these issues, we decided to automate configuration changes and incorporate them into the Raft consensus algorithm.<br>到目前为止，我们已经假设集群的配置(参与一致性算法的服务器集合)是固定的。<br>
在实践中，偶尔的改变配置是必须的，例如在服务器发生故障时进行替换或者改变复制的程度。<br>
尽管这可以通过使整个集群离线，更新配置文件并且随后重启集群来实现，但这也使得集群在转换过程中变得不可用。<br>
另外，如果有任何的手工步骤，则有管理员操作失误的风险。<br>
为了避免这些问题，我们决定将配置的变更自动化并且将其纳入到Raft一致性算法中。<br>For the configuration change mechanism to be safe,<br>
there must be no point during the transition where it is possible for two leaders to be elected for the same term.<br>
Unfortunately, any approach where servers switch directly from the old configuration to the new configuration is unsafe.<br>
It isn’t possible to atomically switch all of the servers at once,<br>
so the cluster can potentially split into two independent majorities during the transition (see Figure 10).<br>为了使得配置变更的过程是安全的，在转换的过程中必须保证不能在同一个任期内选举出两个leader。<br>
不幸的是，任何将旧配置直接切换到新配置的方法都是不安全的。<br>
不可能原子性的一次性切换所有的服务器，因此服务器可能在转换期间被切分为两个独立的多数(如图10所示)。<br>In order to ensure safety, configuration changes must use a two-phase approach.<br>
There are a variety of ways to implement the two phases.<br>
For example, some systems(e.g., [22]) use the first phase to disable the old configuration so it cannot process client requests;<br>
then the second phase enables the new configuration.<br>
In Raft the cluster first switches to a transitional configuration we call joint consensus;<br>
once the joint consensus has been committed, the system then transitions to the new configuration.<br>
The joint consensus combines both the old and new configurations:<br>
<br>Log entries are replicated to all servers in both configurations.
<br>Any server from either configuration may serve as leader
<br>Agreement (for elections and entry commitment) requires separate majorities from both the old and new configurations.
<br><img alt="Pasted image 20240725173130.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173130.png"><br>Figure 11: Timeline for a configuration change.<br>
Dashed lines show configuration entries that have been created but not committed, and solid lines show the latest committed configuration entry.<br>
The leader first creates the Cold,new configuration entry in its log and commits it to Cold,new<br>
(a majority of Cold and a majority of Cnew).<br>
Then it creates the Cnew entry and commits it to a majority of Cnew.<br>
There is no point in time in which Cold and Cnew can both make decisions independently.<br>图11：配置变更的时间线。<br>
虚线标识配置条目已经被创建但还未被提交，而实现标识最新的已提交的配置条目。<br>
leader首先在它的日志中创建Cold,new的配置条目并且向Cold,new(Cold中的大多数以及Cnew中的大多数)提交这一日志。<br>
然后它创建Cnew条目并且向Cnew中的大多数提交这一条目。<br>
没有任何一个时间点可以让Cold和Cnew都能同时独立的做出决定。<br>为了确保安全，配置的变更必须使用一种两阶段的方法。<br>
有很多方法可以实现两阶段。<br>
例如，一些系统通过在一阶段禁用旧的配置因此其无法处理客户端请求，然后二阶段则启用新的配置。<br>
在Raft的集群首先切换到我们成为联合一致(joint consensus)的过渡配置;一旦联合一致已被提交，系统便过度到新的配置。<br>
联合一致结合了旧的和新的配置：<br>
<br>日志条目都会被复制到在这两种配置中所有的服务器上。
<br>新、旧配置中的任一服务器都可以作为leader。
<br>(对于选举和条目提交)达成一致需要在新的和旧的配置中分别获得大多数服务器的同意。
<br>The joint consensus allows individual servers to transition between configurations at different times without compromising safety.<br>
Furthermore, joint consensus allows the cluster to continue servicing client requests throughout the configuration change.<br>联合一致允许单独的服务器在不同的时间内转换配合而不会在安全性上有所妥协。<br>
此外，联合一致允许集群在配置变更的过程中持续的为客户端的请求提供服务。<br>Cluster configurations are stored and communicated using special entries in the replicated log;<br>
Figure 11 illustrates the configuration change process.<br>
When the leader receives a request to change the configuration from Cold to Cnew,<br>
it stores the configuration for joint consensus(Cold,new in the figure) as a log entry and replicates that<br>
entry using the mechanisms described previously.<br>
Once a given server adds the new configuration entry to its log, it uses that configuration for all future decisions<br>
(a server always uses the latest configuration in its log, regardless of whether the entry is committed).<br>
This means that the leader will use the rules of Cold,new to determine when the log entry for Cold,new is committed.<br>
If the leader crashes, a new leader may be chosen under either Cold or Cold,new,<br>
depending on whether the winning candidate has received Cold,new.<br>
In any case, Cnew cannot make unilateral decisions during this period.<br>集群配置通过复制日志中特殊的条目进行存储和通信；图11展示了配置变更的过程。<br>
当leader接受到令配置从Cold(旧配置)到Cnew(新配置)的请求时，<br>
它为了联合一致以一个日志条目的形式存储这个配置(图中的Cold,new)并且使用之前所描述的机制复制这个条目。<br>
一旦给定的服务器将新的配置条目加入了它的日志，它将使用这些配置来指定未来所有的决定(一个服务器总是使用它日志中最后的配置，无论该条目是否是已提交的)。<br>
这意味着leader将使用规则Cold,new来决定何时提交关于Cold,new的日志条目。<br>
如果leader崩溃了，新的leader可能是在Cold或者是Cold,new下选择出来的，这取决于获胜的candidate是否已经收到了Cold,new。<br>
无论如何，Cnew都不能在这个阶段单独的做出决定。<br>Once Cold,new has been committed, neither Cold nor Cnew can make decisions without approval of the other,<br>
and the Leader Completeness Property ensures that only servers with the Cold,new log entry can be elected as leader.<br>
It is now safe for the leader to create a log entry describing Cnew and replicate it to the cluster.<br>
Again, this configuration will take effect on each server as soon as it is seen.<br>
When the new configuration has been committed under the rules of Cnew,<br>
the old configuration is irrelevant and servers not in the new configuration can be shut down.<br>
As shown in Figure 11, there is no time when Cold and Cnew can both make unilateral decisions; this guarantees safety.<br>一旦Cold,new已经提交，Cold或者Cnew都不能在没有另一方同意的情况下做出决定，<br>
并且Leader Completeness特性确保只有拥有Cold,new日志条目的服务器才能被选举为leader。<br>
现在leader可以安全的创建一个描述了Cnew的日志条目并将其在集群中进行复制。<br>
同样的，该配置将在每一个服务器看到其后立即生效。<br>
当新的配置在Cnew的规则下被提交，旧的配置将变得无关紧要并且没有在新配置中的服务器将可以被关闭。<br>
如图11所示，Cold和Cnew不能同时做出单独的决定；这保证了安全性。<br>There are three more issues to address for reconfiguration.<br>
The first issue is that new servers may not initially store any log entries.<br>
If they are added to the cluster in this state, it could take quite a while for them to catch up,<br>
during which time it might not be possible to commit new log entries.<br>
In order to avoid availability gaps, Raft introduces an additional phase before the configuration change,<br>
in which the new servers join the cluster as non-voting members<br>
(the leader replicates log entries to them, but they are not considered for majorities).<br>
Once the new servers have caught up with the rest of the cluster, the reconfiguration can proceed as described above.<br>关于配置变更还存在三个问题需要解决。<br>
第一个问题是，新的服务器可能在初始化时没有存储任何的日志条目。<br>
如果在这种状态下被加入到集群，它可能需要花费很长一段时间才能赶上，在这段时间内都无法提交新的日志条目。<br>
为了避免可用性的差距，Raft在配置变更前引入了一个额外的阶段，新的服务器以无投票权成员(non-voting members)的身份加入集群<br>
(leader复制日志条目给它们，但它们不被认为是大多数的一份子)。<br>
一旦新的服务器能够追上集群中的其它机器，就可以向上述那般执行配置变更。<br>The second issue is that the cluster leader may not be part of the new configuration.<br>
In this case, the leader steps down (returns to follower state) once it has committed the Cnew log entry.<br>
This means that there will be a period of time (while it is committing Cnew)<br>
when the leader is managing a cluster that does not include itself; it replicates log entries but does not count itself in majorities.<br>
The leader transition occurs when Cnew is committed<br>
because this is the first point when the new configuration can operate independently (it will always be possible to choose a leader from Cnew).<br>
Before this point, it may be the case that only a server from Cold can be elected leader.<br>第二个问题是，集群的leader可能不是新配置中的一员。<br>
在这种情况下，一旦Cnew日志条目被提交，leader将会退下(返回到follower状态)。<br>
这意味着存在一段时间(在提交Cnew时)，其中leader管理者一个不包含自己的集群；它复制着日志条目但不把它自己算作大多数中的一员。<br>
当Cnew被提交时将会发生leader的切换，因为这是新配置可以进行独立操作的第一个点位(总是可以在Cnew中选择出一个leader)。<br>
在此之前，只有来自Cold的服务器才有可能被选举为leader。<br>The third issue is that removed servers (those not in Cnew) can disrupt the cluster.<br>
These servers will not receive heartbeats, so they will time out and start new elections.<br>
They will then send RequestVote RPCs with new term numbers, and this will cause the current leader to revert to follower state.<br>
A new leader will eventually be elected, but the removed servers will time out again and the process will repeat,<br>
resulting in poor availability.<br>第三个问题是被移除的服务器(不在Cnew中)可能会中断集群。<br>
这些服务器将不再接收到心跳，所以它们将会超时而启动新的选举。<br>
然后它们将发送有着新任期编号的RequestVote RPC，并且这将导致当前的leader恢复为follower状态。<br>
最终将会有一名新的leader被选举出来，但是被移除的服务器将会再次超时并且重复这一过程，这将导致系统有着较差的可用性。<br>To prevent this problem, servers disregard RequestVote RPCs when they believe a current leader exists.<br>
Specifically, if a server receives a RequestVote RPC within the minimum election timeout of hearing from a current leader,<br>
it does not update its term or grant its vote.<br>
This does not affect normal elections, where each server waits at least a minimum election timeout before starting an election.<br>
However, it helps avoid disruptions from removed servers:<br>
if a leader is able to get heartbeats to its cluster, then it will not be deposed by larger term numbers.<br>为了避免这一问题，服务器将会在它们认为当前leader存在时忽略掉RequestVote RPC。<br>
特别的，如果一个服务器在当前leader最小的选举超时时间内接收到一个RequestVote RPC，它将不会更新它的任期或者发起投票。<br>
这不会影响正常的选举，即每一个服务器在开始一轮选举之前至少等待一个最小的选举超时时间。<br>
然而，它有助于避免移除服务器时的混乱：如果一个leader能够提供集群中的心跳，则它将不会被一个更大的任期编号给取代。<br><br>Raft’s log grows during normal operation to incorporate more client requests, but in a practical system, it cannot grow without bound.<br>
As the log grows longer, it occupies more space and takes more time to replay.<br>
This will eventually cause availability problems without some mechanism to discard obsolete information that has accumulated in the log.<br>Raft的日志在正常操作期间不断增长以满足更多的客户端请求，但是在实际的系统中，日志不能不加限制的增长。<br>
随着日志不断变长，它将占用更多的空间并且花费更长的事件来进行回放。<br>
如果没有一些机制来剔除日志中所累积的过时的信息，这终将造成可用性问题。<br>Snapshotting is the simplest approach to compaction.<br>
In snapshotting, the entire current system state is written to a snapshot on stable storage,<br>
then the entire log up to that point is discarded.<br>
Snapshotting is used in Chubby and ZooKeeper, and the remainder of this section describes snapshotting in Raft.<br>快照是最简单的压缩方法。<br>
在快照中，完整的当前系统状态以快照的形式写入稳定的存储中，然后在这个点位之前的整个日志会被丢弃。<br>
快照被用于Chubby和ZooKeeper中，本届的剩余部分将用于描述Raft中的快照。<br><img alt="Pasted image 20240725173140.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173140.png"><br>Figure 12: A server replaces the committed entries in its log(indexes 1 through 5) with a new snapshot,<br>
which stores just the current state (variables x and y in this example).<br>
The snapshot’s last included index and term serve to position the snapshot in the log preceding entry 6.<br>图12：服务器用新的快照代替其日志中已提交的条目(索引1到5)，该快照只存储了当前的状态(本例中的变量x和y)。<br>
快照中的last included index和term用于定位快照中的条目6之前的日志。<br>Incremental approaches to compaction, such as log cleaning [36] and log-structured merge trees [30, 5], are also possible.<br>
These operate on a fraction of the data at once, so they spread the load of compaction more evenly over time.<br>
They first select a region of data that has accumulated many deleted and overwritten objects,<br>
then they rewrite the live objects from that region more compactly and free the region.<br>
This requires significant additional mechanism and complexity compared to snapshotting,<br>
which simplifies the problem by always operating on the entire data set.<br>
While log cleaning would require modifications to Raft, state machines can implement LSM trees using the same interface as snapshotting.<br>基于增量的压缩方法，例如日志清理和日志结构合并树(LSM tree)也是可行的。<br>
这些操作一次只操作少量的数据，因此它们能随着时间的退役均摊负载。<br>
它们首先选择一片数据区域，其已经积累了很多的被删除和覆盖的对象，然后它们以更加紧凑的方式重写来自这一片区域的存活对象(live objects)并释放这一区域。<br>
与快照压缩相比这显著的引入了额外的机制和复杂度，快照通过始终操作整个数据集合来简化这一问题。<br>
虽然日志清理需要对Raft进行修改，但状态机可以使用与快照相同的接口来实现LSM树。<br>Figure 12 shows the basic idea of snapshotting in Raft.<br>
Each server takes snapshots independently, covering just the committed entries in its log.<br>
Most of the work consists of the state machine writing its current state to the snapshot.<br>
Raft also includes a small amount of metadata in the snapshot:<br>
the last included index is the index of the last entry in the log that the snapshot replaces<br>
(the last entry the state machine had applied), and the last included term is the term of this entry.<br>
These are preserved to support the AppendEntries consistency check for the first log entry following the snapshot,<br>
since that entry needs a previous log index and term.<br>
To enable cluster membership changes (Section 6), the snapshot also includes the latest configuration in the log as of last included index.<br>
Once a server completes writing a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.<br>图12展示了Raft中关于快照的基础思想。<br>
每一个服务器都独立的获得快照，只覆盖它已提交的日志条目。<br>
大部分的工作主要由状态机以快照形式写入它的当前状态组成。<br>
Raft还将少量的元数据包括在了快照中：<br>
last included index是快照代替的日志中的最后一个条目的索引值(状态机已应用的最后一个条目)，并且last included term是这个条目的任期值。<br>
保留这些条目是为了支持快照后面第一个条目的AppendEntries一致性检查，因为这个条目需要前一个日志的索引值和任期值。<br>
要启用集群变更(第6节)，快照还要包括含有last included index日志的最后配置。<br>
一旦一个服务器完成了一个快照的写入，它可能会删除包含last included index之前的所有日志条目，以及之前的任何快照。<br>Although servers normally take snapshots independently, the leader must occasionally send snapshots to followers that lag behind.<br>
This happens when the leader has already discarded the next log entry that it needs to send to a follower.<br>
Fortunately, this situation is unlikely in normal operation: a follower that has kept up with the leader would already have this entry.<br>
However, an exceptionally slow follower or a new server joining the cluster(Section 6) would not.<br>
The way to bring such a follower up-to-date is for the leader to send it a snapshot over the network.<br><img alt="Pasted image 20240725173149.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173149.png"><br>Figure 13: A summary of the InstallSnapshot RPC.<br>
Snapshots are split into chunks for transmission; this gives the follower a sign of life with each chunk, so it can reset its election timer.<br>图13：InstallSnapshot RPC的快照。<br>
快照被分割为块进行传输；每一块都带给了follower其存活的标识，因此follower可以重置其选举计时器。<br>尽管服务器通常独立的生成快照，但leader必须偶尔的向落后的follower发送快照。<br>
当leader已经丢弃了需要发送给follower的下一个日志条目时就会发生这种情况。<br>
幸运的是，这种情况不太可能在正常操作中出现：一个跟上了leader的follower已经有了这个条目了。<br>
然而，一个异常慢的follower或者一个新加入集群的服务器(第6节)将没有这个条目。<br>
让这样的一个follower的日志和leader一样新的方法就是通过网络向它发送一个快照。<br>The leader uses a new RPC called InstallSnapshot to send snapshots to followers that are too far behind; see Figure 13.<br>
When a follower receives a snapshot with this RPC, it must decide what to do with its existing log entries.<br>
Usually the snapshot will contain new information not already in the recipient’s log.<br>
In this case, the follower discards its entire log; it is all superseded by the snapshot<br>
and may possibly have uncommitted entries that conflict with the snapshot.<br>
If instead the follower receives a snapshot that describes a prefix of its log (due to retransmission or by mistake),<br>
then log entries covered by the snapshot are deleted but entries following the snapshot are still valid and must be retained.<br>leader使用一种新的被成为InstallSnapshot的RPC向落后太多的follower发送快照;如图13所示。<br>
当一个follower使用这个RPC接受到一个快照时，它必须决定如何处理它目前已存在的日志条目。<br>
通常，这个快照将包含目前还不在接受者日志中的新信息。<br>
这种情况下，follower将丢弃它全部的日志;其全部被快照所取代，并且被丢弃的日志中可能有着与快照相冲突的但还未提交的条目。<br>
相反，如果follower接受到的快照是它当前日志的前面一部分(由于重传或者出错了)，则被快照所覆盖的日志条目将会被删除但是快照后面的条目依然是有效的并且必须被保留。<br>This snapshotting approach departs from Raft’s strong leader principle, since followers can take snapshots without the knowledge of the leader.<br>
However, we think this departure is justified.<br>
While having a leader helps avoid conflicting decisions in reaching consensus,<br>
consensus has already been reached when snapshotting, so no decisions conflict.<br>
Data still only flows from leaders to followers, just followers can now reorganize their data.<br>这种快照的方式背离了Raft的强leader原则，因为follower可以在leader不知情的情况下生成快照。<br>
然而，我们认为这种背离是值得的。<br>
虽然由一个leader有助于避免在达成一致时产生决策冲突，但生成快照时是已经达成了一致的，所以不会有决策冲突。<br>
数据依然是仅由leader流向follower，但follower现在可以重新组织它们的数据。<br>We considered an alternative leader-based approach in which only the leader would create a snapshot,<br>
then it would send this snapshot to each of its followers.<br>
However, this has two disadvantages.<br>
First, sending the snapshot to each follower would waste network bandwidth and slow the snapshotting process.<br>
Each follower already has the information needed to produce its own snapshots,<br>
and it is typically much cheaper for a server to produce a snapshot from its local state than it is to send and receive one over the network.<br>
Second, the leader’s implementation would be more complex.<br>
For example, the leader would need to send snapshots to followers in parallel with replicating new log entries to them,<br>
so as not to block new client requests.<br>我们考虑过另一种基于leader的方法，其只有leader可以创建快照，然后leader将发送快照给每一个follower。<br>
然而，这样做有两个缺点。<br>
首先，发送快照给每一个follower将浪费网络带宽并且减慢快照的处理。<br>
每一个follower已经有了生成它们自己快照所需要的信息，并且通常基于服务器本地状态来生成快照要比它们通过从网络发送和接收快照的开销要更低。<br>
其次，leader也会被实现的更加复杂。<br>
比如，leader将需要并行的发送快照给follower的同时还要令它们复制新的日志条目，以避免阻塞新的客户端请求。<br>There are two more issues that impact snapshotting performance.<br>
First, servers must decide when to snapshot.<br>
If a server snapshots too often, it wastes disk bandwidth and energy; if it snapshots too infrequently,<br>
it risks exhausting its storage capacity, and it increases the time required to replay the log during restarts.<br>
One simple strategy is to take a snapshot when the log reaches a fixed size in bytes.<br>
If this size is set to be significantly larger than the expected size of a snapshot,<br>
then the disk bandwidth overhead for snapshotting will be small.<br>还有两个问题会影响快照的性能。<br>
首先，服务器必须决定何时生成快照。<br>
如果服务器生成快照太频繁，则将浪费磁盘带宽和能源；如果生成快照太不频繁，则存在耗尽磁盘空间的风险，并且增加重启时回放日志所需的时间。<br>
一种简单的策略时当日志到达一个固定的字节数时生成一个快照。<br>
如果这个大小设置为一个明显大于快照预期大小的值，则用于快照生成的磁盘带宽开销将会很小。<br>The second performance issue is that writing a snapshot can take a significant amount of time,<br>
and we do not want this to delay normal operations.<br>
The solution is to use copy-on-write techniques so that new updates can be accepted without impacting the snapshot being written.<br>
For example, state machines built with functional data structures naturally support this.<br>
Alternatively, the operating system’s copy-on-write support (e.g., fork on Linux)<br>
can be used to create an in-memory snapshot of the entire state machine (our implementation uses this approach).<br>第二个问题是写入一个快照会花费非常多的时间，并且我们不希望这会延迟正常操作。<br>
解决的方案是使用写时复制(copy-on-write)技术,以便可以在不影响快照的写入的同时接受新的更新。<br>
例如，使用函数式数据结构(functional data structures)构建的状态机能自然的支持这一点。<br>
或者，操作系统的写时复制支持(例如，linux中的fork)可以被用于创建整个状态机的内存快照(我们的实现使用了这个方法)。<br><br>This section describes how clients interact with Raft,<br>
including how clients find the cluster leader and how Raft supports linearizable semantics [10].<br>
These issues apply to all consensus-based systems, and Raft’s solutions are similar to other systems.<br>本节描述了客户端如何与Raft交互，包括客户端如何找到集群leader以及Raft是如何支持线性化语义的。<br>
这些问题适用于所有的基于一致性的系统，同时Raft的解决方案也与其它系统是类似的。<br>Clients of Raft send all of their requests to the leader.<br>
When a client first starts up, it connects to a randomly-chosen server.<br>
If the client’s first choice is not the leader,<br>
that server will reject the client’s request and supply information about the most recent leader it has heard from<br>
(AppendEntries requests include the network address of the leader).<br>
If the leader crashes, client requests will time out; clients then try again with randomly-chosen servers.<br>Raft的客户端将它们的所有请求发送给leader。<br>
当客户端第一次启动时，它会随机选择一台服务器并进行连接。<br>
如果客户端第一次选择的不是leader，则服务器将会拒绝客户端的请求并且提供关于它听到的最近的leader的信息(AppendEntries的请求中包括了leader的网络地址)。<br>
如果leader崩溃了，客户端的请求将会超时;客户端则会再一次随机选择一台服务器。<br>Our goal for Raft is to implement linearizable semantics (each operation appears to execute instantaneously,<br>
exactly once, at some point between its invocation and its response).<br>
However, as described so far Raft can execute a command multiple times:<br>
for example, if the leader crashes after committing the log entry but before responding to the client,<br>
the client will retry the command with a new leader, causing it to be executed a second time.<br>
The solution is for clients to assign unique serial numbers to every command.<br>
Then, the state machine tracks the latest serial number processed for each client, along with the associated response.<br>
If it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.<br>我们对于Raft的目标是实现可线性化的语义(每一个操作会立即执行，执行且只执行一次，执行的时机位于请求和响应之间)。<br>
然而，如上所述Raft可以执行执行一条指令多次：例如，如果leader在提交日志条目后但响应客户端之前崩溃了，客户端将会与新的leader重试这条指令，使得该指令被执行了两次。<br>
解决方案是让客户端为每一个指令分配一个唯一的序列号。<br>
然后，状态机追踪为每一个客户端处理的最后的序列号，以及相关的响应。<br>
如果它接受到了一个指令其序列号是已经被执行了的，它将立即返回而不会重新执行该请求。<br>Read-only operations can be handled without writing anything into the log.<br>
However, with no additional measures, this would run the risk of returning stale data,<br>
since the leader responding to the request might have been superseded by a newer leader of which it is unaware.<br>
Linearizable reads must not return stale data, and Raft needs two extra precautions to guarantee this without using the log.<br>
First, a leader must have the latest information on which entries are committed.<br>
The Leader Completeness Property guarantees that a leader has all committed entries,<br>
but at the start of its term, it may not know which those are.<br>
To find out, it needs to commit an entry from its term.<br>
Raft handles this by having each leader commit a blank no-op entry into the log at the start of its term.<br>
Second, a leader must check whether it has been deposed before processing a read-only request<br>
(its information may be stale if a more recent leader has been elected).<br>
Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests.<br>
Alternatively, the leader could rely on the heartbeat mechanism to provide a form of lease [9],<br>
but this would rely on timing for safety (it assumes bounded clock skew).<br>只读操作可以直接被处理而不需要向日志写入任何东西。<br>
然而，如果没有额外的机制，将会有返回过时数据的风险，因为响应请求的leader可能已经被一个新的leader取代了但它自己却没感知到。<br>
线性化的读必须不返回过时数据，并且Raft需要两个额外的预防措施在不使用日志的前提下保证这一点。<br>
首先，leader必须掌握已提交日志条目的最新信息。<br>
leader完整性属性保证了leader有着所有已提交的条目，但在它任期的开始时，它不知道哪些是已提交的条目。<br>
为了找到哪些是已提交的条目，它需要提交一个来自它自己任期的条目。<br>
Raft通过在leader开始其任期时，让每一个leader提交一个空白的no-op条目来处理这一问题。<br>
其次，leader在处理只读请求时必须检查它是否已经被罢黜退位了(如果最新的leader已经被选出，则它的信息可能已经过时了)。<br>
Raft通过让leader在响应只读请求之前与集群中的大多数交换心跳信息来解决这一问题。<br>
或者，leader可以依赖心跳机制来提供一种租约的形式，但这将会依赖于时钟的安全性(假设时间误差是有限的)。<br><br>We have implemented Raft as part of a replicated state machine that stores configuration information for RAMCloud [33]<br>
and assists in failover of the RAMCloud coordinator.<br>
The Raft implementation contains roughly 2000 lines of C++ code, not including tests, comments, or blank lines.<br>
The source code is freely available [23].<br>
There are also about 25 independent third-party open source implementations [34] of Raft in various stages of development,<br>
based on drafts of this paper.<br>
Also, various companies are deploying Raft-based systems [34].<br>我们已经将Raft实现为复制状态机的一部分，其存储RAMCloud的配置信息并且协助RAMCloud协调者进行故障恢复。<br>
Raft的实现包含了大概2000行的C++代码，不包括测试，备注或者空行。<br>
源代码是免费提供的。<br>
基于本论文的草稿，有大约25个独立的、处于不同开发阶段的Raft三方开源实现。<br>
此外，很多公司也部署了基于Raft的系统。<br>The remainder of this section evaluates Raft using three criteria: understandability, correctness, and performance.<br>本节的剩余部分用于在三个方面评估Raft: 可理解性，正确性和性能。<br><br>To measure Raft’s understandability relative to Paxos,<br>
we conducted an experimental study using upper-level undergraduate and graduate students in<br>
an Advanced Operating Systems course at Stanford University and a Distributed Computing course at U.C. Berkeley.<br>
We recorded a video lecture of Raft and another of Paxos, and created corresponding quizzes.<br>
The Raft lecture covered the content of this paper except for log compaction;<br>
the Paxos lecture covered enough material to create an equivalent replicated state machine, including single-decree Paxos,<br>
multi-decree Paxos, reconfiguration, and a few optimizations needed in practice (such as leader election).<br>
The quizzes tested basic understanding of the algorithms and also required students to reason about corner cases.<br>
Each student watched one video, took the corresponding quiz, watched the second video, and took the second quiz.<br>
About half of the participants did the Paxos portion first<br>
and the other half did the Raft portion first in order to account for both individual differences in performance<br>
and experience gained from the first portion of the study.<br>
We compared participants’ scores on each quiz to determine whether participants showed a better understanding of Raft.<br><img alt="Pasted image 20240725173202.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173202.png"><br>Figure 14: A scatter plot comparing 43 participants’ performance on the Raft and Paxos quizzes.<br>
Points above the diagonal (33) represent participants who scored higher for Raft.<br>图14：比较43名参与者在Raft和Paxos测验中表现的散点图。<br>
位于对角线之上的(33个)参与者是Raft分数更高的。<br>为了测量Raft相对于Paxos的可理解性，我们对来自斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式系统课程的高水平本科生和研究生组织了一场学习实验。<br>
我们录制了Raft和Paxos的视频讲座，并且制作了相对应的测验。<br>
Raft的讲座覆盖了本文除日志压缩以外的内容，Paxos的讲座覆盖了相当于创建一个等效的复制状态机的足够多的材料，包括single-decree Paxos，multi-decree Paxos，<br>
刷新配置，以及实践中所需要的一小部分优化(例如leader选举)。<br>
测验测试了学生对算法的基础理解同时也需要学生能推理出极端的case。<br>
每个学生观看第一个视频，然后做相应的测验，再看第二个视频，然后再做第二个视频对应的测验。<br>
为解释本实验第一次学习时获得的经验和表现上的差异，大约一般的实验者先做Paxos那部分的而另一半实验者则先做Raft的那部分。<br>
我们比较了实验者在每一次测验中的分数来确定实现者是否展现出了对Raft有着更好的裂解。<br>We tried to make the comparison between Paxos and Raft as fair as possible.<br>
The experiment favored Paxos in two ways: 15 of the 43 participants reported having some prior experience with Paxos,<br>
and the Paxos video is 14% longer than the Raft video.<br>
As summarized in Table 1, we have taken steps to mitigate potential sources of bias.<br>
All of our materials are available for review [28, 31].<br>我们尝试着使得Paxos和Raft之间的比较尽可能的公平。<br>
该实验在两方面有利于Paxos：43名实验者中的15名报告说曾经有着一些关于Paxos的经验，同时Paxos的视频比Raft的视频要长14%。<br>
如表1所示，我们已经采取措施来减少潜在的来源偏差。<br>
我们所有的材料都是可以审查的。<br>On average, participants scored 4.9 points higher on the Raft quiz than on the Paxos quiz<br>
(out of a possible 60 points, the mean Raft score was 25.7 and the mean Paxos score was 20.8);<br>
Figure 14 shows their individual scores.<br>
A paired t-test states that, with 95% confidence,<br>
the true distribution of Raft scores has a mean at least 2.5 points larger than the true distribution of Paxos scores.<br>平均而言，参与者在Raft测验中的得分要比Paxos的测验中的得分要高4.9分(换算成60分制，意味着Raft的测验分数为25.7同时Paxos的测验分数为20.8)<br>
图14展示了它们各自的分数。<br>
配队t-test表名，有95%的置信度下，Raft的真实分数分布比Paxos的真实分数分布至少要高2.5分。<br>We also created a linear regression model that predicts a new student’s quiz scores based on three factors:<br>
which quiz they took, their degree of prior Paxos experience, and the order in which they learned the algorithms.<br>
The model predicts that the choice of quiz produces a 12.5-point difference in favor of Raft.<br>
This is significantly higher than the observed difference of 4.9 points, because many of the actual students had prior Paxos experience,<br>
which helped Paxos considerably, whereas it helped Raft slightly less.<br>
Curiously, the model also predicts scores 6.3 points lower on Raft for people that have already taken the Paxos quiz;<br>
although we don’t know why, this does appear to be statistically significant.<br><img alt="Pasted image 20240725173212.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173212.png"><br>Figure 15: Using a 5-point scale, participants were asked(left) which algorithm they felt<br>
would be easier to implement in a functioning, correct, and efficient system,<br>
and (right) which would be easier to explain to a CS graduate student.<br>图15：使用5分支，参与者被问到(左侧)他们感觉使用哪种算法更容易去实现一个正常工作的，正确的，和有效的系统，<br>
同时(右侧)是哪种算法对于计算机科学(CS)的研究生来说会更容易解释。<br>我们还创建了一个线性回归模型，其用于预测新生基于三个要素的测验成绩：分别是它们参加的测验，它们之前关于Paxos的经验，以及它们学习算法的顺序。<br>
这个模型预测选择的测验中Raft要比Paxos高12.5分。<br>
这明显高于观察到的4.9分的差异，因为实际上很多学生之前有过Paxos的经验，这有助于对Paxos的理解，而对于Raft的帮助则少很多。<br>
奇怪的是，模型还预测已经参加过Paxos测验的人在Raft的实验上将会低6.3分；即使我们不知道为什么，但这似乎具有统计学的意义。<br>We also surveyed participants after their quizzes to see which algorithm they felt would be easier to implement or explain;<br>
these results are shown in Figure 15.<br>
An overwhelming majority of participants reported Raft would be easier to implement and explain (33 of 41 for each question).<br>
However, these self-reported feelings may be less reliable than participants’ quiz scores,<br>
and participants may have been biased by knowledge of our hypothesis that Raft is easier to understand.<br>我们还在参与者测验后对其进行了调查，询问它们感觉哪种算法更加容易实现或解释；结果如图15所示。<br>
绝大多数参与者表示Raft要更加容易实现和解释(41个被提问者中的33个)<br>
然而，这些自我报告的感受可能不如参与者的测验分数更加可靠，并且参与者可能由于我们假设了Raft更加容易理解而产生偏见。<br>A detailed discussion of the Raft user study is available at [31].<br>有关Raft用户研究的详细讨论，请参见[31]。<br><img alt="Pasted image 20240725173219.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173219.png"><br><br>We have developed a formal specification and a proof of safety for the consensus mechanism described in Section 5.<br>
The formal specification [31] makes the information summarized in Figure 2 completely precise using the TLA+ specification language [17].<br>
It is about 400 lines long and serves as the subject of the proof.<br>
It is also useful on its own for anyone implementing Raft.<br>
We have mechanically proven the Log Completeness Property using the TLA proof system [7].<br>
However, this proof relies on invariants that have not been mechanically checked<br>
(for example, we have not proven the type safety of the specification).<br>
Furthermore, we have written an informal proof [31] of the State Machine Safety property which is complete<br>
(it relies on the specification alone) and rela tively precise (it is about 3500 words long).<br><img alt="Pasted image 20240725173227.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725173227.png"><br>Figure 16: The time to detect and replace a crashed leader.<br>
The top graph varies the amount of randomness in election timeouts, and the bottom graph scales the minimum election timeout.<br>
Each line represents 1000 trials (except for 100 trials for “150–150ms”) and corresponds to a particular choice of election timeouts;<br>
for example, “150–155ms” means that election timeouts were chosen randomly and uniformly between 150ms and 155ms.<br>
The measurements were taken on a cluster of five servers with a broadcast time of roughly 15ms.<br>
Results for a cluster of nine servers are similar.<br>图16：检测和替代一个已崩溃leader的时间。<br>
上图是一系列随机化的选举超时时间，下图是缩放后的最小选举超时时间。<br>
每一行代表了对应于一个特定选举超时时间的1000次实验(除了“150-150ms”的100次实验)；<br>
例如，“150-155ms”意味着选举超时时间是在150ms到155ms间随机且均匀选择的。<br>
测量是在一个有着5台机器的集群上进行的，其广播时间大约为15ms。<br>
由9台服务器组成的集群的结果是类似的。<br>我们已经为第5节所描述的一致性机制提供了形式化规约和安全性证明。<br>
形式化规约使用TLA+规约语言精确的使用了如图2摘要中的信息。<br>
大约由400行长并且可以作为证明的主体来使用。<br>
对于任何一个想实现Raft的人来说也是有用的。<br>
我们已经使用TLA证明系统机械地证明了Log Completeness特性。<br>
然而，这一证明依赖于尚未被机械地检查地不变量(例如，我们还没有证明规约的类型安全性)。<br>
此外，我们也编写了关于状态机安全特性(State Machine Safety property)的非正式证明，该证明是完整的(仅依赖于规约)并且是相对精确的(大约长3500字)。<br><br>Raft’s performance is similar to other consensus algorithms such as Paxos.<br>
The most important case for performance is when an established leader is replicating new log entries.<br>
Raft achieves this using the minimal number of messages (a single round-trip from the leader to half the cluster).<br>
It is also possible to further improve Raft’s performance.<br>
For example, it easily supports batching and pipelining requests for higher throughput and lower latency.<br>
Various optimizations have been proposed in the literature for other algorithms; many of these could be applied to Raft,<br>
but we leave this to future work.<br>Raft的性能与Paxos等其它一致性算法的性能相差无几。<br>
对于性能而言最重要的方面是一个已被选出的leader复制新的日志条目。<br>
Raft通过使用最少数量的消息来实现这一点(从leader到集群中半数机器的单轮往返)。<br>
这也可能进一步的提升Raft的性能。<br>
例如，它可以轻松的支持批处理和流水线(pipelining)请求来获得更高的吞吐量和更低的延迟。<br>
在资料中已经针对其它算法提出了一系列的优化;其中有很多优化也能应用在Raft中，但我们将此留给未来的工作。<br>We used our Raft implementation to measure the performance of Raft’s leader election algorithm and answer two questions.<br>
First, does the election process converge quickly?<br>
Second, what is the minimum downtime that can be achieved after leader crashes?<br>我们使用我们自己的Raft实现来衡量Raft的leader选举算法性能，并且回答两个问题。<br>
第一，选举过程是否迅速的收敛？<br>
第二，在leader崩溃后可以实现的停机宕机时间是多少？<br>To measure leader election, we repeatedly crashed the leader of a cluster of five servers<br>
and timed how long it took to detect the crash and elect a new leader (see Figure 16).<br>
To generate a worst-case scenario, the servers in each trial had different log lengths, so some candidates were not eligible to become leader.<br>
Furthermore, to encourage split votes, our test script triggered<br>
a synchronized broadcast of heartbeat RPCs from the leader before terminating its process<br>
(this approximates the behavior of the leader replicating a new log entry prior to crashing).<br>
The leader was crashed uniformly randomly within its heartbeat interval, which was half of the minimum election timeout for all tests.<br>
Thus, the smallest possible downtime was about half of the minimum election timeout.<br>为了测量leader选举的性能，我们反复的令一个五节点集群中的leader崩溃，并且测量集群多久能检测到崩溃并选举出一个新的leader(见图16)。<br>
为了生成最坏的情况，在每次实验中服务器都有着不同的日志长度，因此一些candidate将没有资格成为leader。<br>
此外，为了促进分裂投票的产生，我们的测试脚本在终止leader进程前触发了一次leader的同步RPC心跳广播(这类似于leader在崩溃前复制新的日志条目的行为)。<br>The top graph in Figure 16 shows that a small amount of randomization in the election timeout is enough to avoid split votes in elections.<br>
In the absence of randomness, leader election consistently took longer than 10 seconds in our tests due to many split votes.<br>
Adding just 5ms of randomness helps significantly, resulting in a median downtime of 287ms.<br>
Using more randomness improves worst-case behavior: with 50ms of randomness the worstcase completion time (over 1000 trials) was 513ms.<br>图16中的上图显示了，只要少量的随机化选举超时时间就足够避免选举时的投票分裂。<br>
在缺乏随机性的情况下，由于许多分裂的投票，在我们的测试中leader选举一直持续了超过10秒钟。<br>
只要增加5ms的随机性就能有显著的帮助，平均的停机时间中位数为287ms。<br>
使用更多的随机性可以改善最坏情况下的行为：有着50ms的随机性时最坏情况下(超过1000次实验)的选举完成时间为513ms。<br>The bottom graph in Figure 16 shows that downtime can be reduced by reducing the election timeout.<br>
With an election timeout of 12–24ms, it takes only 35ms on average to elect a leader (the longest trial took 152ms).<br>
However, lowering the timeouts beyond this point violates Raft’s timing requirement:<br>
leaders have difficulty broadcasting heartbeats before other servers start new elections.<br>
This can cause unnecessary leader changes and lower overall system availability.<br>
We recommend using a conservative election timeout such as 150–300ms;<br>
such timeouts are unlikely to cause unnecessary leader changes and will still provide good availability.<br>图16中的下图显示，可以通过减少选举超时时间来减少停机时间。<br>
由于选举超时时间为12-24ms，选举出一个leader的平均耗时只需要35ms(最长的一次实验花费了152ms)。<br>
然而，超时时间低于这一位点以下会违反Raft的时间需求：leader很难在其它leader发起新一轮选举前进行心跳广播。<br>
这可能会导致不必要的leader变更以及更低的整体系统可用性。<br>
我们推荐使用一个保守的选举超时时间比如150-300ms；这一超时时间不太可能造成不必要的leader变更并且将仍然提供良好的可用性。<br><br>There have been numerous publications related to consensus algorithms, many of which fall into one of the following categories:<br>
<br>Lamport’s original description of Paxos [15], and attempts to explain it more clearly [16, 20, 21].
<br>Elaborations of Paxos, which fill in missing details and modify the algorithm to provide a better foundation for implementation [26, 39, 13].
<br>Systems that implement consensus algorithms, such as Chubby [2, 4], ZooKeeper [11, 12], and Spanner [6].<br>
The algorithms for Chubby and Spanner have not been published in detail, though both claim to be based on Paxos.<br>
ZooKeeper’s algorithm has been published in more detail, but it is quite different from Paxos.
<br>Performance optimizations that can be applied to Paxos [18, 19, 3, 25, 1, 27].
<br>Oki and Liskov’s View-stamped Replication (VR), an alternative approach to consensus developed around the same time as Paxos.<br>
The original description [29] was intertwined with a protocol for distributed transactions,<br>
but the core consensus protocol has been separated in a recent update [22].<br>
VR uses a leader-based approach with many similarities to Raft.
<br>已经有许多与一致性算法有关的出版物了，其中很多都属于以下类目中的一个：<br>
<br>Lamport对于Paxos的原始描述，并且试图更加清晰的进行解释。
<br>对Paxos的细化，其填充了确实的细节并且修改了算法以为实现Paxos提供了一个更好的基础。
<br>实现了共识算法的系统，例如Chubby，ZooKeeper和Spanner。<br>
Chubby和Spanner的算法并没有公布细节，即使它们都声称其基于Paxos。<br>
ZooKeeper的算法公布了更多的细节，但其与Paxos截然不同。
<br>可用于Paxos的性能优化。
<br>Oki和Liskov的Viewstamped Replication (VR)算法，另一种共识算法其被开发的时间点与Paxos相同。<br>
最初的描述与分布式事务的协议混在了一起，但最近的更新中其核心的共识协议已经被分离出来了。<br>
VR采用了一种基于leader的方法,其与Raft存在很多相似之处。
<br>The greatest difference between Raft and Paxos is Raft’s strong leadership:<br>
Raft uses leader election as an essential part of the consensus protocol, and it concentrates as much functionality as possible in the leader.<br>
This approach results in a simpler algorithm that is easier to understand.<br>
For example, in Paxos, leader election is orthogonal to the basic consensus protocol:<br>
it serves only as a performance optimization and is not required for achieving consensus.<br>
However, this results in additional mechanism:<br>
Paxos includes both a two-phase protocol for basic consensus and a separate mechanism for leader election.<br>
In contrast, Raft incorporates leader election directly into the consensus algorithm and uses it as the first of the two phases of consensus.<br>
This results in less mechanism than in Paxos.<br>Raft和Paxos最大的区别在于Raft的强领导性：<br>
Raft将leader选举作为共识协议中必要的组成部分，并尽可能的将很多功能集中在leader身上。<br>
这一策略使得Raft成为了一个更简单的算法，其更容易被理解。<br>
例如，在Paxos中，leader选举基本上与基础的一致性协议无关：<br>
其仅仅用于性能优化并且不是实现共识所必须的。<br>
然而，这产生了额外的机制：<br>
Paxos包括了一个用于基础一致性的两阶段协议以及一个单独的用于leader选举的机制。<br>
相比之下，Raft将leader选举直接纳入共识算法中并且将leader选举作为共识的两阶段中的第一个阶段。<br>
这使得Raft比起Paxos有着更少的机制。<br>Like Raft, VR and ZooKeeper are leader-based and therefore share many of Raft’s advantages over Paxos.<br>
However, Raft has less mechanism that VR or ZooKeeper because it minimizes the functionality in non-leaders.<br>
For example, log entries in Raft flow in only one direction: outward from the leader in AppendEntries RPCs.<br>
In VR log entries flow in both directions (leaders can receive log entries during the election process);<br>
this results in additional mechanism and complexity.<br>
The published description of ZooKeeper also transfers log entries both to and from the leader,<br>
but the implementation is apparently more like Raft [35].<br>与Raft一样，VR和ZooKeeper都是基于leader的并且也共享着很多Raft相对于Paxos的优点。<br>
然而，Raft比起VR或者Zookeeper有着更少的机制，因为它最小化了非leader(non-leaders)的功能。<br>
例如，Raft中的日志条目流向只有一个方向：从leader向外流出的AppendEntries RPC。<br>
在VR中日志条目是双向流动的(leader也可以在选举期间接受日志条目);这引入了额外的机制和复杂度。<br>
ZooKeeper的已公布的描述中也允许日志条目在leader中双向的传输，但其实现明显与Raft更加相似。<br>Raft has fewer message types than any other algorithm for consensus-based log replication that we are aware of.<br>
For example, we counted the message types VR and ZooKeeper use for basic consensus and membership<br>
changes (excluding log compaction and client interaction, as these are nearly independent of the algorithms).<br>
VR and ZooKeeper each define 10 different message types, while Raft has only 4 message types (two RPC requests and their responses).<br>
Raft’s messages are a bit more dense than the other algorithms’, but they are simpler collectively.<br>
In addition, VR and ZooKeeper are described in terms of transmitting entire logs during leader changes;<br>
additional message types will be required to optimize these mechanisms so that they are practical.<br>Raft的消息类型比任何其它已知的、基于日志复制的共识算法都要少。<br>
例如，我们统计了VR和ZooKeeper用于基础共识和成员变更的消息类型数(不包括日志压缩和客户端交互，因为这些与算法几乎是独立的)。<br>
VR和ZooKeeper都定义了10种不同的消息类型，而Raft只有4种(2种RPC的请求以及它们的响应)。<br>
Raft的消息比其它算法的要稍微紧密一些，但总体上更加简单。<br>
此外，VR和ZooKeeper所描述的在任期转换时需要传输完整的日志;所以在实践中需要额外的消息类型来优化这些机制。<br>Raft’s strong leadership approach simplifies the algorithm, but it precludes some performance optimizations.<br>
For example, Egalitarian Paxos (EPaxos) can achieve higher performance under some conditions with a leaderless approach [27].<br>
EPaxos exploits commutativity in state machine commands.<br>
Any server can commit a command with just one round of communication as long as other commands that are proposed concurrently commute with it.<br>
However, if commands that are proposed concurrently do not commute with each other, EPaxos requires an additional round of communication.<br>
Because any server may commit commands, EPaxos balances load well between servers and is able to achieve lower latency than Raft in WAN settings.<br>
However, it adds significant complexity to Paxos.<br>Raft的强领导力方法简化了算法，但是也排除了一些性能优化。<br>
例如，Egalitarian Paxos(EPaxos)可以通过无leader的方法在某些条件下可以获得更高的性能。<br>
EPaxos利用了状态机指令的交换性。<br>
只要同时提出的其它指令能够与之交换，任何服务器都可以仅在一轮通信中提交指令。<br>
然而，如果同时发出的指令不能相互交换，则EPaxos需要额外的一轮通信。<br>
因为任何服务器都能够提交指令，EPaxos能够更好的平衡服务器间的负载并且能够达到比Raft的WAN设置更低的延迟。<br>
然而，这显著的增加了Paxos的复杂性。<br>Several different approaches for cluster membership changes have been proposed or implemented in other work,<br>
including Lamport’s original proposal [15], VR [22], and SMART [24].<br>
We chose the joint consensus approach for Raft because it leverages the rest of the consensus protocol,<br>
so that very little additional mechanism is required for membership changes.<br>
Lamport’s α-based approach was not an option for Raft because it assumes consensus can be reached without a leader.<br>
In comparison to VR and SMART, Raft’s reconfiguration algorithm has the advantage<br>
that membership changes can occur without limiting the processing of normal requests;<br>
in contrast, VR stops all normal processing during configuration changes,<br>
and SMART imposes an α-like limit on the number of outstanding requests.<br>
Raft’s approach also adds less mechanism than either VR or SMART.<br>在其它工作中，几种不同的用于集群成员变更的方法已经被提出或被实现，包括Lamport的原始提案，VR以及SMART。<br>
我们为Raft选择了联合一致的方法，因为它利用了一致性协议的其余部分，因此成员变更只需要增加非常少的额外机制。<br>
Lamport的α-based方法没有被Raft选中，因为它假设可以在没有leader的情况下达成共识。<br>
与VR和SMART相比，Raft的刷新配置的算法有一个优点是可以再不限制正常请求的情况下进行成员变更；<br>
相比之下，VR在配置变更期间停止所有正常的请求处理并且SMART对未完成的请求施加了α-like限制。<br>
Raft的方法相比VR或者SMART也增加了最少的机制。<br><br>Algorithms are often designed with correctness, efficiency, and/or conciseness as the primary goals.<br>
Although these are all worthy goals, we believe that understandability is just as important.<br>
None of the other goals can be achieved until developers render the algorithm into a practical implementation,<br>
which will inevitably deviate from and expand upon the published form.<br>
Unless developers have a deep understanding of the algorithm and can create intuitions about it,<br>
it will be difficult for them to retain its desirable properties in their implementation.<br>算法的设计通常以正确性，效率和/或间接性为主要目标。<br>
尽管这些都是有价值的目标，我们认为可理解性同样重要。<br>
在开发人员将算法转化为一个可行的实现前无法达成任何其它的目标，而实际实现将不可避免的偏离和拓展已发布的形式。<br>
除非开发人员对算法有着很深的理解并且对其产生直觉，否则其将很难在他们的实现中保留理想的特性。<br>In this paper we addressed the issue of distributed consensus, where a widely accepted but impenetrable algorithm,<br>
Paxos, has challenged students and developers for many years.<br>
We developed a new algorithm, Raft, which we have shown to be more understandable than Paxos.<br>
We also believe that Raft provides a better foundation for system building.<br>
Using understandability as the primary design goal changed the way we approached the design of Raft;<br>
as the design progressed we found ourselves reusing a few techniques repeatedly,<br>
such as decomposing the problem and simplifying the state space.<br>
These techniques not only improved the understandability of Raft but also made it easier to convince ourselves of its correctness.<br>在本文中我们讨论了分布式一致性的问题，一种被广泛接受但难于实现的算法Paxos，在多年来一直在挑战着学生和开发者。<br>
我们开发了一种新的算法，Raft，我们已经展示了其比Paxos更加容易理解。<br>
我们也认为Raft为构建系统提供了一个更好的基础。<br>
以可理解性作为主要实现目标改变了我们设计Raft时的方法；随着设计的近战我们发现我们重复的复用了少量技术，例如分解问题和简化状态空间。<br>
这些技术不仅提高了Raft的可理解性也使得我们更容易相信它的正确性。<br><br>The user study would not have been possible without the support of Ali Ghodsi, David Mazieres,<br>
and the students of CS 294-91 at Berkeley and CS 240 at Stanford.<br>
Scott Klemmer helped us design the user study, and Nelson Ray advised us on statistical analysis.<br>
The Paxos slides for the user study borrowed heavily from a slide deck originally created by Lorenzo Alvisi.<br>
Special thanks go to David Mazi`eres and Ezra Hoch for finding subtle bugs in Raft.<br>
Many people provided helpful feedback on the paper and user study materials,<br>
including Ed Bugnion, Michael Chan, Hugues Evrard,Daniel Giffin, Arjun Gopalan, Jon Howell, Vimalkumar Jeyakumar, Ankita Kejriwal,<br>
Aleksandar Kracun, Amit Levy, Joel Martin, Satoshi Matsushita, Oleg Pesok, David Ramos,<br>
Robbert van Renesse, Mendel Rosenblum, Nicolas Schiper, Deian Stefan, Andrew Stone, Ryan Stutsman,<br>
David Terei, Stephen Yang, Matei Zaharia, 24 anonymous conference reviewers (with duplicates), and especially our shepherd Eddie Kohler.<br>
Werner Vogels tweeted a link to an earlier draft, which gave Raft significant exposure.<br>
This work was supported by the Gigascale Systems Research Center and the Multiscale Systems Center,<br>
two of six research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program,<br>
by STAR net, a Semiconductor Research Corporation program sponsored by MARCO and DARPA,<br>
by the National Science Foundation under Grant No. 0963859, and by grants from Facebook, Google, Mellanox, NEC, NetApp, SAP,<br>
and Samsung. Diego Ongaro is supported by The Junglee Corporation Stanford Graduate Fellowship.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/in-search-of-an-understandable-consensus-algorithm(extended-version).html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft 原始论文中英/In Search of an Understandable Consensus Algorithm(Extended Version).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 25 Jul 2024 09:32:41 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172940.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft-原始论文中英/pasted-image-20240725172940.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Raft实现 - 一致性算法介绍]]></title><description><![CDATA[ 
 <br><br><br>
<br>对可靠性有很高要求的系统，通常都会额外部署1至多个机器为备用副本组成主备集群，避免出现单点故障。<br>
有状态的系统需要主节点与备用副本间以某种方式进行数据复制，这样主节点出现故障时就能快速的令备用机器接管系统以达到高可用的目的。
<br>常见的主备复制方式是异步、弱一致性的，例如DNS系统，mysql、redis(7.0之前)等数据库的主备复制，或者通过某种消息中间件来进行解耦，即在CAP中选择了AP(高可用、分区容错)而舍弃了C(强一致性)。<br>
弱一致性的AP相比强一致CP的复制有着许多优点：效率高(多个单次操作可以批量处理)，耦合性低(备份节点挂了也不影响主节点工作)，实现相对简单等等。<br>
但AP复制最大的缺点就是丧失了强一致性，主节点在操作完成响应客户端后，但还未成功同步到备份节点前宕机，对应的变更存在着丢失的风险，因此AP的方案不适用于对一致性有苛刻要求的场合。
<br>最原始的强一致性主备同步，即主节点在每一个备份节点同步完成后才能响应客户端成功的方案效率太低，可用性太差(任意一个备份节点故障就会使得集群不可用)。<br>
因此基于多数派的分布式强一致算法被发明了出来，其中最早被提出的便是Paxos算法。但Paxos算法过于复杂，在分布式环境下有大量的case需要得到正确的实现，因此时至今日也没有多少系统真正的将Paxos落地。
<br><br>
<br>由于Paxos过于复杂的原因，Raft算法被发明了出来。Raft算法在设计时大量参考了Paxos，也是一个基于日志和多数派的一致性算法，但在很多细节上相比Paxos做了许多简化。
<br>因为Raft比Paxos要简单很多，更容易被开发人员理解并最终用于构建实际的系统。因此即使raft算法的性能相比Paxos要差一点，但目前流行的强一致分布式系统基本都是基于Raft算法的。
<br><a data-tooltip-position="top" aria-label="https://raft.github.io/raft.pdf" rel="noopener" class="external-link" href="https://raft.github.io/raft.pdf" target="_blank">raft的论文</a> 中将raft算法的功能分解为4个模块：<br>
<br>leader选举
<br>日志复制
<br>日志压缩
<br>集群成员动态变更
<br>其中前两项“leader选举”和“日志复制”是raft算法的基础，而后两项“日志压缩”和“集群成员动态变更”属于raft算法在功能上的重要优化。<br><a data-tooltip-position="top" aria-label="https://www.cnblogs.com/xiaoxiongcanguan/p/17552027.html" rel="noopener" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17552027.html" target="_blank">raft论文中英翻译</a><br><br>通过raft的论文或者其它相关资料，读者基本能大致理解raft的工作原理。<br>
但纸上得来终觉浅，绝知此事要躬行，亲手实践才能更好的把握raft中的精巧细节，加深对raft算法的理解，更有效的阅读基于raft或其它一致性协议的开源项目源码。<br><br>在这个系列博客中会带领读者一步步实现一个基于raft算法的简易KV数据库,即MyRaft。MyRaft的实现基于原始的raft算法，没有额外的优化，目的是为了保证实现的简单性。<br>
MyRaft实现了raft论文中提到的三个功能，即”leader选举“、”日志复制“和”日志压缩“（在实践中发现“集群成员动态变更”对原有逻辑有较大改动而大幅增加了复杂度，限于个人水平暂不实现）。<br>
三个功能会通过三次迭代实验逐步完成，其中每个迭代都会以博客的形式分享出来。<br><br>![[Pasted image 20240725171642.png]]
Copy<br><br>
<br>由于是MyRaft的第一个迭代，在这个迭代中需要先搭好MyRaft的基础骨架。<br>
raft中的每个节点本质上是一个rpc服务器，同时也是一个rpc的消费者，节点之间通过rpc的方式互相通信。
<br>MyRaft使用的rpc框架是上一个实验中自己实现的MyRpc框架：<br>
博客地址: <a rel="noopener" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17506728.html" target="_blank">https://www.cnblogs.com/xiaoxiongcanguan/p/17506728.html</a><br>
github地址：<a rel="noopener" class="external-link" href="https://github.com/1399852153/MyRpc" target="_blank">https://github.com/1399852153/MyRpc</a> (main分支)
<br><br>
<br>因为lab1中只实现leader选举，简单起见只定义当前所需的api接口，接口参数相比最终的实现也省去了大量当前用不上的字段，后续有需要再进行拓展。
<br>public interface RaftService {

    /**
     * 请求投票 requestVote
     *
     * Receiver implementation:
     * 1. Reply false if term &lt; currentTerm (§5.1)
     * 2. If votedFor is null or candidateId, and candidate’s log is at
     * least as up-to-date as receiver’s log, grant vote (§5.2, §5.4)
     *
     * 接受者需要实现以下功能：
     * 1. 如果参数中的任期值term小于当前自己的任期值currentTerm，则返回false不同意投票给调用者
     * 2. 如果自己还没有投票(FIFO)或者已经投票给了candidateId对应的节点(幂等)，
     *    并且候选人的日志至少与被调用者的日志一样新(比较日志的任期值和索引值)，则投票给调用者(返回值里voteGranted为true)
     * */
    RequestVoteRpcResult requestVote(RequestVoteRpcParam requestVoteRpcParam);

    /**
     * 追加日志条目 AppendEntries
     * */
    AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam);
}
Copy<br>/**
 * 请求投票的RPC接口参数对象
 */
public class RequestVoteRpcParam implements Serializable {

    /**
     * 候选人的任期编号
     * */
    private int term;

    /**
     * 候选人的Id
     * */
    private String candidateId;

    /**
     * 候选人最新日志的索引编号
     * */
    private long lastLogIndex;

    /**
     * 候选人最新日志对应的任期编号
     * */
    private int lastLogTerm;
}
Copy<br>/**
 * 请求投票的RPC接口响应对象
 * */
public class RequestVoteRpcResult implements Serializable {

    /**
     * 被调用者当前的任期值
     * */
    private int term;

    /**
     * 是否同意投票给调用者
     * */
    private boolean voteGranted;
}
Copy<br>/**
 * 追加日志条目的RPC接口参数对象
 * */
public class AppendEntriesRpcParam implements Serializable {

    /**
     * 当前leader的任期值
     * */
    private int term;

    /**
     * leader的id
     * */
    private String leaderId;
}
Copy<br>/**
 * 追加日志条目的RPC接口响应对象
 * */
public class AppendEntriesRpcResult implements Serializable {

    /**
     * 被调用者当前的任期值
     * */
    private int term;

    /**
     * 是否处理成功
     * */
    private boolean success;
}
Copy<br><br>/**
 * raft的rpc服务
 * */
public class RaftRpcServer extends RaftServer {

    private final Registry registry;
    private final RaftNodeConfig currentNodeConfig;

    public RaftRpcServer(RaftConfig raftConfig, Registry registry){
        super(raftConfig);

        this.currentNodeConfig = raftConfig.getCurrentNodeConfig();
        this.registry = registry;
    }

    @Override
    public void init(List&lt;RaftService&gt; otherNodeInCluster) {
        // 先初始化内部模块
        super.init(otherNodeInCluster);

        // 初始化内部的模块后，启动rpc
        initRpcServer();
    }

    public List&lt;RaftService&gt; getRpcProxyList(List&lt;RaftNodeConfig&gt; otherNodeInCluster){
        return initRpcConsumer(otherNodeInCluster);
    }

    private List&lt;RaftService&gt; initRpcConsumer(List&lt;RaftNodeConfig&gt; otherNodeInCluster){
        ConsumerBootstrap consumerBootstrap = new ConsumerBootstrap()
            .registry(registry)
            .loadBalance(new SimpleRoundRobinBalance());

        // 注册消费者
        Consumer&lt;RaftService&gt; consumer = consumerBootstrap.registerConsumer(RaftService.class,new FastFailInvoker());
        RaftService raftServiceProxy = consumer.getProxy();

        List&lt;RaftService&gt; raftRpcConsumerList = new ArrayList&lt;&gt;();
        for(RaftNodeConfig raftNodeConfig : otherNodeInCluster){
            // 使用rpc代理的客户端
            raftRpcConsumerList.add(new RaftRpcConsumer(raftNodeConfig,raftServiceProxy));
        }

        return raftRpcConsumerList;
    }

    private void initRpcServer(){
        URLAddress providerURLAddress = new URLAddress(currentNodeConfig.getIp(),currentNodeConfig.getPort());
        Provider&lt;RaftService&gt; provider = new Provider&lt;&gt;();
        provider.setInterfaceClass(RaftService.class);
        provider.setRef(this);
        provider.setUrlAddress(providerURLAddress);
        provider.setRegistry(registry);
        provider.export();

        NettyServer nettyServer = new NettyServer(providerURLAddress);
        nettyServer.init();
    }
}
Copy<br>public class RaftRpcConsumer implements RaftService {

    private static final Logger logger = LoggerFactory.getLogger(RaftRpcConsumer.class);

    private final RaftNodeConfig targetNodeConfig;
    private final RaftService raftServiceProxy;

    public RaftRpcConsumer(RaftNodeConfig targetNodeConfig, RaftService proxyRaftService) {
        this.targetNodeConfig = targetNodeConfig;
        this.raftServiceProxy = proxyRaftService;
    }

    @Override
    public RequestVoteRpcResult requestVote(RequestVoteRpcParam requestVoteRpcParam) {
        // 强制指定rpc目标的ip/port
        setTargetProviderUrl();
        RequestVoteRpcResult result = raftServiceProxy.requestVote(requestVoteRpcParam);
        return result;
    }

    @Override
    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        // 强制指定rpc目标的ip/port
        setTargetProviderUrl();
        AppendEntriesRpcResult result = raftServiceProxy.appendEntries(appendEntriesRpcParam);
        return result;
    }

    private void setTargetProviderUrl(){
        ConsumerRpcContext consumerRpcContext = ConsumerRpcContextHolder.getConsumerRpcContext();
        consumerRpcContext.setTargetProviderAddress(
            new URLAddress(targetNodeConfig.getIp(),targetNodeConfig.getPort()));
    }
}
Copy<br><br>
<br>raft的论文中提到raft服务中需要持久化的三个要素：currentTerm（当前服务器的任期值）、votedFor(当前服务器在此之前投票给了谁)和logs(raft的操作日志，与本篇博客无关在lab2中才会引入)。
<br>currentTerm和votedFor需要持久化的原因是为了避免raft节点在完成leader选举的投票后宕机，重启恢复后如果这两个数据丢失了就很容易在同一任期内投票给多个候选人而出现集群脑裂(即多个合法leader)。
<br>MyRaft用磁盘文件进行持久化，简单起见在currentTerm或votedFor更新时加写锁，通过原子性的整体刷盘来完成持久化。
<br>public class RaftServerMetaData {

    /**
     * 当前服务器的任期值
     * */
    private int currentTerm;

    /**
     * 当前服务器在此之前投票给了谁？
     * (候选者的serverId，如果还没有投递就是null)
     * */
    private String votedFor;
}
Copy<br>public class RaftServerMetaDataPersistentModule {

    /**
     * 当前服务器的任期值
     * */
    private volatile int currentTerm;

    /**
     * 当前服务器在此之前投票给了谁？
     * (候选者的serverId，如果还没有投递就是null)
     * */
    private volatile String votedFor;

    private final File persistenceFile;

    private final ReentrantReadWriteLock reentrantLock = new ReentrantReadWriteLock();
    private final ReentrantReadWriteLock.WriteLock writeLock = reentrantLock.writeLock();
    private final ReentrantReadWriteLock.ReadLock readLock = reentrantLock.readLock();

    public RaftServerMetaDataPersistentModule(String serverId) {
        String userPath = System.getProperty("user.dir") + File.separator + serverId;

        this.persistenceFile = new File(userPath + File.separator + "raftServerMetaData-" + serverId + ".txt");
        MyRaftFileUtil.createFile(persistenceFile);

        // 读取持久化在磁盘中的数据
        RaftServerMetaData raftServerMetaData = readRaftServerMetaData(persistenceFile);
        this.currentTerm = raftServerMetaData.getCurrentTerm();
        this.votedFor = raftServerMetaData.getVotedFor();
    }

    public int getCurrentTerm() {
        readLock.lock();
        try {
            return currentTerm;
        }finally {
            readLock.unlock();
        }
    }

    public void setCurrentTerm(int currentTerm) {
        writeLock.lock();
        try {
            this.currentTerm = currentTerm;

            // 更新后数据落盘
            persistentRaftServerMetaData(new RaftServerMetaData(this.currentTerm,this.votedFor),persistenceFile);
        }finally {
            writeLock.unlock();
        }
    }

    public String getVotedFor() {
        readLock.lock();
        try {
            return votedFor;
        }finally {
            readLock.unlock();
        }
    }

    public void setVotedFor(String votedFor) {
        writeLock.lock();
        try {
            if(Objects.equals(this.votedFor,votedFor)){
                // 相等的话就不刷新了
                return;
            }
            
            this.votedFor = votedFor;

            // 更新后数据落盘
            persistentRaftServerMetaData(new RaftServerMetaData(this.currentTerm,this.votedFor),persistenceFile);
        }finally {
            writeLock.unlock();
        }
    }

    private static RaftServerMetaData readRaftServerMetaData(File persistenceFile){
        String content = MyRaftFileUtil.getFileContent(persistenceFile);
        if(StringUtils.hasText(content)){
            return JsonUtil.json2Obj(content,RaftServerMetaData.class);
        }else{
            return RaftServerMetaData.getDefault();
        }
    }

    private static void persistentRaftServerMetaData(RaftServerMetaData raftServerMetaData, File persistenceFile){
        String content = JsonUtil.obj2Str(raftServerMetaData);

        MyRaftFileUtil.writeInFile(persistenceFile,content);
    }
}
Copy<br><br><br>raft的leader选举在论文中有较详细的描述，这里说一下我认为的关键细节。<br>
<br>Raft算法中leader扮演着绝对核心的角色，leader负责处理客户端的请求、将操作日志同步给其它的follower节点以及通知follower提交日志等等。<br>
因此Raft集群必须基于多数原则选举出一个存活的leader才能对外提供服务，并且一个任期内只能有一个基于多数票选出的leader。
<br>raft是非拜占庭容错共识算法，rpc通信时交互的双方的请求和响应都是可信的，不会作假，节点运行的行为也符合raft算法的规定。
<br>raft中存在任期term的概念，任期值只会单向递增，可以理解为一个虚拟的时间，是raft实现线性一致性关键的一环。过去的leader(term值更小的)需要服从、追随现任的leader(term值更大的)。
<br>在raft节点刚启动时处于follower追随者状态。如果一段时间内raft节点没有接受到来自leader的定时心跳rpc(logEntry为空的appendEntries)通知时就会发起一轮新的选举。<br>
产生这个现象的原因有很多，比如集群刚刚启动还没有leader；或者之前的leader因为某种原因宕机或与follower的网络通信出现故障等。
<br>发起请求的follower会转变为candidate候选人状态，并首先投票给自己。同时并行的向集群中的其它节点发起请求投票的rpc请求(requestVote),可以理解为给自己拉票。<br>
接收到requestVote请求的节点会根据自身的状态等信息决定是否投票给发起投票的节点。<br>
当candidate获得了集群中超过半数的投票(即包括自己在内的1票加上requestVote返回投票成功的数量超过半数(比如5节点得到3票，6节点得到4票))，则candidate成为当前任期的leader。<br>
如果没有任何一个candidate获得多数选票(没选出leader，可能是分票了，也可能是网络波动等等)，则candidate会将当前任期自增1，则下一次选举超时时会再触发一轮新的选举，循环往复直至选出leader。<br>
<img alt="Pasted image 20240725172248.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172248.png">
<br>leader当选后需要立即向其它的节点发送心跳rpc(logEntry为空的appendEntries)，昭告新leader的产生以抑制其它节点发起新的选举。<br>
心跳rpc必须以一定频率的定时向所有follower发送，发送的时间间隔需要小于设置的选举超时时间。
<br>由于处理requestVote是先到先得的，同一任期内先发起投票请求的candidate会收到票，后发送的会被拒绝。<br>
假如leader宕机了，则每个follower都会在一段时间后触发新一轮选举，如果没有额外的限制，则每个节点并发的发起选举很容易导致分票，即自己投给自己，则难以达成共识(取得多数票)。<br>
Raft的论文中提出了随机化选举超时时间的方案，即每个follower节点的选举超时时间是一个固定值再加上一个随机化的值得到的，这样很难在同一瞬间都触发选举。<br>
随机超时时间更短的follower能够最先发起选举，更快的得到其它节点的投票从而避免分票的情况。<br>
虽然无法完全避免分票，但实践中发现效果很好，随机超时时间下通常少数的几次分票后就能收敛而选出leader来。
<br>注意：raft论文在5.4安全性一节中提到，leader选举对于candidate的日志状态有一定的要求(因为只有拥有完整日志的节点才有资格成为leader，确保leader更替时日志不会丢失)，<br>
但lab1中不支持日志复制，所以MyRaft在lab1的requestVote实现中省略了相关逻辑。
<br><br>下面基于源码展开介绍MyRaft是如何实现raft领导者选举的。<br>大致分为以下几部分：<br>
<br>raft节点配置
<br>raft节点定时选举超时检查
<br>candidate发起选举
<br>leader定时发起心跳广播
<br>raft节点处理requestVote请求
<br><br>
<br>raft服务中有很多参数需要配置，比如服务的ip/port，或者raft相关的选举超时时间、心跳间隔时间等等。
<br>MyRaft中统一放到一个叫RaftConfig的类里维护，后续的功能实现时也会在这个类中进行拓展。
<br>public class RaftConfig {
    
    /**
     * 当前服务节点的id(集群内全局唯一)
     * */
    private final String serverId;

    /**
     * 自己节点的配置
     * */
    private final RaftNodeConfig currentNodeConfig;

    /**
     * 整个集群所有的服务节点的id集合
     * */
    private final List&lt;RaftNodeConfig&gt; raftNodeConfigList;

    /**
     * 集群中多数的值(例如：5节点majorityNum=3,6节点majorityNum=4)
     * */
    private final int majorityNum;

    /**
     * 选举超时时间 单位:秒
     * */
    private int electionTimeout;
    
    /**
     * 选举超时时间的随机化区间 单位：毫秒
     * */
    private Range&lt;Integer&gt; electionTimeoutRandomRange;

    /**
     * 心跳间隔时间 单位：秒
     * */
    private int HeartbeatInternal;
}
Copy<br>public class RaftNodeConfig {

    private String serverId;
    private String ip;
    private int port;
}
Copy<br><br>
<br>MyRaft中将leader选举相关的主要逻辑都集中维护在RaftLeaderElectionModule类中。<br>
lastHeartbeatTime属性用于存储最后一次收到leader心跳的绝对时间，如果当前节点状态不是leader，并且发现lastHeartbeatTime距离当前时间已经超过了指定的选举超时时间则触发选举。
<br>心跳检查的超时逻辑集中在HeartbeatTimeoutCheckTask中。<br>
由于需要引入随机化的心跳超时时间，因此无法使用ScheduledExecutorService的scheduleAtFixedRate方法，改为在每个任务执行完成时再添加一个新任务回去的方式来实现。
<br>/**
 * Raft服务器的leader选举模块
 * */
public class RaftLeaderElectionModule {

    private static final Logger logger = LoggerFactory.getLogger(RaftLeaderElectionModule.class);

    private final RaftServer currentServer;

    /**
     * 最近一次接受到心跳的时间
     * */
    private volatile Date lastHeartbeatTime;

    private final ScheduledExecutorService scheduledExecutorService;

    private final ExecutorService rpcThreadPool;

    public RaftLeaderElectionModule(RaftServer currentServer) {
        this.currentServer = currentServer;
        this.lastHeartbeatTime = new Date();
        this.scheduledExecutorService = Executors.newScheduledThreadPool(3);
        this.rpcThreadPool = Executors.newFixedThreadPool(
                Math.max(currentServer.getOtherNodeInCluster().size() * 2, 1));

        registerHeartbeatTimeoutCheckTaskWithRandomTimeout();
    }

    /**
     * 提交新的延迟任务(带有随机化的超时时间)
     * */
    public void registerHeartbeatTimeoutCheckTaskWithRandomTimeout(){
        int electionTimeout = currentServer.getRaftConfig().getElectionTimeout();
        if(currentServer.getCurrentTerm() &gt; 0 &amp;&amp; currentServer.getRaftConfig().getDebugElectionTimeout() != null){
            // debug的时候多等待一些时间
            electionTimeout = currentServer.getRaftConfig().getDebugElectionTimeout();
        }

        long randomElectionTimeout = getRandomElectionTimeout();
        // 选举超时时间的基础上，加上一个随机化的时间
        long delayTime = randomElectionTimeout + electionTimeout * 1000L;
        logger.debug("registerHeartbeatTimeoutCheckTaskWithRandomTimeout delayTime={}",delayTime);
        scheduledExecutorService.schedule(
            new HeartbeatTimeoutCheckTask(currentServer,this),delayTime,TimeUnit.MILLISECONDS);
    }

    /**
     * 处理投票请求
     * 注意：synchronized修饰防止不同candidate并发的投票申请处理，以FIFO的方式处理
     * */
    public synchronized RequestVoteRpcResult requestVoteProcess(RequestVoteRpcParam requestVoteRpcParam){
        if(this.currentServer.getCurrentTerm() &gt; requestVoteRpcParam.getTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 发起投票的candidate任期小于当前服务器任期，拒绝投票给它
            logger.info("reject requestVoteProcess! term &lt; currentTerm, currentServerId={}",currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 发起投票的节点任期高于当前节点，无条件投票给它(任期高的说了算)
        if(this.currentServer.getCurrentTerm() &lt; requestVoteRpcParam.getTerm()){
            // 刷新元数据
            this.currentServer.refreshRaftServerMetaData(
                new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
            // 任期没它高，自己转为follower
            this.currentServer.setServerStatusEnum(ServerStatusEnum.FOLLOWER);
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
        }

        // term任期值相同，需要避免同一任期内投票给不同的节点而脑裂
        if(this.currentServer.getVotedFor() != null &amp;&amp; !this.currentServer.getVotedFor().equals(requestVoteRpcParam.getCandidateId())){
            // If votedFor is null or candidateId（取反的卫语句）
            // 当前服务器已经把票投给了别人,拒绝投票给发起投票的candidate
            logger.info("reject requestVoteProcess! votedFor={},currentServerId={}",
                currentServer.getVotedFor(),currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 投票校验通过,刷新元数据
        this.currentServer.refreshRaftServerMetaData(
            new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
        this.currentServer.processCommunicationHigherTerm(requestVoteRpcParam.getTerm());
        return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
    }

    public void refreshLastHeartbeatTime(){
        // 刷新最新的接受到心跳的时间
        this.lastHeartbeatTime = new Date();
        // 接受新的心跳,说明现在leader是存活的，清理掉之前的投票信息
        this.currentServer.cleanVotedFor();
    }
    
    private long getRandomElectionTimeout(){
        long min = currentServer.getRaftConfig().getElectionTimeoutRandomRange().getLeft();
        long max = currentServer.getRaftConfig().getElectionTimeoutRandomRange().getRight();

        // 生成[min,max]范围内随机整数的通用公式为：n=rand.nextInt(max-min+1)+min。
        return ThreadLocalRandom.current().nextLong(max-min+1) + min;
    }
}
Copy<br>/**
 * 心跳超时检查任务
 * */
public class HeartbeatTimeoutCheckTask implements Runnable{

    private static final Logger logger = LoggerFactory.getLogger(HeartbeatTimeoutCheckTask.class);

    private final RaftServer currentServer;
    private final RaftLeaderElectionModule raftLeaderElectionModule;

    public HeartbeatTimeoutCheckTask(RaftServer currentServer, RaftLeaderElectionModule raftLeaderElectionModule) {
        this.currentServer = currentServer;
        this.raftLeaderElectionModule = raftLeaderElectionModule;
    }

    @Override
    public void run() {
        if(currentServer.getServerStatusEnum() == ServerStatusEnum.LEADER){
            // leader是不需要处理心跳超时的
            // 注册下一个心跳检查任务
            raftLeaderElectionModule.registerHeartbeatTimeoutCheckTaskWithRandomTimeout();
        }else{
            try {
                doTask();
            }catch (Exception e){
                logger.info("do HeartbeatTimeoutCheckTask error! ignore",e);
            }

            // 注册下一个心跳检查任务
            raftLeaderElectionModule.registerHeartbeatTimeoutCheckTaskWithRandomTimeout();
        }
    }

    private void doTask(){
        logger.debug("do HeartbeatTimeoutCheck start {}",currentServer.getServerId());

        int electionTimeout = currentServer.getRaftConfig().getElectionTimeout();

        // 当前时间
        Date currentDate = new Date();
        Date lastHeartbeatTime = raftLeaderElectionModule.getLastHeartbeatTime();
        long diffTime = currentDate.getTime() - lastHeartbeatTime.getTime();

        logger.debug("currentDate={}, lastHeartbeatTime={}, diffTime={}, serverId={}",
            currentDate,lastHeartbeatTime,diffTime,currentServer.getServerId());
        // 心跳超时判断
        if(diffTime &gt; (electionTimeout * 1000L)){
            logger.info("HeartbeatTimeoutCheck check fail, trigger new election! serverId={}",currentServer.getServerId());

            // 触发新的一轮选举
            triggerNewElection();
        }else{
            // 认定为心跳正常，无事发生
            logger.debug("HeartbeatTimeoutCheck check success {}",currentServer.getServerId());
        }

        logger.debug("do HeartbeatTimeoutCheck end {}",currentServer.getServerId());
    }

    private void triggerNewElection(){
        logger.info("HeartbeatTimeoutCheck check fail, trigger new election! serverId={}",currentServer.getServerId());

        // 距离最近一次接到心跳已经超过了选举超时时间，触发新一轮选举

        // 当前服务器节点当前任期自增1
        currentServer.setCurrentTerm(currentServer.getCurrentTerm()+1);
        // 自己发起选举，先投票给自己
        currentServer.setVotedFor(currentServer.getServerId());
        // 角色转变为CANDIDATE候选者
        currentServer.setServerStatusEnum(ServerStatusEnum.CANDIDATE);

        // 并行的发送请求投票的rpc给集群中的其它节点
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();
        List&lt;Future&lt;RequestVoteRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        // 构造请求参数
        RequestVoteRpcParam requestVoteRpcParam = new RequestVoteRpcParam();
        requestVoteRpcParam.setTerm(currentServer.getCurrentTerm());
        requestVoteRpcParam.setCandidateId(currentServer.getServerId());

        for(RaftService node : otherNodeInCluster){
            Future&lt;RequestVoteRpcResult&gt; future = raftLeaderElectionModule.getRpcThreadPool().submit(
                ()-&gt; {
                    RequestVoteRpcResult rpcResult = node.requestVote(requestVoteRpcParam);
                    // 收到更高任期的处理
                    currentServer.processCommunicationHigherTerm(rpcResult.getTerm());
                    return rpcResult;
                }
            );

            futureList.add(future);
        }

        List&lt;RequestVoteRpcResult&gt; requestVoteRpcResultList = CommonUtil.concurrentGetRpcFutureResult(
            "requestVote", futureList,
            raftLeaderElectionModule.getRpcThreadPool(),1,TimeUnit.SECONDS);

        // 获得rpc响应中决定投票给自己的总票数（算上自己的1票）
        int getRpcVoted = (int) requestVoteRpcResultList.stream().filter(RequestVoteRpcResult::isVoteGranted).count()+1;
        logger.info("HeartbeatTimeoutCheck election, getRpcVoted={}, currentServerId={}",getRpcVoted,currentServer.getServerId());

        // 是否获得大多数的投票
        boolean majorVoted = getRpcVoted &gt;= this.currentServer.getRaftConfig().getMajorityNum();
        if(majorVoted){
            logger.info("HeartbeatTimeoutCheck election result: become a leader! {}, currentTerm={}",currentServer.getServerId(),currentServer.getCurrentTerm());

            // 票数过半成功当选为leader
            currentServer.setServerStatusEnum(ServerStatusEnum.LEADER);
            currentServer.setCurrentLeader(currentServer.getServerId());

            // 成为leader后立马发送一次心跳,抑制其它节点发起新的一轮选举
            // Upon election: send initial empty AppendEntries RPCs (heartbeat) to each server;
            // repeat during idle periods to prevent election timeouts (§5.2)
            HeartbeatBroadcastTask.doHeartbeatBroadcast(currentServer);
        }else{
            // 票数不过半，无法成为leader
            logger.info("HeartbeatTimeoutCheck election result: not become a leader! {}",currentServer.getServerId());
        }

        this.currentServer.cleanVotedFor();
    }
}
Copy<br><br>
<br>在上一节介绍的HeartbeatTimeoutCheckTask中，如果发现有一段时间没有收到心跳后当前节点便会触发新一轮的选举，主要逻辑在triggerNewElection方法中。<br>
triggerNewElection中通过首先令当前term值自增1并投票给自己，然后并行的向集群中的其它节点发送requestVote的rpc请求。
<br>并行处理逻辑通过CommonUtil中的concurrentGetRpcFutureResult方法收集所有的响应结果。<br>
通过future.get设置超时时间，超时则认为是投票失败。
<br>在超时时间内获得所有响应结果后，计算所得到的的票数是否大于半数(&gt;majorityNum)。<br>
如果超过半数则认为选举成功，自己成为合法的leader。当前节点刷新相关的状态数据，同时立即发起一次心跳广播以抑制其它节点发起新的选举。
<br>public class CommonUtil {

    private static final Logger logger = LoggerFactory.getLogger(CommonUtil.class);

    /**
     * 并发的获得future列表的结果
     * */
    public static &lt;T&gt; List&lt;T&gt; concurrentGetRpcFutureResult(
            String info, List&lt;Future&lt;T&gt;&gt; futureList, ExecutorService threadPool, long timeout, TimeUnit timeUnit){
        CountDownLatch countDownLatch = new CountDownLatch(futureList.size());

        List&lt;T&gt; resultList = new ArrayList&lt;&gt;(futureList.size());

        for(Future&lt;T&gt; futureItem : futureList){
            threadPool.execute(()-&gt;{
                try {
                    logger.debug(info + " concurrentGetRpcFutureResult start!");
                    T result = futureItem.get(timeout,timeUnit);
                    logger.debug(info + " concurrentGetRpcFutureResult end!");

                    resultList.add(result);
                } catch (Exception e) {
                    // rpc异常不考虑
                    logger.error( "{} getFutureResult error!",info,e);
                } finally {
                    countDownLatch.countDown();
                }
            });
        }

        try {
            countDownLatch.await();
        } catch (InterruptedException e) {
            throw new MyRaftException("getFutureResult error!",e);
        }

        return resultList;
    }
}
Copy<br><br>
<br>MyRaft中将leader心跳广播相关的逻辑都集中在了RaftHeartbeatBroadcastModule类中，心跳广播任务以RaftConfig中设置的频率定期执行。<br>
需要注意的是，心跳的时间间隔必须配置为明显小于配置的选举超时时间的值(考虑到rpc网络请求延迟以及follower实际处理心跳的耗时)，否则leader心跳将无法抑制follower触发选举。
<br>leader心跳广播的逻辑集中在HeartbeatBroadcastTask中。和发起投票类似，当前节点通过future并发的向集群中的所有节点发送logEntry为空的appendEntries(因为lab1中不涉及日志复制，所以直接去掉了logEntry这个参数字段)。<br>
leader原则上可以不关心follower对于心跳的响应结果，但还是需要检查成功响应中返回的term值。如果发现有其它节点返回了更大的term值，说明集群中可能已经选出了新的leader或者正在进行选举，则当前节点需要退回到follower状态。
<br>/**
 * Raft服务器的心跳广播模块
 * */
public class RaftHeartbeatBroadcastModule {

    private final RaftServer currentServer;

    private final ScheduledExecutorService scheduledExecutorService;

    private final ExecutorService rpcThreadPool;

    public RaftHeartbeatBroadcastModule(RaftServer currentServer) {
        this.currentServer = currentServer;

        this.scheduledExecutorService = Executors.newScheduledThreadPool(1);
        this.rpcThreadPool = Executors.newFixedThreadPool(
            Math.max(currentServer.getOtherNodeInCluster().size() * 2, 1));

        int HeartbeatInternal = currentServer.getRaftConfig().getHeartbeatInternal();

        // 心跳广播任务需要以固定频率执行(scheduleAtFixedRate)
        scheduledExecutorService.scheduleAtFixedRate(
            new HeartbeatBroadcastTask(currentServer,this), 0, HeartbeatInternal, TimeUnit.SECONDS);
    }
}
Copy<br>/**
 * leader心跳广播任务
 * */
public class HeartbeatBroadcastTask implements Runnable{

    private static final Logger logger = LoggerFactory.getLogger(HeartbeatBroadcastTask.class);

    private final RaftServer currentServer;
    private final RaftHeartbeatBroadcastModule raftHeartbeatBroadcastModule;

    private int HeartbeatCount = 0;

    public HeartbeatBroadcastTask(RaftServer currentServer, RaftHeartbeatBroadcastModule raftHeartbeatBroadcastModule) {
        this.currentServer = currentServer;
        this.raftHeartbeatBroadcastModule = raftHeartbeatBroadcastModule;
    }

    @Override
    public void run() {
        if(currentServer.getServerStatusEnum() != ServerStatusEnum.LEADER){
            // 只有leader才需要广播心跳
            return;
        }

        // 心跳广播
        doHeartbeatBroadcast(currentServer);
    }

    /**
     * 做心跳广播
     * @return 是否大多数节点依然认为自己是leader
     * */
    public static boolean doHeartbeatBroadcast(RaftServer currentServer){
        logger.info("do HeartbeatBroadcast start {}",currentServer.getServerId());

        // 先刷新自己的心跳时间
        currentServer.getRaftLeaderElectionModule().refreshLastHeartbeatTime();

        // 并行的发送心跳rpc给集群中的其它节点
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();
        List&lt;Future&lt;AppendEntriesRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        // 构造请求参数(心跳rpc，entries为空)
        AppendEntriesRpcParam appendEntriesRpcParam = new AppendEntriesRpcParam();
        appendEntriesRpcParam.setTerm(currentServer.getCurrentTerm());
        appendEntriesRpcParam.setLeaderId(currentServer.getServerId());

        for(RaftService node : otherNodeInCluster){
            Future&lt;AppendEntriesRpcResult&gt; future = currentServer.getRaftHeartbeatBroadcastModule().getRpcThreadPool().submit(
                ()-&gt; {
                    AppendEntriesRpcResult rpcResult = node.appendEntries(appendEntriesRpcParam);
                    // rpc交互时任期高于当前节点任期的处理
                    currentServer.processCommunicationHigherTerm(rpcResult.getTerm());
                    return rpcResult;
                }
            );

            futureList.add(future);
        }

        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = CommonUtil.concurrentGetRpcFutureResult("doHeartbeatBroadcast",futureList,
            currentServer.getRaftHeartbeatBroadcastModule().getRpcThreadPool(),1, TimeUnit.SECONDS);

        // 通知成功的数量(+1包括自己)
        int successResponseCount = (int) (appendEntriesRpcResultList.stream().filter(AppendEntriesRpcResult::isSuccess).count() + 1);
        if(successResponseCount &gt;= currentServer.getRaftConfig().getMajorityNum()
            &amp;&amp; currentServer.getServerStatusEnum() == ServerStatusEnum.LEADER){
            // 大多数节点依然认为自己是leader,并且广播的节点中没有人任期高于当前节点，让当前节点主动让位
            return true;
        }else{
            // 大多数节点不认为自己是leader（包括广播超时等未接到响应的场景，也认为是广播失败）
            return false;
        }
    }
}
Copy<br><br>处理requestVote请求<br>
<br>MyRaft处理requestVote的逻辑在上面提到的RaftLeaderElectionModule的requestVoteProcess方法中。<br>
raft需要保证每个任期都只能选出一个leader，所以对于特定的term任期需要做到只有一个节点能获得超过半数选票。
<br>因此，requestVoteProcess中会对发起投票的candidate和自己本地的term值进行比对，如果term值比自己低就直接拒绝(过去的leader不是leader，只有现在的leader才是leader)。<br>
每个节点只有一票，如果term值相同则需要确保自己在此之前没有投票给其它candidate。
<br>处理心跳请求<br>
<br>MyRaft的处理心跳请求的逻辑在RaftServer类的appendEntries中。由于lab1没有日志复制的功能，所以认为收到的请求都是心跳请求。
<br>appendEntries中，同样需要比对参数中term和本地term的值，尽可能的确保是真正的leader发来的心跳。<br>
如果校验通过了，则将本地的最后接受到的心跳时间刷新为当前时间，来抑制选举超时检查任务中触发新一轮选举。
<br>public class RaftServer implements RaftService {

    private static final Logger logger = LoggerFactory.getLogger(RaftServer.class);

    /**
     * 当前服务节点的id(集群内全局唯一)
     * */
    private final String serverId;

    /**
     * Raft服务端配置
     * */
    private final RaftConfig raftConfig;

    /**
     * 当前服务器的状态
     * */
    private volatile ServerStatusEnum serverStatusEnum;

    /**
     * raft服务器元数据(当前任期值currentTerm、当前投票给了谁votedFor)
     * */
    private final RaftServerMetaDataPersistentModule raftServerMetaDataPersistentModule;

    /**
     * 当前服务认为的leader节点的Id
     * */
    private volatile String currentLeader;

    /**
     * 集群中的其它raft节点服务
     * */
    protected List&lt;RaftService&gt; otherNodeInCluster;

    private RaftLeaderElectionModule raftLeaderElectionModule;
    private RaftHeartbeatBroadcastModule raftHeartbeatBroadcastModule;

    public RaftServer(RaftConfig raftConfig) {
        this.serverId = raftConfig.getServerId();
        this.raftConfig = raftConfig;
        // 初始化时都是follower
        this.serverStatusEnum = ServerStatusEnum.FOLLOWER;

        // 服务器元数据模块
        this.raftServerMetaDataPersistentModule = new RaftServerMetaDataPersistentModule(raftConfig.getServerId());
    }

    public void init(List&lt;RaftService&gt; otherNodeInCluster){
        // 集群中的其它节点服务
        this.otherNodeInCluster = otherNodeInCluster;

        raftLeaderElectionModule = new RaftLeaderElectionModule(this);
        raftHeartbeatBroadcastModule = new RaftHeartbeatBroadcastModule(this);

        logger.info("raft server init end! otherNodeInCluster={}, currentServerId={}",otherNodeInCluster,serverId);
    }

    @Override
    public RequestVoteRpcResult requestVote(RequestVoteRpcParam requestVoteRpcParam) {
        RequestVoteRpcResult requestVoteRpcResult = raftLeaderElectionModule.requestVoteProcess(requestVoteRpcParam);

        processCommunicationHigherTerm(requestVoteRpcParam.getTerm());

        logger.info("do requestVote requestVoteRpcParam={},requestVoteRpcResult={}, currentServerId={}",
            requestVoteRpcParam,requestVoteRpcResult,this.serverId);

        return requestVoteRpcResult;
    }

    @Override
    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        if(appendEntriesRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("doAppendEntries term &lt; currentTerm");
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
        }

        if(appendEntriesRpcParam.getTerm() &gt;= this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // appendEntries请求中任期值如果大于自己，说明已经有一个更新的leader了，自己转为follower，并且以对方更大的任期为准
            this.serverStatusEnum = ServerStatusEnum.FOLLOWER;
            this.currentLeader = appendEntriesRpcParam.getLeaderId();
            this.raftServerMetaDataPersistentModule.setCurrentTerm(appendEntriesRpcParam.getTerm());
        }

        // 来自leader的心跳处理，清理掉之前选举的votedFor
        this.cleanVotedFor();
        // entries为空，说明是心跳请求，刷新一下最近收到心跳的时间
        raftLeaderElectionModule.refreshLastHeartbeatTime();

        // 心跳请求，直接返回
        return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),true);
    }
}
Copy<br><br>在工程的test目录下，可以启动一个5节点的MyRaft的服务集群(用main方法启动即可)，通过修改其中的RaftClusterGlobalConfig类可以修改相关的配置。<br>
<img alt="Pasted image 20240725172328.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172328.png"><br>public class RaftClusterGlobalConfig {
    
    public static Registry registry = RegistryFactory.getRegistry(
        new RegistryConfig(RegistryCenterTypeEnum.FAKE_REGISTRY.getCode(), "127.0.0.1:2181"));

    /**
     * raft的集群配置
     * */
    public static final List&lt;RaftNodeConfig&gt; raftNodeConfigList = Arrays.asList(
        new RaftNodeConfig("raft-1","127.0.0.1",8001)
        ,new RaftNodeConfig("raft-2","127.0.0.1",8002)
        ,new RaftNodeConfig("raft-3","127.0.0.1",8003)
        ,new RaftNodeConfig("raft-4","127.0.0.1",8004)
        ,new RaftNodeConfig("raft-5","127.0.0.1",8005)
    );

    public static final int electionTimeout = 3;

    public static final Integer debugElectionTimeout = null;

    public static final int HeartbeatInterval = 1;

    /**
     * N次心跳后，leader会自动模拟出现故障(退回follow，停止心跳广播)
     * N&lt;=0代表不触发自动模拟故障
     */
    public static final int leaderAutoFailCount = 0;

    /**
     * 随机化的选举超时时间(毫秒)
     * */
    public static final Range&lt;Integer&gt; electionTimeoutRandomRange = new Range&lt;&gt;(150,500);

    public static void initRaftRpcServer(String serverId){
        RaftNodeConfig currentNodeConfig = RaftClusterGlobalConfig.raftNodeConfigList
            .stream().filter(item-&gt;item.getServerId().equals(serverId)).findAny()
            .orElseThrow(() -&gt; new MyRaftException("serverId must in raftNodeConfigList"));

        List&lt;RaftNodeConfig&gt; otherNodeList = RaftClusterGlobalConfig.raftNodeConfigList
            .stream().filter(item-&gt;!item.getServerId().equals(serverId)).collect(Collectors.toList());

        RaftConfig raftConfig = new RaftConfig(
            currentNodeConfig,RaftClusterGlobalConfig.raftNodeConfigList);
        raftConfig.setElectionTimeout(RaftClusterGlobalConfig.electionTimeout);
        raftConfig.setDebugElectionTimeout(RaftClusterGlobalConfig.debugElectionTimeout);

        raftConfig.setHeartbeatInternal(RaftClusterGlobalConfig.HeartbeatInterval);
        raftConfig.setLeaderAutoFailCount(RaftClusterGlobalConfig.leaderAutoFailCount);
        // 随机化选举超时时间的范围
        raftConfig.setElectionTimeoutRandomRange(RaftClusterGlobalConfig.electionTimeoutRandomRange);

        RaftRpcServer raftRpcServer = new RaftRpcServer(raftConfig, RaftClusterGlobalConfig.registry);

        List&lt;RaftService&gt; raftServiceList = raftRpcServer.getRpcProxyList(otherNodeList);
        // raft服务，启动！
        raftRpcServer.init(raftServiceList);
    }
}
Copy<br>验证lab1中MyRaft leader选举实现的正确性，可以通过以下几个case简单的验证下：<br>
<br>启动5个节点，看看是否能够在短时间内选举出一个leader，leader是否能抑制后续的选举(leader定时心跳有日志能观察到)。
<br>将leader杀掉(5节点集群最多能容忍2个节点故障)，看是否在选举超时后触发新一轮选举，并且成功选出新的leader。
<br>将之前杀掉的leader再启动，看能否成功的回到集群中。
<br><br>在原始的raft算法的leader选举中存在一个问题。具体场景举例如下：<br>
<br>一个5节点的raft集群，突然其中2个follower节点与另外三个节点(包含当前leader)之间出现了网络分区，不同网络分区的节点无法正常通信。
<br>此时3节点所在的网络分区是多数分区，因此可以正常工作。而2个节点所在的分区是少数分区，由于无法接到leader心跳而触发新的选举。<br>
raft的论文中提到，发起新选举需要先将自己的任期值term自增1，然后发起并行的requestVote。<br>
但此时2节点所在的少数分区是无法成功获得大多数选票的，因此在这个分区中的节点会不断的发起一轮又一轮的leader选举，term值也会在很短的时间内快速增长。
<br>在一段时间后网络分区问题恢复后，集群中的所有节点又能互相通信了，此时少数分区节点的term值大概率远大于正常工作的多数分区中的节点。<br>
在少数分区节点收到来自多数分区节点的leader的rpc请求时，其会响应一个更大的term值。此时位于多数分区中的leader会因为响应的term值高于自己而主动退位，集群内会发起一轮新的选举。
<br>从本质上来说，这个分区恢复后进行的新选举是无意义的。且由于进行选举会造成集群短暂的不可用，因此最好能避免这个问题。<br>业界给出的解决方法是在真正的选举前先发起一轮预选举(preVote)。<br>
<br>预选举的操作和选举一样，也是并行的发起requestVote请求，主要的区别在于发起预选举的节点并不事先将term值自增，而是维持不变。节点的状态也在candidate的基础上新增了一个preCandidate状态。
<br>发起预选举的节点需要根据预选举中发起的并行requestVote结果来决定是否开启实际的leader选举。<br>
如果预选举中发起的并行requestVote得到了多数票，则可以接着发起实际的选举。而如果没有得到多数票，则不进行实际的选举。
<br>引入了预选举的机制后，上面所说的网络分区发生时，少数分区的节点由于无法在预选举中获得大多数票，因此只会不断的发起一轮又一轮的预选举。<br>
因此，其term值不会不断增加而是一直维持在分区发生时的值。在分区问题恢复后，其term值一定是小于或等于多数分区内leader的term值，而不会进行一轮无效的选举，从而解决上述的问题。<br>
但需要注意的是，引入预选举机制也会增加正常状况下发起正常选举的开销。
<br>MyRaft为了保持实现的简单性，并没有实现预选举机制。但etcd、sofa-jraft等流行的开源raft系统都是实现了预选举优化的，所以在这里还是简单介绍一下。<br><br>
<br>作为手写raft系列博客的第一篇，在博客的第一、二节简单介绍了raft算法和MyRaft，第3节则详细分析了leader选举的关键细节并基于源码详细分析了MyRaft是如何实现leader选举的。
<br>单纯实现Raft的leader选举并没有什么难度。以我个人的实践经验来说，真正的困难之处在于后续功能的叠加。<br>
由于raft的论文中介绍的几个模块彼此之间是紧密关联的。因此后续日志复制、日志压缩以及成员动态变更这几个功能的逐步实现中，每完成一个都会对上个版本的代码在细节上有不小的调整，大大增加了整体的复杂度。
<br>博客中展示的完整代码在我的github上：<a rel="noopener" class="external-link" href="https://github.com/1399852153/MyRaft" target="_blank">https://github.com/1399852153/MyRaft</a> (release/lab1_leader_election分支)，希望能帮助到对raft算法感兴趣的小伙伴。<br>
内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/raft实现-一致性算法介绍.html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft实现/Raft实现 - 一致性算法介绍.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 25 Jul 2024 09:23:38 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172248.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172248.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Raft实现 - 实现日志压缩]]></title><description><![CDATA[ 
 <br><br>我们知道raft协议是基于日志复制的协议，日志数据是raft的核心。但随着raft集群的持续工作，raft的日志文件将会维护越来越多的日志，而这会带来以下几个问题。<br>
<br>日志文件过大会占用所在机器过多的本地磁盘空间。
<br>对于新加入集群的follower，leader与该follower之间完成日志同步会非常缓慢。
<br>对于自身不进行持久化的状态机，raft节点重启后回放日志也会非常缓慢。
<br>考虑到绝大多数的状态机中存储的数据并不都是新增，而更多的是对已有数据的更新，则状态机中所存储的数据量通常会远小于raft日志的总大小。例如K/V数据库，对相同key的N次操作(整体更新操作)，只有最后一次操作是实际有效的，而在此之前的针对该key的raft日志其实已经没有保存的必要了。<br>
因此raft的作者在论文的日志压缩一节中提到了几种日志压缩的算法(基于快照的、基于LSM树的)，raft选择了更容易理解和实现的、基于状态机快照的算法作为日志压缩的基础。<br><br>raft日志压缩实现中有以下几个关键点：<br>
<br>raft的各个节点可以按照某种策略独立的生成快照(比如定期检测日志文件大小是否超过阈值)，快照的主要内容是状态机当前瞬间所维护的所有数据的快照。<br>
MyRaft的状态机是一个纯内存的K/V数据库，所以快照就是内存中对应Map数据序列化后的内容。
<br>当前状态机中的快照实际上等同于所有已提交日志的顺序执行的最终结果，快照文件生成后会将所有已提交的日志全部删除以达成压缩的目的。<br>
而在处理appendEntries时，leader需要在参数中设置当前传输日志的前一条日志的index和term值，如果此时leader前一条日志恰好是已提交的并且被压缩到快照里而被删除了，则获取不到这个值了。<br>
相对应的，follower也可能出现类似的情况，即当前所有日志都是已提交的并且由于日志压缩被删除了，进行prevIndex/prevTerm校验时，也需要这个数据。<br>
因此，最终的快照中包含了最后一条已提交日志的index和term值这一关键的元数据。
<br>在leader和日志进度较慢的follower进行通信时，如果follower所需要的日志是很早的，而leader这边对应index的日志又被快照压缩而删除了，没法通过appendEntries进行同步。<br>
raft对此新增加了一个rpc接口installSnapshot专门用于解决这个问题。在leader发现follower所需的日志已经被自己压缩到快照里时，则会通过installSnapshot将自己完整的快照直接复制给follower。<br>
由于快照可能很大，所以installSnapshot一次只会传输少量的日志，通过多次的交互后完成整个快照的安装。当follower侧完成了快照同步后，后续所需要同步的日志就都是leader日志文件中还保留的，后续的日志接着使用appendEntries同步即可。
<br>下面开始结合源码分析MyRaft的日志压缩功能<br><br>
<br>raft快照数据由RaftSnapshot对象承载，除了二进制的状态机状态数据外，还包括了快照最后一条日志的index和term的值。
<br>MyRaft关于快照数据读写的逻辑集中维护在SnapshotModule中，简单起见使用一把全局的读写锁来防止并发而不过多的考虑性能。<br>
在SnapshotModule中通过引入临时文件的方式来解决新快照文件在生成过程中可能突然宕机的问题。
<br>/**
 * raft快照对象
 * */
public class RaftSnapshot {

    /**
     * 快照所包含的最后一条log的索引编号
     * */
    private long lastIncludedIndex;

    /**
     * 快照所包含的最后一条log的任期编号
     * */
    private int lastIncludedTerm;

    /**
     * 快照数据
     * (注意：暂不考虑快照过大导致byte数组放不下的情况)
     * */
    private byte[] snapshotData = new byte[0];
}
Copy<br><img alt="Pasted image 20240725172559.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172559.png"><br>public class SnapshotModule {
    private static final Logger logger = LoggerFactory.getLogger(SnapshotModule.class);

    private final RaftServer currentServer;

    private final File snapshotFile;

    private final ReentrantReadWriteLock reentrantLock = new ReentrantReadWriteLock();
    private final ReentrantReadWriteLock.WriteLock writeLock = reentrantLock.writeLock();
    private final ReentrantReadWriteLock.ReadLock readLock = reentrantLock.readLock();

    private static final String snapshotFileName = "snapshot.txt";
    private static final String snapshotTempFileName = "snapshot-temp.txt";

    /**
     * 存放快照实际数据的偏移量(lastIncludedIndex + lastIncludedTerm 共两个字段后存放快照)
     * */
    private static final int actualDataOffset = 4 + 8;

    public SnapshotModule(RaftServer currentServer) {
        this.currentServer = currentServer;

        // 保证目录是存在的
        String snapshotFileDir = getSnapshotFileDir();
        new File(snapshotFileDir).mkdirs();

        snapshotFile = new File(snapshotFileDir + File.separator + snapshotFileName);

        File snapshotTempFile = new File(snapshotFileDir + File.separator + snapshotTempFileName);

        if(!snapshotFile.exists() &amp;&amp; snapshotTempFile.exists()){
            // 快照文件不存在，但是快照的临时文件存在。说明在写完临时文件并重命名之前宕机了(临时文件是最新的完整快照)

            // 将tempFile重命名为快照文件
            snapshotTempFile.renameTo(snapshotFile);

            logger.info("snapshot-temp file rename to snapshot file success!");
        }
    }

    /**
     * 持久化一个新的快照文件
     * (暂不考虑快照太大的问题)
     * */
    public void persistentNewSnapshotFile(RaftSnapshot raftSnapshot){
        logger.info("do persistentNewSnapshotFile raftSnapshot={}",raftSnapshot);
        writeLock.lock();

        try {
            String userPath = getSnapshotFileDir();

            // 新的文件名是tempFile
            String newSnapshotFilePath = userPath + File.separator + snapshotTempFileName;
            logger.info("do persistentNewSnapshotFile newSnapshotFilePath={}", newSnapshotFilePath);

            File snapshotTempFile = new File(newSnapshotFilePath);
            try (RandomAccessFile newSnapshotFile = new RandomAccessFile(snapshotTempFile, "rw")) {
                MyRaftFileUtil.createFile(snapshotTempFile);

                newSnapshotFile.writeInt(raftSnapshot.getLastIncludedTerm());
                newSnapshotFile.writeLong(raftSnapshot.getLastIncludedIndex());
                newSnapshotFile.write(raftSnapshot.getSnapshotData());

                logger.info("do persistentNewSnapshotFile success! raftSnapshot={}", raftSnapshot);
            } catch (IOException e) {
                throw new MyRaftException("persistentNewSnapshotFile error", e);
            }

            // 先删掉原来的快照文件，然后把临时文件重名名为快照文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
            snapshotFile.delete();
            snapshotTempFile.renameTo(snapshotFile);
        }finally {
            writeLock.unlock();
        }
    }

    /**
     * 安装快照
     * */
    public void appendInstallSnapshot(InstallSnapshotRpcParam installSnapshotRpcParam){
        logger.info("do appendInstallSnapshot installSnapshotRpcParam={}",installSnapshotRpcParam);
        writeLock.lock();

        String userPath = getSnapshotFileDir();

        // 新的文件名是tempFile
        String newSnapshotFilePath = userPath + File.separator + snapshotTempFileName;
        logger.info("do appendInstallSnapshot newSnapshotFilePath={}", newSnapshotFilePath);

        File snapshotTempFile = new File(newSnapshotFilePath);
        try (RandomAccessFile newSnapshotFile = new RandomAccessFile(snapshotTempFile, "rw")) {
            MyRaftFileUtil.createFile(snapshotTempFile);

            if(installSnapshotRpcParam.getOffset() == 0){
                newSnapshotFile.setLength(0);
            }

            newSnapshotFile.seek(0);
            newSnapshotFile.writeInt(installSnapshotRpcParam.getLastIncludedTerm());
            newSnapshotFile.writeLong(installSnapshotRpcParam.getLastIncludedIndex());

            // 文件指针偏移，找到实际应该写入快照数据的地方
            newSnapshotFile.seek(actualDataOffset + installSnapshotRpcParam.getOffset());
            // 写入快照数据
            newSnapshotFile.write(installSnapshotRpcParam.getData());

            logger.info("do appendInstallSnapshot success! installSnapshotRpcParam={}", installSnapshotRpcParam);
        } catch (IOException e) {
            throw new MyRaftException("appendInstallSnapshot error", e);
        } finally {
            writeLock.unlock();
        }

        if(installSnapshotRpcParam.isDone()) {
            writeLock.lock();
            try {
                // 先删掉原来的快照文件，然后把临时文件重名名为快照文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
                snapshotFile.delete();
                snapshotTempFile.renameTo(snapshotFile);
            } finally {
                writeLock.unlock();
            }
        }
    }

    /**
     * 没有实际快照数据，只有元数据
     * */
    public RaftSnapshot readSnapshotMetaData(){
        if(this.snapshotFile.length() == 0){
            return null;
        }

        readLock.lock();

        try(RandomAccessFile latestSnapshotRaFile = new RandomAccessFile(this.snapshotFile, "r")) {
            RaftSnapshot raftSnapshot = new RaftSnapshot();
            raftSnapshot.setLastIncludedTerm(latestSnapshotRaFile.readInt());
            raftSnapshot.setLastIncludedIndex(latestSnapshotRaFile.readLong());

            return raftSnapshot;
        } catch (IOException e) {
            throw new MyRaftException("readSnapshotNoData error",e);
        } finally {
            readLock.unlock();
        }
    }

    public RaftSnapshot readSnapshot(){
        logger.info("do readSnapshot");

        if(this.snapshotFile.length() == 0){
            logger.info("snapshotFile is empty!");
            return null;
        }

        readLock.lock();

        try(RandomAccessFile latestSnapshotRaFile = new RandomAccessFile(this.snapshotFile, "r")) {
            logger.info("do readSnapshot");

            RaftSnapshot latestSnapshot = new RaftSnapshot();
            latestSnapshot.setLastIncludedTerm(latestSnapshotRaFile.readInt());
            latestSnapshot.setLastIncludedIndex(latestSnapshotRaFile.readLong());

            // 读取snapshot的实际数据(简单起见，暂不考虑快照太大内存溢出的问题，可以优化为按照偏移量多次读取)
            byte[] snapshotData = new byte[(int) (this.snapshotFile.length() - actualDataOffset)];
            latestSnapshotRaFile.read(snapshotData);
            latestSnapshot.setSnapshotData(snapshotData);

            logger.info("readSnapshot success! readSnapshot={}",latestSnapshot);
            return latestSnapshot;
        } catch (IOException e) {
            throw new MyRaftException("readSnapshot error",e);
        } finally {
            readLock.unlock();
        }
    }

    private String getSnapshotFileDir(){
        return System.getProperty("user.dir")
            + File.separator + currentServer.getServerId()
            + File.separator + "snapshot";
    }
}
Copy<br><br>
<br>相比lab2，lab3中MyRaft的日志模块新增加了一个定时任务，用于检查当前日志文件的大小是否超过了指定的阈值，如果超过了阈值则会触发生成新日志快照的逻辑。<br>
和快照模块类似，考虑到日志文件压缩时可能宕机的问题，同样采用引入临时文件的方法解决。
<br>生成快照的逻辑里先将新的快照通过SnapshotModule持久化，然后将当前已提交的日志从日志文件中删除掉。<br>
日志文件是从前写到后的，直接操作原日志文件会比较麻烦和危险。因此MyRaft将所有未提交的日志写入一个新的临时日志文件后，再通过一次文件名的切换实现对已提交日志的删除。
<br>   /**
   * 构建快照的检查
   * */
    private void buildSnapshotCheck() {
        try {
            if(!readLock.tryLock(1,TimeUnit.SECONDS)){
                logger.info("buildSnapshotCheck lock fail, quick return!");
                return;
            }
        } catch (InterruptedException e) {
            throw new MyRaftException("buildSnapshotCheck tryLock error!",e);
        }

        try {
            long logFileLength = this.logFile.length();
            long logFileThreshold = currentServer.getRaftConfig().getLogFileThreshold();
            if (logFileLength &lt; logFileThreshold) {
                logger.info("logFileLength not reach threshold, do nothing. logFileLength={},threshold={}", logFileLength, logFileThreshold);
                return;
            }

            logger.info("logFileLength already reach threshold, start buildSnapshot! logFileLength={},threshold={}", logFileLength, logFileThreshold);

            byte[] snapshot = currentServer.getKvReplicationStateMachine().buildSnapshot();
            LogEntry lastCommittedLogEntry = readLocalLog(this.lastCommittedIndex);

            RaftSnapshot raftSnapshot = new RaftSnapshot();
            raftSnapshot.setLastIncludedTerm(lastCommittedLogEntry.getLogTerm());
            raftSnapshot.setLastIncludedIndex(lastCommittedLogEntry.getLogIndex());
            raftSnapshot.setSnapshotData(snapshot);

            // 持久化最新的一份快照
            currentServer.getSnapshotModule().persistentNewSnapshotFile(raftSnapshot);
        }finally {
            readLock.unlock();
        }

        try {
            buildNewLogFileRemoveCommittedLog();
        } catch (IOException e) {
            logger.error("buildNewLogFileRemoveCommittedLog error",e);
        }
    }
Copy<br>   /**
     * 构建一个删除了已提交日志的新日志文件(日志压缩到快照里了)
     * */
    private void buildNewLogFileRemoveCommittedLog() throws IOException {
        long lastCommitted = getLastCommittedIndex();
        long lastIndex = getLastIndex();

        // 暂不考虑读取太多日志造成内存溢出的问题
        List&lt;LocalLogEntry&gt; logEntryList;
        if(lastCommitted == lastIndex){
            // (lastCommitted == lastIndex) 所有日志都提交了，创建一个空的新日志文件
            logEntryList = new ArrayList&lt;&gt;();
        }else{
            // 还有日志没提交，把没提交的记录到新的日志文件中
            logEntryList = readLocalLog(lastCommitted+1,lastIndex);
        }

        File tempLogFile = new File(getLogFileDir() + File.separator + logTempFileName);
        MyRaftFileUtil.createFile(tempLogFile);
        try(RandomAccessFile randomAccessTempLogFile = new RandomAccessFile(tempLogFile,"rw")) {

            long currentOffset = 0;
            for (LogEntry logEntry : logEntryList) {
                // 写入日志
                writeLog(randomAccessTempLogFile, logEntry, currentOffset);

                currentOffset = randomAccessTempLogFile.getFilePointer();
            }

            this.currentOffset = currentOffset;
        }

        File tempLogMeteDataFile = new File(getLogFileDir() + File.separator + logMetaDataTempFileName);
        MyRaftFileUtil.createFile(tempLogMeteDataFile);

        // 临时的日志元数据文件写入数据
        refreshMetadata(tempLogMeteDataFile,currentOffset);

        writeLock.lock();
        try{
            // 先删掉原来的日志文件，然后把临时文件重名名为日志文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
            this.logFile.delete();
            boolean renameLogFileResult = tempLogFile.renameTo(this.logFile);
            if(!renameLogFileResult){
                logger.error("renameLogFile error!");
            }

            // 先删掉原来的日志元数据文件，然后把临时文件重名名为日志元数据文件(delete后、重命名前可能宕机，但是没关系，重启后构造方法里做了对应处理)
            this.logMetaDataFile.delete();
            boolean renameTempLogMeteDataFileResult = tempLogMeteDataFile.renameTo(this.logMetaDataFile);
            if(!renameTempLogMeteDataFileResult){
                logger.error("renameTempLogMeteDataFile error!");
            }
        }finally {
            writeLock.unlock();
        }
    }
Copy<br><br><br>相比lab2，在引入了快照压缩功能后，leader侧的日志复制逻辑需要进行一点小小的拓展。<br>
即当要向follower同步某一条日志时，对应日志可能已经被压缩掉了，因此此时需要改为使用installSnapshotRpc来完成快照的安装。<br><img alt="Pasted image 20240725172616.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172616.png"><br>   /**
     * leader向集群广播，令follower复制新的日志条目
     * */
    public List&lt;AppendEntriesRpcResult&gt; replicationLogEntry(LogEntry lastEntry) {
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();

        List&lt;Future&lt;AppendEntriesRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        for(RaftService node : otherNodeInCluster){
            // 并行发送rpc，要求follower复制日志
            Future&lt;AppendEntriesRpcResult&gt; future = this.rpcThreadPool.submit(()-&gt;{
                logger.info("replicationLogEntry start!");

                long nextIndex = this.currentServer.getNextIndexMap().get(node);

                AppendEntriesRpcResult finallyResult = null;

                // If last log index ≥ nextIndex for a follower: send AppendEntries RPC with log entries starting at nextIndex
                while(lastEntry.getLogIndex() &gt;= nextIndex){
                    AppendEntriesRpcParam appendEntriesRpcParam = new AppendEntriesRpcParam();
                    appendEntriesRpcParam.setLeaderId(currentServer.getServerId());
                    appendEntriesRpcParam.setTerm(currentServer.getCurrentTerm());
                    appendEntriesRpcParam.setLeaderCommit(this.lastCommittedIndex);

                    int appendLogEntryBatchNum = this.currentServer.getRaftConfig().getAppendLogEntryBatchNum();

                    // 要发送的日志最大index值
                    // (追进度的时候，就是nextIndex开始批量发送appendLogEntryBatchNum-1条(左闭右闭区间)；如果进度差不多那就是以lastEntry.index为界限全部发送出去)
                    long logIndexEnd = Math.min(nextIndex+(appendLogEntryBatchNum-1), lastEntry.getLogIndex());
                    // 读取出[nextIndex-1,logIndexEnd]的日志(左闭右闭区间),-1往前一位是为了读取出preLog的信息
                    List&lt;LocalLogEntry&gt; localLogEntryList = this.readLocalLog(nextIndex-1,logIndexEnd);

                    logger.info("replicationLogEntry doing! nextIndex={},logIndexEnd={},LocalLogEntryList={}",
                        nextIndex,logIndexEnd,JsonUtil.obj2Str(localLogEntryList));

                    List&lt;LogEntry&gt; logEntryList = localLogEntryList.stream()
                        .map(LogEntry::toLogEntry)
                        .collect(Collectors.toList());

                    // 索引区间大小
                    long indexRange = (logIndexEnd - nextIndex + 1);

                    // 假设索引区间大小为N，可能有三种情况
                    // 1. 查出N条日志，所需要的日志全都在本地日志文件里没有被压缩
                    // 2. 查出1至N-1条日志，部分日志被压缩到快照里 or 就是只有那么多日志(一次批量查5条，但当前总共只写入了3条)
                    // 3. 查出0条日志，需要的日志全部被压缩了(因为是先落盘再广播，如果既没有日志也没有快照那就是有bug)
                    if(logEntryList.size() == indexRange+1){
                        // 查出了区间内的所有日志(case 1)

                        logger.info("find log size match!");
                        // preLog
                        LogEntry preLogEntry = logEntryList.get(0);
                        // 实际需要传输的log
                        List&lt;LogEntry&gt; needAppendLogList = logEntryList.subList(1,logEntryList.size());
                        appendEntriesRpcParam.setEntries(needAppendLogList);
                        appendEntriesRpcParam.setPrevLogIndex(preLogEntry.getLogIndex());
                        appendEntriesRpcParam.setPrevLogTerm(preLogEntry.getLogTerm());
                    }else if(logEntryList.size() &gt; 0 &amp;&amp; logEntryList.size() &lt;= indexRange){
                        // 查出了部分日志(case 2)
                        // 新增日志压缩功能后，查出来的数据个数小于指定的区间不一定就是查到第一条数据，还有可能是日志被压缩了

                        logger.info("find log size not match!");

                        RaftSnapshot readSnapshotNoData = currentServer.getSnapshotModule().readSnapshotMetaData();
                        if(readSnapshotNoData != null){
                            logger.info("has snapshot! readSnapshotNoData={}",readSnapshotNoData);

                            // 存在快照，使用快照里保存的上一条日志信息
                            appendEntriesRpcParam.setPrevLogIndex(readSnapshotNoData.getLastIncludedIndex());
                            appendEntriesRpcParam.setPrevLogTerm(readSnapshotNoData.getLastIncludedTerm());
                        }else{
                            logger.info("no snapshot! prevLogIndex=-1, prevLogTerm=-1");

                            // 没有快照，说明恰好发送第一条日志记录(比如appendLogEntryBatchNum=5，但一共只有3条日志全查出来了)
                            // 第一条记录的prev的index和term都是-1
                            appendEntriesRpcParam.setPrevLogIndex(-1);
                            appendEntriesRpcParam.setPrevLogTerm(-1);
                        }

                        appendEntriesRpcParam.setEntries(logEntryList);
                    } else if(logEntryList.isEmpty()){
                        // 日志压缩把要同步的日志删除掉了，只能使用installSnapshotRpc了(case 3)
                        logger.info("can not find and log entry，maybe delete for log compress");
                        // 快照压缩导致leader更早的index日志已经不存在了

                        // 应该改为使用installSnapshot来同步进度
                        RaftSnapshot raftSnapshot = currentServer.getSnapshotModule().readSnapshot();
                        doInstallSnapshotRpc(node,raftSnapshot,currentServer);

                        // 走到这里，一般是成功的完成了快照的安装。目标follower目前已经有了包括lastIncludedIndex以及之前的所有日志
                        // 如果是因为成为follower快速返回，则再循环一次就结束了
                        nextIndex = raftSnapshot.getLastIncludedIndex() + 1;
                        continue;
                    } else{
                        // 走到这里不符合预期，日志模块有bug
                        throw new MyRaftException("replicationLogEntry logEntryList size error!" +
                            " nextIndex=" + nextIndex + " logEntryList.size=" + logEntryList.size());
                    }

                    logger.info("leader do appendEntries start, node={}, appendEntriesRpcParam={}",node,appendEntriesRpcParam);
                    AppendEntriesRpcResult appendEntriesRpcResult = node.appendEntries(appendEntriesRpcParam);
                    logger.info("leader do appendEntries end, node={}, appendEntriesRpcResult={}",node,appendEntriesRpcResult);

                    finallyResult = appendEntriesRpcResult;
                    // 收到更高任期的处理
                    boolean beFollower = currentServer.processCommunicationHigherTerm(appendEntriesRpcResult.getTerm());
                    if(beFollower){
                        return appendEntriesRpcResult;
                    }

                    if(appendEntriesRpcResult.isSuccess()){
                        logger.info("appendEntriesRpcResult is success, node={}",node);

                        // If successful: update nextIndex and matchIndex for follower (§5.3)

                        // 同步成功了，nextIndex递增一位
                        this.currentServer.getNextIndexMap().put(node,nextIndex+1);
                        this.currentServer.getMatchIndexMap().put(node,nextIndex);

                        nextIndex++;
                    }else{
                        // 因为日志对不上导致一致性检查没通过，同步没成功，nextIndex往后退一位

                        logger.info("appendEntriesRpcResult is false, node={}",node);

                        // If AppendEntries fails because of log inconsistency: decrement nextIndex and retry (§5.3)
                        nextIndex--;
                        this.currentServer.getNextIndexMap().put(node,nextIndex);
                    }
                }

                if(finallyResult == null){
                    // 说明有bug
                    throw new MyRaftException("replicationLogEntry finallyResult is null!");
                }

                logger.info("finallyResult={},node={}",node,finallyResult);

                return finallyResult;
            });

            futureList.add(future);
        }

        // 获得结果
        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = CommonUtil.concurrentGetRpcFutureResult(
                "do appendEntries", futureList,
                this.rpcThreadPool,2, TimeUnit.SECONDS);

        logger.info("leader replicationLogEntry appendEntriesRpcResultList={}",appendEntriesRpcResultList);

        return appendEntriesRpcResultList;
    }
Copy<br><br>前面提到，follower侧在进行日志一致性校验时，也可能出现恰好前一条日志被压缩到快照里的情况。<br>
因此需要在当前日志不存在时，尝试通过SnapshotModule读取快照数据中的前一条日志信息来进行比对。<br>    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        if(appendEntriesRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("doAppendEntries term &lt; currentTerm");
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
        }

        if(appendEntriesRpcParam.getTerm() &gt;= this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // appendEntries请求中任期值如果大于自己，说明已经有一个更新的leader了，自己转为follower，并且以对方更大的任期为准
            this.serverStatusEnum = ServerStatusEnum.FOLLOWER;
            this.currentLeader = appendEntriesRpcParam.getLeaderId();
            this.raftServerMetaDataPersistentModule.setCurrentTerm(appendEntriesRpcParam.getTerm());
        }

        if(appendEntriesRpcParam.getEntries() == null || appendEntriesRpcParam.getEntries().isEmpty()){
            // 来自leader的心跳处理，清理掉之前选举的votedFor
            this.cleanVotedFor();
            // entries为空，说明是心跳请求，刷新一下最近收到心跳的时间
            raftLeaderElectionModule.refreshLastHeartbeatTime();

            long currentLastCommittedIndex = logModule.getLastCommittedIndex();
            logger.debug("doAppendEntries heartbeat leaderCommit={},currentLastCommittedIndex={}",
                appendEntriesRpcParam.getLeaderCommit(),currentLastCommittedIndex);

            if(appendEntriesRpcParam.getLeaderCommit() &gt; currentLastCommittedIndex) {
                // 心跳处理里，如果leader当前已提交的日志进度超过了当前节点的进度，令当前节点状态机也跟上
                // 如果leaderCommit &gt;= logModule.getLastIndex(),说明当前节点的日志进度不足，但可以把目前已有的日志都提交给状态机去执行
                // 如果leaderCommit &lt; logModule.getLastIndex(),说明当前节点进度比较快，有一些日志是leader已复制但还没提交的，把leader已提交的那一部分作用到状态机就行
                long minNeedCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), logModule.getLastIndex());
                pushStatemachineApply(minNeedCommittedIndex);
            }

            // 心跳请求，直接返回
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),true);
        }

        // logEntries不为空，是真实的日志复制rpc

        logger.info("do real log append! appendEntriesRpcParam={}",appendEntriesRpcParam);
        // AppendEntry可靠性校验，如果prevLogIndex和prevLogTerm不匹配，则需要返回false，让leader发更早的日志过来
        {
            LogEntry localPrevLogEntry = logModule.readLocalLog(appendEntriesRpcParam.getPrevLogIndex());
            if(localPrevLogEntry == null){
                // 没有查到prevLogIndex对应的日志，分两种情况
                RaftSnapshot raftSnapshot = snapshotModule.readSnapshotMetaData();
                localPrevLogEntry = new LogEntry();
                if(raftSnapshot == null){
                    // 当前节点日志条目为空,又没有快照，说明完全没有日志(默认任期为-1，这个是约定)
                    localPrevLogEntry.setLogIndex(-1);
                    localPrevLogEntry.setLogTerm(-1);
                }else{
                    // 日志里没有，但是有快照(把快照里记录的最后一条日志信息与leader的参数比对)
                    localPrevLogEntry.setLogIndex(raftSnapshot.getLastIncludedIndex());
                    localPrevLogEntry.setLogTerm(raftSnapshot.getLastIncludedTerm());
                }
            }

            if (localPrevLogEntry.getLogTerm() != appendEntriesRpcParam.getPrevLogTerm()) {
                //  Reply false if log doesn’t contain an entry at prevLogIndex
                //  whose term matches prevLogTerm (§5.3)
                //  本地日志和参数中的PrevLogIndex和PrevLogTerm对不上(对应日志不存在，或者任期对不上)
                logger.info("doAppendEntries localPrevLogEntry not match, localLogEntry={}",localPrevLogEntry);

                return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
            }
        }

        // 走到这里说明找到了最新的一条匹配的记录
        logger.info("doAppendEntries localEntry is match");

        List&lt;LogEntry&gt; newLogEntryList = appendEntriesRpcParam.getEntries();

        // 1. Append any new entries not already in the log
        // 2. If an existing entry conflicts with a new one (same index but different terms),
        //    delete the existing entry and all that follow it (§5.3)
        // 新日志的复制操作（直接整个覆盖掉prevLogIndex之后的所有日志,以leader发过来的日志为准）
        logModule.writeLocalLog(newLogEntryList, appendEntriesRpcParam.getPrevLogIndex());

        // If leaderCommit &gt; commitIndex, set commitIndex = min(leaderCommit, index of last new entry)
        if(appendEntriesRpcParam.getLeaderCommit() &gt; logModule.getLastCommittedIndex()){
            // 如果leaderCommit更大，说明当前节点的同步进度慢于leader，以新的entry里的index为准(更高的index还没有在本地保存(因为上面的appendEntry有效性检查))
            // 如果index of last new entry更大，说明当前节点的同步进度是和leader相匹配的，commitIndex以leader最新提交的为准

            LogEntry lastNewEntry = newLogEntryList.get(newLogEntryList.size()-1);
            long lastCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), lastNewEntry.getLogIndex());
            pushStatemachineApply(lastCommittedIndex);
        }

        // 返回成功
        return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(), true);
    }
Copy<br><br>
<br>MyRaft中，leader侧安装快照的方法实现的比较简单，未考虑快照可能很大的情况，所以直接一股脑将整个快照文件全部读取到内存中来了(在向多个follower并发安装快照时会占用很多的内存，待优化)。
<br>在将快照读取到内存中后，通过一个循环将快照数据按照配置的block大小逐步的发送给follower。在发送完最后一个block数据后，rpc请求参数的done属性会被设置为true标识为同步完成。
<br>    public static void doInstallSnapshotRpc(RaftService targetNode, RaftSnapshot raftSnapshot, RaftServer currentServer){
        int installSnapshotBlockSize = currentServer.getRaftConfig().getInstallSnapshotBlockSize();
        byte[] completeSnapshotData = raftSnapshot.getSnapshotData();

        int currentOffset = 0;
        while(true){
            InstallSnapshotRpcParam installSnapshotRpcParam = new InstallSnapshotRpcParam();
            installSnapshotRpcParam.setLastIncludedIndex(raftSnapshot.getLastIncludedIndex());
            installSnapshotRpcParam.setTerm(currentServer.getCurrentTerm());
            installSnapshotRpcParam.setLeaderId(currentServer.getServerId());
            installSnapshotRpcParam.setLastIncludedTerm(raftSnapshot.getLastIncludedTerm());
            installSnapshotRpcParam.setOffset(currentOffset);

            // 填充每次传输的数据块
            int blockSize = Math.min(installSnapshotBlockSize,completeSnapshotData.length-currentOffset);
            byte[] block = new byte[blockSize];
            System.arraycopy(completeSnapshotData,currentOffset,block,0,blockSize);
            installSnapshotRpcParam.setData(block);

            currentOffset += installSnapshotBlockSize;
            if(currentOffset &gt;= completeSnapshotData.length){
                installSnapshotRpcParam.setDone(true);
            }else{
                installSnapshotRpcParam.setDone(false);
            }

            InstallSnapshotRpcResult installSnapshotRpcResult = targetNode.installSnapshot(installSnapshotRpcParam);

            boolean beFollower = currentServer.processCommunicationHigherTerm(installSnapshotRpcResult.getTerm());
            if(beFollower){
                // 传输过程中发现自己已经不再是leader了，快速结束
                logger.info("doInstallSnapshotRpc beFollower quick return!");
                return;
            }

            if(installSnapshotRpcParam.isDone()){
                // 快照整体安装完毕
                logger.info("doInstallSnapshotRpc isDone!");
                return;
            }
        }
    }
Copy<br><br>
<br>follower侧处理快照安装rpc的逻辑中，除了必要的对参数term大小的检查，就是简单的通过SnapshotModule完成快照的安装工作。
<br>在快照整体成功安装完成后，通过LogModule.compressLogBySnapshot方法将所有已提交的日志全都删除掉，并将之前安装好的快照整体作用到follower自己本地的状态机中。
<br>    public InstallSnapshotRpcResult installSnapshot(InstallSnapshotRpcParam installSnapshotRpcParam) {
        logger.info("installSnapshot start! serverId={},installSnapshotRpcParam={}",this.serverId,installSnapshotRpcParam);

        if(installSnapshotRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply immediately if term &lt; currentTerm
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("installSnapshot term &lt; currentTerm");
            return new InstallSnapshotRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm());
        }

        // 安装快照
        this.snapshotModule.appendInstallSnapshot(installSnapshotRpcParam);

        // 快照已经完全安装好了
        if(installSnapshotRpcParam.isDone()){
            // discard any existing or partial snapshot with a smaller index
            // 快照整体安装完毕，清理掉index小于等于快照中lastIncludedIndex的所有日志(日志压缩)
            logModule.compressLogBySnapshot(installSnapshotRpcParam);

            // Reset state machine using snapshot contents (and load snapshot’s cluster configuration)
            // follower的状态机重新安装快照
            RaftSnapshot raftSnapshot = this.snapshotModule.readSnapshot();
            kvReplicationStateMachine.installSnapshot(raftSnapshot.getSnapshotData());
        }

        logger.info("installSnapshot end! serverId={}",this.serverId);

        return new InstallSnapshotRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm());
    }
Copy<br>    /**
     * discard any existing or partial snapshot with a smaller index
     * 快照整体安装完毕，清理掉index小于等于快照中lastIncludedIndex的所有日志
     * */
    public void compressLogBySnapshot(InstallSnapshotRpcParam installSnapshotRpcParam){
        this.lastCommittedIndex = installSnapshotRpcParam.getLastIncludedIndex();
        if(this.lastIndex &lt; this.lastCommittedIndex){
            this.lastIndex = this.lastCommittedIndex;
        }

        try {
            buildNewLogFileRemoveCommittedLog();
        } catch (IOException e) {
            throw new MyRaftException("compressLogBySnapshot error",e);
        }
    }
Copy<br><br>和lab2中一样，通过启动一个raft集群并触发几个case可以验证MyRaft日志压缩功能的正确性。<br>
<br>启动5个节点，关闭快照压缩功能，正常的进行写入操作。<br>
=&gt; 状态机/日志文件的数据是否符合预期
<br>启动5个节点，开启快照压缩功能，正常的进行写入操作，当日志文件超过阈值触发日志压缩后。<br>
=&gt; 状态机/日志文件/快照文件的数据是否符合预期
<br>启动5个节点，开启快照压缩功能，正常的进行写入操作，主动关闭一个节点。再进行一些写入，令leader生成最新的快照，然后让宕机的节点回到集群，看leader是否通过快照安装来完成快照/日志的同步。<br>
=&gt; 状态机/日志文件/快照文件的数据是否符合预期
<br><br>
<br>作为手写raft系列博客的第三篇，也是目前计划中的最后一篇博客(集群动态变更功能暂不实现)。博客中介绍了Raft的日志压缩功能以及从源码层面上分析MyRaft实现日志压缩功能的细节。
<br>raft是一个相对复杂的算法，因此MyRaft在功能实现上为了追求实现的简单性，舍弃了很多性能方面的优化(比如一把全局大锁防并发，快照数据完整放到内存中处理等等)。<br>
同时分布式系统中处处有并发、机器也时刻可能宕机，需要考虑的细节繁多。所以MyRaft即使通过了我自己设计的一些单测和集成测试的case，但肯定还存在许多尚未被发现bug，还请多多指教。
<br>博客中展示的完整代码在我的github上：<a rel="noopener" class="external-link" href="https://github.com/1399852153/MyRaft" target="_blank">https://github.com/1399852153/MyRaft</a> (release/lab3_log_compaction分支)，希望能帮助到对raft算法感兴趣的小伙伴。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/raft实现-实现日志压缩.html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft实现/Raft实现 - 实现日志压缩.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 25 Jul 2024 09:26:29 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172559.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172559.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Raft实现 - 实现日志复制]]></title><description><![CDATA[ 
 <br><br>在上一篇博客中MyRaft实现了leader选举，为接下来实现日志复制功能打下了基础: <a data-tooltip-position="top" aria-label="https://www.cnblogs.com/xiaoxiongcanguan/p/17569697.html" rel="noopener" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17569697.html" target="_blank">手写raft(一) 实现leader选举</a><br>日志复制是raft最核心也是最复杂的功能，大体上来说一次正常的raft日志复制大致可以简化为以下几步完成：<br>
<br>客户端向raft集群发送一次操作请求(比如kv数据库状态机的写命令(set k1 v1))，如果接受到请求的节点是leader则受理该请求；<br>
如果不是leader则转发给自己认为的leader或者返回leader的地址让客户端向leader重新发送请求(如果过程中小概率发生了leader变更，则循环往复直到leader受理)
<br>leader在接受到请求后，生成对应的raft日志(logEntry)并追加写入到leader节点本地的日志文件中(整个过程需要加锁，防止多个请求并发而日志乱序)。
<br>leader本地日志追加写入完成后，向集群中的所有follower节点广播该日志(并行的rpc appendEntries)，follower接到该rpc请求后也将日志追加写入到自己的本地日志文件中，并返回成功。<br>
当超过半数的follower返回了成功时，leader认为该日志已经成功的复制到了集群中，可以提交到状态机中执行。<br>
如果成功的数量少于半数则认为该客户端请求失败，不提交到状态机中，并将本地写入的日志删除掉，让客户端去重试。
<br>leader提交日志给状态机后，会修改自己的lastApplied字段(最大已提交日志索引编号)，随后通过心跳等rpc交互令follower也提交本地对应索引的日志到状态机中
<br>至此，一次完整的日志复制过程完成，一切正常的情况下整个集群中所有节点的本地日志中都包含了对应请求的日志，每个节点的状态机也执行了对应日志包含的状态机指令。
<br><br><img alt="Pasted image 20240725172412.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172412.png"><br>
<br>上面关于raft日志复制功能的介绍看起来不算复杂，在一切正常的情况下系统似乎能很好的完成任务。但raft是一个分布式系统，分布式系统中会出现各种麻烦的异常情况，在上述任务的每一步、任一瞬间都可能出现网络故障、机器宕机(leader/follower都可能处理到一半就宕机)等问题，<br>
而如何在异常情况下依然能让系统正常完成任务就使得raft日志复制的逻辑变得复杂起来了。
<br>raft论文中对一些异常情况进行了简要介绍，并通过一些示例来证明算法的正确性。但具体落地到代码实现中还有更多的细节需要考虑，这也是为什么raft论文中反复强调算法易理解的重要性。
<br>因为算法容易理解，实现者就能对算法建立起直观的理解，处理异常case时就能更好的理解应该如何做以及为什么这样做是正确的。在下文中将会结合MyRaft的实现源码来详细分析raft是如何处理异常情况的。
<br><br><br>为了实现日志复制功能，lab2版本的MyRaft比起lab1额外新增了3个模块，分别是日志模块LogModule、状态机模块SimpleReplicationStateMachine和客户端模块RaftClient。<br><br>
<br>MyRaft的日志模块用于维护raft日志相关的逻辑，出于减少依赖的原因，MyRaft没有去依赖rocksDb等基于本地磁盘的数据库而是直接操作最原始的日志文件(RandomAccessFile)来实现日志存储功能。
<br>简单起见，LogModule直接通过一把全局的读写锁来防止并发问题。
<br>/**
 * raft日志条目
 * */
public class LogEntry implements Serializable {

    /**
     * 发布日志时的leader的任期编号
     * */
    private int logTerm;

    /**
     * 日志的索引编号
     * */
    private long logIndex;

    /**
     * 具体作用在状态机上的指令
     * */
    private Command command;
}
Copy<br>LogEntry在磁盘文件中存储的示意图<br>
<img alt="Pasted image 20240725172431.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172431.png"><br><a data-tooltip-position="top" aria-label="https://github.com/1399852153/MyRaft/blob/release/lab2_log_replication/raft/src/main/java/myraft/module/LogModule.java" rel="noopener" class="external-link" href="https://github.com/1399852153/MyRaft/blob/release/lab2_log_replication/raft/src/main/java/myraft/module/LogModule.java" target="_blank">logModule类源码</a><br><br>
<br>状态机模块是一个K/V数据模型，本质上就是内存中维护了一个HashMap。状态机的读写操作就是对这个HashMap的读写操作，没有额外的逻辑。
<br>同时为了方便观察状态机中的数据状态，每次进行写操作时都整体刷新这个HashMap中的数据到对应的本地文件中(简单起见暂不考虑同步刷盘的性能问题)。
<br>MyRaft是一个极简的K/V数据库，其只支持最基本的get/set命令，因此作用在状态机中的所有指令天然都是幂等的，是可重复执行的。
<br>/**
 * 指令(取决于实现)
 * */
public interface Command extends Serializable {
}

/**
 * 写操作，把一个key设置为value
 * */
public class SetCommand implements Command {

   private String key;
   private String value;
}

public class GetCommand implements Command{

   private String key;
}
Copy<br><br>
<br>Raft客户端模块就是一个rpc的客户端(不依赖注册中心，基于静态服务配置的点对点rpc)，请求时使用负载均衡随机的获得一个raft服务节点访问。
<br>public class RaftClient {

    private final Registry registry;
    private RaftService raftServiceProxy;
    private List&lt;ServiceInfo&gt; serviceInfoList;
    private final LoadBalance loadBalance = new SimpleRoundRobinBalance();

    public RaftClient(Registry registry) {
        this.registry = registry;
    }

    public void init(){
        ConsumerBootstrap consumerBootstrap = new ConsumerBootstrap()
            .registry(registry);

        // 注册消费者
        Consumer&lt;RaftService&gt; consumer = consumerBootstrap.registerConsumer(RaftService.class,new FastFailInvoker());
        this.raftServiceProxy = consumer.getProxy();
    }

    public void setRaftNodeConfigList(List&lt;RaftNodeConfig&gt; raftNodeConfigList) {
        this.serviceInfoList = raftNodeConfigList.stream().map(item-&gt;{
            ServiceInfo serviceInfo = new ServiceInfo();
            serviceInfo.setServiceName(RaftService.class.getName());
            serviceInfo.setUrlAddress(new URLAddress(item.getIp(),item.getPort()));
            return serviceInfo;
        }).collect(Collectors.toList());
    }

    public String doRequestRetry(Command command, int retryTime){
        RuntimeException ex = new RuntimeException();
        for(int i=0; i&lt;retryTime; i++){
            try {
                return doRequest(command);
            }catch (Exception e){
                ex = new RuntimeException(e);
                System.out.println("doRequestRetry error, retryTime=" + i);
            }
        }

        // n次重试后还是没成功
        throw ex;
    }

    public String doRequest(Command command){
        // 先让负载均衡选择请求任意节点
        ServiceInfo serviceInfo = loadBalance.select(this.serviceInfoList);
        ClientRequestResult clientRequestResult = doRequest(serviceInfo.getUrlAddress(),command);

        if(clientRequestResult.getLeaderAddress() == null){
            if(!clientRequestResult.isSuccess()){
                throw new MyRpcException("doRequest error!");
            }
            // 访问到了leader，得到结果
            return clientRequestResult.getValue();
        }else{
            // leaderAddress不为空，说明访问到了follower，得到follower给出的leader地址
            URLAddress urlAddress = clientRequestResult.getLeaderAddress();
            // 指定leader的地址去发起请求
            ClientRequestResult result = doRequest(urlAddress,command);

            return result.getValue();
        }
    }

    private ClientRequestResult doRequest(URLAddress urlAddress, Command command){
        // 相当于是点对点的rpc，用这种方式比较奇怪，但可以不依赖zookeeper这样的注册中心
        ConsumerRpcContextHolder.getConsumerRpcContext().setTargetProviderAddress(urlAddress);
        ClientRequestParam clientRequestParam = new ClientRequestParam(command);
        ClientRequestResult clientRequestResult = this.raftServiceProxy.clientRequest(clientRequestParam);
        ConsumerRpcContextHolder.removeConsumerRpcContext();

        return clientRequestResult;
    }
}
Copy<br><br><br>
<br>raft是一个强一致的读写模型，只有leader才能对外进行服务。因此raft服务节点收到来自客户端的请求时，需要判断一下自己是否是leader，如果不是leader就返回自己认为的leader地址给客户端，让客户端重试。<br>
raft是一个分布式模型，在出现网络分区等情况下，原来是leader的节点(term更小的老leader)可能并不是目前真正的leader，而这个情况下接到客户端请求的老leader就会错误的处理客户端的请求，因而需要额外的机制来保证raft强一致的读写特性。
<br>线性强一致的写：raft的leader节点在处理客户端请求时会加写锁(线性一致)。提交指令到状态机中执行前，会预先写入一份本地日志，并将本地日志广播到集群中的所有follower节点上。如果自己已经不再是合法的leader，则本地日志的广播是无法在超过半数的节点上执行成功的。<br>
反过来说，只要大多数的节点都成功完成了日志复制的rpc请求(appendEntries)，则该写操作就是强一致下的写，因此可以将命令安全的提交到状态机中并向客户端返回成功。
<br>线性强一致的读：强一致的读就不能让老leader处理读请求，因为很可能老leader相比实际上合法的新leader缺失了一些最新的写操作，而导致返回过时的数据(破坏了强一致读的语义)。因此对于读指令，业界提出了几种常见的确保强一致读的方案。
<br><br>MyRaft考虑到实现的简单性，选择了方案2来实现强一致的读。<br>    public ClientRequestResult clientRequest(ClientRequestParam clientRequestParam) {
        // 不是leader
        if(this.serverStatusEnum != ServerStatusEnum.LEADER){
            if(this.currentLeader == null){
                // 自己不是leader，也不知道谁是leader直接报错
                throw new MyRaftException("current node not leader，and leader is null! serverId=" + this.serverId);
            }

            RaftNodeConfig leaderConfig = this.raftConfig.getRaftNodeConfigList()
                .stream().filter(item-&gt; Objects.equals(item.getServerId(), this.currentLeader)).findAny().get();

            // 把自己认为的leader告诉客户端(也可以改为直接转发请求)
            ClientRequestResult clientRequestResult = new ClientRequestResult();
            clientRequestResult.setLeaderAddress(new URLAddress(leaderConfig.getIp(),leaderConfig.getPort()));

            logger.info("not leader response known leader, result={}",clientRequestResult);
            return clientRequestResult;
        }

        // 是leader，处理读请求(线性一致读)
        if(clientRequestParam.getCommand() instanceof GetCommand){
            // 线性强一致的读，需要先进行一次心跳广播，判断当前自己是否还是leader
            boolean stillBeLeader = HeartbeatBroadcastTask.doHeartbeatBroadcast(this);
            if(stillBeLeader){
                // 还是leader，可以响应客户端
                logger.info("do client read op, still be leader");

                // Read-only operations can be handled without writing anything into the log.
                GetCommand getCommand = (GetCommand) clientRequestParam.getCommand();

                // 直接从状态机中读取就行
                String value = this.kvReplicationStateMachine.get(getCommand.getKey());

                ClientRequestResult clientRequestResult = new ClientRequestResult();
                clientRequestResult.setSuccess(true);
                clientRequestResult.setValue(value);

                logger.info("response getCommand, result={}",clientRequestResult);

                return clientRequestResult;
            }else{
                logger.info("do client read op, not still be leader");

                // 广播后发现自己不再是leader了，报错，让客户端重新自己找leader (客户端和当前节点同时误判，小概率发生)
                throw new MyRaftException("do client read op, but not still be leader!" + this.serverId);
            }
        }

        // 自己是leader，需要处理客户端的写请求

        // 构造新的日志条目
        LogEntry newLogEntry = new LogEntry();
        newLogEntry.setLogTerm(this.raftServerMetaDataPersistentModule.getCurrentTerm());
        // 新日志的索引号为当前最大索引编号+1
        newLogEntry.setLogIndex(this.logModule.getLastIndex() + 1);
        newLogEntry.setCommand(clientRequestParam.getCommand());

        logger.info("handle setCommand, do writeLocalLog entry={}",newLogEntry);

        // 预写入日志
        logModule.writeLocalLog(Collections.singletonList(newLogEntry));

        logger.info("handle setCommand, do writeLocalLog success!");

        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = logModule.replicationLogEntry(newLogEntry);

        logger.info("do replicationLogEntry, result={}",appendEntriesRpcResultList);

        // successNum需要加上自己的1票
        long successNum = appendEntriesRpcResultList.stream().filter(AppendEntriesRpcResult::isSuccess).count() + 1;
        if(successNum &gt;= this.raftConfig.getMajorityNum()){
            // If command received from client: append entry to local log, respond after entry applied to state machine (§5.3)

            // 成功复制到多数节点

            // 设置最新的已提交索引编号
            logModule.setLastCommittedIndex(newLogEntry.getLogIndex());
            // 作用到状态机上
            this.kvReplicationStateMachine.apply((SetCommand) newLogEntry.getCommand());
            // 思考一下：lastApplied为什么不需要持久化？ 状态机指令的应用和更新lastApplied非原子性会产生什么问题？
            logModule.setLastApplied(newLogEntry.getLogIndex());

            // 返回成功
            ClientRequestResult clientRequestResult = new ClientRequestResult();
            clientRequestResult.setSuccess(true);

            return clientRequestResult;
        }else{
            // 没有成功复制到多数,返回失败
            ClientRequestResult clientRequestResult = new ClientRequestResult();
            clientRequestResult.setSuccess(false);

            // 删掉之前预写入的日志条目
            // 思考一下如果删除完成之前，宕机了有问题吗？ 个人感觉是ok的
            logModule.deleteLocalLog(newLogEntry.getLogIndex());

            return clientRequestResult;
        }
    }
Copy<br><br>下面详细介绍leader是如何向集群广播raftLog的。<br>
<br>raft的leader维护了两个非持久化的数据(Volatile state on leaders)，即在当前leader视角下follower节点同步raftLog的进度。<br>
一个数据是nextIndex，代表leader认为的follower应该接收的下一条log的索引值，leader初始化时乐观的估计设置其为leader当前最后一条日志索引值加1(代表着乐观估计follower和leader的日志进度是完全一致的)。<br>
一个数据是matchIndex，代表leader实际确认的follower已接受到的最后一条raft日志的索引值，leader初始化时悲观的将其初始化为0。<br>
由于follower是一个集合，所以论文中通过nextIndex[],matchIndex[]来描述，而在MyRaft中都用Map结构来维护。
<br>leader基于每个follower对应的nextIndex查找出所要发送的日志集合，并行的向所有follower发送appendEntries的rpc请求。<br>
当leader与follower进行rpc交互时，可能follower的日志同步进度并不像leader认为的那样乐观，很可能其实际所拥有的日志索引远小于leader最后一条日志的索引(follower侧的逻辑在下一节分析)。<br>
因此follower在这种情况下会返回失败，此时leader会将对应follower的nextIndex往回退(自减1)，循环往复的交互直到leader发送和follower所需的日志相匹配的那条日志(最坏情况下follower一条日志都没有，leader从第一条日志开始同步)
<br>当follower响应成功后，leader将会更新对应follower的nextIndex和matchIndex的值。当超过半数的follower都响应了对应index日志的appendEntries后，leader认为当前日志已经成功的复制到集群中多数的节点中了，则可以安全的将日志提交到状态机中了。
<br>/**
     * 向集群广播，令follower复制新的日志条目
     * */
    public List&lt;AppendEntriesRpcResult&gt; replicationLogEntry(LogEntry lastEntry) {
        List&lt;RaftService&gt; otherNodeInCluster = currentServer.getOtherNodeInCluster();

        List&lt;Future&lt;AppendEntriesRpcResult&gt;&gt; futureList = new ArrayList&lt;&gt;(otherNodeInCluster.size());

        for(RaftService node : otherNodeInCluster){
            // 并行发送rpc，要求follower复制日志
            Future&lt;AppendEntriesRpcResult&gt; future = this.rpcThreadPool.submit(()-&gt;{
                logger.info("replicationLogEntry start!");

                long nextIndex = this.currentServer.getNextIndexMap().get(node);

                AppendEntriesRpcResult finallyResult = null;

                // If last log index ≥ nextIndex for a follower: send AppendEntries RPC with log entries starting at nextIndex
                while(lastEntry.getLogIndex() &gt;= nextIndex){
                    AppendEntriesRpcParam appendEntriesRpcParam = new AppendEntriesRpcParam();
                    appendEntriesRpcParam.setLeaderId(currentServer.getServerId());
                    appendEntriesRpcParam.setTerm(currentServer.getCurrentTerm());
                    appendEntriesRpcParam.setLeaderCommit(this.lastCommittedIndex);

                    int appendLogEntryBatchNum = this.currentServer.getRaftConfig().getAppendLogEntryBatchNum();

                    // 要发送的日志最大index值
                    // (追进度的时候，就是nextIndex开始批量发送appendLogEntryBatchNum-1条(左闭右闭区间)；如果进度差不多那就是以lastEntry.index为界限全部发送出去)
                    long logIndexEnd = Math.min(nextIndex+(appendLogEntryBatchNum-1), lastEntry.getLogIndex());
                    // 读取出[nextIndex-1,logIndexEnd]的日志(左闭右闭区间),-1往前一位是为了读取出preLog的信息
                    List&lt;LocalLogEntry&gt; localLogEntryList = this.readLocalLog(nextIndex-1,logIndexEnd);

                    logger.info("replicationLogEntry doing! nextIndex={},logIndexEnd={},LocalLogEntryList={}",
                        nextIndex,logIndexEnd,JsonUtil.obj2Str(localLogEntryList));

                    List&lt;LogEntry&gt; logEntryList = localLogEntryList.stream()
                        .map(LogEntry::toLogEntry)
                        .collect(Collectors.toList());

                    // 索引区间大小
                    long indexRange = (logIndexEnd - nextIndex + 1);
                    if(logEntryList.size() == indexRange+1){
                        // 一般情况能查出区间内的所有日志

                        logger.info("find log size match!");
                        // preLog
                        LogEntry preLogEntry = logEntryList.get(0);
                        // 实际需要传输的log
                        List&lt;LogEntry&gt; needAppendLogList = logEntryList.subList(1,logEntryList.size());
                        appendEntriesRpcParam.setEntries(needAppendLogList);
                        appendEntriesRpcParam.setPrevLogIndex(preLogEntry.getLogIndex());
                        appendEntriesRpcParam.setPrevLogTerm(preLogEntry.getLogTerm());
                    }else if(logEntryList.size() &gt; 0 &amp;&amp; logEntryList.size() &lt;= indexRange){
                        logger.info("find log size not match!");
                        // 日志长度小于索引区间值，说明已经查到最前面的日志 (比如appendLogEntryBatchNum=5，但一共只有3条日志全查出来了)
                        appendEntriesRpcParam.setEntries(logEntryList);

                        // 约定好第一条记录的prev的index和term都是-1
                        appendEntriesRpcParam.setPrevLogIndex(-1);
                        appendEntriesRpcParam.setPrevLogTerm(-1);
                    } else{
                        // 正常情况是先持久化然后再广播同步日志，所以size肯定会大于0，也不应该超过索引区间值
                        // 走到这里不符合预期，日志模块有bug
                        throw new MyRaftException("replicationLogEntry logEntryList size error!" +
                            " nextIndex=" + nextIndex + " logEntryList.size=" + logEntryList.size());
                    }

                    logger.info("leader do appendEntries start, node={}, appendEntriesRpcParam={}",node,appendEntriesRpcParam);
                    AppendEntriesRpcResult appendEntriesRpcResult = node.appendEntries(appendEntriesRpcParam);
                    logger.info("leader do appendEntries end, node={}, appendEntriesRpcResult={}",node,appendEntriesRpcResult);

                    finallyResult = appendEntriesRpcResult;
                    // 收到更高任期的处理
                    boolean beFollower = currentServer.processCommunicationHigherTerm(appendEntriesRpcResult.getTerm());
                    if(beFollower){
                        return appendEntriesRpcResult;
                    }

                    if(appendEntriesRpcResult.isSuccess()){
                        logger.info("appendEntriesRpcResult is success, node={}",node);

                        // If successful: update nextIndex and matchIndex for follower (§5.3)

                        // 同步成功了，nextIndex递增一位
                        this.currentServer.getNextIndexMap().put(node,nextIndex+1);
                        this.currentServer.getMatchIndexMap().put(node,nextIndex);

                        nextIndex++;
                    }else{
                        // 因为日志对不上导致一致性检查没通过，同步没成功，nextIndex往后退一位

                        logger.info("appendEntriesRpcResult is false, node={}",node);

                        // If AppendEntries fails because of log inconsistency: decrement nextIndex and retry (§5.3)
                        nextIndex--;
                        this.currentServer.getNextIndexMap().put(node,nextIndex);
                    }
                }

                if(finallyResult == null){
                    // 说明有bug
                    throw new MyRaftException("replicationLogEntry finallyResult is null!");
                }

                logger.info("finallyResult={},node={}",node,finallyResult);

                return finallyResult;
            });

            futureList.add(future);
        }

        // 获得结果
        List&lt;AppendEntriesRpcResult&gt; appendEntriesRpcResultList = CommonUtil.concurrentGetRpcFutureResult(
                "do appendEntries", futureList,
                this.rpcThreadPool,2, TimeUnit.SECONDS);

        logger.info("leader replicationLogEntry appendEntriesRpcResultList={}",appendEntriesRpcResultList);

        return appendEntriesRpcResultList;
    }
Copy<br><br>
<br>相比lab1，lab2版本的appendEntries除了之前已有的针对leader任期相关的校验和处理逻辑外，还新增了日志复制相关的逻辑。
<br>raftLog是顺序保存的，为了日志复制的安全性，raft的follower节点也必须和leader保持一致，将日志按照index索引值以从小到大的顺序存在本地的raft日志文件中。<br>
因此只有在已有第0-第N条日志的情况下，follower才能够安全的将第N+1条日志追加到本地日志文件中。为此raft作者设计了一系列的校验规则来保证这一点。
<br>首先，leader在发起appendEntries命令follower复制第N条(logIndex=N)日志时，会将第前一条日志(N-1)的term值(prevLogTerm)和index值(prevLogIndex)作为参数一并传递，follower会对这两个值进行校验以决定是否能安全的复制日志。<br>
follower会查询出对应索引为prevLogIndex的日志，如果没查出来说明进度没跟上leader那自然要返回复制失败，让leader重试把logIndex更小、更前面的日志发过来。<br>
如果查出来了对应的日志则还需要进一步对比请求参数中的prevLogTerm和follower本地对应日志的term值是否一致，如果一致则校验通过；如果不一致则说明follower保存了一个和leader不一致且未最终提交的日志，也要返回失败，让leader把更前面的日志发过来(具体原理后面会分析)。
<br>当follower这边对于prevLogTerm和prevLogIndex的校验都通过了后，说明leader此时已经找到了follower恰好需要的日志(日志同步的进度匹配上了)，则follower需要将发送过来的日志写入本地日志文件中。<br>
如果leader发过来的日志里对应index的本地日志不存在则直接追加到follower本地日志文件中即可；如果之前已存在则覆盖掉原来的日志即可。
<br>当一开始follower没有追上leader的日志进度时(比如follower宕机了一段时间再回到集群)，follower会一直返回同步失败，leader则会一直向前找直到找到follower所恰好需要的那条日志。在这之后的日志同步就会十分顺利了，直到follower和leader的日志完全一致。
<br>考虑到follower可能和leader的日志进度相差过大，一次回退一个索引值的匹配策略效率并不高。论文中提到一种可行的优化是让follower直接把自己的最后一个日志，或者倒数第N个日志的信息(term和index)返回给leader，leader就能够很快的从正确的日志位点开始同步。<br>
但这样会增加程序的复杂度并降低正常情况下进度一致时日志同步的效率，论文的作者认为出现这一问题的概率很低因此该优化是不必要的，简单起见MyRaft也没有做相关的优化。
<br>follower在成功将日志落盘后，根据请求参数中的leaderCommit值和自己本地的lastCommittedIndex(最大已提交日志索引号)来判断是否应该将本地的raftLog提交到状态机中执行。<br>
如果参数leaderCommit大于本地的lastCommittedIndex，说明leader已经把一些日志提交到状态机中了，而follower还没有。那么follower需要跟上leader的进度(不但日志进度要匹配，状态机进度也要匹配)。<br>
需要提交到状态机中日志最大的index值为leaderCommit和当前最新一条日志的index最小值(index of last new entry)，如果leaderCommit是最小值，说明follower已经赶上了leader，leader提交的日志follower本地都有。<br>
而如果index of last new entry是更小的，说明当前follower还没有追上leader提交到状态机的进度，把当前已有的所有本地日志都提交到状态机中即可(pushStatemachineApply方法)。
<br><br>raft论文在第5.3节重点解释了一下原因。<br>
<br>举个例子，一个5节点的集群，节点编号分别是abcde。在任期1中a是leader，尝试向集群复制一个日志(index=1，term=1，set k1=v1)，但是复制时失败了，只有节点b成功的复制了该日志，而cde都没有持有日志。
<br>恰好这时leader a宕机了，触发新选举后节点c成为了新的leader(任期term=2)。c接受到客户端请求，也尝试向集群复制一个日志(index=1，term=2, set k1=v2)，这个时候节点b有一个index=1，这个时候节点b必须让新的日志覆盖掉老的日志才能和leader保持一致。
<br>c的这次日志复制成功了，bcde四个节点都持有了该日志，则raft集群便可以将这个日志提交到状态机中了，最后集群的所有状态机中k1的值将会是v2，而不是之前复制失败而未提交的v1。<br>
<img alt="Pasted image 20240725172453.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172453.png">
<br>    public AppendEntriesRpcResult appendEntries(AppendEntriesRpcParam appendEntriesRpcParam) {
        if(appendEntriesRpcParam.getTerm() &lt; this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 拒绝处理任期低于自己的老leader的请求

            logger.info("doAppendEntries term &lt; currentTerm");
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
        }

        if(appendEntriesRpcParam.getTerm() &gt;= this.raftServerMetaDataPersistentModule.getCurrentTerm()){
            // appendEntries请求中任期值如果大于自己，说明已经有一个更新的leader了，自己转为follower，并且以对方更大的任期为准
            this.serverStatusEnum = ServerStatusEnum.FOLLOWER;
            this.currentLeader = appendEntriesRpcParam.getLeaderId();
            this.raftServerMetaDataPersistentModule.setCurrentTerm(appendEntriesRpcParam.getTerm());
        }

        if(appendEntriesRpcParam.getEntries() == null || appendEntriesRpcParam.getEntries().isEmpty()){
            // 来自leader的心跳处理，清理掉之前选举的votedFor
            this.cleanVotedFor();
            // entries为空，说明是心跳请求，刷新一下最近收到心跳的时间
            raftLeaderElectionModule.refreshLastHeartbeatTime();

            long currentLastCommittedIndex = logModule.getLastCommittedIndex();
            logger.debug("doAppendEntries heartbeat leaderCommit={},currentLastCommittedIndex={}",
                appendEntriesRpcParam.getLeaderCommit(),currentLastCommittedIndex);

            if(appendEntriesRpcParam.getLeaderCommit() &gt; currentLastCommittedIndex) {
                // 心跳处理里，如果leader当前已提交的日志进度超过了当前节点的进度，令当前节点状态机也跟上
                // 如果leaderCommit &gt;= logModule.getLastIndex(),说明当前节点的日志进度不足，但可以把目前已有的日志都提交给状态机去执行
                // 如果leaderCommit &lt; logModule.getLastIndex(),说明当前节点进度比较快，有一些日志是leader已复制但还没提交的，把leader已提交的那一部分作用到状态机就行
                long minNeedCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), logModule.getLastIndex());
                pushStatemachineApply(minNeedCommittedIndex);
            }

            // 心跳请求，直接返回
            return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),true);
        }

        // logEntries不为空，是真实的日志复制rpc

        logger.info("do real log append! appendEntriesRpcParam={}",appendEntriesRpcParam);
        // AppendEntry可靠性校验，如果prevLogIndex和prevLogTerm不匹配，则需要返回false，让leader发更早的日志过来
        {
            LogEntry localPrevLogEntry = logModule.readLocalLog(appendEntriesRpcParam.getPrevLogIndex());
            if(localPrevLogEntry == null){
                // 当前节点日志条目为空，说明完全没有日志(默认任期为-1，这个是约定)
                localPrevLogEntry = LogEntry.getEmptyLogEntry();
            }

            if (localPrevLogEntry.getLogTerm() != appendEntriesRpcParam.getPrevLogTerm()) {
                //  Reply false if log doesn’t contain an entry at prevLogIndex
                //  whose term matches prevLogTerm (§5.3)
                //  本地日志和参数中的PrevLogIndex和PrevLogTerm对不上(对应日志不存在，或者任期对不上)
                logger.info("doAppendEntries localPrevLogEntry not match, localLogEntry={}",localPrevLogEntry);

                return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(),false);
            }
        }

        // 走到这里说明找到了最新的一条匹配的记录
        logger.info("doAppendEntries localEntry is match");

        List&lt;LogEntry&gt; newLogEntryList = appendEntriesRpcParam.getEntries();

        // 1. Append any new entries not already in the log
        // 2. If an existing entry conflicts with a new one (same index but different terms),
        //    delete the existing entry and all that follow it (§5.3)
        // 新日志的复制操作（直接整个覆盖掉prevLogIndex之后的所有日志,以leader发过来的日志为准）
        logModule.writeLocalLog(newLogEntryList, appendEntriesRpcParam.getPrevLogIndex());

        // If leaderCommit &gt; commitIndex, set commitIndex = min(leaderCommit, index of last new entry)
        if(appendEntriesRpcParam.getLeaderCommit() &gt; logModule.getLastCommittedIndex()){
            // 如果leaderCommit更大，说明当前节点的同步进度慢于leader，以新的entry里的index为准(更高的index还没有在本地保存(因为上面的appendEntry有效性检查))
            // 如果index of last new entry更大，说明当前节点的同步进度是和leader相匹配的，commitIndex以leader最新提交的为准

            LogEntry lastNewEntry = newLogEntryList.get(newLogEntryList.size()-1);
            long lastCommittedIndex = Math.min(appendEntriesRpcParam.getLeaderCommit(), lastNewEntry.getLogIndex());
            pushStatemachineApply(lastCommittedIndex);
        }

        // 返回成功
        return new AppendEntriesRpcResult(this.raftServerMetaDataPersistentModule.getCurrentTerm(), true);
    }

    private void pushStatemachineApply(long lastCommittedIndex){
        long lastApplied = logModule.getLastApplied();

        // If commitIndex &gt; lastApplied: increment lastApplied, apply log[lastApplied] to state machine (§5.3)
        if(lastApplied &lt; lastCommittedIndex){
            // 作用在状态机上的日志编号低于集群中已提交的日志编号，需要把这些已提交的日志都作用到状态机上去
            logger.info("pushStatemachineApply.apply, lastApplied={},lastCommittedIndex={}",lastApplied,lastCommittedIndex);

            // 全读取出来(读取出来是按照index从小到大排好序的)
            List&lt;LocalLogEntry&gt; logEntryList = logModule.readLocalLog(lastApplied+1,lastCommittedIndex);

            logger.info("pushStatemachineApply.apply, logEntryList={}",logEntryList);

            List&lt;SetCommand&gt; setCommandList = logEntryList.stream()
                .filter(item-&gt;item.getCommand() instanceof SetCommand)
                .map(item-&gt;(SetCommand)item.getCommand())
                .collect(Collectors.toList());

            // 按照顺序依次作用到状态机中
            this.kvReplicationStateMachine.batchApply(setCommandList);
        }

        this.logModule.setLastCommittedIndex(lastCommittedIndex);
        this.logModule.setLastApplied(lastCommittedIndex);
    }
Copy<br><br>
<br>前面的例子里提到在老leader宕机触发选举后，新的leader是可能把一些不一致的日志给覆盖清除掉以保证日志一致性。<br>
当leader广播日志并在半数以上follower成功复制后，并提交raftLog到状态机中后如果leader突然宕机了，raft是如何保证新的leader不会清理掉已提交到状态机中的日志的呢？
<br>raft的论文在5.4节安全性一节中提到了这一点，raft作者通过在leader选举过程中follower投票的环节中添加对双方日志的校验来保证已提交到状态机的日志绝对不会被覆盖。<br>
raft论文中保证安全性的核心思路共两点：一是candidate必须和超过半数的follower进行通信并得到选票；二是candidate的日志至少要和follower一样新(即follower有的日志candidate必须本地也有，反之则不需要成立)。<br>
基于这两点后能以此推导：已提交的日志一定在集群中超过半数的节点中存在 + 新当选的leader所包含的日志一定比集群中超过半数的节点更全面(至少一样全面) =&gt; 新的leader一定包含所有已提交的日志(只有包含所有已提交日志的节点才能被选为leader)
<br>那raft是如何在选举投票时令follower和candidate进行日志完整程度比对的呢？raft论文的5.4节中也提到了，具体规则如下：<br>
Raft通过比较两个节点中日志中最后一个条目的索引和任期来决定谁是最新的。

<br>如果两个日志中最后的条目有着不同的任期，则任期较后的日志是更新的。
<br>如果两个日志中最后的条目有着相同的任期，则较长的(注：索引值更大的)那个日志是更新的。


<br>candidate的requestVote的请求中会带上candidate自己最后一条日志的任期(lastLogTerm)和索引值(lastLogIndex),而处理请求的follower也需要查询出自己本地的最后一条日志出来，并基于上述规则比较到底是哪边的日志更新，更全面。<br>
只有当candidate的日志完整程度大于(更新)或等于(一样新)follower本地的日志时，follower才能将选票给到candidate。
<br>有了在选举逻辑中关于日志的完整性的校验，raft的日志复制功能就算基本完成了。而为什么这样的设计能保证日志复制的安全性，不会造成节点间数据的不一致，在raft的论文中有提到，在这里就不再赘述了。<br>
raft论文中给出的关于日志复制正确性的结论并不是那么显然(因为有不少异常的case需要琢磨)，希望读者能通过仔细推敲论文并自己动手实现raft来加深理解。
<br>在上一小节的例子中，假设任期1中的a成功的复制了日志，并且在b、c节点上复制成功，而d、e上没有复制成功。那么如果a在提交日志到状态机后宕机，则只有b、c才可能被选举为leader，因为b、c会拒绝来自d、e的requestVote(b、c的日志比d、e的新)而令其无法获得半数以上的选票，而反过来d、e则会同意投票给b、c。<br> /**
     * 处理投票请求
     * 注意：synchronized修饰防止不同candidate并发的投票申请处理，以FIFO的方式处理
     * */
    public synchronized RequestVoteRpcResult requestVoteProcess(RequestVoteRpcParam requestVoteRpcParam){
        if(this.currentServer.getCurrentTerm() &gt; requestVoteRpcParam.getTerm()){
            // Reply false if term &lt; currentTerm (§5.1)
            // 发起投票的candidate任期小于当前服务器任期，拒绝投票给它
            logger.info("reject requestVoteProcess! term &lt; currentTerm, currentServerId={}",currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 发起投票的节点任期高于当前节点，无条件投票给它(任期高的说了算)
        if(this.currentServer.getCurrentTerm() &lt; requestVoteRpcParam.getTerm()){
            // 刷新元数据
            this.currentServer.refreshRaftServerMetaData(
                new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
            // 任期没它高，自己转为follower
            this.currentServer.setServerStatusEnum(ServerStatusEnum.FOLLOWER);
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
        }

        // term任期值相同，需要避免同一任期内投票给不同的节点而脑裂
        if(this.currentServer.getVotedFor() != null &amp;&amp; !this.currentServer.getVotedFor().equals(requestVoteRpcParam.getCandidateId())){
            // If votedFor is null or candidateId（取反的卫语句）
            // 当前服务器已经把票投给了别人,拒绝投票给发起投票的candidate
            logger.info("reject requestVoteProcess! votedFor={},currentServerId={}",
                currentServer.getVotedFor(),currentServer.getServerId());
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }

        // 考虑日志条目索引以及任期值是否满足条件的情况（第5.4节中提到的安全性）
        // 保证leader必须拥有所有已提交的日志，即发起投票的candidate日志一定要比投票给它的节点更新
        LogEntry lastLogEntry = currentServer.getLogModule().getLastLogEntry();
        logger.info("requestVoteProcess lastLogEntry={}",lastLogEntry);
        if(lastLogEntry.getLogTerm() &gt; requestVoteRpcParam.getLastLogTerm()){
            // If the logs have last entries with different terms, then the log with the later term is more up-to-date.
            // 当前节点的last日志任期比发起投票的candidate更高(比candidate更新)，不投票给它
            logger.info("lastLogEntry.term &gt; candidate.lastLogTerm! voteGranted=false");
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }else if(lastLogEntry.getLogTerm() == requestVoteRpcParam.getLastLogTerm() &amp;&amp;
            lastLogEntry.getLogIndex() &gt; requestVoteRpcParam.getLastLogIndex()){
            // If the logs end with the same term, then whichever log is longer is more up-to-date.
            // 当前节点的last日志和发起投票的candidate任期一样，但是index比candidate的高(比candidate更新)，不投票给它

            logger.info("lastLogEntry.term == candidate.lastLogTerm &amp;&amp; " +
                "lastLogEntry.index &gt; candidate.lastLogIndex! voteGranted=false");
            return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),false);
        }else{
            // candidate的日志至少与当前节点一样新(或者更新)，通过检查，可以投票给它
            logger.info("candidate log at least as new as the current node, valid passed!");
        }

        // 投票校验通过,刷新元数据
        this.currentServer.refreshRaftServerMetaData(
            new RaftServerMetaData(requestVoteRpcParam.getTerm(),requestVoteRpcParam.getCandidateId()));
        this.currentServer.processCommunicationHigherTerm(requestVoteRpcParam.getTerm());
        return new RequestVoteRpcResult(this.currentServer.getCurrentTerm(),true);
    }
Copy<br><br>在raft日志复制的过程中的任意瞬间，集群中的每个节点都可能出现宕机、网络超时等异常情况。下面分析在出现这些异常时，raft是如何保证集群正常工作的。<br><br>client请求报错，client重试直到raft服务集群选举出新的leader后恢复工作。<br><br>client请求报错，重试直到选举出新leader(下面的异常情况client也是一样的处理)。<br>
raft集群会进行选举选出新leader。宕机的老leader在回到集群后对应index的本地日志将会被新leader给覆盖掉。<br><br>当leader广播将日志复制到少数节点中宕机，则可能存在两种情况。<br>
<br>成功落盘的少数节点在新一轮选举中当选leader，则宕机的老leader回到集群后已经落盘的对应raftLog会被保留下来。
<br>未成功落盘的节点当选了新leader，则宕机的老leader已经落盘的对应raftLog将会被覆盖清除掉。<br>
这两种情况都是正确的，因为未提交到状态机中的日志无论是被覆盖清除还是最终被提交，都是合理的。
<br><br>如果对应log成功复制到了多数节点中，则按照上面所分析的raft选举安全性，只有拥有最新raftLog的那多数的节点才有机会当选为新leader。<br>
因此宕机的老leader重新回到集群后，落盘的raftLog将会被保留下来。<br><br>整个集群的处理和3.4一样，宕机的老leader重新回到集群后，落盘的raftLog将会被保留下来。唯一的区别在于宕机leader节点的状态机将会重复执行同一条raftLog。<br>
解决这一问题的方法主要有两种：<br>
<br>要求状态机的具体实现能够容忍raftLog重复的执行，或者设计对相同log幂等的防护(MyRaft的方案，状态机数据不持久化并且只有纯set操作，无自增/自减等非幂等操作)。
<br>raft协议的实现中将lastApplied属性持久化，通过持久化lastApplied的方式来避免宕机恢复后重复执行日志。
<br><a data-tooltip-position="top" aria-label="https://www.zhihu.com/question/382888510/answer/2478166051" rel="noopener" class="external-link" href="https://www.zhihu.com/question/382888510/answer/2478166051" target="_blank">为什么 Raft 的 ApplyIndex 和 CommitIndex 不需要持久化？</a><br><br><br>MyRaft实现了一个非常基础的命令行交互式客户端(RpcCmdInteractiveClient)用于测试MyRaft这一kv数据库的读写功能。<br>/**
 * 命令行交互的客户端
 *
 * 只支持以下命令
 * 1. get [key]
 * 2. set [key] [value]
 * 3. quit
 * */
public class RpcCmdInteractiveClient {

    public static void main(String[] args) {
        // 客户端的超时时间必须大于raft内部rpc的超时时间，否则在节点故障时rpc会一直超时
        DefaultFuture.DEFAULT_TIME_OUT = 3000L;

        RaftClient raftClient = new RaftClient(RaftClusterGlobalConfig.registry);
        raftClient.init();
        raftClient.setRaftNodeConfigList(RaftClusterGlobalConfig.raftNodeConfigList);

        Scanner scan = new Scanner(System.in);

        System.out.println("RpcCmdInteractiveClient start, please input command:");

        while(scan.hasNext()) {
            String input = scan.nextLine();
            if(input.length() == 0){
                continue;
            }

            if (Objects.equals(input, "quit")) {
                scan.close();
                System.out.println("RpcCmdInteractiveClient quit success!");
                return;
            }

            if (input.startsWith("get")) {
                processGetCmd(raftClient,input);
            }else if(input.startsWith("set")){
                processSetCmd(raftClient,input);
            }else{
                System.out.println("un support cmd, please retry！");
            }
        }
    }

    private static void processGetCmd(RaftClient raftClient, String input){
        try {
            String[] cmdItem = input.split(" ");
            if (cmdItem.length != 2) {
                System.out.println("get cmd error, please retry！");
                return;
            }

            String key = cmdItem[1];
            String result = raftClient.doRequestRetry(new GetCommand(key),2);
            System.out.println("processGet result=" + result);
        }catch (Exception e){
            System.out.println("processGet error!");
            e.printStackTrace();
        }
    }

    private static void processSetCmd(RaftClient raftClient, String input){
        try {
            String[] cmdItem = input.split(" ");
            if (cmdItem.length != 3) {
                System.out.println("set cmd error, please retry！");
                return;
            }

            String key = cmdItem[1];
            String value = cmdItem[2];
            String result = raftClient.doRequestRetry(new SetCommand(key, value),2);
            System.out.println("processSet success=" + result);
        }catch (Exception e){
            System.out.println("processSetCmd error!");
            e.printStackTrace();
        }
    }
}
Copy<br><br><img alt="Pasted image 20240725172512.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172512.png"><br><br>在github源码中的test目录下中有RpcClientNode(1-5)类,全部以main方法启动后便可形成一个5节点的raft集群。通过RpcCmdInteractiveClient便可以通过以下几个case简单验证MyRaft关于日志复制的基本功能。<br>
<br>启动所有节点，进行一系列的读写操作。检查每个节点中日志/状态机中的数据是否符合预期
<br>将任意一个节点关闭，删除掉状态机对应的文件(相当于清空了kv状态机里的数据)，重新启动后leader的心跳会触发全量日志再一次作用到状态机中。检查状态机的数据是否符合预期
<br>将任意一个节点关闭，继续进行一系列的读写操作。然后将节点重启恢复，在进行新的写操作后，leader会将宕机时丢失的那部分日志同步到该节点，并且日志是否成功的提交到状态机中执行。检查日志/状态机的数据是否符合预期
<br><br>
<br>作为手写raft系列博客的第二篇，在博客的第1节简单介绍了raft的日志复制功能，第2节详细分析了MyRaft关于日志复制功能的实现源码，第3节通过分析日志复制过程中异常情况的处理来证明raft日志复制功能的正确性。
<br>raft的日志复制功能是raft算法中最复杂的一部分，除了正常执行逻辑以外还包含了大量异常情况的处理。在博客中我结合MyRaft的源码尽可能的将自己理解的各种细节分享出来，希望能帮到对raft实现细节、正确性证明等相关内容感兴趣的读者。
<br>博客中展示的完整代码在我的github上：<a rel="noopener" class="external-link" href="https://github.com/1399852153/MyRaft" target="_blank">https://github.com/1399852153/MyRaft</a> (release/lab2_log_replication分支)，内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/raft实现-实现日志复制.html</link><guid isPermaLink="false">Computer Science/Distributed System/Raft实现/Raft实现 - 实现日志复制.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 25 Jul 2024 09:25:24 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172412.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/distributed-system/raft实现/pasted-image-20240725172412.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[自己动手实现rpc框架(一) 实现点对点的rpc通信]]></title><description><![CDATA[ 
 <br><br>RPC是远过程调用(Remote Procedure Call)的缩写形式，其区别于一个程序内部基本的过程调用(或者叫函数/方法调用)。<br>随着应用程序变得越来越复杂，在单个机器上中仅通过一个进程来运行整个应用程序的方式已经难以满足现实中日益增长的需求。<br>
开发者对应用程序进行模块化的拆分，以分布式部署的方式来降低程序整体的复杂度和提升性能方面的可拓展性(分而治之的思想)。<br>拆分后部署在不同机器上的各个模块无法像之前那样通过内存寻址的方式来互相访问，而是需要通过网络来进行通信。<br>
RPC最主要的功能就是在提供不同模块服务间的网络通信能力的同时，又尽可能的不丢失本地调用时语义的简洁性。rpc可以认为是分布式系统中类似人体经络一样的基础设施，因此有必要对其工作原理有一定的了解。<br><br>要学习rpc的原理，理论上最好的办法就是去看流行的开源框架源码。但dubbo这样成熟的rpc框架由于已经迭代了很多年，为了满足多样的需求而有着复杂的架构和庞大的代码量。对于普通初学者来说往往很难从层层抽象封装中把握住关于rpc框架最核心的内容。<br>MyRpc是我最近在学习MIT6.824分布式系统公开课时，使用java并基于netty实现的一个简易rpc框架，实现的过程中许多地方都参考了dubbo以及一些demo级别的rpc框架。<br>
MyRpc是demo级别的框架，理解起来会轻松不少。在对基础的rpc实现原理有一定了解后，能对后续研究dubbo等开源rpc框架带来很大的帮助。<br>目前MyRpc实现了以下功能<br>
<br>网络通信(netty做客户端、服务端网络交互，服务端使用一个线程池处理业务逻辑)
<br>实现消息的序列化（实现序列化方式的抽象，支持json、hessian、jdk序列化等）
<br>客户端代理生成(目前只实现了jdk动态代理)
<br>服务注册 + 注册中心集成(实现注册中心的抽象，但目前只支持用zookeeper做注册中心)
<br>集群负载均衡策略(实现负载均衡策略的抽象，支持roundRobin轮训，随机等)
<br>使用时间轮，支持设置消费者调用超时时间
<br>限于篇幅，以上功能会拆分为两篇博客分别介绍。其中前3个功能实现了基本的点对点通信的rpc功能，将在本篇博客中结合源码详细分析。<br><img alt="Pasted image 20240725171915.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171915.png"><br><br><br>MyRpc是以netty为基础的，下面展示一个最基础的netty客户端/服务端交互的demo。<br>netty服务端：<br>/**
 * 最原始的netty服务端demo
 * */
public class PureNettyServer {

    public static void main(String[] args) throws InterruptedException {
        ServerBootstrap bootstrap = new ServerBootstrap();
        EventLoopGroup bossGroup = new NioEventLoopGroup(1, new DefaultThreadFactory("NettyServerBoss", true));
        EventLoopGroup workerGroup = new NioEventLoopGroup(8,new DefaultThreadFactory("NettyServerWorker", true));

        bootstrap.group(bossGroup, workerGroup)
            .channel(NioServerSocketChannel.class)
            .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 实际调用业务方法的处理器
                        .addLast("serverHandler", new SimpleChannelInboundHandler&lt;ByteBuf&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf requestByteBuf) {
                                String requestStr = requestByteBuf.toString(CharsetUtil.UTF_8);
                                System.out.println("PureNettyServer read request=" + JsonUtil.json2Obj(requestStr, User.class));

                                // 服务端响应echo
                                ByteBuf byteBuf = Unpooled.copiedBuffer("echo:" + requestStr,CharsetUtil.UTF_8);
                                channelHandlerContext.writeAndFlush(byteBuf);
                            }
                        })
                    ;
                }
            });

        ChannelFuture channelFuture = bootstrap.bind("127.0.0.1", 8888).sync();

        System.out.println("netty server started!");
        // 一直阻塞在这里
        channelFuture.channel().closeFuture().sync();
    }
}
Copy<br>netty客户端：<br>/**
 * 最原始的netty客户端demo
 * */
public class PureNettyClient {

    public static void main(String[] args) throws InterruptedException {
        Bootstrap bootstrap = new Bootstrap();
        EventLoopGroup eventLoopGroup = new NioEventLoopGroup(8,
            new DefaultThreadFactory("NettyClientWorker", true));

        bootstrap.group(eventLoopGroup)
            .channel(NioSocketChannel.class)
            .handler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        .addLast("clientHandler", new SimpleChannelInboundHandler&lt;ByteBuf&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf responseByteBuf) {
                                String responseStr = responseByteBuf.toString(CharsetUtil.UTF_8);
                                System.out.println("PureNettyClient received response=" + responseStr);
                            }
                        })
                    ;
                }
            });

        ChannelFuture channelFuture = bootstrap.connect("127.0.0.1", 8888).sync();
        Channel channel = channelFuture.sync().channel();

        // 发送一个user对象的json串
        User user = new User("Tom",10);
        ByteBuf requestByteBuf = Unpooled.copiedBuffer(JsonUtil.obj2Str(user), CharsetUtil.UTF_8);
        channel.writeAndFlush(requestByteBuf);

        System.out.println("netty client send request success!");
        channelFuture.channel().closeFuture().sync();
    }
}
Copy<br>
<br>demo示例中，netty的服务端启动后绑定在本机127.0.0.1的8888端口上，等待来自客户端的连接。
<br>netty客户端向服务端发起连接请求，在成功建立连接后向服务端发送了一个User对象字符串对应的字节数组。
<br>服务端在接受到这一字节数组后反序列化为User对象并打印在控制台，随后echo响应了一个字符串。客户端在接受到响应后，将echo字符串打印在了控制台上
<br><br>上面展示了一个最基础的netty网络通信的demo，似乎一个点对点的传输功能已经得到了良好的实现。<br>
但作为一个rpc框架，还需要解决tcp传输层基于字节流的消息黏包/拆包问题。<br><br>操作系统实现的传输层tcp协议中，向上层的应用保证尽最大可能的(best effort delivery)、可靠的传输字节流，但并不关心实际传输的数据包是否总是符合应用层的要求。<br>
<br>黏包问题： 假设应用层发送的一次请求数据量比较小(比如0.1kb)，tcp层可能不会在接到应用请求后立即进行传输，而是会稍微等待一小会。<br>
这样如果应用层在短时间内需要传输多次0.1kb的请求，就可以攒在一起批量传输，传输效率会高很多。<br>
但这带来的问题就是接收端一次接受到的数据包内应用程序逻辑上的多次请求黏连在了一起，需要通过一些方法来将其拆分还原为一个个独立的信息给应用层。
<br>拆包问题： 假设应用层发送的一次请求数据量比较大(比如100Mb)，而tcp层的数据包容量的最大值是有限的，所以应用层较大的一次请求数据会被拆分为多个包分开发送。<br>
这就导致接收端接受到的某个数据包其实并不是完整的应用层请求数据，没法直接交给应用程序去使用，<br>
而必须等待后续对应请求的所有数据包都接受完成后，才能组装成完整的请求对象再交给应用层处理。
<br>可以看到，上述的黏包/拆包问题并不能看做是tcp的问题，而是应用层最终需求与tcp传输层功能不匹配导致的问题。<br>
tcp出于传输效率的考虑无法解决这个问题，所以黏包拆包问题最终只能在更上面的应用层自己来处理。
<br>一个数据包中可能同时存在黏包问题和拆包问题(如下图所示)<br>
<img alt="Pasted image 20240725171936.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171936.png"><br><br>解决黏包/拆包问题最核心的思路是，如何知道一个应用层完整请求的边界。<br>
对于黏包问题，基于边界可以独立的拆分出每一个请求；对于拆包问题，如果发现收到的数据包末尾没有边界，则继续等待新的数据包，直到发现边界后再一并上交给应用程序。<br>主流的解决黏包拆包的应用层协议设计方案有三种：<br><br>对于流行的rpc框架，一般都是选用性能与兼容性皆有的方案3：即自己设计一个固定大小的、包含了请求体长度字段的请求头。MyRpc参考dubbo，也设计了一个固定16字节大小的请求头(里面有几个字段暂时没用上)。<br>请求头: MessageHeader<br>/**
 * 共16字节的请求头
 * */
public class MessageHeader implements Serializable {

    public static final int MESSAGE_HEADER_LENGTH = 16;
    public static final int MESSAGE_SERIALIZE_TYPE_LENGTH = 5;
    public static final short MAGIC = (short)0x2233;

    // ================================ 消息头 =================================
    /**
     * 魔数(占2字节)
     * */
    private short magicNumber = MAGIC;

    /**
     * 消息标识(0代表请求事件；1代表响应事件， 占1位)
     * @see MessageFlagEnums
     * */
    private Boolean messageFlag;

    /**
     * 是否是双向请求(0代表oneWay请求；1代表twoWay请求）
     * （双向代表客户端会等待服务端的响应，单向则请求发送完成后即向上层返回成功)
     * */
    private Boolean twoWayFlag;

    /**
     * 是否是心跳消息(0代表正常消息；1代表心跳消息， 占1位)
     * */
    private Boolean eventFlag;

    /**
     * 消息体序列化类型(占5位，即所支持的序列化类型不得超过2的5次方，32种)
     * @see MessageSerializeType
     * */
    private Boolean[] serializeType;

    /**
     * 响应状态(占1字节)
     * */
    private byte responseStatus;

    /**
     * 消息的唯一id（占8字节）
     * */
    private long messageId;

    /**
     * 业务数据长度（占4字节）
     * */
    private int bizDataLength;
}
Copy<br>完整的消息对象: MessageProtocol<br>public class MessageProtocol&lt;T&gt; implements Serializable {
    /**
     * 请求头
     * */
    private MessageHeader messageHeader;

    /**
     * 请求体(实际的业务消息对象)
     * */
    private T bizDataBody;
}
Copy<br><img alt="Pasted image 20240725172037.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172037.png"><br><br>/**
 * rpc请求对象
 * */
public class RpcRequest implements Serializable {

    private static final AtomicLong INVOKE_ID = new AtomicLong(0);

    /**
     * 消息的唯一id（占8字节）
     * */
    private final long messageId;

    /**
     * 接口名
     * */
    private String interfaceName;

    /**
     * 方法名
     * */
    private String methodName;

    /**
     * 参数类型数组(每个参数一项)
     * */
    private Class&lt;?&gt;[] parameterClasses;

    /**
     * 实际参数对象数组(每个参数一项)
     * */
    private Object[] params;

    public RpcRequest() {
        // 每个请求对象生成时都自动生成单机全局唯一的自增id
        this.messageId = INVOKE_ID.getAndIncrement();
    }
}
Copy<br>/**
 * rpc响应对象
 * */
public class RpcResponse implements Serializable {

    /**
     * 消息的唯一id（占8字节）
     * */
    private long messageId;

    /**
     * 返回值
     */
    private Object returnValue;

    /**
     * 异常值
     */
    private Exception exceptionValue;
}
Copy<br><br>在上一节的netty demo中的消息处理器中，一共做了两件事情；一是将原始数据包的字节流转化成了应用程序所需的String对象；二是拿到String对象后进行响应的业务处理(比如打印在控制台上)。<br>
而netty框架允许配置多个消息处理器组成链条，按约定的顺序处理出站/入站的消息；因此从模块化的出发，应该将编码/解码的逻辑和实际业务的处理拆分成多个处理器。<br>在自定义的消息编码器、解码器中进行应用层请求/响应数据的序列化/反序列化，同时处理上述的黏包/拆包问题。<br>编解码工具类<br>public class MessageCodecUtil {

    /**
     * 报文协议编码
     * */
    public static &lt;T&gt; void messageEncode(MessageProtocol&lt;T&gt; messageProtocol, ByteBuf byteBuf) {
        MessageHeader messageHeader = messageProtocol.getMessageHeader();
        // 写入魔数
        byteBuf.writeShort(MessageHeader.MAGIC);

        // 写入消息标识
        byteBuf.writeBoolean(messageHeader.getMessageFlag());
        // 写入单/双向标识
        byteBuf.writeBoolean(messageHeader.getTwoWayFlag());
        // 写入消息事件标识
        byteBuf.writeBoolean(messageHeader.getEventFlag());
        // 写入序列化类型
        for(boolean b : messageHeader.getSerializeType()){
            byteBuf.writeBoolean(b);
        }
        // 写入响应状态
        byteBuf.writeByte(messageHeader.getResponseStatus());
        // 写入消息uuid
        byteBuf.writeLong(messageHeader.getMessageId());

        // 序列化消息体
        MyRpcSerializer myRpcSerializer = MyRpcSerializerManager.getSerializer(messageHeader.getSerializeType());
        byte[] bizMessageBytes = myRpcSerializer.serialize(messageProtocol.getBizDataBody());
        // 获得并写入消息正文长度
        byteBuf.writeInt(bizMessageBytes.length);
        // 写入消息正文内容
        byteBuf.writeBytes(bizMessageBytes);
    }

    /**
     * 报文协议header头解码
     * */
    public static MessageHeader messageHeaderDecode(ByteBuf byteBuf){
        MessageHeader messageHeader = new MessageHeader();
        // 读取魔数
        messageHeader.setMagicNumber(byteBuf.readShort());
        // 读取消息标识
        messageHeader.setMessageFlag(byteBuf.readBoolean());
        // 读取单/双向标识
        messageHeader.setTwoWayFlag(byteBuf.readBoolean());
        // 读取消息事件标识
        messageHeader.setEventFlag(byteBuf.readBoolean());

        // 读取序列化类型
        Boolean[] serializeTypeBytes = new Boolean[MessageHeader.MESSAGE_SERIALIZE_TYPE_LENGTH];
        for(int i=0; i&lt;MessageHeader.MESSAGE_SERIALIZE_TYPE_LENGTH; i++){
            serializeTypeBytes[i] = byteBuf.readBoolean();
        }
        messageHeader.setSerializeType(serializeTypeBytes);

        // 读取响应状态
        messageHeader.setResponseStatus(byteBuf.readByte());
        // 读取消息uuid
        messageHeader.setMessageId(byteBuf.readLong());

        // 读取消息正文长度
        int bizDataLength = byteBuf.readInt();
        messageHeader.setBizDataLength(bizDataLength);

        return messageHeader;
    }

    /**
     * 报文协议正文body解码
     * */
    public static &lt;T&gt; T messageBizDataDecode(MessageHeader messageHeader, ByteBuf byteBuf, Class&lt;T&gt; messageBizDataType){
        // 读取消息正文
        byte[] bizDataBytes = new byte[messageHeader.getBizDataLength()];
        byteBuf.readBytes(bizDataBytes);

        // 反序列化消息体
        MyRpcSerializer myRpcSerializer = MyRpcSerializerManager.getSerializer(messageHeader.getSerializeType());
        return (T) myRpcSerializer.deserialize(bizDataBytes,messageBizDataType);
    }
}
Copy<br>自定义编码器: NettyEncoder<br>public class NettyEncoder&lt;T&gt; extends MessageToByteEncoder&lt;MessageProtocol&lt;T&gt;&gt; {

    @Override
    protected void encode(ChannelHandlerContext channelHandlerContext, MessageProtocol&lt;T&gt; messageProtocol, ByteBuf byteBuf) {
        // 继承自MessageToByteEncoder中，只需要将编码后的数据写入参数中指定的byteBuf中即可
        // MessageToByteEncoder源码逻辑中会自己去将byteBuf写入channel的
        MessageCodecUtil.messageEncode(messageProtocol,byteBuf);
    }
}
Copy<br>自定义解码器: NettyDecoder<br>/**
 * netty 解码器
 */
public class NettyDecoder extends ByteToMessageDecoder {

    private static final Logger logger = LoggerFactory.getLogger(NettyDecoder.class);

    @Override
    protected void decode(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf, List&lt;Object&gt; list){
        do{
            try {
                // 保存读取前的读指针
                int beforeReadIndex = byteBuf.readerIndex();
                MessageDecodeResult messageDecodeResult = decodeHeader(byteBuf);

                if (messageDecodeResult.isNeedMoreData()) {
                    // 出现拆包没有读取到一个完整的rpc请求，还原byteBuf读指针，等待下一次读事件
                    byteBuf.readerIndex(beforeReadIndex);
                    break;
                } else {
                    // 正常解析完一个完整的message，交给后面的handler处理
                    list.add(messageDecodeResult.getMessageProtocol());
                }
            }catch (Exception e){
                // 比如decodeHeader里json序列化失败了等等.直接跳过这个数据包不还原了
                logger.error("NettyDecoder error!",e);
            }

            // 循环，直到整个ByteBuf读取完
        }while(byteBuf.isReadable());
    }
    
    private MessageDecodeResult decodeHeader(ByteBuf byteBuf){
        int readable = byteBuf.readableBytes();
        if(readable &lt; MessageHeader.MESSAGE_HEADER_LENGTH){
            // 无法读取到一个完整的header，说明出现了拆包，等待更多的数据
            return MessageDecodeResult.needMoreData();
        }

        // 读取header头
        MessageHeader messageHeader = MessageCodecUtil.messageHeaderDecode(byteBuf);

        int bizDataLength = messageHeader.getBizDataLength();
        if(byteBuf.readableBytes() &lt; bizDataLength){
            // 无法读取到一个完整的正文内容，说明出现了拆包，等待更多的数据
            return MessageDecodeResult.needMoreData();
        }

        // 基于消息类型标识，解析rpc正文对象
        boolean messageFlag = messageHeader.getMessageFlag();
        if(messageFlag == MessageFlagEnums.REQUEST.getCode()){
            RpcRequest rpcRequest = MessageCodecUtil.messageBizDataDecode(messageHeader,byteBuf,RpcRequest.class);
            MessageProtocol&lt;RpcRequest&gt; messageProtocol = new MessageProtocol&lt;&gt;(messageHeader,rpcRequest);
            // 正确的解析完一个rpc请求消息
            return MessageDecodeResult.decodeSuccess(messageProtocol);
        }else{
            RpcResponse rpcResponse = MessageCodecUtil.messageBizDataDecode(messageHeader,byteBuf,RpcResponse.class);
            MessageProtocol&lt;RpcResponse&gt; messageProtocol = new MessageProtocol&lt;&gt;(messageHeader,rpcResponse);
            // 正确的解析完一个rpc响应消息
            return MessageDecodeResult.decodeSuccess(messageProtocol);
        }
    }
}
Copy<br><br>demo的服务示例:<br>public class User implements Serializable {

    private String name;
    private Integer age;
}
Copy<br>public interface UserService {

    User getUserFriend(User user, String message);
}
Copy<br>public class UserServiceImpl implements UserService {
    @Override
    public User getUserFriend(User user, String message) {
        System.out.println("execute getUserFriend, user=" + user + ",message=" + message);

        // demo返回一个不同的user对象回去
        return new User(user.getName() + ".friend", user.getAge() + 1);
    }
}
Copy<br>netty服务端：<br>public class RpcServer {

    private static final Map&lt;String,Object&gt; interfaceImplMap = new HashMap&lt;&gt;();

    static{
        /**
         * 简单一点配置死实现
         * */
        interfaceImplMap.put(UserService.class.getName(), new UserServiceImpl());
    }

    public static void main(String[] args) throws InterruptedException {
        ServerBootstrap bootstrap = new ServerBootstrap();
        EventLoopGroup bossGroup = new NioEventLoopGroup(1, new DefaultThreadFactory("NettyServerBoss", true));
        EventLoopGroup workerGroup = new NioEventLoopGroup(8,new DefaultThreadFactory("NettyServerWorker", true));

        bootstrap.group(bossGroup, workerGroup)
            .channel(NioServerSocketChannel.class)
            .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 编码、解码处理器
                        .addLast("encoder", new NettyEncoder&lt;&gt;())
                        .addLast("decoder", new NettyDecoder())
                        // 实际调用业务方法的处理器
                        .addLast("serverHandler", new SimpleChannelInboundHandler&lt;MessageProtocol&lt;RpcRequest&gt;&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext ctx, MessageProtocol&lt;RpcRequest&gt; msg) {
                                // 找到本地的方法进行调用，并获得返回值(demo，简单起见直接同步调用)
                                MessageProtocol&lt;RpcResponse&gt; result = handlerRpcRequest(msg);

                                // 将返回值响应给客户端
                                ctx.writeAndFlush(result);
                            }
                        });
                }
            });

        ChannelFuture channelFuture = bootstrap.bind("127.0.0.1", 8888).sync();

        System.out.println("netty server started!");
        // 一直阻塞在这里
        channelFuture.channel().closeFuture().sync();
    }

    private static MessageProtocol&lt;RpcResponse&gt; handlerRpcRequest(MessageProtocol&lt;RpcRequest&gt; rpcRequestMessageProtocol){
        long requestMessageId = rpcRequestMessageProtocol.getMessageHeader().getMessageId();

        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageId(requestMessageId);
        messageHeader.setMessageFlag(MessageFlagEnums.RESPONSE.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(false);
        messageHeader.setSerializeType(rpcRequestMessageProtocol.getMessageHeader().getSerializeType());

        RpcResponse rpcResponse = new RpcResponse();
        rpcResponse.setMessageId(requestMessageId);

        try {
            // 反射调用具体的实现方法
            Object result = invokeTargetService(rpcRequestMessageProtocol.getBizDataBody());

            // 设置返回值
            rpcResponse.setReturnValue(result);
        }catch (Exception e){
            // 调用具体实现类时，出现异常，设置异常的值
            rpcResponse.setExceptionValue(e);
        }

        return new MessageProtocol&lt;&gt;(messageHeader,rpcResponse);
    }

    private static Object invokeTargetService(RpcRequest rpcRequest) throws Exception {
        String interfaceName = rpcRequest.getInterfaceName();
        Object serviceImpl = interfaceImplMap.get(interfaceName);

        // 按照请求里的方法名和参数列表找到对应的方法
        final Method method = serviceImpl.getClass().getMethod(rpcRequest.getMethodName(), rpcRequest.getParameterClasses());

        // 传递参数，反射调用该方法并返回结果
        return method.invoke(serviceImpl, rpcRequest.getParams());
    }
}
Copy<br>netty客户端：<br>public class RpcClientNoProxy {

    public static void main(String[] args) throws InterruptedException {
        Bootstrap bootstrap = new Bootstrap();
        EventLoopGroup eventLoopGroup = new NioEventLoopGroup(8,
            new DefaultThreadFactory("NettyClientWorker", true));

        bootstrap.group(eventLoopGroup)
            .channel(NioSocketChannel.class)
            .handler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 编码、解码处理器
                        .addLast("encoder", new NettyEncoder&lt;&gt;())
                        .addLast("decoder", new NettyDecoder())
                        .addLast("clientHandler", new SimpleChannelInboundHandler&lt;MessageProtocol&gt;() {
                            @Override
                            protected void channelRead0(ChannelHandlerContext channelHandlerContext, MessageProtocol messageProtocol) {
                                System.out.println("PureNettyClient received messageProtocol=" + messageProtocol);
                            }
                        })
                    ;
                }
            });

        ChannelFuture channelFuture = bootstrap.connect("127.0.0.1", 8888).sync();
        Channel channel = channelFuture.sync().channel();

        // 构造消息对象
        MessageProtocol&lt;RpcRequest&gt; messageProtocol = buildMessage();
        // 发送消息
        channel.writeAndFlush(messageProtocol);

        System.out.println("RpcClientNoProxy send request success!");
        channelFuture.channel().closeFuture().sync();
    }

    private static MessageProtocol&lt;RpcRequest&gt; buildMessage(){
        // 构造请求
        RpcRequest rpcRequest = new RpcRequest();
        rpcRequest.setInterfaceName("myrpc.demo.common.service.UserService");
        rpcRequest.setMethodName("getUserFriend");
        rpcRequest.setParameterClasses(new Class[]{User.class,String.class});

        User user = new User("Jerry",10);
        String message = "hello hello!";
        rpcRequest.setParams(new Object[]{user,message});

        // 构造协议头
        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageFlag(MessageFlagEnums.REQUEST.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(true);
        messageHeader.setSerializeType(MessageSerializeType.JSON.getCode());
        messageHeader.setMessageId(rpcRequest.getMessageId());

        return new MessageProtocol&lt;&gt;(messageHeader,rpcRequest);
    }
}
Copy<br><img alt="Pasted image 20240725172059.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172059.png"><br><br>截止目前，我们已经实现了一个点对点rpc客户端/服务端交互的功能，但是客户端这边的逻辑依然比较复杂(buildMessage方法)。<br>
前面提到，rpc中很重要的功能就是保持本地调用时语义的简洁性，即客户端实际使用时是希望直接用以下这种方式来进行调用，而不是去繁琐的处理底层的网络交互逻辑。<br>    User user = new User("Jerry",10);
    String message = "hello hello!";
    // 发起rpc调用并获得返回值
    User userFriend = userService.getUserFriend(user,message);
    System.out.println("userService.getUserFriend result=" + userFriend);
Copy<br>rpc框架需要屏蔽掉构造底层消息发送/接受，序列化/反序列化相关的复杂性，而这时候就需要引入代理模式(动态代理)了。<br>
在MyRpc的底层，我们将客户端需要调用的一个服务(比如UserService)抽象为Consumer对象，服务端的一个具体服务实现抽象为Provider对象。<br>
其中包含了对应的服务的类以及对应的服务地址，客户端这边使用jdk的动态代理生成代理对象，将复杂的、需要屏蔽的消息处理/网络交互等逻辑都封装在这个代理对象中。<br>public class Consumer&lt;T&gt; {

    private Class&lt;?&gt; interfaceClass;
    private T proxy;

    private Bootstrap bootstrap;
    private URLAddress urlAddress;

    public Consumer(Class&lt;?&gt; interfaceClass, Bootstrap bootstrap, URLAddress urlAddress) {
        this.interfaceClass = interfaceClass;
        this.bootstrap = bootstrap;
        this.urlAddress = urlAddress;

        ClientDynamicProxy clientDynamicProxy = new ClientDynamicProxy(bootstrap,urlAddress);

        this.proxy = (T) Proxy.newProxyInstance(
            clientDynamicProxy.getClass().getClassLoader(),
            new Class[]{interfaceClass},
            clientDynamicProxy);
    }

    public T getProxy() {
        return proxy;
    }

    public Class&lt;?&gt; getInterfaceClass() {
        return interfaceClass;
    }
}
Copy<br>public class ConsumerBootstrap {

    private final Map&lt;Class&lt;?&gt;,Consumer&lt;?&gt;&gt; consumerMap = new HashMap&lt;&gt;();
    private final Bootstrap bootstrap;
    private final URLAddress urlAddress;

    public ConsumerBootstrap(Bootstrap bootstrap, URLAddress urlAddress) {
        this.bootstrap = bootstrap;
        this.urlAddress = urlAddress;
    }

    public &lt;T&gt; Consumer&lt;T&gt; registerConsumer(Class&lt;T&gt; clazz){
        if(!consumerMap.containsKey(clazz)){
            Consumer&lt;T&gt; consumer = new Consumer&lt;&gt;(clazz,this.bootstrap,this.urlAddress);
            consumerMap.put(clazz,consumer);
            return consumer;
        }

        throw new MyRpcException("duplicate consumer! clazz=" + clazz);
    }
}
Copy<br>public class Provider&lt;T&gt; {

    private Class&lt;?&gt; interfaceClass;
    private T ref;
    private URLAddress urlAddress;
}
Copy<br><br>/**
 * 客户端动态代理
 * */
public class ClientDynamicProxy implements InvocationHandler {

    private static final Logger logger = LoggerFactory.getLogger(ClientDynamicProxy.class);

    private final Bootstrap bootstrap;
    private final URLAddress urlAddress;

    public ClientDynamicProxy(Bootstrap bootstrap, URLAddress urlAddress) {
        this.bootstrap = bootstrap;
        this.urlAddress = urlAddress;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        Tuple&lt;Object,Boolean&gt; localMethodResult = processLocalMethod(proxy,method,args);
        if(localMethodResult.getRight()){
            // right为true,代表是本地方法，返回toString等对象自带方法的执行结果，不发起rpc调用
            return localMethodResult.getLeft();
        }

        logger.debug("ClientDynamicProxy before: methodName=" + method.getName());

        // 构造请求和协议头
        RpcRequest rpcRequest = new RpcRequest();
        rpcRequest.setInterfaceName(method.getDeclaringClass().getName());
        rpcRequest.setMethodName(method.getName());
        rpcRequest.setParameterClasses(method.getParameterTypes());
        rpcRequest.setParams(args);

        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageFlag(MessageFlagEnums.REQUEST.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(true);
        messageHeader.setSerializeType(GlobalConfig.messageSerializeType.getCode());
        messageHeader.setResponseStatus((byte)'a');
        messageHeader.setMessageId(rpcRequest.getMessageId());

        logger.debug("ClientDynamicProxy rpcRequest={}", JsonUtil.obj2Str(rpcRequest));

        ChannelFuture channelFuture = bootstrap.connect(urlAddress.getHost(),urlAddress.getPort()).sync();
        Channel channel = channelFuture.sync().channel();
        // 通过Promise，将netty的异步转为同步,参考dubbo DefaultFuture
        DefaultFuture&lt;RpcResponse&gt; defaultFuture = DefaultFutureManager.createNewFuture(channel,rpcRequest);

        channel.writeAndFlush(new MessageProtocol&lt;&gt;(messageHeader,rpcRequest));
        logger.debug("ClientDynamicProxy writeAndFlush success, wait result");

        // 调用方阻塞在这里
        RpcResponse rpcResponse = defaultFuture.get();

        logger.debug("ClientDynamicProxy defaultFuture.get() rpcResponse={}",rpcResponse);

        return processRpcResponse(rpcResponse);
    }

    /**
     * 处理本地方法
     * @return tuple.right 标识是否是本地方法， true是
     * */
    private Tuple&lt;Object,Boolean&gt; processLocalMethod(Object proxy, Method method, Object[] args) throws Exception {
        // 处理toString等对象自带方法，不发起rpc调用
        if (method.getDeclaringClass() == Object.class) {
            return new Tuple&lt;&gt;(method.invoke(proxy, args),true);
        }
        String methodName = method.getName();
        Class&lt;?&gt;[] parameterTypes = method.getParameterTypes();
        if (parameterTypes.length == 0) {
            if ("toString".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.toString(),true);
            } else if ("hashCode".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.hashCode(),true);
            }
        } else if (parameterTypes.length == 1 &amp;&amp; "equals".equals(methodName)) {
            return new Tuple&lt;&gt;(proxy.equals(args[0]),true);
        }

        // 返回null标识非本地方法，需要进行rpc调用
        return new Tuple&lt;&gt;(null,false);
    }

    private Object processRpcResponse(RpcResponse rpcResponse){
        if(rpcResponse.getExceptionValue() == null){
            // 没有异常，return正常的返回值
            return rpcResponse.getReturnValue();
        }else{
            // 有异常，往外抛出去
            throw new MyRpcRemotingException(rpcResponse.getExceptionValue());
        }
    }
}
Copy<br><br>/**
 * 客户端 rpc响应处理器
 */
public class NettyRpcResponseHandler extends SimpleChannelInboundHandler&lt;MessageProtocol&lt;RpcResponse&gt;&gt; {

    private static final Logger logger = LoggerFactory.getLogger(NettyRpcResponseHandler.class);

    @Override
    protected void channelRead0(ChannelHandlerContext channelHandlerContext, MessageProtocol&lt;RpcResponse&gt; rpcResponseMessageProtocol) throws Exception {
        logger.debug("NettyRpcResponseHandler channelRead0={}",JsonUtil.obj2Str(rpcResponseMessageProtocol));

        // 触发客户端的future，令其同步阻塞的线程得到结果
        DefaultFutureManager.received(rpcResponseMessageProtocol.getBizDataBody());
    }
}
Copy<br>public class DefaultFutureManager {

    private static Logger logger = LoggerFactory.getLogger(DefaultFutureManager.class);

    public static final Map&lt;Long,DefaultFuture&gt; DEFAULT_FUTURE_CACHE = new ConcurrentHashMap&lt;&gt;();

    public static void received(RpcResponse rpcResponse){
        Long messageId = rpcResponse.getMessageId();

        logger.debug("received rpcResponse={},DEFAULT_FUTURE_CACHE={}",rpcResponse,DEFAULT_FUTURE_CACHE);
        DefaultFuture defaultFuture = DEFAULT_FUTURE_CACHE.remove(messageId);

        if(defaultFuture != null){
            logger.debug("remove defaultFuture success");
            if(rpcResponse.getExceptionValue() != null){
                // 异常处理
                defaultFuture.completeExceptionally(rpcResponse.getExceptionValue());
            }else{
                // 正常返回
                defaultFuture.complete(rpcResponse);
            }
        }else{
            logger.debug("remove defaultFuture fail");
        }
    }

    public static DefaultFuture createNewFuture(Channel channel, RpcRequest rpcRequest){
        DefaultFuture defaultFuture = new DefaultFuture(channel,rpcRequest);

        return defaultFuture;
    }
}
Copy<br><br>public class RpcClientProxy {

    public static void main(String[] args) throws InterruptedException {
        Bootstrap bootstrap = new Bootstrap();
        EventLoopGroup eventLoopGroup = new NioEventLoopGroup(8, new DefaultThreadFactory("NettyClientWorker", true));

        bootstrap.group(eventLoopGroup)
            .channel(NioSocketChannel.class)
            .handler(new ChannelInitializer&lt;SocketChannel&gt;() {
                @Override
                protected void initChannel(SocketChannel socketChannel) {
                    socketChannel.pipeline()
                        // 编码、解码处理器
                        .addLast("encoder", new NettyEncoder&lt;&gt;())
                        .addLast("decoder", new NettyDecoder())

                        // 响应处理器
                        .addLast("clientHandler", new NettyRpcResponseHandler())
                    ;
                }
            });

        ConsumerBootstrap consumerBootstrap = new ConsumerBootstrap(bootstrap, new URLAddress("127.0.0.1", 8888));
        Consumer&lt;UserService&gt; userServiceConsumer = consumerBootstrap.registerConsumer(UserService.class);

        // 获得UserService的代理对象
        UserService userService = userServiceConsumer.getProxy();

        User user = new User("Jerry", 10);
        String message = "hello hello!";
        // 发起rpc调用并获得返回值
        User userFriend = userService.getUserFriend(user, message);
        System.out.println("userService.getUserFriend result=" + userFriend);
    }
}
Copy<br>可以看到，引入了代理模式后的使用方式就变得简单很多了。<br>
到这一步，我们已经实现了一个点对点的rpc通信的能力，并且如博客开头中所提到的，没有丧失本地调用语义的简洁性。<br><br>
<br>这篇博客是我关于Mit6.824分布式系统公开课lab的第一篇博客，按照计划会将实现简易版rpc和raft k/v数据库的心得以博客的形式分享出来，希望能帮助到对分布式系统相关技术的小伙伴。
<br>打个广告：对于英语不好(没法直接啃生肉)但又对国外著名的计算机公开课(涉及操作系统、数据库、分布式系统、编译原理、计算机网络、算法等等)感兴趣的小伙伴，可以咨询simviso购买中英翻译质量很高的公开课视频(比如Mit6.824，b站上开放了一部分免费的视频：<a rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV1x7411M7Sf" target="_blank">https://www.bilibili.com/video/BV1x7411M7Sf</a>)。
<br>博客中展示的完整代码在我的github上：<a rel="noopener" class="external-link" href="https://github.com/1399852153/MyRpc" target="_blank">https://github.com/1399852153/MyRpc</a> (release/lab1分支)，内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/自己动手实现rpc框架(一)-实现点对点的rpc通信.html</link><guid isPermaLink="false">Computer Science/Distributed System/RPC框架的实现/自己动手实现rpc框架(一) 实现点对点的rpc通信.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 25 Jul 2024 09:21:13 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171915.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725171915.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[自己动手实现rpc框架(二) 实现集群间rpc通信]]></title><description><![CDATA[ 
 <br><br>上一篇博客中MyRpc框架实现了基本的点对点rpc通信功能。而在这篇博客中我们需要实现MyRpc的集群间rpc通信功能。<br>
<br><a data-tooltip-position="top" aria-label="https://www.cnblogs.com/xiaoxiongcanguan/p/17506728.html" rel="noopener" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17506728.html" target="_blank">自己动手实现rpc框架(一) 实现点对点的rpc通信</a>
<br>上篇博客的点对点rpc通信实现中，客户端和服务端的ip地址和端口都是固定配置死的。而通常为了提升服务总负载，客户端和服务端都是以集群的方式部署的(水平拓展)，客户端和服务端的节点都不止1个。<br>集群条件下出现了很多新的问题需要解决：<br>
<br>对于某一特定服务，客户端该如何知道当前环境下哪些机器能提供这一服务?
<br>服务端集群中的某些节点如果发生了变化(比如老节点下线或宕机)，客户端该如何及时的感知到，而不会调用到已经停止服务的节点上？
<br>存在多个服务端时，客户端应该向哪一个服务端节点发起请求？怎样才能使得每个服务端的负载尽量均衡，而不会让某些服务端饥饿或者压力过大。
<br><br>
<br>针对第一个问题，最先想到的自然是直接在每个客户端都配置一个固定的服务端节点列表，但这一方案无法很好的解决服务端节点动态变化的问题。<br>
如果一个服务端节点下线了，就需要人工的去修改每个客户端那里维护的服务端节点列表的话，在集群节点数量较多、服务端节点上下线频繁的场景下是不可接受的。
<br>解决这一问题的思路是服务端节点信息的中心化，将服务端节点的信息都集中维护在一个地方。<br>
服务端在启动成功后将自己的信息注册在上面(服务注册)，而客户端也能实时的查询出最新的服务端列表(服务发现)。<br>
这个统一维护服务端节点信息的地方被叫做注册中心，一般是以独立服务的形式与rpc的服务端/客户端机器部署在同一环境内。
<br>由于节点信息的中心化，所以注册中心需要具备高可用能力(集群部署来提供容错能力)，避免单点故障而导致整个rpc集群的不可用。<br>
同时在服务端节点因为一些原因不可用时能实时的感知并移除掉对应节点，同时通知监听变更客户端(解决第二个关于provider信息实时性的问题)。<br>
因此zookeeper、eureka、nacos、etcd等等具备上述能力的中间件都被广泛的用作rpc框架的注册中心。
<br><br>MyRpc目前支持使用zookeeper作为注册中心。<br>
zookeeper作为一个高性能的分布式协调器，存储的数据以ZNode节点树的形式存在。ZNode节点有两种属性，有序/无序，持久/临时。<br>
<br>rpc框架中一般设置一个持久的根路径节点用于与zk上存储其它的业务数据作区分(例如/my_rpc)。
<br>在根节点下有着代表着某一特定服务的子节点，其也是持久节点。服务子节点的路径名是标识接口的唯一名称(比如包名+类名：myrpc.demo.common.service.UserService)
<br>而服务节点下则可以存储各种关于provider、consumer等等相关的元数据。<br>
MyRpc中为了简单起见，服务节点的子节点直接就是对应特定provider注册的临时节点。临时节点中数据保存了provider的ip/port等必要的信息。<br>
由于是临时节点，在provider因为各种故障而不可用而导致与zookeeper的连接断开，zookeeper会在等待一小会后将该临时节点删除，并通知监听该服务的客户端以刷新客户端的对应配置。
<br><img alt="Pasted image 20240725172146.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172146.png"><br>注册中心接口<br>/**
 * 注册中心的抽象
 * */
public interface Registry {

    /**
     * 服务注册
     * */
    void doRegistry(ServiceInfo serviceInfo);

    /**
     * 服务发现
     * */
    List&lt;ServiceInfo&gt; discovery(String serviceName);
}
Copy<br>zookeeper注册中心实现(原始的zk客户端)<br>/**
 * 简易的zk注册中心(原始的zk客户端很多地方都需要用户去处理异常，但为了更简单的展示zk注册中心的使用，基本上没有处理这些异常情况)
 * */
public class ZookeeperRegistry implements Registry{

    private static final Logger logger = LoggerFactory.getLogger(ZookeeperRegistry.class);

    private final ZooKeeper zooKeeper;

    private final ConcurrentHashMap&lt;String,List&lt;ServiceInfo&gt;&gt; serviceInfoCacheMap = new ConcurrentHashMap&lt;&gt;();

    public ZookeeperRegistry(String zkServerAddress) {
        try {
            this.zooKeeper = new ZooKeeper(zkServerAddress,2000, event -&gt; {});

            // 确保root节点是一定存在的
            createPersistentNode(MyRpcRegistryConstants.BASE_PATH);
        } catch (Exception e) {
            throw new MyRpcException("init zkClient error",e);
        }
    }

    @Override
    public void doRegistry(ServiceInfo serviceInfo) {
        // 先创建永久的服务名节点
        createServiceNameNode(serviceInfo.getServiceName());
        // 再创建临时的providerInfo节点
        createProviderInfoNode(serviceInfo);
    }

    @Override
    public List&lt;ServiceInfo&gt; discovery(String serviceName) {
        return serviceInfoCacheMap.computeIfAbsent(serviceName,(key)-&gt; findProviderInfoList(serviceName));
    }

    private String getServiceNameNodePath(String serviceName){
        return MyRpcRegistryConstants.BASE_PATH + "/" + serviceName;
    }

    // ================================ zk工具方法 ==================================
    private void createServiceNameNode(String serviceName){
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceName);

            // 服务名节点是永久节点
            createPersistentNode(serviceNameNodePath);
            logger.info("createServiceNameNode success! serviceNameNodePath={}",serviceNameNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createServiceNameNode error",e);
        }
    }

    private void createProviderInfoNode(ServiceInfo serviceInfo){
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceInfo.getServiceName());
            // 子节点用一个uuid做path防重复
            String providerInfoNodePath = serviceNameNodePath + "/" + UUID.randomUUID();

            String providerInfoJsonStr = JsonUtil.obj2Str(serviceInfo);
            // providerInfo节点是临时节点(如果节点宕机了，zk的连接断开一段时间后，临时节点会被自动删除)
            zooKeeper.create(providerInfoNodePath, providerInfoJsonStr.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
            logger.info("createProviderInfoNode success! path={}",providerInfoNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createProviderInfoNode error",e);
        }
    }

    private void createPersistentNode(String path){
        try {
            if (zooKeeper.exists(path, false) == null) {
                // 服务名节点是永久节点
                zooKeeper.create(path, "".getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            }
        }catch (Exception e){
            throw new MyRpcException("createPersistentNode error",e);
        }
    }

    private List&lt;ServiceInfo&gt; findProviderInfoList(String serviceName){
        String serviceNameNodePath = getServiceNameNodePath(serviceName);

        List&lt;ServiceInfo&gt; serviceInfoList = new ArrayList&lt;&gt;();
        try {
            List&lt;String&gt; providerInfoPathList = zooKeeper.getChildren(serviceNameNodePath, new ZookeeperListener(serviceNameNodePath));
            for(String providerInfoPath : providerInfoPathList){
                try{
                    String fullProviderInfoPath = serviceNameNodePath + "/" + providerInfoPath;
                    byte[] data = zooKeeper.getData(fullProviderInfoPath,false,null);
                    String jsonStr = new String(data,StandardCharsets.UTF_8);
                    ServiceInfo serviceInfo = JsonUtil.json2Obj(jsonStr,ServiceInfo.class);

                    serviceInfoList.add(serviceInfo);
                }catch (Exception e){
                    logger.error("findProviderInfoList getData error",e);
                }
            }

            logger.info("findProviderInfoList={}",JsonUtil.obj2Str(serviceInfoList));
            return serviceInfoList;
        } catch (Exception e) {
            throw new MyRpcException("findProviderInfoList error",e);
        }
    }

    private class ZookeeperListener implements Watcher{
        private final String path;
        private final String serviceName;

        public ZookeeperListener(String serviceName) {
            this.path = getServiceNameNodePath(serviceName);
            this.serviceName = serviceName;
        }

        @Override
        public void process(WatchedEvent event) {
            logger.info("ZookeeperListener process! path={}",path);

            try {
                // 刷新缓存
                List&lt;ServiceInfo&gt; serviceInfoList = findProviderInfoList(path);
                serviceInfoCacheMap.put(serviceName,serviceInfoList);
            } catch (Exception e) {
                logger.error("ZookeeperListener getChildren error! path={}",path,e);
            }
        }
    }
}
Copy<br>zookeeper注册中心实现(curator客户端，通过自动重试等操作解决了原生客户端的一些坑)<br>public class ZkCuratorRegistry implements Registry {
    private static final Logger logger = LoggerFactory.getLogger(ZkCuratorRegistry.class);

    private CuratorFramework curatorZkClient;

    private final ConcurrentHashMap&lt;String, List&lt;ServiceInfo&gt;&gt; serviceInfoCacheMap = new ConcurrentHashMap&lt;&gt;();
    private static ConcurrentHashMap&lt;String, PathChildrenCache&gt; nodeCacheMap = new ConcurrentHashMap&lt;&gt;();


    public ZkCuratorRegistry(String zkServerAddress) {
        try {
            this.curatorZkClient = CuratorFrameworkFactory.newClient(zkServerAddress, new ExponentialBackoffRetry(3000, 1));
            this.curatorZkClient.start();
        } catch (Exception e) {
            throw new MyRpcException("init zkClient error", e);
        }
    }

    @Override
    public void doRegistry(ServiceInfo serviceInfo) {
        // 先创建永久的服务名节点
        createServiceNameNode(serviceInfo.getServiceName());
        // 再创建临时的providerInfo节点
        createProviderInfoNode(serviceInfo);
    }

    @Override
    public List&lt;ServiceInfo&gt; discovery(String serviceName) {
        return this.serviceInfoCacheMap.computeIfAbsent(serviceName,(key)-&gt;{
            List&lt;ServiceInfo&gt; serviceInfoList = findProviderInfoList(serviceName);

            // 创建对子节点的监听
            String serviceNodePath = getServiceNameNodePath(serviceName);
            PathChildrenCache pathChildrenCache = new PathChildrenCache(curatorZkClient, serviceNodePath, true);
            try {
                pathChildrenCache.start();
                nodeCacheMap.put(serviceName,pathChildrenCache);
                pathChildrenCache.getListenable().addListener(new ZkCuratorListener(serviceName));
            } catch (Exception e) {
                throw new MyRpcException("PathChildrenCache start error!",e);
            }

            return serviceInfoList;
        });
    }

    private void createServiceNameNode(String serviceName) {
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceName);

            // 服务名节点是永久节点
            if (curatorZkClient.checkExists().forPath(serviceNameNodePath) == null) {
                curatorZkClient.create()
                    .creatingParentsIfNeeded()
                    .withMode(CreateMode.PERSISTENT)
                    .forPath(serviceNameNodePath);
            }

            logger.info("createServiceNameNode success! serviceNameNodePath={}", serviceNameNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createServiceNameNode error", e);
        }
    }

    private void createProviderInfoNode(ServiceInfo serviceInfo) {
        try {
            String serviceNameNodePath = getServiceNameNodePath(serviceInfo.getServiceName());
            // 子节点用一个uuid做path防重复
            String providerInfoNodePath = serviceNameNodePath + "/" + UUID.randomUUID();

            String providerInfoJsonStr = JsonUtil.obj2Str(serviceInfo);

            // providerInfo节点是临时节点(如果节点宕机了，zk的连接断开一段时间后，临时节点会被自动删除)
            curatorZkClient.create()
                .withMode(CreateMode.EPHEMERAL)
                .forPath(providerInfoNodePath, providerInfoJsonStr.getBytes(StandardCharsets.UTF_8));
            logger.info("createProviderInfoNode success! path={}", providerInfoNodePath);
        } catch (Exception e) {
            throw new MyRpcException("createProviderInfoNode error", e);
        }
    }

    private String getServiceNameNodePath(String serviceName) {
        return MyRpcRegistryConstants.BASE_PATH + "/" + serviceName;
    }

    private List&lt;ServiceInfo&gt; findProviderInfoList(String serviceName) {
        String serviceNameNodePath = getServiceNameNodePath(serviceName);

        try {
            List&lt;String&gt; providerInfoPathList = curatorZkClient.getChildren().forPath(serviceNameNodePath);
            List&lt;ServiceInfo&gt; serviceInfoList = new ArrayList&lt;&gt;();

            for(String providerInfoPath : providerInfoPathList){
                try{
                    String fullProviderInfoPath = serviceNameNodePath + "/" + providerInfoPath;
                    byte[] data = curatorZkClient.getData().forPath(fullProviderInfoPath);
                    String jsonStr = new String(data,StandardCharsets.UTF_8);
                    ServiceInfo serviceInfo = JsonUtil.json2Obj(jsonStr,ServiceInfo.class);

                    serviceInfoList.add(serviceInfo);
                }catch (Exception e){
                    logger.error("findProviderInfoList getData error",e);
                }
            }

            logger.info("findProviderInfoList={}",JsonUtil.obj2Str(serviceInfoList));
            return serviceInfoList;
        } catch (Exception e) {
            throw new MyRpcException("findProviderInfoList error",e);
        }
    }

    private class ZkCuratorListener implements PathChildrenCacheListener {
        private final String serviceName;

        public ZkCuratorListener(String serviceName) {
            this.serviceName = serviceName;
        }

        @Override
        public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) {
            logger.info("ZookeeperListener process! serviceName={}",serviceName);

            try {
                // 刷新缓存
                List&lt;ServiceInfo&gt; serviceInfoList = findProviderInfoList(serviceName);
                serviceInfoCacheMap.put(serviceName,serviceInfoList);
            } catch (Exception e) {
                logger.error("ZookeeperListener getChildren error! serviceName={}",serviceName,e);
            }
        }
    }
}
Copy<br><br>现在客户端已经能通过服务发现得到实时的provider集合了，那么客户端发起请求时应该如何决定向哪个provider发起请求以实现provider侧的负载均衡呢？<br>
<br>常见的负载均衡算法有很多，MyRpc抽象出了负载均衡算法的接口，并实现了最简单的两种负载均衡算法(无权重的纯随机 + roundRobin)。
<br>在实际的环境中，每个provider可能机器的配置、网络延迟、运行时的动态负载、请求处理的延迟等都各有不同，优秀的负载均衡算法能够通过预先的配置和采集运行时的各项指标来计算出最优的请求顺序。<br>
MyRpc实现的负载均衡算法在这里只起到一个抛砖引玉的参考作用。
<br>负载均衡接口<br>/**
 * 负载均衡选择器
 * */
public interface LoadBalance {

    ServiceInfo select(List&lt;ServiceInfo&gt; serviceInfoList);
}
Copy<br>随机负载均衡<br>/**
 * 无权重，纯随机的负载均衡选择器
 * */
public class RandomLoadBalance implements LoadBalance{
    @Override
    public ServiceInfo select(List&lt;ServiceInfo&gt; serviceInfoList) {
        int selectedIndex = ThreadLocalRandom.current().nextInt(serviceInfoList.size());
        return serviceInfoList.get(selectedIndex);
    }
}
Copy<br>/**
 * 无权重的轮训负载均衡（后续增加带权重的轮训）
 * */
public class SimpleRoundRobinBalance implements LoadBalance{

    private final AtomicInteger count = new AtomicInteger();

    @Override
    public ServiceInfo select(List&lt;ServiceInfo&gt; serviceInfoList) {
        if(serviceInfoList.isEmpty()){
            throw new MyRpcException("serviceInfoList is empty!");
        }

        // 考虑一下溢出，取绝对值
        int selectedIndex = Math.abs(count.getAndIncrement());
        return serviceInfoList.get(selectedIndex % serviceInfoList.size());
    }
}
Copy<br><br>由于需要通过网络发起rpc调用，比起本地调用很容易因为网络波动、远端机器故障等原因而导致调用失败。<br>
客户端有时希望能通过重试等方式屏蔽掉可能出现的偶发错误，尽可能的保证rpc请求的成功率，最好rpc框架能解决这个问题。但另一方面，能够安全重试的基础是下游服务能够做到幂等，否则重复的请求会带来意想不到的后果，而不幂等的下游服务只能至多调用一次。<br>
因此rpc框架需要能允许用户不同的服务可以有不同的集群服务调用方式，这样幂等的服务可以配置成可自动重试N次的failover调用、只能调用1次的fast-fail调用或者广播调用等等方式。<br>MyRpc的Invoker接口用于抽象上述的不同集群调用方式，并简单的实现了failover和fast-fail等多种调用方式（参考dubbo）。<br>public interface InvokerCallable {
    RpcResponse invoke(NettyClient nettyClient);
}
Copy<br>/**
 * 不同的集群调用方式
 * */
public interface Invoker {

    RpcResponse invoke(InvokerCallable callable, String serviceName,
                                      Registry registry, LoadBalance loadBalance);
}
Copy<br>/**
 * 快速失败，无论成功与否调用1次就返回
 * */
public class FastFailInvoker implements Invoker {

  private static final Logger logger = LoggerFactory.getLogger(FastFailInvoker.class);

  @Override
  public RpcResponse invoke(InvokerCallable callable, String serviceName,
                            Registry registry, LoadBalance loadBalance) {
    List&lt;ServiceInfo&gt; serviceInfoList = registry.discovery(serviceName);
    logger.debug("serviceInfoList.size={},serviceInfoList={}",serviceInfoList.size(), JsonUtil.obj2Str(serviceInfoList));
    NettyClient nettyClient = InvokerUtil.getTargetClient(serviceInfoList,loadBalance);
    logger.info("ClientDynamicProxy getTargetClient={}", nettyClient);

    // fast-fail，简单的调用一次就行，有错误就直接向上抛
    return callable.invoke(nettyClient);
  }
}
Copy<br>/**
 * 故障转移调用(如果调用出现了错误，则重试指定次数)
 * 1 如果重试过程中成功了，则快读返回
 * 2 如果重试了指定次数后还是没成功，则抛出异常
 * */
public class FailoverInvoker implements Invoker {

    private static final Logger logger = LoggerFactory.getLogger(FailoverInvoker.class);

    private final int defaultRetryCount = 2;
    private final int retryCount;

    public FailoverInvoker() {
        this.retryCount = defaultRetryCount;
    }

    public FailoverInvoker(int retryCount) {
        this.retryCount = Math.max(retryCount,1);
    }

    @Override
    public RpcResponse invoke(InvokerCallable callable, String serviceName, Registry registry, LoadBalance loadBalance) {
        MyRpcException myRpcException = null;

        for(int i=0; i&lt;retryCount; i++){
            List&lt;ServiceInfo&gt; serviceInfoList = registry.discovery(serviceName);
            logger.debug("serviceInfoList.size={},serviceInfoList={}",serviceInfoList.size(), JsonUtil.obj2Str(serviceInfoList));
            NettyClient nettyClient = InvokerUtil.getTargetClient(serviceInfoList,loadBalance);
            logger.info("ClientDynamicProxy getTargetClient={}", nettyClient);

            try {
                RpcResponse rpcResponse = callable.invoke(nettyClient);
                if(myRpcException != null){
                    // 虽然最终重试成功了，但是之前请求失败过
                    logger.warn("FailRetryInvoker finally success, but there have been failed providers");
                }
                return rpcResponse;
            }catch (Exception e){
                myRpcException = new MyRpcException(e);

                logger.warn("FailRetryInvoker callable.invoke error",e);
            }
        }

        // 走到这里说明经过了retryCount次重试依然不成功，myRpcException一定不为null
        throw myRpcException;
    }
}
Copy<br><br>/**
 * 客户端动态代理
 * */
public class ClientDynamicProxy implements InvocationHandler {

    private static final Logger logger = LoggerFactory.getLogger(ClientDynamicProxy.class);

    private final Registry registry;
    private final LoadBalance loadBalance;
    private final Invoker invoker;

    public ClientDynamicProxy(Registry registry, LoadBalance loadBalance, Invoker invoker) {
        this.registry = registry;
        this.loadBalance = loadBalance;
        this.invoker = invoker;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        Tuple&lt;Object,Boolean&gt; localMethodResult = processLocalMethod(proxy,method,args);
        if(localMethodResult.getRight()){
            // right为true,代表是本地方法，返回toString等对象自带方法的执行结果，不发起rpc调用
            return localMethodResult.getLeft();
        }

        logger.debug("ClientDynamicProxy before: methodName=" + method.getName());

        String serviceName = method.getDeclaringClass().getName();

        // 构造请求和协议头
        RpcRequest rpcRequest = new RpcRequest();
        rpcRequest.setInterfaceName(method.getDeclaringClass().getName());
        rpcRequest.setMethodName(method.getName());
        rpcRequest.setParameterClasses(method.getParameterTypes());
        rpcRequest.setParams(args);

        MessageHeader messageHeader = new MessageHeader();
        messageHeader.setMessageFlag(MessageFlagEnums.REQUEST.getCode());
        messageHeader.setTwoWayFlag(false);
        messageHeader.setEventFlag(true);
        messageHeader.setSerializeType(GlobalConfig.messageSerializeType.getCode());
        messageHeader.setResponseStatus((byte)'a');
        messageHeader.setMessageId(rpcRequest.getMessageId());

        logger.debug("ClientDynamicProxy rpcRequest={}", JsonUtil.obj2Str(rpcRequest));

        RpcResponse rpcResponse = this.invoker.invoke((nettyClient)-&gt;{
            Channel channel = nettyClient.getChannel();
            // 将netty的异步转为同步,参考dubbo DefaultFuture
            DefaultFuture&lt;RpcResponse&gt; newDefaultFuture = DefaultFutureManager.createNewFuture(channel,rpcRequest);

            try {
                nettyClient.send(new MessageProtocol&lt;&gt;(messageHeader,rpcRequest));

                // 调用方阻塞在这里
                return newDefaultFuture.get();
            } catch (Exception e) {
                throw new MyRpcException("InvokerCallable error!",e);
            }
        },serviceName,registry,loadBalance);

        logger.debug("ClientDynamicProxy defaultFuture.get() rpcResponse={}",rpcResponse);

        return processRpcResponse(rpcResponse);
    }

    /**
     * 处理本地方法
     * @return tuple.right 标识是否是本地方法， true是
     * */
    private Tuple&lt;Object,Boolean&gt; processLocalMethod(Object proxy, Method method, Object[] args) throws Exception {
        // 处理toString等对象自带方法，不发起rpc调用
        if (method.getDeclaringClass() == Object.class) {
            return new Tuple&lt;&gt;(method.invoke(proxy, args),true);
        }
        String methodName = method.getName();
        Class&lt;?&gt;[] parameterTypes = method.getParameterTypes();
        if (parameterTypes.length == 0) {
            if ("toString".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.toString(),true);
            } else if ("hashCode".equals(methodName)) {
                return new Tuple&lt;&gt;(proxy.hashCode(),true);
            }
        } else if (parameterTypes.length == 1 &amp;&amp; "equals".equals(methodName)) {
            return new Tuple&lt;&gt;(proxy.equals(args[0]),true);
        }

        // 返回null标识非本地方法，需要进行rpc调用
        return new Tuple&lt;&gt;(null,false);
    }

    private Object processRpcResponse(RpcResponse rpcResponse){
        if(rpcResponse.getExceptionValue() == null){
            // 没有异常，return正常的返回值
            return rpcResponse.getReturnValue();
        }else{
            // 有异常，往外抛出去
            throw new MyRpcRemotingException(rpcResponse.getExceptionValue());
        }
    }
}
Copy<br><br>客户端发起请求后，可能由于网络原因，可能由于服务端负载过大等原因而迟迟无法收到回复。<br>
出于性能或者自身业务的考虑，客户端不能无限制的等待下去，因此rpc框架需要能允许客户端设置请求的超时时间。在一定的时间内如果无法收到响应则需要抛出超时异常，令调用者及时的感知到问题。<br>在客户端侧DefaultFuture.get方法，指定超时时间是可以做到这一点的。<br>
但其依赖底层操作系统的定时任务机制，虽然超时时间的精度很高(nanos级别)，但在高并发场景下性能不如时间轮。<br>
具体原理可以参考我之前的博客：<a data-tooltip-position="top" aria-label="https://www.cnblogs.com/xiaoxiongcanguan/p/17128575.html" rel="noopener" class="external-link" href="https://www.cnblogs.com/xiaoxiongcanguan/p/17128575.html" target="_blank">时间轮TimeWheel工作原理解析</a><br>MyRpc参考dubbo，引入时间轮来实现客户端设置请求超时时间的功能。<br>public class DefaultFutureManager {

    private static final Logger logger = LoggerFactory.getLogger(DefaultFutureManager.class);

    public static final Map&lt;Long,DefaultFuture&gt; DEFAULT_FUTURE_CACHE = new ConcurrentHashMap&lt;&gt;();
    public static final HashedWheelTimer TIMER = new HashedWheelTimer();

    public static void received(RpcResponse rpcResponse){
        Long messageId = rpcResponse.getMessageId();

        logger.debug("received rpcResponse={},DEFAULT_FUTURE_CACHE={}",rpcResponse,DEFAULT_FUTURE_CACHE);
        DefaultFuture defaultFuture = DEFAULT_FUTURE_CACHE.remove(messageId);

        if(defaultFuture != null){
            logger.debug("remove defaultFuture success");
            if(rpcResponse.getExceptionValue() != null){
                // 异常处理
                defaultFuture.completeExceptionally(rpcResponse.getExceptionValue());
            }else{
                // 正常返回
                defaultFuture.complete(rpcResponse);
            }
        }else{
            // 可能超时了，接到响应前已经remove掉了这个future(超时和实际接到请求都会调用received方法)
            logger.debug("remove defaultFuture fail");
        }
    }

    public static DefaultFuture createNewFuture(Channel channel, RpcRequest rpcRequest){
        DefaultFuture defaultFuture = new DefaultFuture(channel,rpcRequest);
        // 增加超时处理的逻辑
        newTimeoutCheck(defaultFuture);

        return defaultFuture;
    }

    public static DefaultFuture getFuture(long messageId){
        return DEFAULT_FUTURE_CACHE.get(messageId);
    }

    /**
     * 增加请求超时的检查任务
     * */
    public static void newTimeoutCheck(DefaultFuture defaultFuture){
        TimeoutCheckTask timeoutCheckTask = new TimeoutCheckTask(defaultFuture.getMessageId());
        TIMER.newTimeout(timeoutCheckTask, defaultFuture.getTimeout(), TimeUnit.MILLISECONDS);
    }
}
Copy<br>public class TimeoutCheckTask implements TimerTask {

    private final long messageId;

    public TimeoutCheckTask(long messageId) {
        this.messageId = messageId;
    }

    @Override
    public void run(Timeout timeout) {
        DefaultFuture defaultFuture = DefaultFutureManager.getFuture(this.messageId);
        if(defaultFuture == null || defaultFuture.isDone()){
            // 请求已经在超时前返回，处理过了,直接返回即可
            return;
        }

        // 构造超时的响应
        RpcResponse rpcResponse = new RpcResponse();
        rpcResponse.setMessageId(this.messageId);
        rpcResponse.setExceptionValue(new MyRpcTimeoutException(
            "request timeout：" + defaultFuture.getTimeout() + " channel=" + defaultFuture.getChannel()));

        DefaultFutureManager.received(rpcResponse);
    }
}
Copy<br><br>
<br>经过两个迭代，目前MyRpc已经是一个麻雀虽小五脏俱全的rpc框架了。<br>
虽然无论在功能上还是在各种细节的处理上都还有很多需要优化的地方，但作为一个demo级别的框架，其没有过多的抽象封装，更有利于rpc框架的初学者去理解。
<br>做为Mit6.824课程学习的一部分，rpc的实现到此就暂时告一段落。后续我会继续分享实现简易raft kv数据库的学习心得。
<br>博客中展示的完整代码在我的github上：<a rel="noopener" class="external-link" href="https://github.com/1399852153/MyRpc" target="_blank">https://github.com/1399852153/MyRpc</a> (release/lab2分支)，内容如有错误，还请多多指教。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/自己动手实现rpc框架(二)-实现集群间rpc通信.html</link><guid isPermaLink="false">Computer Science/Distributed System/RPC框架的实现/自己动手实现rpc框架(二) 实现集群间rpc通信.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 25 Jul 2024 09:22:05 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172146.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/distributed-system/rpc框架的实现/pasted-image-20240725172146.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[分布式任务调度框架]]></title><description><![CDATA[ 
 <br>前段时间，公司要改造现有的单节点调度为分布式任务调度，然后就研究了目前市面上主流的开源分布式任务调度框架，用起来就一个感觉：麻烦！特别是之前在一个类里写了好多个调度任务，改造起来更加麻烦。我这人又比较懒，总感觉用了别人写好的工具还要改一大堆，心里就有点不舒服。于是我就想自己写一个框架，毕竟自己觉得分布式任务调度在所有分布式系统中是最简单的，因为一般公司任务调度本身不可能同时调度海量的任务，很大的并发，改造成分布式主要还是为了分散任务到多个节点，以便同一时间处理更多的任务。后面有一天，我在公司前台取快递，看到这样一个现象:我们好几个同事(包括我)在前台那从头到尾看快递是不是自己的，是自己的就取走，不是就忽略，然后我就收到了启发。这个场景类比到分布式调度系统中，我们可以认为是快递公司或者快递员已经把每个快递按照我们名字电话分好了快递，我们只需要取走自己的就行了。但是从另外一个角度看，也可以理解成我们每个人都是从头到尾看了所有快递，然后按照某种约定的规则，如果是自己的快递就拿走，不是自己的就忽略继续看下一个。如果把快递想象成任务，一堆人去拿一堆快递也可以很顺利的拿到各自的快递，那么一堆节点自己去取任务是不是也可以很好的处理各自的任务呢?<br>传统的分布式任务调度都有一个调度中心，这个调度中心也都要部署称多节点的集群，避免单点故障，然后还有一堆执行器，执行器负责执行调度中心分发的任务。按照上面的启发，我的思路是放弃中心式的调度中心直接由各个执行器节点去公共的地方按照约定的规则去取任务，然后执行。设计示意图如下<br><img alt="Pasted image 20240507114701.png" src="https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/pasted-image-20240507114701.png">有人可能怀疑那任务db库不是有单点问题吗，我想反问下，难道其他的分布式任务调度框架没有这个问题吗？针对数据库单点我们可以单独类似业务库那样考虑高可用方案，这里不是这篇文章的讨论重点。很明显我们重点放在执行节点那里到底怎么保证高可用，单个任务不会被多个节点同时执行，单个节点执行到一半突然失联了，这个任务怎么办等复杂的问题。后续我们使用未经修饰的代码的方式一一解决这个问题（未经修饰主要是没有优化结构流水账式的代码风格，主要是很多人包括我自己看别人源码时总是感觉晕头转向的，仿佛置身迷宫般，看起来特别费劲，可能是我自己境界未到吧）<br>既然省略了集中式的调度，那么既然叫任务调度很明显必须要有调度的过程，不然多个节点去抢一个任务怎么避免冲突呢?我这里解决方式是：首先先明确一个任务的几种状态:待执行，执行中，有异常，已完成。每个节点起一个线程一直去查很快就要开始执行的待执行任务，然后遍历这些任务，使用乐观锁的方式先更新这个任务的版本号（版本号+1）和状态（变成执行中），如果更新成功就放入节点自己的延时队列中等待执行。由于每个节点的线程都是去数据库查待执行的任务，很明显变成执行中的任务下次就不会被其他节点再查询到了，至于对于那些在本节点更新状态之前就查到的待执行任务也会经过乐观锁尝试后更新失败从而跳过这个任务，这样就可以避免一个任务同时被多个节点重复执行。关键代码如下:<br>package com.rdpaas.task.scheduler;

import com.rdpaas.task.common.*;
import com.rdpaas.task.config.EasyJobConfig;
import com.rdpaas.task.repository.NodeRepository;
import com.rdpaas.task.repository.TaskRepository;
import com.rdpaas.task.strategy.Strategy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import javax.annotation.PostConstruct;

import java.util.Date;
import java.util.List;
import java.util.concurrent.*;

/**
 * 任务调度器
 * @author rongdi
 * @date 2019-03-13 21:15
 */
@Component
public class TaskExecutor {

    private static final Logger logger = LoggerFactory.getLogger(TaskExecutor.class);

    @Autowired
    private TaskRepository taskRepository;

    @Autowired
    private NodeRepository nodeRepository;

    @Autowired
    private EasyJobConfig config;/**
     * 创建任务到期延时队列
      */
	    private DelayQueue&lt;DelayItem&lt;Task&gt;&gt; taskQueue = new DelayQueue&lt;&gt;();

    /**
     * 可以明确知道最多只会运行2个线程，直接使用系统自带工具就可以了
     */
    private ExecutorService bossPool = Executors.newFixedThreadPool(2);

    /**
     * 声明工作线程池
     */
    private ThreadPoolExecutor workerPool;


    @PostConstruct
    public void init() {
/**
         * 自定义线程池，初始线程数量corePoolSize，线程池等待队列大小queueSize，当初始线程都有任务，并且等待队列满后
         * 线程数量会自动扩充最大线程数maxSize，当新扩充的线程空闲60s后自动回收.自定义线程池是因为Executors那几个线程工具
         * 各有各的弊端，不适合生产使用
         */
        workerPool = new ThreadPoolExecutor(config.getCorePoolSize(), config.getMaxPoolSize(), 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(config.getQueueSize()));
        /**
         * 执行待处理任务加载线程
         */
        bossPool.execute(new Loader());
        /**
         * 执行任务调度线程
         */
        bossPool.execute(new Boss());
    
    }

    class Loader implements Runnable {

        @Override
        public void run() {
            for(;;) {
                try { 
            /**
                     * 查找还有指定时间(单位秒)开始的主任务列表
                     */
                    List&lt;Task&gt; tasks = taskRepository.listPeddingTasks(config.getFetchDuration());
                    if(tasks == null || tasks.isEmpty()) {
                        continue;
                    }
                    for(Task task:tasks) {
                        
                        task.setStatus(TaskStatus.DOING);
                        task.setNodeId(config.getNodeId());
                        /**
                         * 使用乐观锁尝试更新状态，如果更新成功，其他节点就不会更新成功。如果在查询待执行任务列表
                         * 和当前这段时间有节点已经更新了这个任务，version必然和查出来时候的version不一样了,这里更新
                         * 必然会返回0了
                         */
                        int n = taskRepository.updateWithVersion(task);
                        Date nextStartTime = task.getNextStartTime();
                        if(n == 0 || nextStartTime == null) {
                            continue;
                        }
                        /**
                         * 封装成延时对象放入延时队列
                         */
                        task = taskRepository.get(task.getId());
                        DelayItem&lt;Task&gt; delayItem = new DelayItem&lt;Task&gt;(nextStartTime.getTime() - new Date().getTime(), task);
                        taskQueue.offer(delayItem);
                        
                    }
                    Thread.sleep(config.getFetchPeriod());
                } catch(Exception e) {
                    logger.error("fetch task list failed,cause by:{}", e);
                }
            }
        }
        
    }
    
    class Boss implements Runnable {
        @Override
        public void run() {
            for (;;) {
                try {
                     /**
                     * 时间到了就可以从延时队列拿出任务对象,然后交给worker线程池去执行
                     */
                    DelayItem&lt;Task&gt; item = taskQueue.take();
                    if(item != null &amp;&amp; item.getItem() != null) {
                        Task task = item.getItem();
                        workerPool.execute(new Worker(task));
                    }
                     
                } catch (Exception e) {
                    logger.error("fetch task failed,cause by:{}", e);
                }
            }
        }

    }

    class Worker implements Runnable {

        private Task task;

        public Worker(Task task) {
            this.task = task;
        }

        @Override
        public void run() {
            logger.info("Begin to execute task:{}",task.getId());
            TaskDetail detail = null;
            try {
                //开始任务
                detail = taskRepository.start(task);
                if(detail == null) return;
                //执行任务
                task.getInvokor().invoke();
                //完成任务
                finish(task,detail);
                logger.info("finished execute task:{}",task.getId());
            } catch (Exception e) {
                logger.error("execute task:{} error,cause by:{}",task.getId(), e);
                try {
                    taskRepository.fail(task,detail,e.getCause().getMessage());
                } catch(Exception e1) {
                    logger.error("fail task:{} error,cause by:{}",task.getId(), e);
                }
            }
        }

    }

    /**
     * 完成子任务，如果父任务失败了，子任务不会执行
     * @param task
     * @param detail
     * @throws Exception
     */
    private void finish(Task task,TaskDetail detail) throws Exception {

        //查看是否有子类任务
        List&lt;Task&gt; childTasks = taskRepository.getChilds(task.getId());
        if(childTasks == null || childTasks.isEmpty()) {
            //当没有子任务时完成父任务
            taskRepository.finish(task,detail);
            return;
        } else {
            for (Task childTask : childTasks) {
                //开始任务
                TaskDetail childDetail = null;
                try {
                    //将子任务状态改成执行中
                    childTask.setStatus(TaskStatus.DOING);
                    childTask.setNodeId(config.getNodeId());
                    //开始子任务
                    childDetail = taskRepository.startChild(childTask,detail);
                    //使用乐观锁更新下状态，不然这里可能和恢复线程产生并发问题
                    int n = taskRepository.updateWithVersion(childTask);
                    if (n &gt; 0) {
                        //再从数据库取一下，避免上面update修改后version不同步
                        childTask = taskRepository.get(childTask.getId());
                        //执行子任务
                        childTask.getInvokor().invoke();
                        //完成子任务
                        finish(childTask, childDetail);
                    }
                } catch (Exception e) {
                    logger.error("execute child task error,cause by:{}", e);
                    try {
                        taskRepository.fail(childTask, childDetail, e.getCause().getMessage());
                    } catch (Exception e1) {
                        logger.error("fail child task error,cause by:{}", e);
                    }
                }
            }
            /**
             * 当有子任务时完成子任务后再完成父任务
             */
            taskRepository.finish(task,detail);

        }

    }

}
Copy<br>如上所述，可以保证一个任务同一个时间只会被一个节点调度执行。这时候如果部署多个节点，正常应该可以很顺利的将任务库中的任务都执行到，就像一堆人去前台取快递一样，可以很顺利的拿走所有快递。毕竟对于每个快递不是自己的就是其他人的，自己的快递也不会是其他人的。但是这里的调度和取快递有一点不一样，取快递的每个人都知道怎么去区分到底哪个快递是自己的。这里的调度完全没这个概念，完全是哪个节点运气好使用乐观锁更新了这个任务状态就是哪个节点的。总的来说区别就是需要一个约定的规则，快递是不是自己的，直接看快递上的名字和手机号码就知道了。任务到底该不该自己执行我们也可以出一个这种规则，明确哪些任务那些应该是哪些节点可以执行，从而避免无谓的锁竞争。这里可以借鉴负载均衡的那些策略，目前我想实现如下规则:<br>
<br>id_hash : 按照任务自增id的对节点个数取余，余数值和当前节点的实时序号匹配，可以匹配就可以拿走执行，否则请自觉忽略掉这个任务。
<br>least_count：最少执行任务的节点优先去取任务。
<br>weight：按照节点权重去取任务。
<br>default： 默认先到先得，没有其它规则。
<br>根据上面规则也可以说是任务的负载均衡策略可以知道除了默认规则，其余规则都需要知道全局的节点信息，比如节点执行次数，节点序号，节点权重等，所以我们需要给节点添加一个心跳，隔一个心跳周期上报一下自己的信息到数据库，心跳核心代码如下:<br>/**
     * 创建节点心跳延时队列
      */
    private DelayQueue&lt;DelayItem&lt;Node&gt;&gt; heartBeatQueue = new DelayQueue&lt;&gt;();
 　　/**
     * 可以明确知道最多只会运行2个线程，直接使用系统自带工具
     */
    private ExecutorService bossPool = Executors.newFixedThreadPool(2);
　　　　 @PostConstruct
    public void init() {
        /**
         * 如果恢复线程开关是开着，并且心跳开关也是开着
         */
        if(config.isRecoverEnable() &amp;&amp; config.isHeartBeatEnable()) {
            /**
             * 初始化一个节点到心跳队列，延时为0，用来注册节点
             */
            heartBeatQueue.offer(new DelayItem&lt;&gt;(0,new Node(config.getNodeId())));
            /**
             * 执行心跳线程
             */
            bossPool.execute(new HeartBeat());
            /**
             * 执行异常恢复线程
             */
            bossPool.execute(new Recover());
        }
    }
 　　class HeartBeat implements Runnable {
        @Override
        public void run() {
            for(;;) {
                try {
                    /**
                     * 时间到了就可以从延时队列拿出节点对象，然后更新时间和序号，
                     * 最后再新建一个超时时间为心跳时间的节点对象放入延时队列，形成循环的心跳
                     */
                    DelayItem&lt;Node&gt; item = heartBeatQueue.take();
                    if(item != null &amp;&amp; item.getItem() != null) {
                        Node node = item.getItem();
                        handHeartBeat(node);
                    }
                    heartBeatQueue.offer(new DelayItem&lt;&gt;(config.getHeartBeatSeconds() * 1000,new Node(config.getNodeId())));
                } catch (Exception e) {
                    logger.error("task heart beat error,cause by:{} ",e);
                }
            }
        }
    }

    /**
     * 处理节点心跳
     * @param node
     */
    private void handHeartBeat(Node node) {
        if(node == null) {
            return;
        }
        /**
         * 先看看数据库是否存在这个节点
         * 如果不存在：先查找下一个序号，然后设置到node对象中，最后插入
         * 如果存在：直接根据nodeId更新当前节点的序号和时间
         */
        Node currNode= nodeRepository.getByNodeId(node.getNodeId());
        if(currNode == null) {
            node.setRownum(nodeRepository.getNextRownum());
            nodeRepository.insert(node);
        } else  {
            nodeRepository.updateHeartBeat(node.getNodeId());
        }

    }
Copy<br>数据库有了节点信息后，我们就可以实现各种花式的取任务的策略了，代码如下:<br>/**
 * 抽象的策略接口
 * @author rongdi
 * @date 2019-03-16 12:36
 */
public interface Strategy {

    /**
     * 默认策略
     */
    String DEFAULT = "default";
    
    /**
     * 按任务ID hash取余再和自己节点序号匹配
     */
    String ID_HASH = "id_hash";
    
    /**
     * 最少执行次数
     */
    String LEAST_COUNT = "least_count";
    
    /**
     * 按节点权重
     */
    String WEIGHT = "weight";
    
    
    public static Strategy choose(String key) {
        switch(key) {
            case ID_HASH:
                return new IdHashStrategy();
            case LEAST_COUNT:
                return new LeastCountStrategy();
            case WEIGHT:
                return new WeightStrategy();
            default:
                return new DefaultStrategy();
        }
    }
    
    public boolean accept(List&lt;Node&gt; nodes,Task task,Long myNodeId);
    
}
Copy<br>/**
 * 按照任务ID hash方式针对有效节点个数取余，然后余数+1后和各个节点的顺序号匹配，
 * 这种方式效果其实等同于轮询，因为任务id是自增的
 * @author rongdi
 * @date 2019-03-16
 */
public class IdHashStrategy implements Strategy {

    /**
     * 这里的nodes集合必然不会为空，外面调度那判断了，而且是按照nodeId的升序排列的
     */
    @Override
    public boolean accept(List&lt;Node&gt; nodes, Task task, Long myNodeId) {
        int size = nodes.size();
        long taskId = task.getId();
        /**
         * 找到自己的节点
         */
        Node myNode = nodes.stream().filter(node -&gt; node.getNodeId() == myNodeId).findFirst().get();
        return myNode == null ? false : (taskId % size) + 1 == myNode.getRownum();
    }

}
Copy<br>/**
 * 最少处理任务次数策略，也就是每次任务来了，看看自己是不是处理任务次数最少的，是就可以消费这个任务
 * @author rongdi
 * @date 2019-03-16 21:56
 */
public class LeastCountStrategy implements Strategy {

    @Override
    public boolean accept(List&lt;Node&gt; nodes, Task task, Long myNodeId) {

        /**
         * 获取次数最少的那个节点,这里可以类比成先按counts升序排列然后取第一个元素
         * 然后是自己就返回true
         */
        Optional&lt;Node&gt; min = nodes.stream().min((o1, o2) -&gt; o1.getCounts().compareTo(o2.getCounts()));
        
        return min.isPresent()? min.get().getNodeId() == myNodeId : false;
    }
    
}
Copy<br>/**
 * 按权重的分配策略,方案如下，假如
 * 节点序号   1     ,2     ,3       ,4
 * 节点权重   2     ,3     ,3       ,2
 * 则取余后 0,1 | 2,3,4 | 5,6,7 | 8,9
 * 序号1可以消费按照权重的和取余后小于2的
 * 序号2可以消费按照权重的和取余后大于等于2小于2+3的
 * 序号3可以消费按照权重的和取余后大于等于2+3小于2+3+3的
 * 序号3可以消费按照权重的和取余后大于等于2+3+3小于2+3+3+2的
 * 总结：本节点可以消费的按照权重的和取余后大于等于前面节点的权重和小于包括自己的权重和的这个范围
 * 不知道有没有大神有更好的算法思路
 * @author rongdi
 * @date 2019-03-16 23:16
 */
public class WeightStrategy implements Strategy {

    @Override
    public boolean accept(List&lt;Node&gt; nodes, Task task, Long myNodeId) {
        Node myNode = nodes.stream().filter(node -&gt; node.getNodeId() == myNodeId).findFirst().get();
        if(myNode == null) {
            return false;
        }
        /**
         * 计算本节点序号前面的节点的权重和
         */
        int preWeightSum = nodes.stream().filter(node -&gt; node.getRownum() &lt; myNode.getRownum()).collect(Collectors.summingInt(Node::getWeight));
        /**
         * 计算全部权重的和
         */
        int weightSum = nodes.stream().collect(Collectors.summingInt(Node::getWeight));
        /**
         * 计算对权重和取余的余数
         */
        int remainder = (int)(task.getId() % weightSum);
        return remainder &gt;= preWeightSum &amp;&amp; remainder &lt; preWeightSum + myNode.getWeight();
    }
    
}
Copy<br>然后我们再改造下调度类:<br>/**
     * 获取任务的策略
     */
    private Strategy strategy;


    @PostConstruct
    public void init() {
        /**
         * 根据配置选择一个节点获取任务的策略
         */
        strategy = Strategy.choose(config.getNodeStrategy());
        /**
         * 自定义线程池，初始线程数量corePoolSize，线程池等待队列大小queueSize，当初始线程都有任务，并且等待队列满后
         * 线程数量会自动扩充最大线程数maxSize，当新扩充的线程空闲60s后自动回收.自定义线程池是因为Executors那几个线程工具
         * 各有各的弊端，不适合生产使用
         */
        workerPool = new ThreadPoolExecutor(config.getCorePoolSize(), config.getMaxPoolSize(), 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(config.getQueueSize()));
        /**
         * 执行待处理任务加载线程
         */
        bossPool.execute(new Loader());
        /**
         * 执行任务调度线程
         */
        bossPool.execute(new Boss());
    
    }

    class Loader implements Runnable {

        @Override
        public void run() {
            for(;;) {
                try { 
                    /**
                     * 先获取可用的节点列表
                     */
                    List&lt;Node&gt; nodes = nodeRepository.getEnableNodes(config.getHeartBeatSeconds() * 2);
                    if(nodes == null || nodes.isEmpty()) {
                        continue;
                    }
                    /**
                     * 查找还有指定时间(单位秒)开始的主任务列表
                     */
                    List&lt;Task&gt; tasks = taskRepository.listPeddingTasks(config.getFetchDuration());
                    if(tasks == null || tasks.isEmpty()) {
                        continue;
                    }
                    for(Task task:tasks) {
                        
                        boolean accept = strategy.accept(nodes, task, config.getNodeId());
                        /**
                         * 不该自己拿就不要抢
                         */
                        if(!accept) {
                            continue;
                        }
                        task.setStatus(TaskStatus.DOING);
                        task.setNodeId(config.getNodeId());
                        /**
                         * 使用乐观锁尝试更新状态，如果更新成功，其他节点就不会更新成功。如果在查询待执行任务列表
                         * 和当前这段时间有节点已经更新了这个任务，version必然和查出来时候的version不一样了,这里更新
                         * 必然会返回0了
                         */
                        int n = taskRepository.updateWithVersion(task);
                        Date nextStartTime = task.getNextStartTime();
                        if(n == 0 || nextStartTime == null) {
                            continue;
                        }
                        /**
                         * 封装成延时对象放入延时队列
                         */
                        task = taskRepository.get(task.getId());
                        DelayItem&lt;Task&gt; delayItem = new DelayItem&lt;Task&gt;(nextStartTime.getTime() - new Date().getTime(), task);
                        taskQueue.offer(delayItem);
                        
                    }
                    Thread.sleep(config.getFetchPeriod());
                } catch(Exception e) {
                    logger.error("fetch task list failed,cause by:{}", e);
                }
            }
        }
        
    }
Copy<br>如上可以通过各种花式的负载策略来平衡各个节点获取的任务，同时也可以显著降低各个节点对同一个任务的竞争。但是还有个问题，假如某个节点拿到了任务更新成了执行中，执行到一半，没执行完也没发生异常，突然这个节点由于各种原因挂了，那么这时候这个任务永远没有机会再执行了。这就是传说中的占着茅坑不拉屎。解决这个问题可以用最终一致系统常见的方法，异常恢复线程。在这种场景下只需要检测一下指定心跳超时时间（比如默认3个心跳周期）下没有更新心跳时间的节点所属的未完成任务，将这些任务状态重新恢复成待执行，并且下次执行时间改成当前就可以了。核心代码如下:<br>class Recover implements Runnable {
        @Override
        public void run() {
            for (;;) {
                try {
                    /**
                     * 查找需要恢复的任务,这里界定需要恢复的任务是任务还没完成，并且所属执行节点超过3个
                     * 心跳周期没有更新心跳时间。由于这些任务由于当时执行节点没有来得及执行完就挂了，所以
                     * 只需要把状态再改回待执行，并且下次执行时间改成当前时间，让任务再次被调度一次
                     */
                    List&lt;Task&gt; tasks = taskRepository.listRecoverTasks(config.getHeartBeatSeconds() * 3);
                    if(tasks == null || tasks.isEmpty()) {
                        return;
                    }
                   /**
                    * 先获取可用的节点列表
                    */
                   List&lt;Node&gt; nodes = nodeRepository.getEnableNodes(config.getHeartBeatSeconds() * 2);
                   if(nodes == null || nodes.isEmpty()) {
                       return;
                   }
                   long maxNodeId = nodes.get(nodes.size() - 1).getNodeId();
                    for (Task task : tasks) {
                        /**
                         * 每个节点有一个恢复线程，为了避免不必要的竞争,从可用节点找到一个最靠近任务所属节点的节点
                         */
                        long currNodeId = chooseNodeId(nodes,maxNodeId,task.getNodeId());
                        long myNodeId = config.getNodeId();
                        /**
                         * 如果不该当前节点处理直接跳过
                         */
                        if(currNodeId != myNodeId) {
                            continue;
                        }
                        /**
                         * 直接将任务状态改成待执行，并且节点改成当前节点
                         */
                        task.setStatus(TaskStatus.PENDING);
                        task.setNextStartTime(new Date());
                        task.setNodeId(config.getNodeId());
                        taskRepository.updateWithVersion(task);
                    }
                    Thread.sleep(config.getRecoverSeconds() * 1000);
                } catch (Exception e) {
                    logger.error("Get next task failed,cause by:{}", e);
                }
            }
        }

    }
　　/**
     * 选择下一个节点
     * @param nodes
     * @param maxNodeId
     * @param nodeId
     * @return
     */
    private long chooseNodeId(List&lt;Node&gt; nodes,long maxNodeId,long nodeId) {
        if(nodes.size() == 0 || nodeId &gt;= maxNodeId) {
            return nodes.get(0).getNodeId();
        }
        return nodes.stream().filter(node -&gt; node.getNodeId() &gt; nodeId).findFirst().get().getNodeId();
    }
Copy<br>如上为了避免每个节点的异常恢复线程对同一个任务做无谓的竞争，每个异常任务只能被任务所属节点ID的下一个正常节点去恢复。这样处理后就能确保就算出现了上面那种任务没执行完节点挂了的情况，一段时间后也可以自动恢复。总的来说上面那些不考虑优化应该可以做为一个还不错的任务调度框架了。如果你们以为这样就完了，我只能说抱歉了，还有，哈哈！前面提到我是嫌弃其它任务调度用起来麻烦，特别是习惯用spring的注解写调度的，那些很可能一个类里写了n个带有@Scheduled注解的调度方法，这样改造起来更加麻烦，我是希望做到如下方式就可以直接整合到分布式任务调度里：<br>/**
 * 测试调度功能
 * @author rongdi
 * @date 2019-03-17 16:54
 */
@Component
public class SchedulerTest {

    @Scheduled(cron = "0/10 * * * * ?")
    public void test1() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间1:"+sdf.format(new Date()));
    }
    
    @Scheduled(cron = "0/20 * * * * ?",parent = "test1")
    public void test2() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间2:"+sdf.format(new Date()));
    }

    @Scheduled(cron = "0/10 * * * * ?",parent = "test2")
    public void test3() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间3:"+sdf.format(new Date()));
    }

    @Scheduled(cron = "0/10 * * * * ?",parent = "test3")
    public void test4() throws InterruptedException {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Thread.sleep(2000);
        System.out.println("当前时间4:"+sdf.format(new Date()));
    }

}
Copy<br>为了达到上述目标，我们还需要在spring启动后加载自定义的注解（名称和spring的一样），代码如下:<br>/**
 * spring容器启动完后，加载自定义注解
 * @author rongdi
 * @date 2019-03-15 21:07
 */
@Component
public class ContextRefreshedListener implements ApplicationListener&lt;ContextRefreshedEvent&gt; {

    @Autowired
    private TaskExecutor taskExecutor;

    /**
     * 用来保存方法名/任务名和任务插入后数据库的ID的映射,用来处理子任务新增用
     */
    private Map&lt;String,Long&gt; taskIdMap = new HashMap&lt;&gt;();

    @Override
    public void onApplicationEvent(ContextRefreshedEvent event) {
        /**
         * 判断根容器为Spring容器，防止出现调用两次的情况（mvc加载也会触发一次）
          */
        if(event.getApplicationContext().getParent()==null){
            /**
             * 判断调度开关是否打开
             * 如果打开了：加载调度注解并将调度添加到调度管理中
             */
            ApplicationContext context = event.getApplicationContext();
            Map&lt;String,Object&gt; beans = context.getBeansWithAnnotation(org.springframework.scheduling.annotation.EnableScheduling.class);
            if(beans == null) {
                return;
            }
            /**
             * 用来存放被调度注解修饰的方法名和Method的映射
             */
            Map&lt;String,Method&gt; methodMap = new HashMap&lt;&gt;();
            /**
             * 查找所有直接或者间接被Component注解修饰的类，因为不管Service，Controller等都包含了Component，也就是
             * 只要是被纳入了spring容器管理的类必然直接或者间接的被Component修饰
             */
            Map&lt;String,Object&gt; allBeans = context.getBeansWithAnnotation(org.springframework.stereotype.Component.class);
            Set&lt;Map.Entry&lt;String,Object&gt;&gt; entrys = allBeans.entrySet();
            /**
             * 遍历bean和里面的method找到被Scheduled注解修饰的方法,然后将任务放入任务调度里
             */
            for(Map.Entry entry:entrys){
                Object obj = entry.getValue();
                Class clazz = obj.getClass();
                Method[] methods = clazz.getMethods();
                for(Method m:methods) {
                    if(m.isAnnotationPresent(Scheduled.class)) {
                        methodMap.put(clazz.getName() + Delimiters.DOT + m.getName(),m);
                    }
                }
            }
            /**
             * 处理Sheduled注解
             */
            handleSheduledAnn(methodMap);
            /**
             * 由于taskIdMap只是启动spring完成后使用一次，这里可以直接清空
             */
            taskIdMap.clear();
        }
    }

    /**
     * 循环处理方法map中的所有Method
     * @param methodMap
     */
    private void handleSheduledAnn(Map&lt;String,Method&gt; methodMap) {
        if(methodMap == null || methodMap.isEmpty()) {
            return;
        }
        Set&lt;Map.Entry&lt;String,Method&gt;&gt; entrys = methodMap.entrySet();
        /**
         * 遍历bean和里面的method找到被Scheduled注解修饰的方法,然后将任务放入任务调度里
         */
        for(Map.Entry&lt;String,Method&gt; entry:entrys){
            Method m = entry.getValue();
            try {
                handleSheduledAnn(methodMap,m);
            } catch (Exception e) {
                e.printStackTrace();
                continue;
            }
        }
    }

    /**
     * 递归添加父子任务
     * @param methodMap
     * @param m
     * @throws Exception
     */
    private void handleSheduledAnn(Map&lt;String,Method&gt; methodMap,Method m) throws Exception {
        Class&lt;?&gt; clazz = m.getDeclaringClass();
        String name = m.getName();
        Scheduled sAnn = m.getAnnotation(Scheduled.class);
        String cron = sAnn.cron();
        String parent = sAnn.parent();
        /**
         * 如果parent为空，说明该方法代表的任务是根任务，则添加到任务调度器中，并且保存在全局map中
         * 如果parent不为空，则表示是子任务，子任务需要知道父任务的id
         * 先根据parent里面代表的方法全名或者方法名（父任务方法和子任务方法在同一个类直接可以用方法名，
         * 不然要带上类的全名）从taskIdMap获取父任务ID
         * 如果找不到父任务ID，先根据父方法全名在methodMap找到父任务的method对象，调用本方法递归下去
         * 如果找到父任务ID，则添加子任务
         */
        if(StringUtils.isEmpty(parent)) {
            if(!taskIdMap.containsKey(clazz.getName() + Delimiters.DOT + name)) {
                Long taskId = taskExecutor.addTask(name, cron, new Invocation(clazz, name, new Class[]{}, new Object[]{}));
                taskIdMap.put(clazz.getName() + Delimiters.DOT + name, taskId);
            }
        } else {
            String parentMethodName = parent.lastIndexOf(Delimiters.DOT) == -1 ? clazz.getName() + Delimiters.DOT + parent : parent;
            Long parentTaskId = taskIdMap.get(parentMethodName);
            if(parentTaskId == null) {
                Method parentMethod = methodMap.get(parentMethodName);
                handleSheduledAnn(methodMap,parentMethod);
                /**
                 * 递归回来一定要更新一下这个父任务ID
                 */
                parentTaskId = taskIdMap.get(parentMethodName);
            }
            if(parentTaskId != null &amp;&amp; !taskIdMap.containsKey(clazz.getName() + Delimiters.DOT + name)) {
                Long taskId = taskExecutor.addChildTask(parentTaskId, name, cron, new Invocation(clazz, name, new Class[]{}, new Object[]{}));
                taskIdMap.put(clazz.getName() + Delimiters.DOT + name, taskId);
            }

        }


    }
}
Copy<br>上述代码就完成了spring初始化完成后加载了自己的自定义任务调度的注解，并且也受spring的调度开关@EnableScheduling的控制，实现无缝整合到spring或者springboot中去，达到了我这种的懒人的要求。<br>好了其实写这个框架差不多就用了5天业余时间，估计会有一些隐藏的坑，不过明显的坑我自己都解决了，开源出来的目的既是为了抛砖引玉，也为了广大屌丝程序员提供一种新的思路，希望对大家有所帮助，同时也希望大家多帮忙找找bug，一起来完善这个东西,大神们请忽略。文笔不好，主要是好久没写作文了，请大家多多担待。详细的流水账式的源码加上长篇大论式的汉语注释尽情查看:<a rel="noopener" class="external-link" href="https://github.com/rongdi/easy-job" target="_blank">https://github.com/rongdi/easy-job</a>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/分布式任务调度框架.html</link><guid isPermaLink="false">Computer Science/Distributed System/分布式任务调度框架/分布式任务调度框架.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 25 Jul 2024 09:27:09 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/pasted-image-20240507114701.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/distributed-system/分布式任务调度框架/pasted-image-20240507114701.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Basic System Architecture]]></title><description><![CDATA[ 
 <br>In this article, we’ll introduce some of the fundamental approaches to scaling a software system. The type of systems this series of articles is oriented towards are the Internet-facing systems we all utilize every day. I’ll let you name your favorite. These systems accept requests from users through Web and mobile interfaces, store and retrieve data based on user requests or events (e.g. a GPS-based system), and have some intelligent features such as providing recommendations or providing notifications based on previous user interactions.<br>We’ll start with a simple system design and show how it can be scaled. In the process, several concepts will be introduced that we’ll cover in much more detail later in this series. Hence this article gives a broad overview of these concepts and how they aid in scalability — truly a whirlwind tour! If you’ve missed Part 1 in this series, <a data-tooltip-position="top" aria-label="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" rel="noopener" class="external-link" href="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" target="_blank">here it is</a>!<br><br>Virtually all massive-scale systems start off small and grow due to their success. It’s common, and sensible, to start with a development framework such as Ruby on Rails or Django or equivalent that promotes rapid development to get a system quickly up and running. A typical, very simple software architecture for ‘starter’ systems that closely resembles what you get with rapid development frameworks is shown in Figure 1. This comprises a client tier, application service tier, and a database tier. If you use Rails or equivalent, you also get a framework that hardwires a Model-View-Controller (MVC) pattern for Web application processing and an Object-Relational Mapper (ORM) that generates SQL queries.<br>With this architecture, users submit requests to the application from their mobile app or Web browser across the Internet. The magic of Internet networking delivers these requests to the application service which is running on a machine hosted in some corporate or commercial cloud data center. Communications uses a standard network protocol, typically HTTP.<br>The application service runs code that supports an application programming interface (API) that clients use to format data and send HTTP requests to. Upon receipt of a request, the service executes the code associated with the requested API. In the process, it may read from or write to a database, depending on the semantics of the API. When the request is complete, the server sends the results to the client to display in their app or browser.<br><img alt="Basic system architecture" src="https://miro.medium.com/v2/resize:fit:584/1*MTjAf75AZDXNDLO7iWgsxQ.png" referrerpolicy="no-referrer"><br>Figure 1 Basic Multi-Tier Distributed Systems Architecture<br>Many systems conceptually look exactly like this. The application service code exploits an execution environment that enables multiple requests from multiple users to be processed simultaneously. There’s a myriad of these application server technologies — JEE and Spring for Java, <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Flask_(web_framework)" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Flask_(web_framework)" target="_blank">Flask for Python</a> – that are widely used in this scenario.<br>This approach leads to what is generally known as a monolithic architecture. Single, monolithic services grow in complexity as the application becomes more feature-rich. This eventually makes it hard to modify and test rapidly, and their execution footprint can become extremely heavyweight as all the API implementations run in the same application service.<br>Still, as long as request loads stay relatively low, this application architecture can suffice. The service has the capacity to process requests with consistently low latency. If request loads keep growing, this means eventually latencies will grow as the server has insufficient CPU/memory capacity for the concurrent request volume and hence requests will take longer to process. In these circumstances, our single server is overloaded and has become a bottleneck.<br>In this case, the first strategy for scaling is usually to ‘scale up’ the application service hardware. For example, if your application is running on AWS, you might upgrade your server from a modest t3.xlarge instance with 4 (virtual) CPUs and 16GBs of memory to a t3.2xlarge instance <a data-tooltip-position="top" aria-label="https://aws.amazon.com/ec2/instance-types/" rel="noopener" class="external-link" href="https://aws.amazon.com/ec2/instance-types/" target="_blank">which doubles the number of vCPUs and memory available for the application</a>.<br>Scale up is simple. It gets many real-world applications a long way to supporting larger workloads. It obviously just costs more money for hardware, but that’s scaling for you.<br>It’s inevitable however that for many applications the load will grow to a level that will swamp a single server node, no matter how many CPUs and how much memory you have. That’s when you need a new strategy — namely scaling out, or horizontal scaling, that we touched on <a data-tooltip-position="top" aria-label="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" rel="noopener" class="external-link" href="https://medium.com/swlh/building-scalable-distributed-systems-part-1-introduction-to-scalable-systems-9ca471fd77d7" target="_blank">in the first article in this series</a>.<br><br>Scaling out relies on the ability to replicate a service in the architecture and run multiple copies on multiple server nodes. Requests from clients are distributed across the replicas so that in theory, if we have N replicas, each server node processes {#requests/N} requests. This simple strategy increases an application’s capacity and hence scalability.<br>To successfully scale out an application, we need two fundamental elements in our design. As illustrated in Figure 2, these are:<br>A Load balancer: All user requests are sent to a load balancer, which chooses a service replica to process the request. Various strategies exist for choosing a service, all with the core aim of keeping each processing resource equally busy. The load balancer also relays the responses from the service back to the client. Most load balancers belong to a class of Internet components known as <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Reverse_proxy" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Reverse_proxy" target="_blank">reverse proxies</a>, which control access to server resources for client requests. As an intermediary, reverse proxies add an extra network hop for a request, and hence need to be extremely low latency to minimize the overheads they introduce. There are many off-the-shelf load balancing solutions as well as cloud-provider specific ones, and we’ll cover the general characteristics of these in much more detail in a later article.<br>Stateless services: For load balancing to be effective and share requests evenly, the load balancer must be free to send consecutive requests from the same client to different service instances for processing. This means the API implementations in the services must retain no knowledge, or state, associated with an individual client’s session. When a user accesses an application, a user session is created by the service, and a unique session identified is managed internally to identify the sequence of user interactions and track session state. A classic example of session state is a shopping cart. To use a load balancer effectively, the data representing the current contents of a user’s cart must be stored somewhere — typically a data store — such that any service replica can access this state when it receives a request as part of a user session. In Figure 2 this is labeled as a Session Store.<br><img alt="Scaling out the service tier with a load balancer" src="https://miro.medium.com/v2/resize:fit:802/1*ZoDFOdsaG-hVdRegLsoR4w.png" referrerpolicy="no-referrer"><br>Figure 2 Scale out Architecture<br>Scale out is attractive as, in theory, you can keep adding new (virtual) hardware and services to handle increased user request loads and keep latencies consistent and low. As soon as you see latencies rising, you deploy another service instance. This requires no code changes and hence is relatively cheap — you just pay for the hardware you deploy.<br>Scale out has another highly attractive feature. If one of the services fails, the requests it is processing will be lost. But as the failed service manages no session state, these requests can be simply reissued by the client and sent to another service instance for processing. This means the application is resilient to failures in the application service software and hardware, thus enhancing the application’s availability. Availability is a key feature of distributed systems and one we will discuss in-depth in a later article too.<br>Unfortunately, as with any engineering solution, simple scaling out has limits. As you add new service instances, the request processing capacity grows, potentially infinitely. At some stage, however, reality will bite and the capability of your single database to provide low latency query responses will diminish. Slow queries will mean longer response times for clients. If requests keep arriving faster than they are being processed, some system components will fail as resources will be exhausted and clients will see exceptions and request timeouts. Essentially your database has become a bottleneck that you must engineer away in order to scale your application further.<br><br>Scaling up by increasing the number of CPUs, memory and disks in a database server can go a long way to increasing a system’s capacity. For example, at the time of writing Google Cloud Platform can provision a SQL database on a db-n1-highmem-96 node, which has 96 vCPUs, 624GB of memory, 30TBs of disk and can support 4000 connections. This will cost somewhere between $6K and $16K per year, which sounds a good deal to me. Scaling up is a very common database scalability strategy.<br>Large databases need constant care and attention from highly skilled database administrators to keep them tuned and running fast. There’s a lot of wizardry in this job — e.g. query tuning, disk partitioning, indexing, on-node caching — and hence database administrators are valuable people that you want to be very nice to. They can make your application services highly responsive indeed.<br>In conjunction with scale up, a highly effective approach is to query the database as infrequently as possible in your services. This can be achieved by employing distributed caching in the service tier. A cache stores recently retrieved and commonly accessed database results in memory so they can be quickly retrieved without placing a burden on the database. For data that is frequently read and changes rarely, your processing logic must be modified to first check a distributed cache, such as a <a data-tooltip-position="top" aria-label="https://redis.io/" rel="noopener" class="external-link" href="https://redis.io/" target="_blank">Redis</a> or <a data-tooltip-position="top" aria-label="https://memcached.org/" rel="noopener" class="external-link" href="https://memcached.org/" target="_blank">memcached</a> store. These cache technologies are essentially distributed Key-Value stores with very simple APIs. This scheme is illustrated in Figure 3. Note that the Session Store from Figure 2 has disappeared. This is because we can use a general-purpose distributed cache to store session identifiers along with application data.<br>Accessing the cache requires a remote call from your service, but if the data you need is in the cache, on a fast network this is far less expensive than querying the database instance. Introducing a caching layer also requires your processing logic to be modified to check for cached data. If what you want is not in the cache — a cache miss — your code must still query the database and load the results into the cache as well as return the results to the caller. You also need to decide when to remove or invalidate cached results — this depends on your application’s tolerance to serving stale results to clients and the volume of data you cache.<br><img alt="Introducing a distributed cache" src="https://miro.medium.com/v2/resize:fit:762/1*7KFWeMMfq814m5pEnp21_Q.png" referrerpolicy="no-referrer"><br>Figure 3 Introducing Distributed Caching<br>A well-designed caching scheme can be absolutely invaluable in scaling a system. Caching works great for data that rarely changes and is accessed frequently, such as inventory, event and contact data. If you can handle a large percentage, like 80% or more, of read requests from your cache, then you effectively buy extra capacity at your databases as they are not involved in handling requests.<br>Still, many systems need to rapidly access many terabytes and larger data stores, which make a single database effectively prohibitive. In these systems, a distributed database is needed.<br><br>There are more distributed database technologies around in 2020 than you probably want to imagine. It’s a complex area, and one we’ll cover extensively in another article. In very general terms, there are two major categories:<br>
<br>Distributed SQL stores from major vendors such as Oracle and IBM. These enable organizations to scale out their SQL database relatively seamlessly by storing the data across multiple disks that are queried by multiple database engine replicas. These multiple engines logically appear to the application as a single database, hence minimizing code changes.
<br>Distributed so-called NoSQL stores from a whole array of vendors. These products use a variety of data models and query languages. They distribute data across multiple nodes that run the database engine, each with their own locally attached storage. Again, the location of the data is transparent to the application and typically controlled by the design of the data model through hashing functions on database keys. Leading products in this category are Cassandra, MongoDB and Neo4j.<br>
<img alt="Figure 4 Scaling the Data Tier using a Distributed Database" src="https://miro.medium.com/v2/resize:fit:922/1*i34jGfA1SXllJoszNSz6Jw.png" referrerpolicy="no-referrer">
<br>Figure 4 Scaling the Data Tier using a Distributed Database<br>Figure 4 shows how our architecture incorporates a distributed database. As the data volumes grow, a distributed database has features to enable the number of storage nodes to be increased. As nodes are added (or removed), the data managed across all nodes is rebalanced to attempt to ensure the processing and storage capacity of each node is equally utilized.<br>Distributed databases also promote availability. They support replicating each data storage node so if one node fails or cannot be accessed due to network problems, another copy of the data is available. The models utilized for replication and the trade-offs these require (spoiler — consistency) are fodder for another article.<br>If you are utilizing a major cloud provider, there are also two deployment choices for your data tier. You can deploy your own virtual resources and build, configure, and administer your own distributed database servers. Alternatively, you can utilize cloud-hosted databases such as DynamoDB. The latter category simplifies the administrative effort associated with managing, monitoring, and scaling the database, as many of these tasks essentially become the responsibility of the cloud provider you choose. As usual, the no free lunch principle applies.<br><br>Any realistic system that we need to scale will have many different services that interact to process a request. For example, accessing a Web page on the Amazon.com site can require in excess of <a data-tooltip-position="top" aria-label="https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html" rel="noopener" class="external-link" href="https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html" target="_blank">100 different services being called</a> before an aggregate response is returned to the user.<br>The beauty of the stateless, load-balanced, cached architecture we are elaborating in this article is that we can extend the core design principles and build a multi-tiered, multi-service application. In fulfilling a request, a service can call one or more dependent services, which in turn are replicated and load-balanced. A simple example is shown in Figure 5. There are many nuances in how the services interact, and <a data-tooltip-position="top" aria-label="https://medium.com/@i.gorton/six-rules-of-thumb-for-scaling-software-architectures-a831960414f9" rel="noopener" class="external-link" href="https://medium.com/@i.gorton/six-rules-of-thumb-for-scaling-software-architectures-a831960414f9" target="_blank">how applications ensure rapid responses from dependent services</a>. Again, we’ll cover these in detail in later articles.<br><img alt="Figure 5 Scaling the Processing Capacity across multiple tiers" src="https://miro.medium.com/v2/resize:fit:970/1*5QvIxJtIO5B_bxbyPtmqNg.png" referrerpolicy="no-referrer"><br>Figure 5 Scaling the Processing Capacity across multiple tiers<br>This design also promotes having different, load-balanced services at each tier in the architecture. For example, Figure 6 illustrates two replicated Internet-facing services that both utilized a core service providing database access. Each service is load balanced and employs caching to provide high performance and availability. This design is often used, for example, to provide a service for Web clients and a service for mobile clients, each of which can be scaled independently based on the load they experience. Its commonly called the <a data-tooltip-position="top" aria-label="https://samnewman.io/patterns/architectural/bff/" rel="noopener" class="external-link" href="https://samnewman.io/patterns/architectural/bff/" target="_blank">Backend For Frontend (BFF) pattern</a>.<br><img alt="Figure 6 Scalable Architecture with Multiple Services" src="https://miro.medium.com/v2/resize:fit:1114/1*wGBhGR8D9ZuWutOeYiMJvQ.png" referrerpolicy="no-referrer"><br>Figure 6 Scalable Architecture with Multiple Services<br>In addition, by breaking the application into multiple independent services, we can scale each based on the service demand. If for example, we see an increasing volume of requests from mobile users and decreasing volumes from Web users, we can provision different numbers of instances for each service to satisfy demand. This is a major advantage of refactoring monolithic applications into multiple independent services, which can be separately built, tested, deployed, and scaled.<br><br>Most client application requests expect a response. A user might want to see all auction items for a given product category or see the real estate that is available for sale in a given location. In these examples, the client sends a request and waits until a response is received. The time interval between sending the request and receiving the result is the latency of the request. We can decrease latencies by using caching and precalculated responses, but many requests will still result in database access.<br>A similar scenario exists for requests that update data in an application. If a user updates their delivery address immediately prior to placing an order, the new delivery address must be persisted so that the user can confirm the address before they hit the ‘purchase’ button. This is known as ‘read your own writes’. The latency, in this case, includes the time for the database write, which is confirmed by the response the user receives.<br>Some update requests however can be successfully responded to without fully persisting the data in a database. For example, the skiers and snowboarders amongst you will be familiar with lift ticket scanning systems that check you have a valid pass to ride the lifts that day. They also record which lifts you take, the time you get on, and so on. Nerdy skier/snowboarders can then use the resort’s mobile app to see how many lifts they ride in a day.<br>As a person waits to get on a lift, a scanner device validates the pass using an RFID chip reader. The information about the rider, lift, and time is then sent over the Internet to a data capture service operated by the ski resort. The lift rider doesn’t have to wait for this update to occur, as the latency could slow down the lift loading process. There’s also no expectation from the lift rider that they can instantly use their app to ensure this data has been captured. They just get on the lift, talk smack with their friends, and plan their next run.<br>Service implementations can exploit this type of scenario to reduce latencies and improve responsiveness. The data about the event is sent to the service, which acknowledges receipt and concurrently stores the data in a remote queue for subsequent writing to the database. Writing a message to a queue is much faster than writing to a database, and this enables the request to be successfully acknowledged much more quickly. Another backend service is deployed to read messages from the queue and write the data to the database. When the user checks their lift rides — maybe 3 hours or 3 days later — the data has been persisted successfully.<br>The basic architecture to implement this approach is illustrated in Figure 7.<br><img alt="Figure 7 Increasing Responsiveness with Queueing" src="https://miro.medium.com/v2/resize:fit:1054/1*Te6RjrHDEQ3scq_kVbpwEQ.png" referrerpolicy="no-referrer"><br>Figure 7 Increasing Responsiveness with Queuing<br>Whenever the results of a write operation are not immediately needed, an application can use this approach to improve responsiveness and hence scalability. Many queueing technologies exist that applications can utilize, and we’ll discuss how these operate in later articles. These queueing platforms all provide asynchronous communications. A producer service writes to the queue, which acts as temporary storage, while another consumer service removes messages from the queue and makes the necessary updates to, in our example, a database that stores skier lift ride details.<br>The key is that the data eventually gets persisted. Eventually typically means a few seconds at most, but use cases that employ this design should be resilient to longer delays without impacting the user experience.<br><br>This chapter has provided a whirlwind tour of the major approaches we can utilize to scale out an Internet-facing system as a collection of communicating services and distributed databases. Much detail has been brushed over, and as you all no doubt know, in software systems the devil is in the detail.<br>Another area this chapter has skirted around is the subject of software architecture. We’ve used the term services for distributed components in an architecture that implement application business logic and database access. These services are independently deployed processes that communicate using remote communications mechanisms such as HTTP. In architectural terms, these services are most closely mirrored by those in the Service Oriented Architecture (SOA) pattern, an established architectural approach for building distributed systems. A more modern evolution of this approach revolves around microservices. These tend to be more cohesive, encapsulated services that promote continuous development and deployment.<br>If you’d like a much more in-depth discussion of these, and software architecture issues in general, then Mark Richards’ and Neal Ford’s book is an excellent place to start.<br>Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering Approach 1st Edition, O’Reilly Media, 2020<br>Finally, there’s a class of big data software architectures that address some of the issues that rise to the fore with very large data collections. One of the most prominent is data reprocessing. This occurs when raw data that has already been stored and analyzed needs to be re-analyzed due to code changes. This reprocessing may occur due to software fixes, or the introduction of new algorithms that can derive more insights from the original raw data. There’s a good discussion of the Lambda and Kappa architectures, both of which are prominent in this space, in this article.<br>Jay Krepps, <a data-tooltip-position="top" aria-label="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" rel="noopener" class="external-link" href="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" target="_blank">Questioning the Lambda Architecture</a>,]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/building-scalable-distributed-systems-distributed-system-architecture-blueprint-a-whirlwind-tour.html</link><guid isPermaLink="false">Computer Science/Distributed System/Building Scalable Distributed Systems - Distributed System Architecture Blueprint - A Whirlwind Tour.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:28 GMT</pubDate><enclosure url="https://miro.medium.com/v2/resize:fit:584/1*MTjAf75AZDXNDLO7iWgsxQ.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:584/1*MTjAf75AZDXNDLO7iWgsxQ.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[What is Scalability?]]></title><description><![CDATA[ 
 <br>The last 20 years have seen unprecedented growth in the size, complexity, and capacity of software systems. This rate of growth is hardly likely to slow down in the next 20 years — what these future systems will look like is close to unimaginable right now. The one thing we can guarantee is that more and more software systems will need to be built with constant growth — more requests, more data, more analysis — as a primary design driver.<br>Scalable is the term used in software engineering to describe software systems that can accommodate growth. In this first part of the series, we will explore what precisely is meant by the ability to scale — known, not surprisingly, as scalability. We’ll also describe a few examples that put hard numbers on the capabilities and characteristics of contemporary applications and give a brief history of the origins of the massive systems we routinely build today. Finally, we will describe two general principles for achieving scalability that will recur in various forms throughout the rest of this series of articles and examine the indelible link between scalability and cost.<br><br>Intuitively, scalability is a pretty straightforward concept. If we ask Wikipedia for a definition, it tells us “scalability is the property of a system to handle a growing amount of work by adding resources to the system”. We all know how we scale a highway system — we add more traffic lanes so it can handle a greater number of vehicles. Some of my favorite people know how to scale beer production — they add more capacity in terms of the number and size of brewing vessels, the number of staff to perform and manage the brewing process, and the number of kegs they can fill with tasty fresh brews. Think of any physical system — a transit system, an airport, elevators in a building — and how we increase capacity is pretty obvious.<br>Unlike physical systems, software is somewhat amorphous. It is not something you can point at, see, touch, feel, and get a sense of how it behaves internally from external observation. It’s a digital artifact. At its core, the stream of 1’s and 0’s that make up executable code and data are hard for anyone to tell apart. So, what does scalability mean in terms of a software system?<br>Put very simply, and without getting into definition wars, scalability defines a software system’s capability to handle growth in some dimension of its operations. Examples of operational dimensions are:<br>
<br>the number of simultaneous user or external (e.g. sensor) requests a system can process
<br>the amount of data a system can effectively process and manage
<br>the value that can be derived from the data a system stores
<br>For example, imagine a major supermarket chain is rapidly opening new stores and increasing the number of self-checkout kiosks in every store. This requires the core supermarket software systems to:<br>
<br>Handle increased volume from item sale scanning without decreased response time. Instantaneous responses to item scans are necessary to keep customers happy.
<br>Process and store the greater data volumes generated from increased sales. This data is needed for inventory management, accounting, planning and likely many other functions.
<br>Derive ‘real-time’ (e.g. hourly) sales data summaries from each store, region and country and compare to historical trends. This trend data can help highlight unusual events in regions (e.g. unexpected weather conditions, large crowds at events, etc.) and help the stores affected quickly respond.
<br>Evolve the stock ordering prediction subsystem to be able to correctly anticipate sales (and hence the need for stock reordering) as the number of stores and customers grow
<br>These dimensions are effectively the scalability requirements of a system. If over a year, the supermarket chain opens 100 new stores and grows sales by 400 times (some of the new stores are big), then the software system needs to scale to provide the necessary processing capacity to enable the supermarket to operate efficiently. If the systems don’t scale, we could lose sales as customers are unhappy. We might hold stock that will not be sold quickly, increasing costs. We might miss opportunities to increase sales by responding to local circumstances with special offerings. All these reduce customer satisfaction and profits. None are good for business.<br>Successfully scaling is therefore crucial for our imaginary supermarket’s business growth, and is in fact the lifeblood of many modern internet applications. But for most business and Government systems, scalability is not a primary quality requirement in the early stages of development and deployment. New features to enhance usability and utility become the drivers of our development cycles. As long as performance is adequate under normal loads, we keep adding user-facing features to enhance the system’s business value.<br>Still, it’s not uncommon for systems to evolve into a state where enhanced performance and scalability become a matter of urgency or even survival. Attractive features and high utility breed success, which brings more requests to handle and more data to manage. This often heralds a tipping point, where design decisions that made sense under light loads are now suddenly technical debt. External trigger events often cause these tipping points — look in the March/April 2020 media at the many reports of Government Unemployment and supermarket online ordering sites crashing under demand caused by the coronavirus pandemic.<br>Increasing a systems’ capacity in some dimension by increasing resources is commonly called scaling up or scaling out — we’ll explore the difference between these later in this series. In addition, unlike physical systems, it is often equally important to be able to scale down the capacity of a system to reduce costs. The canonical example of this is Netflix, which has a predictable regional diurnal load that it needs to process. Simply, a lot more people are watching Netflix in any geographical region at 9 pm than are at 5 am. This enables Netflix to reduce its processing resources during times of lower load. This saves the cost of running the processing nodes that are used in the Amazon cloud, as well as societally worthy things such as reducing data center power consumption. Compare this to a highway. At night when few cars are on the road, we don’t retract lanes (except for repairs). The full road capacity is available for the few drivers to go as fast as they like.<br>There’s a lot more to consider about scalability in software systems, but let’s come back to these issues after examining the scale of some contemporary software systems circa 2020.<br><br>Looking ahead in this technology game is always fraught with danger. In 2008 I wrote [1]:<br>“While petabyte datasets and gigabit data streams are today’s frontiers for data-intensive applications, no doubt 10 years from now we’ll fondly reminisce about problems of this scale and be worrying about the difficulties that looming exascale applications are posing.”<br>Reasonable sentiments, it is true, but exascale? That’s almost commonplace in today’s world. Google reported multiple exabytes of <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=eNliOm9NtCM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=eNliOm9NtCM" target="_blank">Gmail in 2014</a>, and by now, do all Google services manage a yottabyte or more? I don’t know. I’m not even sure I know what a yottabyte is! Google won’t tell us about their storage, but I wouldn’t bet against it. Similarly, how much data do Amazon store in the various AWS data stores for their clients. And how many requests does, say, DynamoDB process per second collectively, for all client applications supported. Think about these things for too long and your head will explode.<br>A great source of information that sometimes gives insights into contemporary operational scales are the major Internet company’s technical blogs. There are also Web sites analyzing Internet traffic that are highly illustrative of traffic volumes. Let’s take a couple of ‘point in time’ examples to illustrate a few things we do know today. Bear in mind these will look almost quaint in a year or four.<br>
<br>Facebook’s engineering blog describes <a data-tooltip-position="top" aria-label="https://engineering.fb.com/data-infrastructure/scribe/" rel="noopener" class="external-link" href="https://engineering.fb.com/data-infrastructure/scribe/" target="_blank">Scribe</a>, their solution for collecting, aggregating, and delivering petabytes of log data per hour, with low latency and high throughput. Facebook’s computing infrastructure comprises millions of machines, each of which generates log files that capture important events relating to system and application health. Processing these log files, for example, from a Web server, can give development teams insights into their application’s behavior and performance, and support fault finding. Scribe is a custom buffered queuing solution that can transport logs from servers at a rate of several terabytes per second and deliver them to downstream analysis and data warehousing systems. That, my friends, is a lot of data!
<br>You can see live Internet traffic for numerous services at <a data-tooltip-position="top" aria-label="http://www.internetlivestats.com" rel="noopener" class="external-link" href="http://www.internetlivestats.com" target="_blank">www.internetlivestats.com</a>. Dig around and you’ll find statistics like Google handles around 3.5 billion search requests a day, Instagram uploads about 65 million photos per day, and there is something like 1.7 billion web sites. It is a fun site with lots of information to amaze you. Note the data is not really ‘live’, just estimates based on statistical analyses of multiple data sources.
<br>In 2016 Google published a paper describing the <a data-tooltip-position="top" aria-label="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" rel="noopener" class="external-link" href="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext" target="_blank">characteristics of their code base</a>. Amongst the many startling facts reported is: “The repository contains 86TBs of data, including approximately two billion lines of code in nine million unique source files.” Remember, this was 2016.
<br>Still, real, concrete data on the scale of the services provided by major Internet sites remain shrouded in commercial-in-confidence secrecy. Luckily, we can get some deep insights into the request and data volumes handled at Internet scale through the annual usage report from one tech company. You can browse their incredibly detailed <a data-tooltip-position="top" aria-label="https://www.pornhub.com/insights/2019-year-in-review" rel="noopener" class="external-link" href="https://www.pornhub.com/insights/2019-year-in-review" target="_blank">usage statistics here from 2019</a>. It’s a fascinating glimpse into the capabilities of massive scale systems. Beware though, this is Pornhub.com. The report is not for the squeamish. Here’s one PG-13 illustrative data point — they had 42 billion visits in 2019! I’ll let interested readers browse the data in the report to their heart's content. Some of the statistics will definitely make your eyes bulge!<br><br>I am sure many readers will have trouble believing there was civilized life without Internet search, YouTube, and social media. By coincidence, the day I type this sentence is the 15 year anniversary of the first video <a data-tooltip-position="top" aria-label="https://kogo.iheart.com/content/2020-04-23-youtube-celebrates-15th-anniversary-by-featuring-first-video-ever-posted/" rel="noopener" class="external-link" href="https://kogo.iheart.com/content/2020-04-23-youtube-celebrates-15th-anniversary-by-featuring-first-video-ever-posted/" target="_blank">being uploaded to YouTube</a>. Only 15 years. Yep, it is hard for even me to believe. There’s been a lot of wine under the bridge since then. I can’t remember how we survived!<br>So, let’s take a brief look back in time at how we arrived at the scale of today’s systems. This is from a personal perspective — one which started at college in 1981 when my class of 60 had access to a shared lab of 8 state-of-the-art so-called <a data-tooltip-position="top" aria-label="http://pcmuseum.tripod.com/comphis4.html" rel="noopener" class="external-link" href="http://pcmuseum.tripod.com/comphis4.html" target="_blank"><em></em></a>microcomputers. By today’s standards, micro they were not.<br><br>An age dominated by mainframe and minicomputers. These were basically timeshared multiuser systems where users interacted with the machines via ‘dumb’ terminals. Personal computers emerged in the early 1980s and developed throughout the decade to become useful business and (relatively) powerful development machines. They were rarely networked, however, especially early in the decade. The <a data-tooltip-position="top" aria-label="https://www.internetsociety.org/internet/history-internet/brief-history-internet/" rel="noopener" class="external-link" href="https://www.internetsociety.org/internet/history-internet/brief-history-internet/" target="_blank">first limited incarnation of the Internet emerged during this time</a>. By the end of the 1980s, development labs, universities, and increasingly businesses had email and access to exotic internet-based resources such as <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Usenet" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Usenet" target="_blank">Usenet discussion forums</a> — think of a relatively primitive and incredibly polite Reddit.<br><br>Personal computers and networking technology, both LANs and WANS, continued to improve dramatically through this period. This created an environment ripe for the creation of the World Wide Web (WWW) as we know it today. The catalyst was the HTTP/HTML technology that had been <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web" target="_blank">pioneered at CERN by Tim Berners-Lee</a> during the 1980s. In 1993 CERN made the WWW technology available on a royalty-free basis. And the rest is history — a platform for information sharing and money-making had been created. By 1995, the number of web sites was tiny, but the seeds of the future were planted with companies like Yahoo! in 1994 and Amazon and eBay in 1995<br><br>During this period, the number of web sites grew from <a data-tooltip-position="top" aria-label="https://www.nngroup.com/articles/100-million-websites/" rel="noopener" class="external-link" href="https://www.nngroup.com/articles/100-million-websites/" target="_blank">around 10,000 to 10 million</a>, a truly explosive growth period. Networking bandwidth and access also grew rapidly, with initially dial-up modems for home users (yep, dial-up) and then early broadband technologies becoming available.<br>This surge in users with Internet access heralded a profound change in how we had to think about building systems. Take for example a retail bank. Before providing online services, it was possible to accurately predict the loads the bank’s business systems would experience. You knew how many people worked in the bank and used the internal systems, how many terminals/PCs were connected to the bank’s networks, how many ATMs you had to support, and the number and nature of connections to other financial institutions. Armed with this knowledge, we could build systems that support say a maximum of 3000 concurrent users, safe in the knowledge that this number could not be exceeded. Growth would also be relatively slow, and probably most of the time (eg outside business hours) the load would be a lot less than the peak. This made our software design decisions and hardware provisioning a lot easier.<br>Now imagine our retail bank decides to let all customers have Internet banking access. And the bank has 5 million customers. What is our maximum load now? How will the load be dispersed during a business day? When are the peak periods? What happens if we run a limited-time promotion to try and sign up new customers? Suddenly our relatively simple and contained business systems environment is disrupted by the higher average and peak loads and unpredictability you see from Internet-based user populations.<br>During this period, companies like Amazon, eBay, Google, Yahoo! and the like were pioneering many of the design principles and early versions of advanced technologies for highly scalable systems. They had to, as their request loads and data volumes were growing exponentially.<br><br>The late 1990s and early 2000’s saw massive investments in, and technological innovations from so-called ‘dot com’ companies, all looking to provide innovative and valuable online businesses. Spending was huge, and not all investments were well-targeted. This led to a little event called the ‘<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Dot-com_bubble" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Dot-com_bubble" target="_blank">dot com crash</a>’ during 2000/2001. By 2002 the technology landscape was littered with failed investments — anyone remember Pets.Com? Nope. Me neither. About 50% of dot com’s disappeared during this period. Of those that survived, albeit, with much lower valuations, many have become the staples we all know and use today.<br>The number of Web sites grew from around 10 to 80 million during this period, and new service and business models emerged. In 2005, YouTube was launched. 2006 saw Facebook become available to the public. In the same year, Amazon Web Services, which had low key beginnings in 2004, relaunched with its S3 and EC2 services. The modern era of Internet-scale computing and cloud-hosted systems was born.<br><br>We now live in a world with nearly 2 billion web sites, of which about 20% are active. There are something like <a data-tooltip-position="top" aria-label="https://www.internetworldstats.com/stats.htm" rel="noopener" class="external-link" href="https://www.internetworldstats.com/stats.htm" target="_blank">4 billion Internet users</a>. Huge data centers operated by public cloud operators like AWS, GCP and Azure, along with a myriad of private data centers, for example <a data-tooltip-position="top" aria-label="https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-behind-twitter-scale.html" rel="noopener" class="external-link" href="https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-behind-twitter-scale.html" target="_blank">Twitter’s operational infrastructure</a>, are scattered around the planet. Clouds host millions of applications, with engineers provisioning and operating their computational and data storage systems using sophisticated cloud management portals. Powerful, feature-rich cloud services make it possible for us to build, deploy, and scale our systems literally with a few clicks of a mouse. All you must do is pay your cloud provider bill at the end of the month.<br>This is the world that this series of articles targets. A world where our applications need to exploit the key principles for building scalable systems and leverage highly scalable infrastructure platforms. Bear in mind, in modern applications, most of the code executed is not written by your organization. It is part of the containers, databases, messaging systems, and other components that you compose into your application through API calls and build directives. This makes the selection and use of these components at least as important as the design and development of your own business logic. They are architectural decisions that are not easy to change.<br><br>As we have already discussed, the basic aim of scaling a system is to increase its capacity in some application-specific dimension. A common dimension is increasing the number of requests that a system can process in a given time period. This is known as the system’s throughput. Let’s use an analogy to explore two basic principles we have available to us for scaling our systems and increasing throughput.<br>In 1932, one of the world’s great icons, <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Sydney_Harbour_Bridge" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Sydney_Harbour_Bridge" target="_blank">the Sydney Harbor Bridge</a>, was opened. Now it is a fairly safe assumption that traffic volumes in 2020 are somewhat higher than in 1932. If you have driven over the bridge at peak hour in the last 30 years, then you know that its capacity is exceeded considerably every day. So how do we increase throughput on physical infrastructures such as bridges?<br>This issue became very prominent in Sydney in the 1980s, when it was realized that the capacity of the harbor crossing had to be increased. The solution was the rather less iconic <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Sydney_Harbour_Tunnel" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Sydney_Harbour_Tunnel" target="_blank">Sydney Harbor</a> tunnel, which essentially follows the same route underneath the harbor. This provides 4 more lanes of traffic and hence added roughly 1/3rd more capacity to harbor crossings.<br>In not too far away Auckland, their <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge" target="_blank">harbor bridge</a> also had a capacity problem as it was built in 1959 with only 4 lanes. In essence, they adopted the same solution as Sydney, namely to increase capacity. But rather than build a tunnel, they ingeniously doubled the number of lanes by expanding the bridge with the hilariously named ‘<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge#'Nippon_clip-ons'" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge#'Nippon_clip-ons'" target="_blank">Nippon Clipons’</a>, which widened the bridge on each side. Ask a Kiwi to say ‘Nippon Clipons’ and you will understand why this is funny.<br>These examples illustrate the first strategy we have in software systems to increase capacity. We basically replicate the software processing resources to provide more capacity to handle requests and thus increase throughput, as shown in Figure 1. These replicated processing resources are analogous to the laneways on bridges, providing a mostly independent processing pathway for a stream of arriving requests. Luckily, in cloud-based software systems, replication can be achieved at the click of a mouse, and we can effectively replicate our processing resources thousands of times. We have it a lot easier than bridge builders in that respect.<br><img alt="With one server we can process 100 requests per second. If we deploy 3 servers, we can process 300 requests per second." src="https://miro.medium.com/v2/resize:fit:1062/1*N1__-Yxk_qEcuFqzY8gyZg.png" referrerpolicy="no-referrer"><br>Figure 1 Increasing Capacity through Replication<br>The second strategy for scalability can also be illustrated with our bridge example. In Sydney, some observant people realized that in the mornings a lot more vehicles cross the bridge from north to south, and in the afternoon we see the reverse pattern. A smart solution was therefore devised — allocate more of the lanes to the high demand direction in the morning, and sometime in the afternoon, switch this around. This effectively increased the capacity of the bridge without allocating any new resources — we optimized the resources we already had available.<br>We can follow this same approach in software to scale our systems. If we can somehow optimize our processing, by maybe using more efficient algorithms, adding extra indexes in our databases to speed up queries, or even rewriting our server in a faster programming language, we can increase our capacity without increasing our resources. The canonical example of this is Facebook’s creation of (the now discontinued) <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/HipHop_for_PHP" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/HipHop_for_PHP" target="_blank">HipHop for PHP</a>, which increased the speed of Facebook’s web page generation by up to 6 times by compiling PHP code to C++. This enabled the site to process many more requests with the same resources.<br>We’ll revisit these two design principles — namely replication and optimization — many times in the remainder of this series. You will see that there are many complex implications of adopting these principles that arise from the fact that we are building distributed systems. Distributed systems have properties that make designing scalable systems ‘interesting’, where interesting in this context has both positive and negative connotations.<br><br>Let’s take a trivial hypothetical example to examine the relationship between scalability and costs. Assume we have a Web-based (e.g. web server and database) system that can service a load of 100 concurrent requests with a mean response time of 1 second. We get a business requirement to scale up this system to handle 1000 concurrent requests with the same response time. Without making any changes, a simple load test of this system reveals the performance shown in Figure 2 (left). As the request load increases, we see the mean response time steadily grow to 10 seconds with the projected load. Clearly this is not scalable and cannot satisfy our requirements in its current deployment configuration.<br><img alt="On the left, response times grow rapidly. One the right, response times stay flat as load increases." src="https://miro.medium.com/v2/resize:fit:1400/1*JKhgncDUTSwPe0lw5iIZGA.png" referrerpolicy="no-referrer"><br>Figure 2 Scaling an application. (Left) — non-scalable performance. (Right) — scalable performance<br>Clearly some engineering effort is needed in order to achieve the required performance. Figure 2 (right) shows the system’s performance after it has been modified. It now provides the specified response time with 1000 concurrent requests. Hence, we have successfully scaled the system. Party time!<br>A major question looms, however. Namely, how much effort and resources were required to achieve this performance? Perhaps it was simply a case of scaling up by running the Web server on a more powerful (virtual) machine. Performing such reprovisioning on a cloud might take 30 minutes at most. Slightly more complex would be reconfiguring the system to scale out and run multiple instances of the Web server to increase capacity. Again, this should be a simple, low-cost configuration change for the application, with no code changes needed. These would be excellent outcomes.<br>However, scaling a system isn’t always so easy. The reasons for this are many and varied, but here are some possibilities:<br>
<br>the database becomes less responsive with 1000 requests per second, requiring an upgrade to a new machine
<br>the Web server generates a lot of content dynamically and this reduces response time under load. A possible solution is to alter the code to more efficiently generate the content, thus reducing processing time per request.
<br>the request load creates hot spots in the database when many requests try to access and update the same records simultaneously. This requires a schema redesign and subsequent reloading of the database, as well as code changes to the data access layer.
<br>the Web server framework that was selected emphasized ease of development over scalability. The model it enforces means that the code simply cannot be scaled to meet the request load requirements, and a complete rewrite is required. Another framework? Another programming language even?
<br>There’s a myriad of other potential causes, but hopefully, these illustrate the increasing effort that might be required as we move from possibility (1) to possibility (4).<br>Now let’s assume option (1), upgrading the database server, requires 15 hours of effort and a thousand dollars extra cloud costs per month for a more powerful server. This is not prohibitively expensive. And let’s assume option (4), a rewrite of the Web application layer, requires 10,000 hours of development due to implementing in a new language (e.g. Java instead of Ruby). Options (2) and (3) fall somewhere in between options (1) and (4). The cost of 10,000 hours of development is seriously significant. Even worse, while the development is underway, the application may be losing market share and hence money due to its inability to satisfy client requests loads. These kinds of situations can cause systems and businesses to fail.<br>This simple scenario illustrates how resource and effort costs are inextricably tied to scalability. If a system is not designed intrinsically to scale, then the downstream costs and resources of increasing its capacity to meet requirements may be massive. For some applications, such as <a data-tooltip-position="top" aria-label="https://www.bloomberg.com/news/articles/2014-09-24/obamacare-website-costs-exceed-2-billion-study-finds" rel="noopener" class="external-link" href="https://www.bloomberg.com/news/articles/2014-09-24/obamacare-website-costs-exceed-2-billion-study-finds" target="_blank">Healthcare.gov</a>, these (more than $2 billion) costs are borne and the system is modified to eventually meet business needs. For others, such as Oregon’s health care exchange, an inability to scale rapidly at low cost can be an expensive ($303million) death knell.<br>We would never expect someone would attempt to scale up the capacity of a family home to become a 50-floor office building. The home doesn’t have the architecture, materials, and foundations for this to be even a remote possibility without being completely demolished and rebuilt. Similarly, we shouldn’t expect software systems that do not employ scalable architectures, mechanisms and technologies to be quickly evolved to meet greater capacity needs. The foundations of scale need to be built in from the beginning, with the recognition that the components will evolve over time. By employing design and development principles that promote scalability, we can more rapidly and cheaply scale systems to meet rapidly growing demands.<br>Software systems that can be scaled exponentially while costs grow linearly are known as hyperscale systems, defined as:<br>“Hyperscale systems exhibit exponential growth in computational and storage capabilities while exhibiting linear growth rates in the costs of resources required to build, operate, support, and evolve the required software and hardware resources.”<br>You can read more about hyperscale systems <a data-tooltip-position="top" aria-label="https://www.researchgate.net/publication/318049054_Chapter_2_Hyperscalability_-_The_Changing_Face_of_Software_Architecture" rel="noopener" class="external-link" href="https://www.researchgate.net/publication/318049054_Chapter_2_Hyperscalability_-_The_Changing_Face_of_Software_Architecture" target="_blank">in this article</a>[3].<br><br>The ability to scale an application quickly and cost-effectively should be a defining quality of the software architecture of contemporary Internet-facing applications. We have two basic ways to achieve scalability, namely increasing system capacity, typically through replication, and performance optimization of system components. The rest of this series of articles will delve deeply into how these two basic principles manifest themselves in constructing scalable distributed systems. Get ready for a wild ride.<br><br>
<br>
Ian Gorton, Paul Greenfield, Alex Szalay, and Roy Williams. 2008. Data-Intensive Computing in the 21st Century. Computer 41, 4 (April 2008), 30–32.

<br>
Rachel Potvin and Josh Levenberg. 2016. Why Google stores billions of lines of code in a single repository. Commun. ACM 59, 7 (July 2016), 78–87.

<br>
Ian Gorton (2017). Chapter 2. Hyperscalability — The Changing Face of Software Architecture. 10.1016/B978–0–12–805467–3.00002–8.

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/building-scalable-distributed-systems-introduction-to-scalable-systems.html</link><guid isPermaLink="false">Computer Science/Distributed System/Building Scalable Distributed Systems - Introduction to Scalable Systems.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:28 GMT</pubDate><enclosure url="https://miro.medium.com/v2/resize:fit:1062/1*N1__-Yxk_qEcuFqzY8gyZg.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:1062/1*N1__-Yxk_qEcuFqzY8gyZg.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[我为什么认为 Actor 不合适 CPU 密集任务]]></title><description><![CDATA[ 
 <br>从面向对象来说，面向对象中，将世间万物看作一个个对象，从相同对象中总结（抽象）出一个个类，比如人类，猫类等等，但是面向对象理论在落实的编程语言中时，没有考虑到世界线问题，即世界的时间流转问题，举个例子：<br>课堂上，老师在讲台上讲课（ teacher 对象的 speak 方法正在执行中），下边的学生在说话（student 对象的 speak 也在执行）。从现实生活中看，这个是自然而然的，但是，在计算机世界， 讲课和说话操作都需要在thread 中才能执行。所以，object 和 thread 就有匹配问题了，理论上，你应该为每一个 object 都匹配一个 thread, 毕竟现实中就是这样，老师和学生都有各自的 thread,可以同时执行操作。但是计算机世界中的问题就是不可能为每一个 object 起一个 thread。所以面向对象编程语言在计算机中实现时，线程问题做的就会比较奇怪了。<br>而 Actor  model 实际上是屏蔽了 thread 概念的（不接触thread,也不需要理解thread概念）。 Actor  model 中假设所有的 Actor 之间的运行是并行的，就和现实世界一样的。老师 Actor 和学生的 Actor 的运行底层是不相关的，相关的是老师在 speak（相当于群发消息），学生也在 speak（单发给同桌或者给后座的人），老师听到有人在说话（接收一个消息），然后老师拿粉笔打了说话的学生（单发一个粉笔头给那个学生）。<br>由此可见， Actor 是比面向对象更接近现实的模型。<br>从上一个问题来看， Actor 模型比oo更接近现实世界，但是，落实在计算机中时，就涉及到， Actor 怎么执行的问题。因为计算机中没有那么多的thread, 所以 Actor 在实现时，必然是通过少量的 thread 来执行大量的 Actor 实体。 即在一个 thread 中分时执行多个 Actor 。当然，这是开发 Actor 模型的人需要关注的问题，用 Actor 模型的人可以不关注。<br>所以，这里就引出了这个问题：如果在 Actor 中执行 CPU 密集任务会怎么样？<br>结论就是，CPU 密集任务把 Actor 的底层执行机制给拖垮了，thread 在执行一个 Actor 时，因耗时太长，并占用大量 CPU 资源，导致其他 Actor 无法执行。 Actor 的执行机制出现断档， Actor 世界出现 stop the world。STW 问题对于 Actor 本身不会有啥影响，反正世界都停了，无非就是多等一会儿。但是对于我们这个上帝来说，这个 Actor 世界已经不能流畅运行了。<br>所以，不管是 go, 还是 akka,或者 vert.x，都会提醒你，不要在 Actor 或类 Actor 实例中执行 block 太长的任务。<br>所以，怎么解决这个问题呢？<br>
答案是：使用异步。<br>
异步操作就可以把一个 block 变成不 block 的任务，从而在 Actor 世界中实现目标。<br>最后，再考虑一下用 Actor 模型来执行 CPU 密集任务的问题。<br>如果在 Actor 中执行一个 CPU 密集任务，首先会导致其他 Actor 的执行被推迟，其次，系统本身的 Actor （比如 log, monitor 等）也会推迟。如果能接受这个影响，那么用 Actor 来执行 CPU 密集任务也没有问题。<br>另一种方案是，把CPU密集 Actor 放到单独的 executor 中，与其他 Actor 分离，充分利用多线程特性]]></description><link>https://muqiuhan.github.io/wiki/computer-science/distributed-system/我为什么认为-actor-不合适-cpu-密集任务.html</link><guid isPermaLink="false">Computer Science/Distributed System/我为什么认为 Actor 不合适 CPU 密集任务.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Tue, 14 May 2024 14:56:37 GMT</pubDate></item><item><title><![CDATA[Linux中的硬连接和软连接]]></title><description><![CDATA[ 
 <br>Linux链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。默认情况下，ln命令产生硬链接。<br><br>硬连接指通过索引节点来进行连接。在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号(Inode Index)。在Linux中，多个文件名指向同一索引节点是存在的。一般这种连接就是硬连接。硬连接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬连接到重要文件，以防止“误删”的功能。其原因如上所述，因为对应该目录的索引节点有一个以上的连接。只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个连接被删除后，文件的数据块及目录的连接才会被释放。也就是说，文件真正删除的条件是与之相关的所有硬连接文件均被删除。<br><br>另外一种连接称之为符号连接（Symbolic Link），也叫软连接。软链接文件有类似于Windows的快捷方式。它实际上是一个特殊的文件。在符号连接中，文件实际上是一个文本文件，其中包含的有另一文件的位置信息。<br>2. 通过实验加深理解<br>[root@Linux]$ touch f1 #创建一个测试文件f1
[root@Linux]$ ln f1 f2 #创建f1的一个硬连接文件f2
[root@Linux]$ ln -s f1 f3 #创建f1的一个符号连接文件f3
[root@Linux]$ ls -li # -i参数显示文件的inode节点信息
total 0
9797648 -rw-r--r-- 2 root root 0 Apr 21 08:11 f1
9797648 -rw-r--r-- 2 root root 0 Apr 21 08:11 f2
9797649 lrwxrwxrwx 1 root root 2 Apr 21 08:11 f3 -&gt; f1
Copy<br>从上面的结果中可以看出，硬连接文件f2与原文件f1的inode节点相同，均为9797648，然而符号连接文件的inode节点不同。<br>[root@Linux]$ echo "I am f1 file" &gt;&gt;f1
[root@Linux]$ cat f1
I am f1 file
[root@Linux]$ cat f2
I am f1 file
[root@Linux]$ cat f3
I am f1 file
[root@Linux]$ rm -f f1
[root@Linux]$ cat f2
I am f1 file
[root@Linux]$ cat f3
cat: f3: No such file or directory
Copy<br>通过上面的测试可以看出：当删除原始文件f1后，硬连接f2不受影响，但是符号连接f1文件无效。<br>3. 总结<br>
<br>删除符号连接f3,对f1,f2无影响；
<br>删除硬连接f2，对f1,f3也无影响；
<br>删除原文件f1，对硬连接f2没有影响，导致符号连接f3失效；
<br>同时删除原文件f1,硬连接f2，整个文件会真正的被删除。
<br><br><br>在Linux的文件系统中，保存在磁盘分区中的文件不管是什么类型都给它分配一个编号，称为索引节点号inode 。<br>
<br>软连接，其实就是新建立一个文件，这个文件就是专门用来指向别的文件的（那就和windows 下的快捷方式的那个文件有很接近的意味）。软链接产生的是一个新的文件，但这个文件的作用就是专门指向某个文件的，删了这个软连接文件，那就等于不需要这个连接，和原来的存在的实体原文件没有任何关系，但删除原来的文件，则相应的软连接不可用（cat那个软链接文件，则提示“没有该文件或目录“）
<br>硬连接是不会建立inode的，他只是在文件原来的inode link count域再增加1而已，也因此硬链接是不可以跨越文件系统的。相反是软连接会重新建立一个inode，当然inode的结构跟其他的不一样，他只是一个指明源文件的字符串信息。一旦删除源文件，那么软连接将变得毫无意义。而硬链接删除的时候，系统调用会检查inode link count的数值，如果他大于等于1，那么inode不会被回收。因此文件的内容不会被删除。
<br>硬链接实际上是为文件建一个别名，链接文件和原文件实际上是同一个文件。可以通过ls -i来查看一下，这两个文件的inode号是同一个，说明它们是同一个文件；而软链接建立的是一个指向，即链接文件内的内容是指向原文件的指针，它们是两个文件。
<br>软链接可以跨文件系统，硬链接不可以；
<br>软链接可以对一个不存在的文件名(filename)进行链接（当然此时如果你vi这个软链接文件，linux会自动新建一个文件名为filename的文件）,硬链接不可以（其文件必须存在，inode必须存在）；
<br>软链接可以对目录进行连接，硬链接不可以。
<br>两种链接都可以通过命令 ln 来创建。ln 默认创建的是硬链接。
<br>使用 -s 开关可以创建软链接。
<br><br><br>链接简单说实际上是一种文件共享的方式，是 POSIX 中的概念，主流文件系统都支持链接文件。<br><br>你可以将链接简单地理解为 Windows 中常见的快捷方式（或是 OS X 中的替身），Linux 中常用它来解决一些库版本的问题，通常也会将一些目录层次较深的文件链接到一个更易访问的目录中。在这些用途上，我们通常会使用到软链接（也称符号链接）。<br><br>下面我们进入正题，来探讨一下软硬两种链接到底有什么区别？<br>首先，从使用的角度讲，两者没有任何区别，都与正常的文件访问方式一样，支持读写，如果是可执行文件的话也可以直接执行。<br>那区别在哪呢？在底层的原理上。<br>为了解释清楚，我们首先在自己的一个工作目录下创建一个文件，然后对这个文件进行链接的创建：<br>$ touch myfile &amp;&amp; echo "This is a plain text file." &gt; myfile
$ cat myfile
 
This is a plain text file.
Copy<br>现在我们创建了一个普通地不能再普通的文件了。然后我们对它创建一个硬链接，并查看一下当前目录：<br>$ ln myfile hard
$ ls -li
 
25869085 -rw-r--r-- 2 unixzii staff 27 7 8 17:39 hard
25869085 -rw-r--r-- 2 unixzii staff 27 7 8 17:39 myfile
Copy<br>在 ls 结果的最左边一列，是文件的 inode 值，你可以简单把它想成 C 语言中的指针。它指向了物理硬盘的一个区块，事实上文件系统会维护一个引用计数，只要有文件指向这个区块，它就不会从硬盘上消失。<br>你也看到了，这两个文件就如同一个文件一样，inode 值相同，都指向同一个区块。<br>然后我们修改一下刚才创建的 hard 链接文件：<br>$ echo "New line" &gt;&gt; hard
$ cat myfile
 
This is a plain text file.
New line
Copy<br>可以看到，这两个文件果真就是一个文件。<br>
下面我们看看软链接（也就是符号链接）和它有什么区别。<br>$ ln -s myfile soft
$ ls -li
 
25869085 -rw-r--r-- 2 unixzii staff 36 7 8 17:45 hard
25869085 -rw-r--r-- 2 unixzii staff 36 7 8 17:45 myfile
25869216 lrwxr-xr-x 1 unixzii staff 6 7 8 17:47 soft -&gt; myfile
Copy<br>诶，你会发现，这个软链接的 inode 竟然不一样啊，并且它的文件属性上也有一个 l 的 flag，这就说明它与之前我们创建的两个文件根本不是一个类型。<br>下面我们试着删除 myfile 文件，然后分别输出软硬链接的文件内容：<br>$ rm myfile
$ cat hard
 
This is a plain text file.
New line
Copy<br>$ cat soft
 
cat: soft: No such file or directory
Copy<br>之前的硬链接没有丝毫地影响，因为它 inode 所指向的区块由于有一个硬链接在指向它，所以这个区块仍然有效，并且可以访问到。<br>
然而软链接的 inode 所指向的内容实际上是保存了一个绝对路径，当用户访问这个文件时，系统会自动将其替换成其所指的文件路径，然而这个文件已经被删除了，所以自然就会显示无法找到该文件了。<br>为验证这一猜想，我们再向这个软链接写点东西：<br>$ echo "Something" &gt;&gt; soft
$ ls
 
hard myfile soft
Copy<br>可以看到，刚才删除的 myfile 文件竟然又出现了！这就说明，当我们写入访问软链接时，系统自动将其路径替换为其所代表的绝对路径，并直接访问那个路径了。<br>
<br>硬链接： 与普通文件没什么不同，inode 都指向同一个文件在硬盘中的区块
<br>软链接： 保存了其代表的文件的绝对路径，是另外一种文件，在硬盘上有独立的区块，访问时替换自身路径。<br>
<img alt="Linux中的硬链接和软链接.png" src="https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬链接和软链接.png">
<br><br><br>当把原文件myfile删除之后，通过链接是无法访问，通过硬链接还是可以访问的  <br>
<br>软连接其实保存的是原文件的路径，访问软连接就是通过绝对路径进行访问的。访问不到原文件说明--文件消失了或者文件移动到其他目录下面了  
<br>把原myfile文件删除之后，但是通过硬链接还是可以访问的
<br>在这个场景下，文件并没有移动或者不移动之说。文件就是一块物理存储区域的内容。只有当指向存储区的指针全部消失，存储区才会被回收（内容也就没了）。所谓的文件名，只是指向存储区的一个flag。软链接是这个flag的路径，先找到这个flag，再访问存储区。硬链接是新建了一个不同文件名但指向同一个存储区的flag。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬连接和软连接.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/Linux中的硬连接和软连接/Linux中的硬连接和软连接.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:38 GMT</pubDate><enclosure url="https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬链接和软链接.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/linux中的硬连接和软连接/linux中的硬链接和软链接.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Inter-process communication in Linux -- Shared storage]]></title><description><![CDATA[ 
 <br>This is the first article in a series&nbsp;about <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Inter-process_communication" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Inter-process_communication" target="_blank">interprocess communication</a> (IPC) in Linux. The series uses code examples in C to clarify the following IPC mechanisms:<br>
<br>Shared files
<br>Shared memory (with semaphores)
<br>Pipes (named and unnamed)
<br>Message queues
<br>Sockets
<br>Signals
<br>This article reviews some core concepts before moving on to the first two of these mechanisms: shared files and shared memory.<br><br>A process is a program in execution, and each process has its own address space, which comprises the memory locations that the process is allowed to access. A process has one or more threads of execution, which are sequences of executable instructions: a single-threaded process has just one thread, whereas a multi-threaded process has more than one thread. Threads within a process share various resources, in particular, address space. Accordingly, threads within a process can communicate straightforwardly through shared memory, although some modern languages (e.g., Go) encourage a more disciplined approach such as the use of thread-safe channels. Of interest here is that different processes, by default, do not share memory.<br>There are various ways to launch processes that then communicate, and two ways dominate in the examples that follow:<br>
<br>A terminal is used to start one process, and perhaps a different terminal is used to start another.
<br>The system function fork is called within one process (the parent) to spawn another process (the child).
<br>The first examples take the terminal approach. The <a data-tooltip-position="top" aria-label="http://condor.depaul.edu/mkalin" rel="noopener" class="external-link" href="http://condor.depaul.edu/mkalin" target="_blank">code examples</a> are available in a ZIP file on my website.<br><br>Programmers are all too familiar with file access, including the many pitfalls (non-existent files, bad file permissions, and so on) that beset the use of files in programs. Nonetheless, shared files may be the most basic IPC mechanism. Consider the relatively simple case in which one process (producer) creates and writes to a file, and another process (consumer) reads from this same file:<br>         writes  +-----------+  reads
producer--------&gt;| disk file |&lt;-------consumer
                 +-----------+
Copy<br>The obvious challenge in using this IPC mechanism is that a race condition might arise: the producer and the consumer might access the file at exactly the same time, thereby making the outcome indeterminate. To avoid a race condition, the file must be locked in a way that prevents a conflict between a write operation and any another operation, whether a read or a write. The locking API in the standard system library can be summarized as follows:<br>
<br>A producer should gain an exclusive lock on the file before writing to the file. An exclusive lock can be held by one process at most, which rules out a race condition because no other process can access the file until the lock is released.
<br>A consumer should gain at least a shared lock on the file before reading from the file. Multiple readers can hold a shared lock at the same time, but no writer can access a file when even a single reader holds a shared lock.
<br>A shared lock promotes efficiency. If one process is just reading a file and not changing its contents, there is no reason to prevent other processes from doing the same. Writing, however, clearly demands exclusive access to a file.<br>The standard I/O library includes a utility function named fcntl that can be used to inspect and manipulate both exclusive and shared locks on a file. The function works through a file descriptor, a non-negative integer value that, within a process, identifies a file. (Different file descriptors in different processes may identify the same physical file.) For file locking, Linux provides the library function flock, which is a thin wrapper around fcntl. The first example uses the fcntl function to expose API details.<br><br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;string.h&gt;

#define FileName "data.dat"
#define DataString "Now is the winter of our discontent\nMade glorious summer by this sun of York\n"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1); /* EXIT_FAILURE */
}

int main() {
  struct flock lock;
  lock.l_type = F_WRLCK;    /* read/write (exclusive versus shared) lock */
  lock.l_whence = SEEK_SET; /* base for seek offsets */
  lock.l_start = 0;         /* 1st byte in file */
  lock.l_len = 0;           /* 0 here means 'until EOF' */
  lock.l_pid = getpid();    /* process id */

  int fd; /* file descriptor to identify a file within a process */
  if ((fd = open(FileName, O_RDWR | O_CREAT, 0666)) &lt; 0)  /* -1 signals an error */
    report_and_exit("open failed...");

  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0) /** F_SETLK doesn't block, F_SETLKW does **/
    report_and_exit("fcntl failed to get lock...");
  else {
    write(fd, DataString, strlen(DataString)); /* populate data file */
    fprintf(stderr, "Process %d has written to data file...\n", lock.l_pid);
  }

  /* Now release the lock explicitly. */
  lock.l_type = F_UNLCK;
  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)
    report_and_exit("explicit unlocking failed...");

  close(fd); /* close the file: would unlock if needed */
  return 0;  /* terminating the process would unlock as well */
}
Copy<br>The main steps in the producer program above can be summarized as follows:<br>
<br>The program declares a variable of type struct flock, which represents a lock, and initializes the structure's five fields. The first initialization:  
lock.l_type = F_WRLCK; /* exclusive lock */
Copy
  makes the lock an exclusive (read-write) rather than a shared (read-only) lock. If the producer gains the lock, then no other process will be able to write or read the file until the producer releases the lock, either explicitly with the appropriate call to fcntl or implicitly by closing the file. (When the process terminates, any opened files would be closed automatically, thereby releasing the lock.)<br>

<br>The program then initializes the remaining fields. The chief effect is that the entire file is to be locked. However, the locking API allows only designated bytes to be locked. For example, if the file contains multiple text records, then a single record (or even part of a record) could be locked and the rest left unlocked.
<br>The first call to fcntl:  
if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)
Copy
  tries to lock the file exclusively, checking whether the call succeeded. In general, the fcntl function returns -1 (hence, less than zero) to indicate failure. The second argument F_SETLK means that the call to fcntl does not block: the function returns immediately, either granting the lock or indicating failure. If the flag F_SETLKW (the W at the end is for wait) were used instead, the call to fcntl would block until gaining the lock was possible. In the calls to fcntl, the first argument fd is the file descriptor, the second argument specifies the action to be taken (in this case,&nbsp;F_SETLK for setting the lock), and the third argument is the address of the lock structure (in this case,&nbsp;&amp;lock).<br>

<br>If the producer gains the lock, the program writes two text records to the file.
<br>After writing to the file, the producer changes the lock structure's l_type field to the unlock value:  
lock.l_type = F_UNLCK;
Copy
  and calls fcntl to perform the unlocking operation. The program finishes up by closing the file and exiting.<br>

<br><br>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;

#define FileName "data.dat"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1); /* EXIT_FAILURE */
}

int main() {
  struct flock lock;
  lock.l_type = F_WRLCK;    /* read/write (exclusive) lock */
  lock.l_whence = SEEK_SET; /* base for seek offsets */
  lock.l_start = 0;         /* 1st byte in file */
  lock.l_len = 0;           /* 0 here means 'until EOF' */
  lock.l_pid = getpid();    /* process id */

  int fd; /* file descriptor to identify a file within a process */
  if ((fd = open(FileName, O_RDONLY)) &lt; 0)  /* -1 signals an error */
    report_and_exit("open to read failed...");

  /* If the file is write-locked, we can't continue. */
  fcntl(fd, F_GETLK, &amp;lock); /* sets lock.l_type to F_UNLCK if no write lock */
  if (lock.l_type != F_UNLCK)
    report_and_exit("file is still write locked...");

  lock.l_type = F_RDLCK; /* prevents any writing during the reading */
  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)
    report_and_exit("can't get a read-only lock...");

  /* Read the bytes (they happen to be ASCII codes) one at a time. */
  int c; /* buffer for read bytes */
  while (read(fd, &amp;c, 1) &gt; 0)    /* 0 signals EOF */
    write(STDOUT_FILENO, &amp;c, 1); /* write one byte to the standard output */

  /* Release the lock explicitly. */
  lock.l_type = F_UNLCK;
  if (fcntl(fd, F_SETLK, &amp;lock) &lt; 0)
    report_and_exit("explicit unlocking failed...");

  close(fd);
  return 0;
}
Copy<br>The consumer program is more complicated than necessary to highlight features of the locking API. In particular, the consumer program first checks whether the file is exclusively locked and only then tries to gain a shared lock. The relevant code is:<br>lock.l_type = F_WRLCK;
...
fcntl(fd, F_GETLK, &amp;lock); /* sets lock.l_type to F_UNLCK if no write lock */
if (lock.l_type != F_UNLCK)
  report_and_exit("file is still write locked...");
Copy<br>The F_GETLK operation specified in the fcntl call checks for a lock, in this case, an exclusive lock given as F_WRLCK in the first statement above. If the specified lock does not exist, then the fcntl call automatically changes the lock type field to F_UNLCK to indicate this fact. If the file is exclusively locked, the consumer terminates. (A more robust version of the program might have the consumer sleep a bit and try again several times.)<br>If the file is not currently locked, then the consumer tries to gain a shared (read-only) lock (F_RDLCK). To shorten the program, the F_GETLK call to fcntl could be dropped because the F_RDLCK call would fail if a read-write lock already were held by some other process. Recall that a read-only lock does prevent any other process from writing to the file, but allows other processes to read from the file. In short, a shared lock can be held by multiple processes. After gaining a shared lock, the consumer program reads the bytes one at a time from the file, prints the bytes to the standard output, releases the lock, closes the file, and terminates.<br>Here is the output from the two programs launched from the same terminal with % as the command line prompt:<br>% ./producer
Process 29255 has written to data file...

% ./consumer
Now is the winter of our discontent
Made glorious summer by this sun of York
Copy<br>In this first code example, the data shared through IPC is text: two lines from Shakespeare's play Richard III. Yet, the shared file's contents could be voluminous, arbitrary bytes (e.g., a digitized movie), which makes file sharing an impressively flexible IPC mechanism. The downside is that file access is relatively slow, whether the access involves reading or writing. As always, programming comes with tradeoffs. The next example has the upside of IPC through shared memory, rather than shared files, with a corresponding boost in performance.<br><br>Linux systems provide two separate APIs for shared memory: the legacy System V API and the more recent POSIX one. These APIs should never be mixed in a single application, however. A downside of the POSIX approach is that features are still in development and dependent upon the installed kernel version, which impacts code portability. For example, the POSIX API, by default, implements shared memory as a memory-mapped file: for a shared memory segment, the system maintains a backing file with corresponding contents. Shared memory under POSIX can be configured without a backing file, but this may impact portability. My example uses the POSIX API with a backing file, which combines the benefits of memory access (speed) and file storage (persistence).<br>The shared-memory example has two programs, named memwriter and memreader, and uses a semaphore to coordinate their access to the shared memory. Whenever shared memory comes into the picture with a writer, whether in multi-processing or multi-threading, so does the risk of a memory-based race condition; hence, the semaphore is used to coordinate (synchronize) access to the shared memory.<br>The memwriter program should be started first in its own terminal. The memreader program then can be started (within a dozen seconds) in its own terminal. The output from the memreader is:<br>This is the way the world ends...
Copy<br>Each source file has documentation at the top explaining the link flags to be included during compilation.<br>Let's start with a review of how semaphores work as a synchronization mechanism. A general semaphore also is called a counting semaphore, as it has a value (typically initialized to zero) that can be incremented. Consider a shop that rents bicycles, with a hundred of them in stock, with a program that clerks use to do the rentals. Every time a bike is rented, the semaphore is incremented by one; when a bike is returned, the semaphore is decremented by one. Rentals can continue until the value hits 100 but then must halt until at least one bike is returned, thereby decrementing the semaphore to 99.<br>A binary semaphore is a special case requiring only two values: 0 and 1. In this situation, a semaphore acts as a mutex: a mutual exclusion construct. The shared-memory example uses a semaphore as a mutex. When the semaphore's value is 0, the memwriter alone can access the shared memory. After writing, this process increments the semaphore's value, thereby allowing the memreader to read the shared memory.<br><br>/** Compilation: gcc -o memwriter memwriter.c -lrt -lpthread **/
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/mman.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;semaphore.h&gt;
#include &lt;string.h&gt;
#include "shmem.h"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1);
}

int main() {
  int fd = shm_open(BackingFile,      /* name from smem.h */
                    O_RDWR | O_CREAT, /* read/write, create if needed */
                    AccessPerms);     /* access permissions (0644) */
  if (fd &lt; 0) report_and_exit("Can't open shared mem segment...");

  ftruncate(fd, ByteSize); /* get the bytes */

  caddr_t memptr = mmap(NULL,       /* let system pick where to put segment */
                        ByteSize,   /* how many bytes */
                        PROT_READ | PROT_WRITE, /* access protections */
                        MAP_SHARED, /* mapping visible to other processes */
                        fd,         /* file descriptor */
                        0);         /* offset: start at 1st byte */
  if ((caddr_t) -1  == memptr) report_and_exit("Can't get segment...");

  fprintf(stderr, "shared mem address: %p [0..%d]\n", memptr, ByteSize - 1);
  fprintf(stderr, "backing file:       /dev/shm%s\n", BackingFile );

  /* semaphore code to lock the shared mem */
  sem_t* semptr = sem_open(SemaphoreName, /* name */
                           O_CREAT,       /* create the semaphore */
                           AccessPerms,   /* protection perms */
                           0);            /* initial value */
  if (semptr == (void*) -1) report_and_exit("sem_open");

  strcpy(memptr, MemContents); /* copy some ASCII bytes to the segment */

  /* increment the semaphore so that memreader can read */
  if (sem_post(semptr) &lt; 0) report_and_exit("sem_post");

  sleep(12); /* give reader a chance */

  /* clean up */
  munmap(memptr, ByteSize); /* unmap the storage */
  close(fd);
  sem_close(semptr);
  shm_unlink(BackingFile); /* unlink from the backing file */
  return 0;
}
Copy<br>Here's an overview of how the memwriter and memreader programs communicate through shared memory:<br>
<br>The memwriter program, shown above, calls the shm_open function to get a file descriptor for the backing file that the system coordinates with the shared memory. At this point, no memory has been allocated. The subsequent call to the misleadingly named function ftruncate:  
ftruncate(fd, ByteSize); /* get the bytes */
Copy
  allocates ByteSize bytes, in this case, a modest 512 bytes. The memwriter and memreader programs access the shared memory only, not the backing file. The system is responsible for synchronizing the shared memory and the backing file.<br>

<br>The memwriter then calls the mmap function:  
caddr_t memptr = mmap(NULL,       /* let system pick where to put segment */
                      ByteSize,   /* how many bytes */
                      PROT_READ | PROT_WRITE, /* access protections */
                      MAP_SHARED, /* mapping visible to other processes */
                      fd,         /* file descriptor */
                      0);         /* offset: start at 1st byte */
Copy
  to get a pointer to the shared memory. (The memreader makes a similar call.) The pointer type caddr_t starts with a c for calloc, a system function that initializes dynamically allocated storage to zeroes. The memwriter uses the memptr for the later write operation, using the library strcpy (string copy) function.<br>

<br>At this point, the memwriter is ready for writing, but it first creates a semaphore to ensure exclusive access to the shared memory. A race condition would occur if the memwriter were writing while the memreader was reading. If the call to sem_open succeeds:  
sem_t* semptr = sem_open(SemaphoreName, /* name */
                         O_CREAT,       /* create the semaphore */
                         AccessPerms,   /* protection perms */
                         0);            /* initial value */
Copy
  then the writing can proceed. The SemaphoreName (any unique non-empty name will do) identifies the semaphore in both the memwriter and the memreader. The initial value of zero gives the semaphore's creator, in this case,&nbsp;the memwriter, the right to proceed, in this case, to the write operation.<br>

<br>After writing, the memwriter increments the semaphore value to 1:  
if (sem_post(semptr) &lt; 0) ..
Copy
  with a call to the sem_post function. Incrementing the semaphore releases the mutex lock and enables the memreader to perform its read operation. For good measure, the memwriter also unmaps the shared memory from the memwriter address space:
munmap(memptr, ByteSize); /* unmap the storage *
Copy
  This bars the memwriter from further access to the shared memory.<br>

<br><br>/** Compilation: gcc -o memreader memreader.c -lrt -lpthread **/
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/mman.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;semaphore.h&gt;
#include &lt;string.h&gt;
#include "shmem.h"

void report_and_exit(const char* msg) {
  perror(msg);
  exit(-1);
}

int main() {
  int fd = shm_open(BackingFile, O_RDWR, AccessPerms);  /* empty to begin */
  if (fd &lt; 0) report_and_exit("Can't get file descriptor...");

  /* get a pointer to memory */
  caddr_t memptr = mmap(NULL,       /* let system pick where to put segment */
                        ByteSize,   /* how many bytes */
                        PROT_READ | PROT_WRITE, /* access protections */
                        MAP_SHARED, /* mapping visible to other processes */
                        fd,         /* file descriptor */
                        0);         /* offset: start at 1st byte */
  if ((caddr_t) -1 == memptr) report_and_exit("Can't access segment...");

  /* create a semaphore for mutual exclusion */
  sem_t* semptr = sem_open(SemaphoreName, /* name */
                           O_CREAT,       /* create the semaphore */
                           AccessPerms,   /* protection perms */
                           0);            /* initial value */
  if (semptr == (void*) -1) report_and_exit("sem_open");

  /* use semaphore as a mutex (lock) by waiting for writer to increment it */
  if (!sem_wait(semptr)) { /* wait until semaphore != 0 */
    int i;
    for (i = 0; i &lt; strlen(MemContents); i++)
      write(STDOUT_FILENO, memptr + i, 1); /* one byte at a time */
    sem_post(semptr);
  }

  /* cleanup */
  munmap(memptr, ByteSize);
  close(fd);
  sem_close(semptr);
  unlink(BackingFile);
  return 0;
}
Copy<br>In both the memwriter and memreader programs, the shared-memory functions of main interest are shm_open and mmap: on success, the first call returns a file descriptor for the backing file, which the second call then uses to get a pointer to the shared memory segment. The calls to shm_open are similar in the two programs except that the memwriter program creates the shared memory, whereas the memreader only accesses this already created memory:<br>int fd = shm_open(BackingFile, O_RDWR | O_CREAT, AccessPerms); /* memwriter */
int fd = shm_open(BackingFile, O_RDWR, AccessPerms);           /* memreader */
Copy<br>With a file descriptor in hand, the calls to mmap are the same:<br>caddr_t memptr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
Copy<br>The first argument to mmap is NULL, which means that the system determines where to allocate the memory in virtual address space. It's possible (but tricky) to specify an address instead. The MAP_SHARED flag indicates that the allocated memory is shareable among processes, and the last argument (in this case, zero) means that the offset for the shared memory should be the first byte. The size argument specifies the number of bytes to be allocated (in this case, 512), and the protection argument indicates that the shared memory can be written and read.<br>When the memwriter program executes successfully, the system creates and maintains the backing file; on my system, the file is /dev/shm/shMemEx, with shMemEx as my name (given in the header file shmem.h) for the shared storage. In the current version of the memwriter and memreader programs, the statement:<br>shm_unlink(BackingFile); /* removes backing file */
Copy<br>removes the backing file. If the unlink statement is omitted, then the backing file persists after the program terminates.<br>The memreader, like the memwriter, accesses the semaphore through its name in a call to sem_open. But the memreader then goes into a wait state until the memwriter increments the semaphore, whose initial value is 0:<br>if (!sem_wait(semptr)) { /* wait until semaphore != 0 */
Copy<br>Once the wait is over, the memreader reads the ASCII bytes from the shared memory, cleans up, and terminates.<br>The shared-memory API includes operations explicitly to synchronize the shared memory segment and the backing file. These operations have been omitted from the example to reduce clutter and&nbsp;keep the&nbsp;focus on the memory-sharing and semaphore code.<br>The memwriter and memreader programs are likely to execute without inducing a race condition even if the semaphore code is removed: the memwriter creates the shared memory segment and writes immediately to it; the memreader cannot even access the shared memory until this has been created. However, best practice requires that shared-memory access is synchronized whenever a write operation is in the mix, and the semaphore API is important enough to be highlighted in a code example.<br><br>The shared-file and shared-memory examples show how processes can communicate through shared storage, files in one case and memory segments in the other. The APIs for both approaches are relatively straightforward. Do these approaches have a common downside? Modern applications often deal with streaming data, indeed, with massively large streams of data. Neither the shared-file nor the shared-memory approaches are well suited for massive data streams. Channels of one type or another are better suited. Part 2 thus introduces channels and message queues, again with code examples in C.<br>[<a data-tooltip-position="top" aria-label="https://opensource.com/downloads/guide-inter-process-communication-linux" rel="noopener" class="external-link" href="https://opensource.com/downloads/guide-inter-process-communication-linux" target="_blank">Download</a> the complete guide to inter-process communication in Linux]<br><br>More Linux resources<br>
<br><a data-tooltip-position="top" aria-label="https://developers.redhat.com/cheat-sheets/linux-commands-cheat-sheet/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://developers.redhat.com/cheat-sheets/linux-commands-cheat-sheet/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Linux commands cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://developers.redhat.com/cheat-sheets/advanced-linux-commands/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://developers.redhat.com/cheat-sheets/advanced-linux-commands/?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Advanced Linux commands cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://www.redhat.com/en/services/training/rh024-red-hat-linux-technical-overview?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://www.redhat.com/en/services/training/rh024-red-hat-linux-technical-overview?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Free online course: RHEL Technical Overview</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/downloads/cheat-sheet-networking?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://opensource.com/downloads/cheat-sheet-networking?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Linux networking cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/downloads/cheat-sheet-selinux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://opensource.com/downloads/cheat-sheet-selinux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">SELinux cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/downloads/linux-common-commands-cheat-sheet?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://opensource.com/downloads/linux-common-commands-cheat-sheet?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Linux common commands cheat sheet</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/resources/what-are-linux-containers?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://opensource.com/resources/what-are-linux-containers?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">What are Linux containers?</a>
<br><a data-tooltip-position="top" aria-label="https://opensource.com/tags/linux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" rel="noopener" class="external-link" href="https://opensource.com/tags/linux?intcmp=70160000000h1jYAAQ&amp;utm_source=intcallout&amp;utm_campaign=linuxcontent" target="_blank">Our latest Linux articles</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/inter-process-communication-in-linux-shared-storage.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/Inter-process communication in Linux -- Shared storage.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:31 GMT</pubDate></item><item><title><![CDATA[udevd Operation and Configuration]]></title><description><![CDATA[ 
 <br>The udevd daemon operates as follows:<br>
<br>The kernel sends udevd a notification event, called a uevent, through an internal network link.
<br>udevd loads all of the attributes in the uevent.
<br>udevd parses its rules, filters and updates the uevent based on<br>
those rules, and takes actions or sets more attributes accordingly. An incoming uevent that udevd receives from the kernel might look like this:
<br>ACTION=change
DEVNAME=sde
DEVPATH=/devices/pci0000:00/0000:00:1a.0/usb1/1-1/1-1.2/1-1.2:1.0/host4/
target4:0:0/4:0:0:3/block/sde
DEVTYPE=disk
DISK_MEDIA_CHANGE=1
MAJOR=8
MINOR=64
SEQNUM=2752
SUBSYSTEM=block
UDEV_LOG=3
Copy<br>This particular event is a change to a device. After receiving the uevent, udevd knows the name of the device, the sysfs device path, and a number of other attributes associated with the properties; it is now ready to start processing rules.<br>
The rules files are in the /lib/udev/rules.d and /etc/udev/rules.d directories. The rules in /lib are the defaults, and the rules in /etc are overrides. A full explanation of the rules would be tedious, and you can learn much more from the udev(7) manual page, but here is some basic information about how udevd reads them:<br>
<br>udevd reads rules from start to finish of a rules file.
<br>After reading a rule and possibly executing its action, udevd<br>
continues reading the current rules file for more applicable rules.
<br>There are directives (such as GOTO) to skip over parts of rules files<br>
if necessary. These are usually placed at the top of a rules file to skip over the entire file if it’s irrelevant to a particular device that udevd is configuring.
<br>Let’s look at the symbolic links from the /dev/sda example. Those links were defined by rules in /lib/udev/rules.d/60-persistent-storage.rules. Inside, you’ll see the following lines:<br># ATA
KERNEL=="sd*[!0-9]|sr*", ENV{ID_SERIAL}!="?*", SUBSYSTEMS=="scsi",
ATTRS{vendor}=="ATA", IMPORT{program}="ata_id --export $devnode"
# ATAPI devices (SPC-3 or later)
KERNEL=="sd*[!0-9]|sr*", ENV{ID_SERIAL}!="?*", SUBSYSTEMS=="scsi",
ATTRS{type}=="5",ATTRS{scsi_level}=="[6-9]*", IMPORT{program}="ata_id --
export $devnode"
Copy<br>These rules match ATA disks and optical media presented through the kernel’s SCSI subsystem. You can see that there are a few rules to catch different ways the devices may be represented, but the idea is that udevd will try to match a device starting with sd or sr but without a number (with the KERNEL =="sd*[!0-9]|sr*" expression), as well as a subsystem (SUBSYSTEMS=="scsi"), and, finally, some other attributes, depending on the type of device. If all of those conditional expressions are true in either of the rules, udevd moves to the next and final expression:<br>IMPORT{program}="ata_id --export $tempnode"
Copy<br>This is not a conditional. Instead, it’s a directive to import variables from the /lib/udev/ata_id command. If you have such a disk, try it yourself on the command line. It will look like this:<br># /lib/udev/ata_id --export /dev/sda
ID_ATA=1
ID_TYPE=disk
ID_BUS=ata
ID_MODEL=WDC_WD3200AAJS-22L7A0
ID_MODEL_ENC=WDC\x20WD3200AAJS22L7A0\x20\x20\x20\x20\x20\x20\x20\x20\x20
\x20
\x20\x20\x20\x20\x20\x20\x20\x20\x20
ID_REVISION=01.03E10
ID_SERIAL=WDC_WD3200AAJS-22L7A0_WD-WMAV2FU80671
--snip--
Copy<br>The import now sets the environment so that all of the variable names in this output are set to the values shown. For example, any rule that follows will now recognize ENV{ID_TYPE} as disk. In the two rules we’ve seen so far, of particular note is ID_SERIAL. In each rule, this conditional appears second:<br>ENV{ID_SERIAL}!="?*"
Copy<br>This expression evaluates to true if ID_SERIAL is not set. Therefore, if ID_SERIAL is set, the conditional is false, the entire current rule does not apply, and udevd moves to the next rule.<br>
Why is this here? The purpose of these two rules is to run ata_id to find the serial number of the disk device and then add these attributes to the current working copy of the uevent. You’ll find this general pattern in many udev rules. With ENV{ID_SERIAL} set, udevd can now evaluate this rule later on in the rules file, which looks for any attached SCSI disks:<br>KERNEL=="sd*|sr*|cciss*", ENV{DEVTYPE}=="disk", ENV{ID_SERIAL}=="?
*",SYMLINK+="disk/by-id/$env{ID_BUS}-$env{ID_SERIAL}"
Copy<br>You can see that this rule requires ENV{ID_SERIAL} to be set, and it has one directive:<br>SYMLINK+="disk/by-id/$env{ID_BUS}-$env{ID_SERIAL}"
Copy<br>This directive tells udevd to add a symbolic link for the incoming device. So now you know where the device symbolic links came from! You may be wondering how to tell a conditional expression from a directive.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/linux/udevd-operation-and-configuration.html</link><guid isPermaLink="false">Computer Science/Operating System/Linux/udevd Operation and Configuration.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 22 Mar 2024 08:42:15 GMT</pubDate></item><item><title><![CDATA[FreeBSD 允许非 Root 用户使用 80 端口]]></title><description><![CDATA[ 
 <br>In experimenting with FreeNAS jails I wanted to allow a web service to use port 80. Normally 80 is a high order port reserved for root-level processes for security reasons. Since this is a FreeBSD jail and not a full on system I’m not worried about this.<br>The command to do so is fairly simple (thanks to <a data-tooltip-position="top" aria-label="http://hyber.org/privbind.yaws" rel="noopener" class="external-link" href="http://hyber.org/privbind.yaws" target="_blank">this page</a> for information)<br>sysctl net.inet.ip.portrange.reservedhigh=0<br>The above command is not permanent; to make it so add it to /etc/sysctl.conf:<br>echo "net.inet.ip.portrange.reservedhigh=0" &gt;&gt; /etc/sysctl.conf]]></description><link>https://muqiuhan.github.io/wiki/computer-science/operating-system/freebsd-允许非-root-用户使用-80-端口.html</link><guid isPermaLink="false">Computer Science/Operating System/FreeBSD 允许非 Root 用户使用 80 端口.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Wed, 15 May 2024 05:54:22 GMT</pubDate></item><item><title><![CDATA[并行处理的历史]]></title><description><![CDATA[ 
 <br>
原文《Parallel Processing, 1980 to 2020》Robert Kuhn, David Padua
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/other/并行处理的历史/并行处理的历史.html</link><guid isPermaLink="false">Computer Science/OTHER/并行处理的历史/并行处理的历史.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 22 Mar 2024 09:03:11 GMT</pubDate></item><item><title><![CDATA[多线程，的一点小想法]]></title><description><![CDATA[ 
 <br>标准的做法是：如果计算机有 32 个核心，那么开一个 FIFO 队列，把要计算的数据扔到这个队列里，然后开 36 个线程，每个线程去队列中取数据，算完一个再去队列中取一下个数据....直到队列中没数据。<br>CPU 密集型任务比 CPU 核心数 (如果 CPU 有超线程技术，可以按算上超线程的核心数) 略多几个是最快的，过多的线程反而会更慢，没有空闲的核心，过多的线程只能等待，并没有并行运算效果，反而线程不停的切来切去，浪费性能，通常每次切换过程需要耗费大约 1000 个 CPU 时钟周期。<br>略多几个的原因是，CPU 在真正运算开始前，需要等待数据被装入寄存器，而数据从内存装入寄存器，也需要一些时间，略多几个，可以让 CPU 在等待数据就绪时，切换到数据已经就绪的线程上开始运算。 (多发射的乱序执行也是解决内存速度跟不上cpu的一种有效手段，但是并不是切换到另一个线程)<br>如果数据在内存中连续，数据量不大 (相对CPU高速缓存) ，那么装载数据到寄存器的开销就会比较小，就可以附加少一点的线程，比如 1~2 个就够了，反之，数据分散，缓存命中率低，就需要多几个线程，比如 6~8 个。<br>另外，还可以考虑使用 SIMD 加速计算，如果数据量再多，还可以考虑CUDA-GPU加速。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/other/多线程，的一点小想法.html</link><guid isPermaLink="false">Computer Science/OTHER/多线程，的一点小想法.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 13 May 2024 00:50:32 GMT</pubDate></item><item><title><![CDATA[C++ 20 实现 string split]]></title><description><![CDATA[ 
 <br>C++20引入了范围库ranges，其中提供的两个范围适配器std::split、std::lazy_split可以使我们以一种更为优雅的形式实现split:<br>#include &lt;concept&gt;
#include &lt;ranges&gt;
#include &lt;algorithm&gt;
#include &lt;format&gt;
#include &lt;iostream&gt;

#define stdr  std::ranges
#define stdrv std::ranges::views

template&lt;template&lt;typename&gt; typename Container = std::vector, typename Arg = std::string_view&gt;
auto Split(std::string_view str, std::string_view delimiter)
{
	Container&lt;Arg&gt; myCont;
	auto temp = str 
		| stdrv::split(delimiter)
		| stdrv::transform([](auto&amp;&amp; r)
			{
				return Arg(std::addressof(*r.begin()), stdr::distance(r));
			});
	auto iter = std::inserter(myCont, myCont.end());
	stdr::for_each(temp, [&amp;](auto&amp;&amp; x) { iter = {x.begin(), x.end()}; });
	return myCont;
}
int main()
{
	std::string str = "Hello233C++20233and233New233Spilt";
	std::string delimiter = "233";
	auto&amp;&amp; strCont = Split&lt;std::list, std::string&gt;(str, delimiter);
	stdr::for_each(strCont, [](auto&amp;&amp; x) { std::cout &lt;&lt; std::format("{} ", x); });
}
//output: Hello C++20 and New Spilt
Copy<br>C++20没有提供关键的 ranges::to&lt;container&gt;函数，导致demo中还需要额外封装并手写for_each来写入数据，等到C++23实装了该函数，split的实现会比现在简洁优雅的多，真正做到方便泛用、无需封装：<br>auto&amp;&amp; strCont = str
	| stdrv::lazy_split(delimiter)
	| stdr ::to&lt;std::vector&lt;std::string&gt;&gt;;
Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/c++/c++-20-实现-string-split.html</link><guid isPermaLink="false">Computer Science/Programming Language/C++/C++ 20 实现 string split.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 22 Mar 2024 09:03:04 GMT</pubDate></item><item><title><![CDATA[C++ vector 的 push_back 和 emplace_back]]></title><description><![CDATA[ 
 <br>/// Inserts a new element at the end of the vector, right after its current last element. This new element is constructed in place using args as the arguments for its constructor.
/// This effectively increases the container size by one, which causes an automatic reallocation of the allocated storage space if -and only if- the new vector size surpasses the current vector capacity.
/// The element is constructed in-place by calling allocator_traits::construct with args forwarded.
///A similar member function exists, push_back, which either copies or moves an existing object into the container.
template &lt;class... Args&gt;
void emplace_back (Args&amp;&amp;... args);
Copy<br>push_back 会构造一个临时对象，这个临时对象会被拷贝或者移入到容器中，然而 emplace_back 会直接根据传入的参数在容器的适当位置进行构造而避免拷贝或者移动。<br>传统观点认为 push_back 会构造一个临时对象，这个临时对象会被移入到 v 中，然而 emplace_back 会直接根据传入的参数在适当位置进行构造而避免拷贝或者移动。从标准库代码的实现角度来说这是对的，但是对于提供了优化的编译器来讲，上面示例中最后两行表达式生成的代码其实没有区别。<br>真正的区别在于，emplace_back 更加强大，它可以调用任何类型的（只要存在）构造函数。而 push_back 会更加严谨，它只调用隐式构造函数。隐式构造函数被认为是安全的。如果能够通过对象 T 隐式构造对象 U，就认为 U 能够完整包含 T 的所有内容，这样将 T 传递给 U 通常是安全的。正确使用隐式构造的例子是用 std::uint32_t 对象构造 std::uint64_t 对象，错误使用隐式构造的例子是用 double 构造 std::uint8_t。<br>如果想要调用显示构造函数，那么就调用 emplace_back。如果只希望调用隐式构造函数，那么请使用更加安全的 push_back:<br>std::vector&lt;std::unique_ptr&lt;T&gt;&gt; v;
T a;
v.emplace_back(std::addressof(a)); // compiles
v.push_back(std::addressof(a)); // fails to compile
Copy<br>std::unique_ptr&lt;T&gt; 包含了显示构造函数通过 T* 进行构造。因为 emplace_back 能够调用显示构造函数，所以传递一个裸指针并不会产生编译错误。然而，当 v 超出了作用域，std::unique_ptr&lt;T&gt; 的析构函数会尝试 delete 类型 T* 的指针，而类型 T* 的指针并不是通过 new 来分配的，因为它保存的是栈对象的地址，因此 delete 行为是未定义的。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/c++/c++-vector-的-push_back-和-emplace_back.html</link><guid isPermaLink="false">Computer Science/Programming Language/C++/C++ vector 的 push_back 和 emplace_back.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 13 May 2024 03:10:10 GMT</pubDate></item><item><title><![CDATA[C++ 的 Trait]]></title><description><![CDATA[ 
 <br>
C++ 的 traits 技术，是一种约定俗称的技术方案，用来为同一类数据（包括自定义数据类型和内置数据类型）提供统一的类型名（traits），这样可以统一的操作函数，例如 advance(), swap(), encode()/decode() 等。
<br>例如，拥有义类型Foo, Bar，以及编译器自带类型 int, double, string，我们想要为这些不同的类型提供统一的编码函数 decode() 。<br>除了使用 trait 技术之外，函数重载和模板函数 + 内置字段也可以实现，前者每增加一种数据类型就需要重新实现一个函数，而同一类数据（int, unsinged int）可以使用同样的编码方法。我们想要的是针对同一种数据类型，只编写一个函数。后者对于系统自定义变量 int, double 而言，是无法在其内部定义 type 的。<br>traits 技术的关键在于，使用另外的模板类 type_traits 来保存不同数据类型的 type，这样就可以兼容自定义数据类型和内置数据类型:<br>// 定义数据 type 类
enum Type {
  TYPE_1,
  TYPE_2,
  TYPE_3
}
Copy<br>对于自定义类型，在类内部定义 type，然后在 traits 类中定义同样的 type:<br>// 自定义数据类型
class Foo {
public:
  Type type = TYPE_1;
};
class Bar {
public:
  Type type = TYPE_2;
};
template&lt;typename T&gt;
struct type_traits {
  Type type = T::type;
}
Copy<br>对于内置数据类型，使用模板类的特化为自定义类型生成独有的 type_traits:<br>// 内置数据类型
template&lt;typename int&gt;
struct type_traits {
  Type type = Type::TYPE_1;
}
template&lt;typename double&gt;
struct type_traits {
  Type type = Type::TYPE_3;
}
Copy<br>这样就可以为不同数据类型生成统一的模板函数<br>// 统一的编码函数
template&lt;typename T&gt;
void decode&lt;const T&amp; data, char* buf) {
  if(type_traits&lt;T&gt;::type == Type::TYPE_1) {
    ...
  }
  else if(type_traits&lt;T&gt;::type == Type::TYPE_2) {
    ...
  }
}
Copy<br>总结<br>
<br>traits 技术的关键在于使用第三方模板类 traits，利用模板特化的功能, 实现对自定义数据和编译器内置数据的统一
<br>这个例子使用了枚举变量来表示数据类型，而实际操作中通常使用不同的类来表示不同的类型，这样可以在编写模板函数时更好的优化。
<br>tratis 技术常见于标准库的实现中，但对日常开发中降低代码冗余也有很好的借鉴意义
<br>C++20 提供了Concept 的特性，使用Concept 可以使得实现类似的功能更加方便
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/c++/c++-的-trait.html</link><guid isPermaLink="false">Computer Science/Programming Language/C++/C++ 的 Trait.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 12 May 2024 12:49:46 GMT</pubDate></item><item><title><![CDATA[My fading frustration with ClojureScript]]></title><description><![CDATA[ 
 <br>I’ve talked about <a data-tooltip-position="top" aria-label="https://mauricio.szabo.link/blog/2018/04/05/my-frustration-with-clojurescript/" rel="noopener" class="external-link" href="https://mauricio.szabo.link/blog/2018/04/05/my-frustration-with-clojurescript/" target="_blank">at another post</a> on how ClojureScript frustrates me, mostly because I was doing some Node.JS work and Figwheel simply wasn’t working correctly. Now, it’s time to revisit these points:<br>A little update: I talked a little with Thomas Heller, Shadow-CLJS creator, and he pointed me some issues with this article, so I’ll update it acordingly<br><br>Figwheel and Lein are not the best tools to work with ClojureScript. Since I discovered shadow-cljs, things are working way better than before: I can reload ClojureScript code from any target, and I’m even experimenting with Hubot (and it works really fine too). The only thing I’m missing is my profiles.clj file, but I can live with that (and I can always use Shadow with Lein if I need profiles.clj).<br>Also, I’m working on a new package for Atom (and in the future, for another editors too) called <a data-tooltip-position="top" aria-label="http://github.com/mauricioszabo/atom-chlorine/" rel="noopener" class="external-link" href="http://github.com/mauricioszabo/atom-chlorine/" target="_blank">Chlorine</a>. One of the ideas is to offer better ClojureScript support (we have Autocomplete now!), using Socket REPL for solutions (even self-hosted REPLs like Lumo and Plank) and even wrap UNREPL protocol in Clojure. So far, is still in the very beginning but things are looking promising!<br><br>Forget Figwheel at all: Shadow-CLJS is probably the best tooling for ClojureScript ever. It auto-reloads ClojureScript code for the browser, for node.js, for node modules, and it integrates with almost everything that you want. It controls release optimizations, have sensible defaults, and even have post-compile hooks (so you can hook Clojure code to do something after some compilation phases). Also, it integrates with node-modules (no more maven-wrappers for JS libraries!) and have some warnings when you use some kind of ClojureScript code that would break :advanced compilation. And, let’s not forget that you can control the refresh reload phase, it adds a userful :include-macros in ns form (that will include all macros from the namespace being required), and controls exports in a sane manner. But first let’s begin with the feature that I found most useful: :before-load-async.  <br>When you’re developing ClojureScript code, one of the killer features is the abilty to reload your code after save, so at any moment you have the most recent version of your code running. Let’s see how it works in practice: imagine you’re developing a simple static website (no react, no vue.js, nothing – just a simple site that’ll calculate the sum of two input boxes). You already have the HTML file with two input boxes (IDs “n1” and “n2”) and a “results” div. You’ll want to add Shadow-CLJS in this project.<br>Simple: just npm install shadow-cljs, then npx shadow-cljs init, and edit the shadow-cljs.edn file. You’ll want to add a build targeting browser, so the file will look like this:<br><br>Most of the keys should be self-explanatory. The ones that are not are:<br>
<br>:asset-path mostly says that, relative to the root of your webserver, where assets will be put – for instance, suppose that your webserver will load static assets from the public directory. Then, your config will be :output-dir "public/js", so that the compiler will generate code in a folder that your webserver will be able to serve, and :asset-path "js", so the compiler will generate “requires” relative to “js” folder, not “public/js”
<br>:devtools registers a “hook function” that’ll be called when Shadow-CLJS have compiled the source paths, and it is ready to reload our code. This “reload function” will receive a parameter (let’s call it done) that, when called, it informs shadow-cljs that it’s time to use the new versions of our functions. So, let’s first look at the main function:
<br><br>You can start our compiler with npx shadow-cljs watch app, and yes, we’re using jQuery. Also, there’s a prn function that’ll be used to debug. If you include js/main.js as a script in your webpage, this code will work – but it’ll not reload. So, after these functions, we can add our reload function that’ll just call our main function after informing shadow that we did our cleanup:<br><br>If you change your code in any way – maybe change the math operation to multiplication – you’ll see that our page changes correctly. You’ll also see, in the console, that we’re printing our numbers twice per change. This is because jQuery’s .on method stacks callbacks: this means that we’re still listening to our old code. What we have to do is to clean our callbacks before reloading our code, and this is quite simple with Shadow:<br><br>UPDATE: As Thomas Heller told me, :before-load-async is used do signal my application that a reload is due, so I can control what to happen before the reload, and also to inform shadow that it can continue reloading our code. If we do any async stuff to reload code – let’s say, resolve a promise (something like (-&gt; clear-things (.then done)) – or if the reload uses some async code, this will not work. In this case, it’s better to register another callback, :after-load, and use it to re-activate our code, something like:<br><br><br>Now, this probably misses some points about why this is completely awesome. So, let’s imagine a different use-case: a reload function for an Atom editor’s package. In Atom, almost every side-effect function that you can call will return an Disposable object: one that you can dispose and nullify the effect (for instance, if you listen to changes in an editor, you can .dispose() those changes and un-listen to changes).<br>What you can do is to create a CompositeDisposable, and when you refresh, you .dispose everything, re-creates the CompositeDisposable, then re-adds your effects. This means that while you’re developing your plug-in, you can stop subscriptions, delete your commands, add new commands to editor, re-load subscriptions, all just saving your code. If this is not awesome, I don’t really know what it is. The code to do it is surprisingly simple:<br><br>For projects like Atom or other editors’ plug-ins (like VSCode, NeoVim, Oni) this give a temendous power. Also, this can be used while developing extensions for browsers, or projects when it’s tedious to unload everything, reload everything, just to see you have called a function with the arguments swapped…<br><br>With a simple npx shadow-cljs release app, Shadow will try to compile your code with :advanced optimizations. But sometimes, this won’t work: imagine that in our jQuery example, :advanced will rename jQuery to something else, so things will not work. There are two options you can use: inside your build id (in our case, :app) you can add the key :compiler-options {:infer-externs :auto}. This will throw an warning when you’re using code that Shadow-CLJS can’t infer. Also, you can use npx shadow-cljs release --debug app to compile with :advanced features, but will pretty-print files and will make meaningful names (so you can debug where things went wrong).<br>This is not exclusive of Shadow-CLJS, but the compiler just make easier to access these options. Now, what is exclusive from Shadow-CLJS is an extension to ns form (UPDATE: it is not. As Thomas told me, it uses a different implementation and have more checks, but it works in CLJS too):<br><br>This means no more :refer-macros or :require-macros. This magically finds the macros in that required namespace, and allows then to be used in the current ns. Simpler code, less things to remember, and cleaner. Also, I never had the compiler problems that I had with figwheel – that it compiles a version, then sometime later you clean things and it didn’t compile anymore. Or, that it compiles a version, and there’s a runtime exception with a compiler error.<br>Yet, for now, there are still problems with .cljc files that have macros, and some toolings that I was unable to make it work (cljs-complete being one of the most critical for me). Even with these limitations, developing with ClojureScript is now my primary choice for JS projects (when I can choose), even if there’s already JS code in production (because it integrates with existing code like a breeze too). It wasn’t before, and this was all thanks to a better tooling.<br>So yes, tools make all the difference!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/clojure/my-fading-frustration-with-clojurescript.html</link><guid isPermaLink="false">Computer Science/Programming Language/Clojure/My fading frustration with ClojureScript.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 15 Jan 2024 12:10:29 GMT</pubDate></item><item><title><![CDATA[Building a self-contained game in CSharp under 8 kilobytes]]></title><description><![CDATA[ 
 <br><br>Jan 03, 2020<br>28 minute read<br>
NOTE: This article captures a point in time in the past. While the general information is still correct, the CoreRT project got folded into <a data-tooltip-position="top" aria-label="https://learn.microsoft.com/dotnet/core/deploying/native-aot/" rel="noopener" class="external-link" href="https://learn.microsoft.com/dotnet/core/deploying/native-aot/" target="_blank">Native AOT publishing</a> in .NET 7 and is now a supported part of .NET. The information about sizes is no longer accurate (and much better), neither is the information about support for dynamic code (both interpreter and JIT are unsupported).
<br>As someone who grew up in the times of 1.44 MB floppy disks and 56 kbit modems, I’ve always liked small programs. I could fit many small programs on a floppy disk I carried with me. If a program couldn’t fit on my floppy disk, I started thinking about why - does it have a lot of graphics? Is there music? Can the program do many complex things? Or is it simply bloated?<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" target="_blank"></a><img alt="3.5&quot; floppies, photo by Brett Jordan on Unsplash" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" referrerpolicy="no-referrer"><br>3.5" floppies, photo by Brett Jordan on Unsplash<br>These days, disk space became so cheap (and huge flashdrives so ubiquitous) that people gave up on optimizing for size.<br>One place where size still matters is transfers: when transferring a program over a wire, megabytes equate to seconds. A fast 100 MBit connection can only push through 12 megabytes per second in the best case. If on the other end of the wire is a person waiting for a download to finish, the difference between five seconds and one second can have meaningful impact on their experience.<br>The person could be exposed to the transfer times either directly (user is downloading a program over network), or indirectly (a serverless service is getting deployed to respond to a web request).<br>People typically perceive anything faster than 0.1 seconds as instant, 3.0 seconds is about the limit for user’s flow to stay uninterrupted, and you would have a hard time to keep the user engaged after 10 seconds.<br>
While smaller is not essential anymore, it’s still better.
<br>This article came out as an experiment to find out just how small a useful self-contained C# executable can be. Can C# apps hit the sizes where users would consider the download times instant? Would it enable C# to be used in places where it isn’t used right now?<br><br>A self-contained application is an application that includes everything that’s necessary for it to run on a vanilla installation of the operating system.<br>The C# compiler belongs to a group of compilers targeting a virtual machine (Java and Kotlin being another notable members of the group): the output of the C# compiler is an executable that requires some sort of virtual machine (VM) to execute. One can’t just install a barebone operating system and expect to be able to run programs produced by the C# compiler on it.<br>At least on Windows, it used to be the case that one could rely on a machine-wide installation of the .NET Framework to run the outputs of the C# compiler. Nowadays there are many Windows SKUs that no longer carry the framework with it (IoT, Nano Server, ARM64,…). .NET Framework also doesn’t support the latest enhancements to the C# language. It’s kind of on its way out.<br>For a C# app to be self-contained, it needs to include the runtime and all the class libraries it uses. It’s a lot of stuff to fit into the 8 kB that we budget for!<br><br>We’re going to build a clone of the Snake game. Here’s the finished product:<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/02-snake.gif" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/02-snake.gif" target="_blank"></a><img alt="Snake game" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/02-snake.gif" referrerpolicy="no-referrer"><br>Snake game<br>If you’re not interested in the game mechanics, feel free to skip over to the interesting parts where we shrink the game from 65 megabytes to 8 kilobytes in 9 steps (scroll down to where you see graphs).<br>The game will run in text mode and we’ll use the box drawing characters to draw the snake. I’m sure Vulcan or DirectX would be a lot more fun, but we’ll get by with System.Console.<br><br>We’re going to build a no-allocation game - and by no-allocation I don’t mean the “don’t allocate in the game loop” that is common among C# game devs. I mean “the new keyword with reference types is forbidden in the entire codebase”. The reasons for that will become apparent at the final stretch of shrinking the game.<br>With such restriction in place, one might wonder if there’s any point in using C# after all: without the new keyword, we won’t be using the garbage collector, we can’t throw exceptions, etc. - a language like C would work just as well.<br>One reason to use C# is “because we can”. The other reason is testability and code sharing - while the game as a whole is no-allocation, it doesn’t mean that parts of it couldn’t be reused in a different project that doesn’t have such constrains. For example, parts of the game could be included from an xUnit project to get unit test coverage. If one chooses C to build the game, things have to stay constrained by what C can do even when the code is reused from elsewhere. But since C# provides a good mix of high level and low level constructs, we can follow the “high level by default, low level when necessary” philosophy.<br>To reach the 8 kB deployment size, the low level part will be necessary.<br><br>Let’s start with a struct that represents the frame buffer. Frame buffer is a component that holds the pixels (or in this case characters) to be drawn to the screen.<br>unsafe struct FrameBuffer
{
    public const int Width = 40;
    public const int Height = 20;
    public const int Area = Width * Height;

    fixed char _chars[Area];

    public void SetPixel(int x, int y, char character)
    {
        _chars[y * Width + x] = character;
    }

    public void Clear()
    {
        for (int i = 0; i &lt; Area; i++)
            _chars[i] = ' ';
    }

    public readonly void Render()
    {
        Console.SetCursorPosition(0, 0);

        const ConsoleColor snakeColor = ConsoleColor.Green;

        Console.ForegroundColor = snakeColor;

        for (int i = 1; i &lt;= Area; i++)
        {
            char c = _chars[i - 1];

            if (c == '*' || (c &gt;= 'A' &amp;&amp; c &lt;= 'Z') || (c &gt;= 'a' &amp;&amp; c &lt;= 'z'))
            {
                Console.ForegroundColor = c == '*' ? ConsoleColor.Red : ConsoleColor.White;
                Console.Write(c);
                Console.ForegroundColor = snakeColor;
            }
            else
                Console.Write(c);

            if (i % Width == 0)
            {
                Console.SetCursorPosition(0, i / Width - 1);
            }
        }
    }
}
Copy<br>We provide methods to set individual pixels, clear the frame buffer, and to render the contents of the frame buffer into System.Console. The rendering step special cases a couple characters so that we get colorful output without having to keep track of color for each pixel of the frame buffer.<br>One interesting thing to call out is the fixed char _chars[Area] field: this is the C# syntax to declare a <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/unsafe-code-pointers/fixed-size-buffers" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/unsafe-code-pointers/fixed-size-buffers" target="_blank">fixed array</a>. A fixed array is an array whose individual elements are a part of the struct. You can think of it as a shortcut for a set of fields char _char_0, _char_1, _char_2, _char_3,... _char_Area that one can access as an array. The size of this array needs to be a compile time constant so that the size of the entire struct is fixed.<br>We can’t go overboard with the size of a fixed array because being a part of a struct, the array needs to live on the stack and stacks tend to be limited to a small number of bytes (1 MB per thread, typically). But 40  20  2 bytes (width  height  sizeof(char)) should be fine.<br>Next thing we need is a random number generator. The one that comes with .NET is a reference type (for good reasons!) and we forbid ourselves the new keyword - we can’t use it. A simple struct will do:<br>struct Random
{
    private uint _val;

    public Random(uint seed)
    {
        _val = seed;
    }

    public uint Next() =&gt; _val = (1103515245 * _val + 12345) % 2147483648;
}
Copy<br>This random number generator is not great, but we don’t need anything sophisticated.<br>Now we only need something that wraps the snake logic. Time for a Snake struct:<br>struct Snake
{
    public const int MaxLength = 30;

    private int _length;

    // Body is a packed integer that packs the X coordinate, Y coordinate, and the character
    // for the snake's body.
    // Only primitive types can be used with C# `fixed`, hence this is an `int`.
    private unsafe fixed int _body[MaxLength];

    private Direction _direction;
    private Direction _oldDirection;

    public Direction Course
    {
        set
        {
            if (_oldDirection != _direction)
                _oldDirection = _direction;

            if (_direction - value != 2 &amp;&amp; value - _direction != 2)
                _direction = value;
        }
    }

    public unsafe Snake(byte x, byte y, Direction direction)
    {
        _body[0] = new Part(x, y, DirectionToChar(direction, direction)).Pack();
        _direction = direction;
        _oldDirection = direction;
        _length = 1;
    }

    public unsafe bool Update()
    {
        Part oldHead = Part.Unpack(_body[0]);
        Part newHead = new Part(
            (byte)(_direction switch
            {
                Direction.Left =&gt; oldHead.X == 0 ? FrameBuffer.Width - 1 : oldHead.X - 1,
                Direction.Right =&gt; (oldHead.X + 1) % FrameBuffer.Width,
                _ =&gt; oldHead.X,
            }),
            (byte)(_direction switch
            {
                Direction.Up =&gt; oldHead.Y == 0 ? FrameBuffer.Height - 1 : oldHead.Y - 1,
                Direction.Down =&gt; (oldHead.Y + 1) % FrameBuffer.Height,
                _ =&gt; oldHead.Y,
            }),
            DirectionToChar(_direction, _direction)
            );

        oldHead = new Part(oldHead.X, oldHead.Y, DirectionToChar(_oldDirection, _direction));

        bool result = true;

        for (int i = 0; i &lt; _length - 1; i++)
        {
            Part current = Part.Unpack(_body[i]);
            if (current.X == newHead.X &amp;&amp; current.Y == newHead.Y)
                result = false;
        }

        _body[0] = oldHead.Pack();

        for (int i = _length - 2; i &gt;= 0; i--)
        {
            _body[i + 1] = _body[i];
        }

        _body[0] = newHead.Pack();

        _oldDirection = _direction;

        return result;
    }

    public unsafe readonly void Draw(ref FrameBuffer fb)
    {
        for (int i = 0; i &lt; _length; i++)
        {
            Part p = Part.Unpack(_body[i]);
            fb.SetPixel(p.X, p.Y, p.Character);
        }
    }

    public bool Extend()
    {
        if (_length &lt; MaxLength)
        {
            _length += 1;
            return true;
        }
        return false;
    }

    public unsafe readonly bool HitTest(int x, int y)
    {
        for (int i = 0; i &lt; _length; i++)
        {
            Part current = Part.Unpack(_body[i]);
            if (current.X == x &amp;&amp; current.Y == y)
                return true;
        }

        return false;
    }

    private static char DirectionToChar(Direction oldDirection, Direction newDirection)
    {
        const string DirectionChangeToChar = "│┌?┐┘─┐??└│┘└?┌─";
        return DirectionChangeToChar[(int)oldDirection * 4 + (int)newDirection];
    }

    // Helper struct to pack and unpack the packed integer in _body.
    readonly struct Part
    {
        public readonly byte X, Y;
        public readonly char Character;

        public Part(byte x, byte y, char c)
        {
            X = x;
            Y = y;
            Character = c;
        }

        public int Pack() =&gt; X &lt;&lt; 24 | Y &lt;&lt; 16 | Character;
        public static Part Unpack(int packed) =&gt; new Part((byte)(packed &gt;&gt; 24), (byte)(packed &gt;&gt; 16), (char)packed);
    }

    public enum Direction
    {
        Up, Right, Down, Left
    }
}
Copy<br>The state that a snake needs to track is:<br>
<br>the coordinates of each pixel that represents the snake’s body,
<br>the current length of the snake,
<br>the current direction of the snake,
<br>past direction of the snake (in case we need to draw the “bend” character instead of a straight line)
<br>The snake provides methods to Extend the length of snake by one (returns false if the snake is already at full length), to HitTest a pixel with the snake’s body, to Draw the snake into a FrameBuffer and to Update the snake’s position as a response to a game tick (returns false if the snake ate itself). There’s also a property to set the current Course of the snake.<br>We use the same fixed array trick that we used in the frame buffer to keep the snake no-allocation. It means the maximum length of the snake has to be a compile time constant.<br>The last thing we need is the game loop:<br>struct Game
{
    enum Result
    {
        Win, Loss
    }

    private Random _random;

    private Game(uint randomSeed)
    {
        _random = new Random(randomSeed);
    }

    private Result Run(ref FrameBuffer fb)
    {
        Snake s = new Snake(
            (byte)(_random.Next() % FrameBuffer.Width),
            (byte)(_random.Next() % FrameBuffer.Height),
            (Snake.Direction)(_random.Next() % 4));

        MakeFood(s, out byte foodX, out byte foodY);

        long gameTime = Environment.TickCount64;

        while (true)
        {
            fb.Clear();

            if (!s.Update())
            {
                s.Draw(ref fb);
                return Result.Loss;
            }

            s.Draw(ref fb);

            if (Console.KeyAvailable)
            {
                ConsoleKeyInfo ki = Console.ReadKey(intercept: true);
                switch (ki.Key)
                {
                    case ConsoleKey.UpArrow:
                        s.Course = Snake.Direction.Up; break;
                    case ConsoleKey.DownArrow:
                        s.Course = Snake.Direction.Down; break;
                    case ConsoleKey.LeftArrow:
                        s.Course = Snake.Direction.Left; break;
                    case ConsoleKey.RightArrow:
                        s.Course = Snake.Direction.Right; break;
                }
            }

            if (s.HitTest(foodX, foodY))
            {
                if (s.Extend())
                    MakeFood(s, out foodX, out foodY);
                else
                    return Result.Win;
            }

            fb.SetPixel(foodX, foodY, '*');

            fb.Render();

            gameTime += 100;

            long delay = gameTime - Environment.TickCount64;
            if (delay &gt;= 0)
                Thread.Sleep((int)delay);
            else
                gameTime = Environment.TickCount64;
        }
    }

    void MakeFood(in Snake snake, out byte foodX, out byte foodY)
    {
        do
        {
            foodX = (byte)(_random.Next() % FrameBuffer.Width);
            foodY = (byte)(_random.Next() % FrameBuffer.Height);
        }
        while (snake.HitTest(foodX, foodY));
    }

    static void Main()
    {
        Console.SetWindowSize(FrameBuffer.Width, FrameBuffer.Height);
        Console.SetBufferSize(FrameBuffer.Width, FrameBuffer.Height);
        Console.Title = "See Sharp Snake";
        Console.CursorVisible = false;

        FrameBuffer fb = new FrameBuffer();

        while (true)
        {
            Game g = new Game((uint)Environment.TickCount64);
            Result result = g.Run(ref fb);

            string message = result == Result.Win ? "You win" : "You lose";

            int position = (FrameBuffer.Width - message.Length) / 2;
            for (int i = 0; i &lt; message.Length; i++)
            {
                fb.SetPixel(position + i, FrameBuffer.Height / 2, message[i]);
            }

            fb.Render();

            Console.ReadKey(intercept: true);
        }
    }
}
Copy<br>We use the random number generator to generate a random position and direction of the snake, we randomly place the food on the game surface, making sure it doesn’t overlap the snake, and start the game loop.<br>Within the game loop we ask the snake to update its position and check whether it ate itself. We then draw the snake, check the keyboard for input, hit-test the snake with the food, and render everything to the console.<br>That’s pretty much it. Let’s see where we are in terms of size.<br><br>I’ve placed the game in <a data-tooltip-position="top" aria-label="https://github.com/MichalStrehovsky/SeeSharpSnake" rel="noopener" class="external-link" href="https://github.com/MichalStrehovsky/SeeSharpSnake" target="_blank">a GitHub repo</a> so that you can follow along. The project file will produce the game in different configurations depending on the Mode property passed to publish. To produce the default configuration with CoreCLR, run:<br>$ dotnet publish -r win-x64 -c Release
Copy<br>This will produce a single EXE file that has whopping 65 MB. The produced EXE includes the game, the .NET Runtime, and the base class libraries that are the standard part of .NET. You might say “still better than Electron” and call it good, but let’s see if we can do better.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/03-graph1.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/03-graph1.png" target="_blank"></a><img alt="Starting point" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/03-graph1.png" referrerpolicy="no-referrer"><br>Starting point<br><br>IL trimming shipped with .NET Core 3.0 - trimming removes unused code from your app by scanning the entire program and removing assemblies that are unreferenced. To use it with the project, pass a PublishTrimmed property to publish. Like so:<br>$ dotnet publish -r win-x64 -c Release /p:PublishTrimmed=true
Copy<br>With this setting, the game shrinks to 25 MB. It’s a nice 60% reduction, but far from our 8 kB goal.<br>Trimming has more aggressive settings that are not publicly exposed and they could bring this down further, but in the end, we’re going to be limited by the size of the CoreCLR runtime itself - coreclr.dll - at 5.3 MB. We might have reached a dead end on the road to a 8 kB game.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/04-graph2.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/04-graph2.png" target="_blank"></a><img alt="After trimming" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/04-graph2.png" referrerpolicy="no-referrer"><br>After trimming<br><br><a data-tooltip-position="top" aria-label="https://www.mono-project.com/" rel="noopener" class="external-link" href="https://www.mono-project.com/" target="_blank">Mono</a> is another .NET runtime that for many is the synonym for Xamarin. To build a single executable with the C# snake, we can use the mkbundle tool that comes with Mono:<br>$ mkbundle SeeSharpSnake.dll --simple -o SeeSharpSnake.exe
Copy<br>This will produce a 12.3 MB executable that depends on mono-2.0-sgen.dll that itself has 5.9 MB - so we’re looking at 18.2 MB in total. When trying to launch it, I hit “Error mapping file: mono_file_map_error failed”, but I’m going to trust that except for this bug, things would work with Mono and the result would be 18.2 MB.<br>Unlike CoreCLR, Mono also depends on the Visual C++ runtime redistributable library that is not available in a default Windows installation: to keep the goal of the app being self-contained, we need to carry this library with the app. This increases the footprint of the application by another megabyte or so.<br>We would likely be able to make things smaller by adding trimming to the mix, but we’re going to hit the same problem as with CoreCLR - the size of the runtime (mono-2.0-sgen.dll) is 5.9 MB (plus the size of the C++ runtime libraries on top of it), and represents the floor of where any possible IL-level optimization could bring us.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/05-graph3.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/05-graph3.png" target="_blank"></a><img alt="With Mono" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/05-graph3.png" referrerpolicy="no-referrer"><br>With Mono<br><br>It is clear that to get anywhere near the 8 kB goal, we need to take the runtime out of the app. The only .NET runtime where this is possible is <a data-tooltip-position="top" aria-label="https://github.com/dotnet/corert" rel="noopener" class="external-link" href="https://github.com/dotnet/corert" target="_blank">CoreRT</a>. While it’s common to call CoreRT a “runtime”, it’s closer to being a “runtime library”. It’s not a virtual machine like CoreCLR or Mono - the CoreRT’s runtime is just a set of functions that support ahead of time generated native code produced by CoreRT’s ahead of time compiler.<br>CoreRT comes with libraries that make CoreRT look like any other .NET runtime: there’s a library that adds GC, library that adds support for reflection, library that adds a JIT, library that adds an interpreter, etc. But all of those libraries are optional (and that includes the GC).<br>More on how CoreRT differs from CoreCLR and Mono is in <a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2019/05/01/fight-the-global-warming-compile-your-csharp-apps-ahead-of-time/" rel="noopener" class="external-link" href="https://migeel.sk/blog/2019/05/01/fight-the-global-warming-compile-your-csharp-apps-ahead-of-time/" target="_blank">this article</a>. When I was reading about the runtime of the <a data-tooltip-position="top" aria-label="https://theartofmachinery.com/2017/06/04/what_is_the_d_runtime.html" rel="noopener" class="external-link" href="https://theartofmachinery.com/2017/06/04/what_is_the_d_runtime.html" target="_blank">D language</a>, it reminded me of CoreRT a lot. The article is an interesting read too.<br>Let’s see where we’re with the default CoreRT configuration:<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT
Copy<br>This comes down to 4.7 MB. It’s the smallest so far, but still not good enough.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/06-graph4.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/06-graph4.png" target="_blank"></a><img alt="With CoreRT" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/06-graph4.png" referrerpolicy="no-referrer"><br>With CoreRT<br>The CoreRT ahead of time compiler offers a <a data-tooltip-position="top" aria-label="http://aka.ms/OptimizeCoreRT" rel="noopener" class="external-link" href="http://aka.ms/OptimizeCoreRT" target="_blank">vast number</a> of settings that affect code generation. By default, the compiler tries to maximize the generated code speed and compatibility with other .NET runtimes at the expense of the size of the generated executable.<br>The compiler has a built-in trimmer that removes unused code. The “CoreRT-Moderate” setting that we define in the Snake project relaxes one of the restrictions on removing unused code that allows more removal. We also ask the compiler to trade program speed for some extra bytes. Most .NET programs would work just fine in this mode.<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-Moderate
Copy<br>We’re now at 4.3 MB.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/07-graph5.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/07-graph5.png" target="_blank"></a><img alt="With stuff removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/07-graph5.png" referrerpolicy="no-referrer"><br>With stuff removed<br><br>I’ve grouped a couple more compilation options into a “high savings” mode. This mode is going to remove support for things that many apps would notice, but Snake (being the low level thing that it is) won’t.<br>We are going to remove:<br>
<br>Stack trace data for framework implementation details
<br>Exception messages in framework-thrown exceptions
<br>Support for non-English locales
<br>EventSource instrumentation
<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-High
Copy<br>We’ve reached 3.0 MB. This is 5% of what we started with, but CoreRT has one more trick up its sleeve.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/08-graph6.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/08-graph6.png" target="_blank"></a><img alt="With more stuff removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/08-graph6.png" referrerpolicy="no-referrer"><br>With more stuff removed<br><br>Substantial part of the CoreRT runtime libraries is dedicated to the implementation of the .NET reflection surface area. Because CoreRT is an ahead-of-time-compiled runtime-library-based .NET implementation, it doesn’t need most of the data structures a typical VM-based runtime (like CoreCLR and Mono) needs. This data includes things like names of types, methods, signatures, base types, etc. CoreRT embeds this data because programs using .NET reflection need it, but not because it’s needed for the runtime to operate. I call this data “the reflection tax”, because that’s what it is for the runtime.<br>CoreRT supports a <a data-tooltip-position="top" aria-label="https://github.com/dotnet/corert/blob/master/Documentation/using-corert/reflection-free-mode.md" rel="noopener" class="external-link" href="https://github.com/dotnet/corert/blob/master/Documentation/using-corert/reflection-free-mode.md" target="_blank">reflection-free mode</a> that avoids this tax. You might feel that a lot of .NET code wouldn’t work without reflection and you might be right, but a surprising amount of things do work: Gui.cs, System.IO.Pipelines, or even a basic WinForms app. Snake will definitely work, so let’s turn this mode on:<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-ReflectionFree
Copy<br>We’re now at 1.2 MB. The reflection tax is a pretty heavy tax!<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/09-graph7.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/09-graph7.png" target="_blank"></a><img alt="With more stuff removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/09-graph7.png" referrerpolicy="no-referrer"><br>With more stuff removed<br><br>Now we’ve reached the end of what’s possible with the .NET SDK and we need to get our hands dirty. What we’re going to do now is starting to be ridiculous and I wouldn’t expect anyone else to do this. We’re going to rely on the implementation details of the CoreRT compiler and runtime.<br>As we saw earlier, CoreRT is a set of runtime libraries coupled with an ahead of time compiler. What if we replace the runtime libraries with a minimal reimplementation? We’ve decided not to use the garbage collector and that makes this job much more feasible.<br>Let’s start with the easy things:<br>namespace System.Threading
{
    static class Thread
    {
        [DllImport("api-ms-win-core-synch-l1-2-0")]
        public static extern void Sleep(int delayMs);
    }
}

namespace System
{
    static class Environment
    {
        [DllImport("api-ms-win-core-sysinfo-l1-1-0")]
        private static extern long GetTickCount64();

        public static long TickCount64 =&gt; GetTickCount64();
    }
}
Copy<br>There - we just reimplemented Thread.Sleep and Environment.TickCount64 (for Windows) while avoiding all dependencies on the existing runtime library.<br>Let’s do the same for the subset of System.Console that the game uses:<br>namespace System
{
    static class Console
    {
        private enum BOOL : int
        {
            FALSE = 0,
            TRUE = 1,
        }

        [DllImport("api-ms-win-core-processenvironment-l1-1-0")]
        private static unsafe extern IntPtr GetStdHandle(int c);

        private readonly static IntPtr s_outputHandle = GetStdHandle(-11);

        private readonly static IntPtr s_inputHandle = GetStdHandle(-10);

        [DllImport("api-ms-win-core-console-l2-1-0.dll", EntryPoint = "SetConsoleTitleW")]
        private static unsafe extern BOOL SetConsoleTitle(char* c);
        public static unsafe string Title
        {
            set
            {
                fixed (char* c = value)
                    SetConsoleTitle(c);
            }
        }

        [StructLayout(LayoutKind.Sequential)]
        struct CONSOLE_CURSOR_INFO
        {
            public uint Size;
            public BOOL Visible;
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleCursorInfo(IntPtr handle, CONSOLE_CURSOR_INFO* cursorInfo);

        public static unsafe bool CursorVisible
        {
            set
            {
                CONSOLE_CURSOR_INFO cursorInfo = new CONSOLE_CURSOR_INFO
                {
                    Size = 1,
                    Visible = value ? BOOL.TRUE : BOOL.FALSE
                };
                SetConsoleCursorInfo(s_outputHandle, &amp;cursorInfo);
            }
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleTextAttribute(IntPtr handle, ushort attribute);

        public static ConsoleColor ForegroundColor
        {
            set
            {
                SetConsoleTextAttribute(s_outputHandle, (ushort)value);
            }
        }

        [StructLayout(LayoutKind.Sequential)]
        private struct KEY_EVENT_RECORD
        {
            public BOOL KeyDown;
            public short RepeatCount;
            public short VirtualKeyCode;
            public short VirtualScanCode;
            public short UChar;
            public int ControlKeyState;
        }

        [StructLayout(LayoutKind.Sequential)]
        private struct INPUT_RECORD
        {
            public short EventType;
            public KEY_EVENT_RECORD KeyEvent;
        }

        [DllImport("api-ms-win-core-console-l1-2-0", EntryPoint = "PeekConsoleInputW", CharSet = CharSet.Unicode)]
        private static unsafe extern BOOL PeekConsoleInput(IntPtr hConsoleInput, INPUT_RECORD* lpBuffer, uint nLength, uint* lpNumberOfEventsRead);

        public static unsafe bool KeyAvailable
        {
            get
            {
                uint nRead;
                INPUT_RECORD buffer;
                while (true)
                {
                    PeekConsoleInput(s_inputHandle, &amp;buffer, 1, &amp;nRead);

                    if (nRead == 0)
                        return false;

                    if (buffer.EventType == 1 &amp;&amp; buffer.KeyEvent.KeyDown != BOOL.FALSE)
                        return true;

                    ReadConsoleInput(s_inputHandle, &amp;buffer, 1, &amp;nRead);
                }
            }
        }

        [DllImport("api-ms-win-core-console-l1-2-0", EntryPoint = "ReadConsoleInputW", CharSet = CharSet.Unicode)]
        private static unsafe extern BOOL ReadConsoleInput(IntPtr hConsoleInput, INPUT_RECORD* lpBuffer, uint nLength, uint* lpNumberOfEventsRead);

        public static unsafe ConsoleKeyInfo ReadKey(bool intercept)
        {
            uint nRead;
            INPUT_RECORD buffer;
            do
            {
                ReadConsoleInput(s_inputHandle, &amp;buffer, 1, &amp;nRead);
            }
            while (buffer.EventType != 1 || buffer.KeyEvent.KeyDown == BOOL.FALSE);

            return new ConsoleKeyInfo((char)buffer.KeyEvent.UChar, (ConsoleKey)buffer.KeyEvent.VirtualKeyCode, false, false, false);
        }

        struct SMALL_RECT
        {
            public short Left, Top, Right, Bottom;
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleWindowInfo(IntPtr handle, BOOL absolute, SMALL_RECT* consoleWindow);

        public static unsafe void SetWindowSize(int x, int y)
        {
            SMALL_RECT rect = new SMALL_RECT
            {
                Left = 0,
                Top = 0,
                Right = (short)(x - 1),
                Bottom = (short)(y - 1),
            };
            SetConsoleWindowInfo(s_outputHandle, BOOL.TRUE, &amp;rect);
        }

        [StructLayout(LayoutKind.Sequential)]
        struct COORD
        {
            public short X, Y;
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleScreenBufferSize(IntPtr handle, COORD size);

        public static void SetBufferSize(int x, int y)
        {
            SetConsoleScreenBufferSize(s_outputHandle, new COORD { X = (short)x, Y = (short)y });
        }

        [DllImport("api-ms-win-core-console-l2-1-0")]
        private static unsafe extern BOOL SetConsoleCursorPosition(IntPtr handle, COORD position);

        public static void SetCursorPosition(int x, int y)
        {
            SetConsoleCursorPosition(s_outputHandle, new COORD { X = (short)x, Y = (short)y });
        }

        [DllImport("api-ms-win-core-console-l1-2-0", EntryPoint = "WriteConsoleW")]
        private static unsafe extern BOOL WriteConsole(IntPtr handle, void* buffer, int numChars, int* charsWritten, void* reserved);

        public static unsafe void Write(char c)
        {
            int dummy;
            WriteConsole(s_outputHandle, &amp;c, 1, &amp;dummy, null);
        }
    }
}
Copy<br>Let’s rebuild the game with this replacement framework:<br>$ dotnet publish -r win-x64 -c Release /p:Mode=CoreRT-ReflectionFree /p:IncludePal=true
Copy<br>Unsurprisingly, this didn’t save us much. The APIs we’re replacing are already relatively lightweight, and rewriting them only gains a couple kilobytes that are not worth mentioning. But this is an important stepping stone to the last step in our journey.<br><br>The remaining 1.2 MB of code and data in the Snake game is there to support things we don’t see, but are there - ready in case we need them. There’s the garbage collector, support for exception handling, the code to format and print stack traces to the console when an unhandled exception happens, and many other things that are “under the hood”.<br>The compiler could detect that none of this is needed and avoid generating them, but what we’re trying to do is so weird that it’s not worth adding compiler features to support it. The way to avoid it is to simply provide an alternative runtime library.<br>Let’s start with redefining a minimal version of the base types:<br>namespace System
{
    public class Object
    {
        // The layout of object is a contract with the compiler.
        public IntPtr m_pEEType;
    }
    public struct Void { }

    // The layout of primitive types is special cased because it would be recursive.
    // These really don't need any fields to work.
    public struct Boolean { }
    public struct Char { }
    public struct SByte { }
    public struct Byte { }
    public struct Int16 { }
    public struct UInt16 { }
    public struct Int32 { }
    public struct UInt32 { }
    public struct Int64 { }
    public struct UInt64 { }
    public struct IntPtr { }
    public struct UIntPtr { }
    public struct Single { }
    public struct Double { }

    public abstract class ValueType { }
    public abstract class Enum : ValueType { }

    public struct Nullable&lt;T&gt; where T : struct { }
    
    public sealed class String
    {
        // The layout of the string type is a contract with the compiler.
        public readonly int Length;
        public char _firstChar;

        public unsafe char this[int index]
        {
            [System.Runtime.CompilerServices.Intrinsic]
            get
            {
                return Internal.Runtime.CompilerServices.Unsafe.Add(ref _firstChar, index);
            }
        }
    }
    public abstract class Array { }
    public abstract class Delegate { }
    public abstract class MulticastDelegate : Delegate { }

    public struct RuntimeTypeHandle { }
    public struct RuntimeMethodHandle { }
    public struct RuntimeFieldHandle { }

    public class Attribute { }
}

namespace System.Runtime.CompilerServices
{
    internal sealed class IntrinsicAttribute : Attribute { }

    public class RuntimeHelpers
    {
        public static unsafe int OffsetToStringData =&gt; sizeof(IntPtr) + sizeof(int);
    }
}

namespace System.Runtime.InteropServices
{
    public enum CharSet
    {
        None = 1,
        Ansi = 2,
        Unicode = 3,
        Auto = 4,
    }

    public sealed class DllImportAttribute : Attribute
    {
        public string EntryPoint;
        public CharSet CharSet;
        public DllImportAttribute(string dllName) { }
    }

    public enum LayoutKind
    {
        Sequential = 0,
        Explicit = 2,
        Auto = 3,
    }

    public sealed class StructLayoutAttribute : Attribute
    {
        public StructLayoutAttribute(LayoutKind layoutKind) { }
    }
}
namespace Internal.Runtime.CompilerServices
{
    public static unsafe partial class Unsafe
    {
        // The body of this method is generated by the compiler.
        // It will do what Unsafe.Add is expected to do. It's just not possible to express it in C#.
        [System.Runtime.CompilerServices.Intrinsic]
        public static extern ref T Add&lt;T&gt;(ref T source, int elementOffset);
    }
}
Copy<br>At this point let’s forgo the project file and dotnet CLI and launch the individual tools directly. We start by launching the C# compiler (CSC). I recommend launching these commands from the “x64 Native Tools Command Prompt for VS 2019” - it’s in your Start menu if you have Visual Studio installed. The right version of tools is on the PATH in that window.<br>The /noconfig, /nostdlib, and /runtimemetadataversion are the magic switches needed to compile something that defines System.Object. I chose the .ilexe file extension instead of .exe because .exe will be used for the finished product.<br>$ csc.exe /debug /O /noconfig /nostdlib /runtimemetadataversion:v4.0.30319 MiniBCL.cs Game\FrameBuffer.cs Game\Random.cs Game\Game.cs Game\Snake.cs Pal\Thread.Windows.cs Pal\Environment.Windows.cs Pal\Console.Windows.cs /out:zerosnake.ilexe /langversion:latest /unsafe
Copy<br>This will successfully compile the IL bytecode version of the game with the C# compiler. We still need some sort of runtime to execute it.<br>Let’s try to feed this to the CoreRT ahead of time compiler to generate native code from the IL. If you followed the steps above, you’ll find ilc.exe, the CoreRT ahead of time compiler, in your NuGet package cache (somewhere like %USERPROFILE%.nuget\packages\runtime.win-x64.microsoft.dotnet.ilcompiler\1.0.0-alpha-27402–01\Tools).<br>$ ilc.exe zerosnake.ilexe -o zerosnake.obj --systemmodule zerosnake --Os -g
Copy<br>This is going to crash with “Expected type ‘Internal.Runtime.CompilerHelpers.StartupCodeHelpers’ not found in module ‘zerosnake’”. Turns out that besides the obvious minimum that a managed developer would expect, there’s also a minimum that the CoreRT compiler needs to compile the input.<br>Let’s skip to the chase and add what’s needed:<br>namespace Internal.Runtime.CompilerHelpers
{
    // A class that the compiler looks for that has helpers to initialize the
    // process. The compiler can gracefully handle the helpers not being present,
    // but the class itself being absent is unhandled. Let's add an empty class.
    class StartupCodeHelpers
    {
    }
}

namespace System
{
    // A special type that the compiler uses to implement generic interfaces
    // (e.g. IEnumerable&lt;T&gt;) on arrays. Our arrays won't implement any generic interfaces.
    class Array&lt;T&gt; : Array { }
}

namespace System.Runtime.InteropServices
{
    // Custom attribute that marks a class as having special "Call" intrinsics.
    // The compiler has special logic handling types with this attribute.
    internal class McgIntrinsicsAttribute : Attribute { }
}

namespace System.Runtime.CompilerServices
{
    // A class responsible for running static constructors. The compiler will call into this
    // code to ensure static constructors run and that they only run once.
    [System.Runtime.InteropServices.McgIntrinsics]
    internal static class ClassConstructorRunner
    {
        private static unsafe IntPtr CheckStaticClassConstructionReturnNonGCStaticBase(ref StaticClassConstructionContext context, IntPtr nonGcStaticBase)
        {
            CheckStaticClassConstruction(ref context);
            return nonGcStaticBase;
        }

        private static unsafe void CheckStaticClassConstruction(ref StaticClassConstructionContext context)
        {
            // Very simplified class constructor runner. In real world, the class constructor runner
            // would need to be able to deal with potentially multiple threads racing to initialize
            // a single class, and would need to be able to deal with potential deadlocks
            // between class constructors.

            // If the class is already initialized, we're done.
            if (context.initialized == 1)
                return;

            // Mark the class as initialized.
            context.initialized = 1;

            // Run the class constructor.
            Call&lt;int&gt;(context.cctorMethodAddress);
        }

        // This is a special compiler intrinsic that calls the method pointed to by pfn.
        // The compiler generates code for this and we can just mark it `extern`.
        // Once C# gets proper function pointer support (planned for C# 9), this won't be needed.
        [System.Runtime.CompilerServices.Intrinsic]
        private static extern T Call&lt;T&gt;(System.IntPtr pfn);
    }

    // This data structure is a contract with the compiler. It holds the address of a static
    // constructor and a flag that specifies whether the constructor already executed.
    [System.Runtime.InteropServices.StructLayout(System.Runtime.InteropServices.LayoutKind.Sequential)]
    public struct StaticClassConstructionContext
    {
        // Pointer to the code for the static class constructor method. This is initialized by the
        // binder/runtime.
        public IntPtr cctorMethodAddress;

        // Initialization state of the class. This is initialized to 0. Every time managed code checks the
        // cctor state the runtime will call the classlibrary's CheckStaticClassConstruction with this context
        // structure unless initialized == 1. This check is specific to allow the classlibrary to store more
        // than a binary state for each cctor if it so desires.
        public int initialized;
    }
}
Copy<br>Let’s rebuild the IL bytecode with this newly added code and re-rerun ILC.<br>$ csc.exe /debug /O /noconfig /nostdlib /runtimemetadataversion:v4.0.30319 MiniRuntime.cs MiniBCL.cs Game\FrameBuffer.cs Game\Random.cs Game\Game.cs Game\Snake.cs Pal\Thread.Windows.cs Pal\Environment.Windows.cs Pal\Console.Windows.cs /out:zerosnake.ilexe /langversion:latest /unsafe
$ ilc.exe zerosnake.ilexe -o zerosnake.obj --systemmodule zerosnake --Os -g
Copy<br>Now we have zerosnake.obj - a standard object file that is no different from object files produced by other native compilers such as C or C++. The last step is linking it. We’ll use the link.exe tool that should be on the PATH of our “x64 Native Tools Command Prompt” (you might need to install the C/C++ development tools in Visual Studio).<br>link.exe /debug:full /subsystem:console zerosnake.obj /entry:__managed__Main
Copy<br>The __managed__Main symbol name is a contract with the compiler - it’s the name of the managed entrypoint of the program that ILC created.<br>But it doesn’t work:<br>error LNK2001: unresolved external symbol RhpPInvoke
error LNK2001: unresolved external symbol SetConsoleTextAttribute
error LNK2001: unresolved external symbol WriteConsoleW
error LNK2001: unresolved external symbol GetStdHandle
...
fatal error LNK1120: 17 unresolved externals
Copy<br>Some of these symbols look familiar - the linker doesn’t know where to look for the Windows APIs we call. Let’s add the import libraries for those:<br>$ link.exe /debug:full /subsystem:console zerosnake.obj /entry:__managed__Main kernel32.lib ucrt.lib
Copy<br>This looks better - only 4 unresolved symbols:<br>error LNK2001: unresolved external symbol RhpPInvoke
error LNK2001: unresolved external symbol RhpPInvokeReturn
error LNK2001: unresolved external symbol RhpReversePInvoke2
error LNK2001: unresolved external symbol RhpReversePInvokeReturn2
fatal error LNK1120: 4 unresolved externals
Copy<br>The remaining missing symbols are helpers that the compiler expects to find in the runtime library. The fact they’re missing is only discovered at the time of linking because these helpers are typically implemented in assembly and the compiler only refers to them by their symbolic name (as opposed to other compiler-required types and methods we provided above).<br>The helpers set up and tear down the stack frames when native code calls into managed code, and managed code calls into native code. This is necessary for the GC to operate. Since we don’t have a GC, let’s stub them out with a piece of C# and another magical attribute that the compiler understands.<br>namespace System.Runtime
{
    // Custom attribute that the compiler understands that instructs it
    // to export the method under the given symbolic name.
    internal sealed class RuntimeExportAttribute : Attribute
    {
        public RuntimeExportAttribute(string entry) { }
    }
}

namespace Internal.Runtime.CompilerHelpers
{
    class StartupCodeHelpers
    {
        // The containing type for these methods doesn't matter.
        // Let's park them in StarupCodeHelpers.
        
        [System.Runtime.RuntimeExport("RhpReversePInvoke2")]
        static void RhpReversePInvoke2(System.IntPtr frame) { }
        [System.Runtime.RuntimeExport("RhpReversePInvokeReturn2")]
        static void RhpReversePInvokeReturn2(System.IntPtr frame) { }
        [System.Runtime.RuntimeExport("RhpPInvoke")]
        static void RhpPinvoke(System.IntPtr frame) { }
        [System.Runtime.RuntimeExport("RhpPInvokeReturn")]
        static void RhpPinvokeReturn(System.IntPtr frame) { }
    }
}
Copy<br>After rebuilding the C# source code with these modifications and re-running ILC, the linking will finally succeed.<br>We’re now at 27 kilobytes and the game still works!<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/10-graph8.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/10-graph8.png" target="_blank"></a><img alt="With runtime removed" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/10-graph8.png" referrerpolicy="no-referrer"><br>With runtime removed<br><br>The remaining kilobytes can be shaved off by using tricks native developers use to shrink their native apps.<br>We’re going to:<br>
<br>Disable incremental linking
<br>Strip relocation information
<br>Merge similar sections within the executable
<br>Set internal alignment within the executable to a small value
<br>$ link.exe /debug:full /subsystem:console zerosnake.obj /entry:__managed__Main kernel32.lib ucrt.lib /merge:.modules=.rdata /merge:.pdata=.rdata /incremental:no /DYNAMICBASE:NO /filealign:16 /align:16
Copy<br>Success! 8176 bytes!<br>The game still works, and interestingly, it’s still fully debuggable - feel free to open the EXE in Visual Studio (File -&gt; Open Solution), open one of the C# files that are part of the game, set a breakpoint in it, hit F5 to launch the EXE, and see the breakpoint getting hit. You can disable optimizations in ILC to make the executable even more debuggable - just drop the –Os argument.<br><a data-tooltip-position="top" aria-label="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/11-graph9.png" rel="noopener" class="external-link" href="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/11-graph9.png" target="_blank"></a><img alt="After messing with linker" src="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/11-graph9.png" referrerpolicy="no-referrer"><br>After messing with linker<br><br>The executable still carries some data that is not essential - the ILC compiler just doesn’t expose command line options to disable their generation.<br>One of those data structures that gets generated but we don’t need is GC information for the individual methods. CoreRT has a precise garbage collector that requires each method to describe where references to GC heap are at each instruction of the method body. Since we don’t have a garbage collector in the Snake game, this data is unnecessary. Other runtimes (e.g. Mono) use a conservative garbage collector that doesn’t require this data (it simply assumes any piece of the stack and CPU registers could be a GC reference) - a conservative garbage collector trades GC performance for extra size savings. The precise garbage collector used in CoreRT can operate in conservative mode too, but it hasn’t been hooked up yet. It’s a potential future addition that we could then leverage to make things even smaller.<br>Maybe one day we can make a simplified version of our game fit into a 512 byte boot sector. Until then, happy hacking!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/csharp/building-a-self-contained-game-in-csharp-under-8-kilobytes.html</link><guid isPermaLink="false">Computer Science/Programming Language/CSharp/Building a self-contained game in CSharp under 8 kilobytes.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:28 GMT</pubDate><enclosure url="https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://migeel.sk/blog/2020/01/03/building-a-self-contained-game-in-csharp-under-8-kilobytes/01-floppies.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Isomorphic .NET Support in Extism]]></title><description><![CDATA[ 
 <br>In the early days of computing, software was pretty much locked into specific hardware setups. Then came operating systems, providing a layer of abstraction over the hardware. However, that still didn’t quite cut it. Businesses wanted to write their apps once and have them work on all sorts of platforms. This demand gave birth to higher-level languages that brought along their own runtimes, making it possible for software to run seamlessly on different operating systems.<br>Today, many organizations manage a multitude of programming languages for distinct tasks. While several runtimes, such as .NET’s Common Language Runtime (“CLR”) and Oracle’s Java Virtual Machine (“JVM”), support multiple languages, they still operate within isolated ecosystems. Each ecosystem has its package managers and libraries, often leading to unnecessary code duplication and duplicated efforts.<br>WebAssembly (Wasm) to the rescue! Wasm allows developers to create applications and libraries in their language of choice, meaning you can build a library in one language and smoothly integrate it with others.<br>Microsoft has long recognized Wasm’s potential and has heavily invested in <a data-tooltip-position="top" aria-label="https://dotnet.microsoft.com/en-us/apps/aspnet/web-apps/blazor" rel="noopener" class="external-link" href="https://dotnet.microsoft.com/en-us/apps/aspnet/web-apps/blazor" target="_blank">Blazor</a>. Blazor runs .NET code in the browser. This way, C# and F# programmers can reuse their existing skills to write interactive apps for the web. In .NET 8, Microsoft is adding experimental support for WASI too. Which means .NET apps compiled to Wasm can now run on the server, computers, and mobile phones!<br>That’s where Extism steps in. Languages have varying levels of support for running and compiling to Wasm, and capabilities may change depending on what runtime is being used. Extism ensures that your plugins will run consistently across languages and platforms. We’ve <a data-tooltip-position="top" aria-label="https://dylibso.com/blog/why-extism/" rel="noopener" class="external-link" href="https://dylibso.com/blog/why-extism/" target="_blank">previously talked about the added value of Extism</a>, but in a nutshell, Extism offers a set of unified and user-friendly Plugin Development Kits (PDKs) and Software Development Kits (SDKs) that make it easy for you to develop and run plugins in your preferred programming language.<br><br>Extism makes it all the more convenient to build Wasm plugins. For example, consider this scenario: you have a messaging bot platform, and you want to empower your users to create their own bots. Typically, messaging platforms provide webhooks that users can use to write bots. But we want to invert the situation: we run the users code on our own infrastructure! Here’s a glimpse of how it could look when you use the <a data-tooltip-position="top" aria-label="https://github.com/extism/dotnet-pdk" rel="noopener" class="external-link" href="https://github.com/extism/dotnet-pdk" target="_blank">.NET Extism PDK</a>:<br>  static class Plugin
{
    [UnmanagedCallersOnly(EntryPoint = "bot_name")]
    public static void BotName()
    {
        Pdk.SetOutput("weather bot");
    }

    [UnmanagedCallersOnly(EntryPoint = "respond")]
    public static void Respond()
    {
        var message = Pdk.GetInputString();

        if (message.Contains("hi", StringComparison.OrdinalIgnoreCase))
        {
            Reply("Hello :-)");
        }
        else if (message.Contains("weather", StringComparison.OrdinalIgnoreCase))
        {
            // Get secrets and configuration from the Host
            if (!Pdk.TryGetConfig("weather-api-key", out var apiKey))
            {
                throw new Exception("Beep boop malfunction detected: API Key is not configured!");
            }

            var block = MemoryBlock.Find(Env.GetUserInfo());
            var json = block.ReadString();
            var userInfo = JsonSerializer.Deserialize&lt;UserInfo&gt;(json);

            // Call HTTP APIs
            var query = $"https://api.weatherapi.com/v1/current.json?key={apiKey}&amp;q={userInfo.City}&amp;aqi=no";
            var response = Pdk.SendRequest(new HttpRequest(query));
            var responseJson = response.Body.ReadString();
            var apiResponse = JsonSerializer.Deserialize&lt;ApiResponse&gt;(responseJson);

            Reply($"The current temparature in {userInfo.City} is {apiResponse.current.temp_c}°C");
        }
        else
        {
            Reply("""
                Hi, I am the weather bot. Commands:
                1. Hi
                2. How's the weather?
                """);
        }
    }

    static void Reply(string message)
    {
        var block = Pdk.Allocate(message);
        Env.SendMessage(block.Offset);
    }
}

// Import functions from the host
static class Env
{
    [DllImport("extism", EntryPoint = "send_message")]
    public static extern void SendMessage(ulong offset);

    [DllImport("extism", EntryPoint = "user_info")]
    public static extern ulong GetUserInfo();
}
Copy<br>This small example demonstrates how we can easily import functions from the host, share data between the host and the plugin, and make HTTP requests. The host has full control over which HTTP hosts the plugin can make requests to and which files the plugin has access to.<br>We have also exported two functions for the host: bot_name provides the name of the bot and respond can process a message sent by the user. The .NET Extism PDK add the following capabilities on top of the .NET WASI SDK:<br>
<br>Support for DllImport for importing functions from the host.
<br>Support for UnmanagedCallersOnly for exporting C# and F# functions.
<br>A global exception handler that propagates the exceptions back to the host.
<br>Easily share data between the plugin and the host.<br>
Extism has PDKs for <a data-tooltip-position="top" aria-label="https://extism.org/docs/category/write-a-plug-in" rel="noopener" class="external-link" href="https://extism.org/docs/category/write-a-plug-in" target="_blank">many popular programming languages</a>, so your users can write plugins in their favorite programming language and it would still work the same way!
<br><br>Running plugins is equally easy thanks to our <a data-tooltip-position="top" aria-label="https://extism.org/docs/category/integrate-into-your-codebase" rel="noopener" class="external-link" href="https://extism.org/docs/category/integrate-into-your-codebase" target="_blank">Host SDKs</a>, here is how you’d call the plugin above using our <a data-tooltip-position="top" aria-label="https://github.com/extism/dotnet-sdk" rel="noopener" class="external-link" href="https://github.com/extism/dotnet-sdk" target="_blank">.NET SDK</a>:<br>var manifest = new Manifest(new PathWasmSource("Plugin.wasm"))
{
	// Provide configurations and secrets for the plugins
	Config = new Dictionary&lt;string, string&gt;
	{
		{ "weather-api-key", Environment.GetEnvironmentVariable("weather-api-key") }
	},

 	// Control which HTTP hosts the plugins can call
 	AllowedHosts = ["api.weatherapi.com"]
};

// Provide custom host functions
var functions = new[]
{
    HostFunction.FromMethod("send_message", IntPtr.Zero, (CurrentPlugin plugin, long offset) =&gt;
    {
        var message = plugin.ReadString(offset);
        Console.WriteLine($"bot says: {message}");
    }),

    HostFunction.FromMethod("user_info", IntPtr.Zero, (CurrentPlugin plugin) =&gt;
    {
        var json = JsonSerializer.Serialize(new UserInfo
        {
            FullName = "John Smith",
            City = "New York"
        });

        return plugin.WriteString(json);
    })
};

var plugin = new Plugin(manifest, functions, withWasi: true);

// Call functions exported by the plugin
var botName = plugin.Call("bot_name", "");
Console.WriteLine($"Bot Name: {botName}");

// Call respond with an empty input
plugin.Call("respond", "");

while (true)
{
    Console.Write("&gt; ");
    var message = Console.ReadLine();

    // Easily cancel plugin calls
    var cts = new CancellationTokenSource();
    cts.CancelAfter(TimeSpan.FromSeconds(1));

    plugin.Call("respond", message, cts.Token);
}
Copy<br>And here is the result of running the host:<br>PS D:\x\dotnet\isomophic-dotnet-demo\Host&gt; dotnet run
Bot Name: weather bot
bot says: Hi, I am the weather bot. Commands:
1. Hi
2. How's the weather?
&gt; hi
bot says: Hello :-)
&gt; weather?
bot says: The current temparature in New York is 10.6°C
&gt;
Copy<br>While this example is very simple, it serves as the foundation for a robust and highly extensible bot framework. Utilizing Wasm for this scenario offers a host of advantages:<br>
<br>Sandboxed Plugins: Wasm ensures that plugins operate within a secure sandbox. They can only access resources such as memory, files, and sockets if explicitly allowed by the Host.
<br>Language Flexibility: Your users can develop plugins in their preferred programming language.
<br>Resource Management: You can set precise limits on memory usage and execution timeouts, enhancing control over your bot framework.
<br>Less network traffic: Since you’re running the plugins on your own server, there is no need for webhooks which can improve the responsiveness of the bots.<br>
The complete code for this sample is <a data-tooltip-position="top" aria-label="https://github.com/dylibso/isomorphic-dotnet-demo" rel="noopener" class="external-link" href="https://github.com/dylibso/isomorphic-dotnet-demo" target="_blank">available on GitHub</a>.
<br>Finally, we want to thank the incredible .NET team for their invaluable support and guidance through the WASI SDK challenges. We’re excited about the future of .NET in Wasm.<br><a data-tooltip-position="top" aria-label="https://twitter.com/mhmd_azeez" rel="noopener" class="external-link" href="https://twitter.com/mhmd_azeez" target="_blank">Muhammad Azeez</a>, Senior Software Engineer]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/csharp/isomorphic-.net-support-in-extism.html</link><guid isPermaLink="false">Computer Science/Programming Language/CSharp/Isomorphic .NET Support in Extism.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:32 GMT</pubDate></item><item><title><![CDATA[10 Tips for Productive FSharp Scripting]]></title><description><![CDATA[<a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:I" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#I</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:time" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#time</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:fsharp" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#fsharp</a> <a class="tag" href="https://muqiuhan.github.io/wiki/?query=tag:ndclondon" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ndclondon</a> 
 <br>Scott Hanselman recently had a <a data-tooltip-position="top" aria-label="http://www.hanselman.com/blog/InteractiveCodingWithCAndFREPLsScriptCSOrTheVisualStudioInteractiveWindow.aspx" rel="noopener" class="external-link" href="http://www.hanselman.com/blog/InteractiveCodingWithCAndFREPLsScriptCSOrTheVisualStudioInteractiveWindow.aspx" target="_blank">nice post on C# and F# REPLs</a>, which reminded me of the time I started using F# scripts. Over time, I found out a couple of small tricks, which helped make the experience productive. I found about them mainly by accident, so I figured, let’s see if I can list them in one place! Some of these are super simple, some probably a bit obscure, but hopefully, one of them at least will make your path towards scripting nirvana an easier one…<br>
Note: these tips are not necessarily ordered by usefulness. For that matter, there might or might not be exactly 10 of them :)
<br><br>You can use the F# Interactive 2 ways: you can directly type code into FSI, the F# Interactive window, or you can write code in an .fsx file, and select pieces of the code you want to execute. I recommend the second approach, for at least two reasons. First, FSI is a very primitive environment, .fsx files provide a much richer experience (IntelliSense). Then this encourages writing clean scripts you can reuse later.<br>
This is not specific to scripts, but… if you are on Visual Studio, do yourself a service and install the <a data-tooltip-position="top" aria-label="http://fsprojects.github.io/VisualFSharpPowerTools/" rel="noopener" class="external-link" href="http://fsprojects.github.io/VisualFSharpPowerTools/" target="_blank">Visual F# Power Tools</a> - you’ll get nice things such as better code highlighting, refactoring, and more.
<br>To execute code interactively, simply type code in an .fsx file, select a block of code, and hit Alt + Enter. The selected code will be evaluated, and the result will show up in the FSI window. In Visual Studio, you can also select code and right-click “Execute in Interactive”, but shortcuts are way faster.<br>
You can also execute a single-line with Alt + '. I rarely use this option, but this can save you time because you don’t need to select the entire line of code.
<br>
In case the keyboard shortcuts to send code to FSI do not work anymore (ReSharper used to over-write them in the past), you can reset them in Visual Studio, by going to Tools / Options / Environment / Keyboard. The 2 commands you need to map are EditorContextMenus.CodeWindow.ExecuteInInteractive and EditorContextMenus.CodeWindow.ExecuteLineInInteractive.
<br>You can also use these shortcuts from a regular .fs file, which can be handy if you want to validate that a piece of code is behaving the way you want.<br>
Interactive coding is by far my main usage for scripts - I use it extensively to prototype designs, run dumb tasks, or explore data or libraries. I realized recently that a few of my C# friends use LinqPad for the same purpose.
<br><br>While I encourage working primarily from .fsx files, the FSI window is also very helpful. I use it primarily for small verifications. For instance, I might have in my script file code like this:<br>let add x y =
  x + y
Copy<br>Once I send it for evaluation into FSI, I will see the following show up in FSI:<br>val add : x:int -&gt; y:int -&gt; int
&gt;
Copy<br>My function add is now in memory, in my FSI session; I can start typing in the FSI window and use it:<br>&gt; add 1 2;;
val it : int = 3
&gt;
Copy<br>Enter does not trigger execution in FSI. The ;; indicates to FSI “Please execute everything I just typed, up to that point”. This is useful if you want to type multiple lines of code in FSI, and execute them as a block.<br>
it: in our add 1 2 example, the result showed up as it. We simply ran add, but didn’t assign the result to anything. it now contains the result, until we run another expression. If you want to re-use that value, you can assign it in FSI, by doing for instance let x = it;;.′
<br>
Once a value is loaded in your FSI session, it will remain there, available to you until you shadow it (in the example above, x will remain available, until I run for instance let x = 42;;). This is extremely convenient: for instance, you can load a data file once let data = File.ReadAllLines path, and keep using data for as long as you want, without having to reload it between code changes.
<br>
FSI often shows an abbreviated version of values for large items. For instance, [1..999] will show up as val it : int list = [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78; 79; 80; 81; 82; 83; 84; 85; 86; 87; 88; 89; 90; 91; 92; 93; 94; 95; 96; 97; 98; 99; 100; ...] - note the … at the end, which indicate that there is more.
<br>What if you inadvertently started a very long computation, or an infinite loop? In Visual Studio, you can either kill the session entirely, by right-clicking over the FSI window and selecting “Reset Interactive Session” or Ctrl + Alt + R, or cancel the latest evaluation you requested (“Cancel Interactive Evaluation”, or Ctrl + Break.).<br><br>Besides interactive scripting, you can also run a script from the command line, by using FSI.exe:<br>&gt;fsi.exe "C:\myscript.fsx"<br>
FSI.exe is typically located at C:\Program Files (x86)\Microsoft SDKs\F#\4.0\Framework\v4.0. You can also install it separately, see <a data-tooltip-position="top" aria-label="http://fsharp.org/" rel="noopener" class="external-link" href="http://fsharp.org/" target="_blank">fsharp.org/use</a> section for instructions for various platforms.
<br>You can define different behaviors in your script, depending on whether it is run interactively or from the command line, like this:<br>#if INTERACTIVE
let msg = "Interactive"
#else
let msg = "Not Interactive"
#endif

printfn "%s" msg
Copy<br>Updated, Sep 19: thanks Matt Klein for <a data-tooltip-position="top" aria-label="http://stackoverflow.com/q/39581342/114519" rel="noopener" class="external-link" href="http://stackoverflow.com/q/39581342/114519" target="_blank">pointing the issue</a>.<br>For more information on FSI from the command line, <a data-tooltip-position="top" aria-label="https://msdn.microsoft.com/en-us/library/dd233175.aspx" rel="noopener" class="external-link" href="https://msdn.microsoft.com/en-us/library/dd233175.aspx" target="_blank">check the reference page here</a>.<br>Updated, Feb 20: <a data-tooltip-position="top" aria-label="https://twitter.com/genTauro42" rel="noopener" class="external-link" href="https://twitter.com/genTauro42" target="_blank">Ramon Soto Mathiesen</a> points out that <a data-tooltip-position="top" aria-label="https://twitter.com/genTauro42/status/696407757835132928" rel="noopener" class="external-link" href="https://twitter.com/genTauro42/status/696407757835132928" target="_blank">Tip 9 also applies to the command line</a>.<br><br>Sometimes, your script will reference another resource; for instance, you need to read the contents of a .txt file somewhere. You can use absolute path, as in:<br>File.ReadAllLines @"C:/data/myfile.txt"
Copy<br>
Pre-pending a string with @ makes it a verbatim string, and ignore escape sequences, such as \.
<br>
Use / rather than \, so that path work both on Windows and Mono.
<br>However, if that resource lives in a location relative to your script, consider using relative path, so that you can move your script folder around without breaking it.<br>Relative paths can be a bit tricky; for instance, running the following code interactively…<br>System.Environment.CurrentDirectory
Copy<br>… produces a potentially unexpected result in FSI:<br>val it : string = "C:\Users\Mathias Brandewinder\AppData\Local\Temp"
&gt;
Copy<br>You can avoid these issues by using built-in constants, which refer respectively to the directory where the script lives, the script file name, and the current line of the script:<br>__SOURCE_DIRECTORY__
__SOURCE_FILE__
__LINE__
Copy<br>So if your folder structure was along these lines…<br>root
  /src/script.fsx
  /data/data.txt
Copy<br>… you could refer to the data file data.txt from your script like this:<br>let path = System.IO.Path.Combine(__SOURCE_DIRECTORY__,"..","data/data.txt")
System.IO.File.ReadAllText path
Copy<br><br>By default, FSI loads FSharp.Core and nothing else. If you want to use System.DateTime, you will need to first open System in your script. If you want to use an assembly that is not part of the standard .NET distribution, you will need to reference it first using #r. Imagine for instance that you installed the Nuget package fsharp.data; to use it in your script, you would do something like:<br>#r @"../packages/FSharp.Data.2.2.5/lib/net40/FSharp.Data.dll"
open FSharp.Data
Copy<br>
When you execute open System in interactive, don’t worry if nothing seems to happen: the only result is a new &gt; showing up in FSI.
<br>For assemblies that are part of .NET but not referenced by default, you can use a shorter version:<br>#r @"System.Xaml"
open System.Xaml
Copy<br>
In Visual Studio, you can right-click a reference from Solution Explorer, and send to F# interactive. You can then directly open it, and start using it in FSI.
<br>Updated, Feb 20: <a data-tooltip-position="top" aria-label="https://twitter.com/sergey_tihon" rel="noopener" class="external-link" href="https://twitter.com/sergey_tihon" target="_blank">Sergey Tihon</a> shared an interesting comment, explaining where Tip 5 can sometimes go wrong. I’d say, try Tip 5 first, but be aware that this might at times not quite work:<br>
<a data-tooltip-position="top" aria-label="https://twitter.com/brandewinder" rel="noopener" class="external-link" href="https://twitter.com/brandewinder" target="_blank">@brandewinder</a> don't load assemblies like in Tip 5 ) <a rel="noopener" class="external-link" href="https://t.co/Owft1NmPoo" target="_blank">https://t.co/Owft1NmPoo</a>
— Sergey Tihon (@sergey_tihon) <a data-tooltip-position="top" aria-label="https://twitter.com/sergey_tihon/status/696395229285523456" rel="noopener" class="external-link" href="https://twitter.com/sergey_tihon/status/696395229285523456" target="_blank">February 7, 2016</a>
<br>Updated, Feb 20: <a data-tooltip-position="top" aria-label="https://twitter.com/dsyme" rel="noopener" class="external-link" href="https://twitter.com/dsyme" target="_blank">F# open source contributor Don Syme</a> share a related nice trick:<br>
<a data-tooltip-position="top" aria-label="https://twitter.com/jeroldhaas" rel="noopener" class="external-link" href="https://twitter.com/jeroldhaas" target="_blank">@jeroldhaas</a> <a data-tooltip-position="top" aria-label="https://twitter.com/sergey_tihon" rel="noopener" class="external-link" href="https://twitter.com/sergey_tihon" target="_blank">@sergey_tihon</a> <a data-tooltip-position="top" aria-label="https://twitter.com/brandewinder" rel="noopener" class="external-link" href="https://twitter.com/brandewinder" target="_blank">@brandewinder</a> Use <a data-tooltip-position="top" aria-label="https://twitter.com/hashtag/I?src=hash" rel="noopener" class="external-link" href="https://twitter.com/hashtag/I?src=hash" target="_blank"></a><a href="https://muqiuhan.github.io/wiki?query=tag:I" class="tag" target="_blank" rel="noopener">#I</a> SOURCE_DIRECTORY, it is wondrous, very satisfying. All relative paths then work
— Don Syme (@dsyme) <a data-tooltip-position="top" aria-label="https://twitter.com/dsyme/status/696429115184955393" rel="noopener" class="external-link" href="https://twitter.com/dsyme/status/696429115184955393" target="_blank">February 7, 2016</a>
<br><br>The Nuget package manager is useful to consume existing packages. However, by default, Nuget stores assemblies in a folder that includes the package version number. This is very impractical for a script. In our example above, if fsharp.data gets an update, our script reference will be broken once we update the Nuget package:<br>#r @"../packages/FSharp.Data.2.2.5/lib/net40/FSharp.Data.dll"<br>Fixing the script requires manually editing the version number in the path, which quickly becomes a pain. <a data-tooltip-position="top" aria-label="https://fsprojects.github.io/Paket/" rel="noopener" class="external-link" href="https://fsprojects.github.io/Paket/" target="_blank"><strong></strong></a>Paket provides a better experience, because it stores packages without the version number, in this case, under:<br>#r @"../packages/FSharp.Data/lib/net40/FSharp.Data.dll"<br>Your scripts will now gracefully handle version number changes.<br>If you end up consuming numerous packages, you can make your life even easier, by referencing paths where assemblies might be searched for, using #I:<br>#I @"../packages/
#r @"FSharp.Data/lib/net40/FSharp.Data.dll"
Copy<br>
If your primary goal is to “just script”, consider using <a data-tooltip-position="top" aria-label="https://atom.io/" rel="noopener" class="external-link" href="https://atom.io/" target="_blank">Atom</a> or <a data-tooltip-position="top" aria-label="https://code.visualstudio.com/" rel="noopener" class="external-link" href="https://code.visualstudio.com/" target="_blank">VSCode</a>, with the <a data-tooltip-position="top" aria-label="http://ionide.io/" rel="noopener" class="external-link" href="http://ionide.io/" target="_blank">Ionide plugin</a>. You can create and run free-standing F# scripts, with beautiful <a data-tooltip-position="top" aria-label="http://ionide.io/#paket-integration" rel="noopener" class="external-link" href="http://ionide.io/#paket-integration" target="_blank">Paket integration</a>.
<br><br>You might want to use the code from an existing file in your script. Suppose that we have a code file Code.fs somewhere, looking like this:<br>namespace Mathias

module Common =
  let hello name = sprintf "Hello, %s" name
Copy<br>You can use that code from your script, by using the #load directive:<br>#load "Code.fs"
open Mathias.Common
hello "World"
Copy<br>
You might have to close and re-open the script file if you end up changing the contents of the file.
<br>
If the file you are attempting to load contains references to other assemblies or files, you might get an error on the #load statement: “One or more errors in loaded files. The namespace or module … is not defined”. Simply reference the missing assemblies above the #load statement, so that your script uses the same dependencies as the file it refers to.
<br><br>Another handy directive, #time, turns on basic profiling. Once it is executed, for every block of code you send for execution you will see timing and garbage collection information. For instance, running this code…<br>#time
[| 1 .. 10000000 |] |&gt; Array.map (fun x -&gt; x * x)
Copy<br>… will produce the following in FSI:<br>--&gt; Timing now on

Real: 00:00:00.887, CPU: 00:00:00.828, GC gen0: 2, gen1: 2, gen2: 2
val it : int [] =
  [|1; 4; 9; 16; 25; 36; 49; // snipped for brevity
Copy<br>We get the wall time and CPU time it took, as well as some information about garbage collection in generations 0, 1 and 2. This would not replace a full-blown profiler, but this is an awfully convenient tool to figure out quickly if there are obvious ways to improve a piece of code.<br>Note that every time you execute #time, the timer will be switched from on to off, or vice-versa. This is not always convenient; you can also explicitly set it to the desired state, like this:<br>#time "on"
// everything now is timed
#time "off"
Copy<br>
If you are interested in profiling, you should take a look at <a data-tooltip-position="top" aria-label="http://www.privateeye.io/" rel="noopener" class="external-link" href="http://www.privateeye.io/" target="_blank">PrivateEye</a>; check out <a data-tooltip-position="top" aria-label="https://twitter.com/gregyoung" rel="noopener" class="external-link" href="https://twitter.com/gregyoung" target="_blank">Greg Young</a>’s <a data-tooltip-position="top" aria-label="https://vimeo.com/131637366" rel="noopener" class="external-link" href="https://vimeo.com/131637366" target="_blank">talk at NDC Oslo 2015</a> to get a feel for what it does.
<br><br>Hat tip to <a data-tooltip-position="top" aria-label="https://twitter.com/rickasaurus" rel="noopener" class="external-link" href="https://twitter.com/rickasaurus" target="_blank">Rick Minerich</a> for that one. I’ll refer you to his blog post to see how to <a data-tooltip-position="top" aria-label="http://richardminerich.com/2013/03/setting-up-fsharp-interactive-for-machine-learning-with-large-datasets/" rel="noopener" class="external-link" href="http://richardminerich.com/2013/03/setting-up-fsharp-interactive-for-machine-learning-with-large-datasets/" target="_blank">set FSI to 64 bits to handle large datasets</a>.<br><br>Did you know that you could…<br>
<br><a data-tooltip-position="top" aria-label="https://channel9.msdn.com/Events/Visual-Studio/Visual-Studio-2015-Final-Release-Event/Six-Quick-Picks-from-Visual-F-40" rel="noopener" class="external-link" href="https://channel9.msdn.com/Events/Visual-Studio/Visual-Studio-2015-Final-Release-Event/Six-Quick-Picks-from-Visual-F-40" target="_blank">debug an F# script? (around 0:12:35 in)</a>
<br><a data-tooltip-position="top" aria-label="http://www.swensensoftware.com/fseye" rel="noopener" class="external-link" href="http://www.swensensoftware.com/fseye" target="_blank">inspect the objects in your FSI session with <strong></strong>?</a>FsEye
<br>change the FSI font size in Tools/Options/Environment/Fonts and Colors/Show Settings for/F# Interactive?
<br>add your own pretty-printer to FSI, <a data-tooltip-position="top" aria-label="https://github.com/mathnet/mathnet-numerics/blob/master/src/FSharp/MathNet.Numerics.fsx" rel="noopener" class="external-link" href="https://github.com/mathnet/mathnet-numerics/blob/master/src/FSharp/MathNet.Numerics.fsx" target="_blank">like this</a>?
<br>mess with your coworkers’ mental sanity, by executing (* (opening a multiline comment) in FSI? (credit: <a data-tooltip-position="top" aria-label="https://twitter.com/tomaspetricek" rel="noopener" class="external-link" href="https://twitter.com/tomaspetricek" target="_blank">Tomas</a>)
<br>simplify loading references with Visual Studio and Power Tools? (credit: <a data-tooltip-position="top" aria-label="https://twitter.com/kitlovesfsharp" rel="noopener" class="external-link" href="https://twitter.com/kitlovesfsharp" target="_blank">Kit Eason</a>, see details in comments section).
<br>And again… if you are not using the <a data-tooltip-position="top" aria-label="http://fsprojects.github.io/VisualFSharpPowerTools/" rel="noopener" class="external-link" href="http://fsprojects.github.io/VisualFSharpPowerTools/" target="_blank">Visual F# Power Tools</a>, you are missing out:<br>
"Don't let your friends try <a data-tooltip-position="top" aria-label="https://twitter.com/hashtag/fsharp?src=hash" rel="noopener" class="external-link" href="https://twitter.com/hashtag/fsharp?src=hash" target="_blank"></a><a href="https://muqiuhan.github.io/wiki?query=tag:fsharp" class="tag" target="_blank" rel="noopener">#fsharp</a> without installing <a data-tooltip-position="top" aria-label="https://twitter.com/FSPowerTools" rel="noopener" class="external-link" href="https://twitter.com/FSPowerTools" target="_blank">@FSPowerTools</a>." <a data-tooltip-position="top" aria-label="https://twitter.com/dsyme" rel="noopener" class="external-link" href="https://twitter.com/dsyme" target="_blank">@dsyme</a> at <a data-tooltip-position="top" aria-label="https://twitter.com/hashtag/ndclondon?src=hash" rel="noopener" class="external-link" href="https://twitter.com/hashtag/ndclondon?src=hash" target="_blank"></a><a href="https://muqiuhan.github.io/wiki?query=tag:ndclondon" class="tag" target="_blank" rel="noopener">#ndclondon</a>
— Tomas Petricek (@tomaspetricek) <a data-tooltip-position="top" aria-label="https://twitter.com/tomaspetricek/status/687934127627186176" rel="noopener" class="external-link" href="https://twitter.com/tomaspetricek/status/687934127627186176" target="_blank">January 15, 2016</a>
<br>That’s what I got! I am sure I forgot some - do you have a useful or favorite trick to share?]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/10-tips-for-productive-fsharp-scripting.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/10 Tips for Productive FSharp Scripting.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 May 2024 13:23:17 GMT</pubDate></item><item><title><![CDATA[Amoeba optimization method using FSharp]]></title><description><![CDATA[ 
 <br>My favorite column in MSDN Magazine is Test Run; it was originally focused on testing, but the author, James McCaffrey, has been focusing lately on topics revolving around numeric optimization and machine learning, presenting a variety of methods and approaches. I quite enjoy his work, with one minor gripe –his examples are all coded in C#, which in my opinion is really too bad, because the algorithms would gain much clarity if written in F# instead.<br>Back in June 2013, he published a piece on <a data-tooltip-position="top" aria-label="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" rel="noopener" class="external-link" href="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" target="_blank">Amoeba Method Optimization using C#</a>. I hadn’t seen that approach before, and found it intriguing. I also found the C# code a bit too hairy for my feeble brain to follow, so I decided to rewrite it in F#.<br>In a nutshell, the Amoeba approach is a heuristic to find the minimum of a function. Its proper respectable name is the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method" target="_blank">Nelder-Nead method</a>. The reason it is also called the Amoeba method is because of the way the algorithm works: in its simple form, it starts from a triangle, the “Amoeba”; at each step, the Amoeba “probes” the value of 3 points in its neighborhood, and moves based on how much better the new points are. As a result, the triangle is iteratively updated, and behaves a bit like an Amoeba moving on a surface.<br>Before going into the actual details of the algorithm, here is how my final result looks like. You can find the entire code <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Amoeba" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/Amoeba" target="_blank">here on GitHub</a>, with some usage examples in the Sample.fsx script file. Let’s demo the code in action: in a script file, we load the Amoeba code, and use the same function the article does, the <a data-tooltip-position="top" aria-label="http://mathworld.wolfram.com/RosenbrockFunction.html" rel="noopener" class="external-link" href="http://mathworld.wolfram.com/RosenbrockFunction.html" target="_blank">Rosenbrock function</a>. We transform the function a bit, so that it takes a Point (an alias for an Array of floats, essentially a vector) as an input, and pass it to the solve function, with the domain where we want to search, in that case, [ –10.0; 10.0 ] for both x and y:<br>#load "Amoeba.fs"
 
open Amoeba
open Amoeba.Solver
 
let g (x:float) y =
100. * pown (y - x * x) 2 + pown (1. - x) 2
 
let testFunction (x:Point) =
g x.[0] x.[1]
 
solve Default [| (-10.,10.); (-10.,10.) |] testFunction 1000
Copy<br>Running this in the F# interactive window should produce the following:<br>val it : Solution = (0.0, [|1.0; 1.0|]) 
&gt;
Copy<br>The algorithm properly identified that the minimum is 0, for a value of x = 1.0 and y = 1.0. Note that results may vary: this is a heuristic, which starts with a random initial amoeba, so each run could produce slightly different results, and might at times epically fail.<br>So how does the algorithm work?<br>I won’t go into full detail on the implementation, but here are some points of interest. At each iteration, the Amoeba has a collection of candidate solutions, Points that could be a Solution, with their value (the value of the function to be minimized at that point). These points can be ordered by value, and as such, always have a best and worst point. The following picture, which I lifted from the article, shows what points the Amoeba is probing:<br><img alt="Amoeba" src="https://mathias-brandewinder.github.io//assets/amoeba.png" referrerpolicy="no-referrer"><br>Source: <a data-tooltip-position="top" aria-label="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" rel="noopener" class="external-link" href="http://msdn.microsoft.com/en-us/magazine/dn201752.aspx" target="_blank">“Amoeba Optimization Method in C#”</a><br>The algorithm constructs a Centroid, the average of all current solutions except the worst one, and attempts to replace the Worst with 3 candidates: a Contracted, Reflected and Expanded solution. If none of these is satisfactory (the rules are pretty straightforward in the code), the Amoeba shrinks towards the Best solution. In other words, first the Amoeba searches for new directions to explore by trying to replace its current Worst solution, and if no good change is found, it shrinks on itself, narrowing down around its current search zone towards its current Best candidate.<br>If you consider the diagram, clearly all transformations are a variation on the same theme: take the Worst solution and the Centroid, and compute a new point by stretching it by different values: –50% for contraction, +100% for reflection, and +200% for expansion. For that matter, the shrinkage can also be represented as a stretch of –50% towards the Best point.<br>This is what I ended up with:<br>type Point = float []
type Settings = { Alpha:float; Sigma:float; Gamma:float; Rho:float; Size:int }
 
let stretch ((X,Y):Point*Point) (s:float) =
Array.map2 (fun x y -&gt; x + s * (x - y)) X Y
 
let reflected V s = stretch V s.Alpha
let expanded V s = stretch V s.Gamma
let contracted V s = stretch V s.Rho
Copy<br>I defined Point as an alias for an array of floats, and a Record type Settings to hold the parameters that describe the transformation. The function stretch takes a pair of points and a float (by how much to stretch), and computes the resulting Point by taking every coordinate, and going by a ratio s from x towards y. From then on, defining the 3 transforms is trivial; they just use different values from the settings.<br>Now that we have the Points represented, the other part of the algorithm requires evaluating a function at each of these points. That part was done with a couple types:<br>type Solution = float * Point
type Objective = Point -&gt; float
 
type Amoeba =
{ Dim:int; Solutions:Solution [] } // assumed to be sorted by fst value
member this.Size = this.Solutions.Length
member this.Best = this.Solutions.[0]
member this.Worst = this.Solutions.[this.Size - 1]
 
let evaluate (f:Objective) (x:Point) = f x, x
let valueOf (s:Solution) = fst s
Copy<br>A Solution is a tuple, a pair associating a Point and the value of the function at that point. The function we are trying to minimize, the Objective, takes in a point, and returns a float. We can then define an Amoeba as an array of Solutions, which is assumed to be sorted. Nothing guarantees that the Solutions are ordered, which bugged me for a while; I was tempted to make that type private or internal, but this would have caused some extra hassle for testing, so I decided not to bother with it. I added a few convenience methods on the Amoeba, to directly extract the Best and Worst solutions, and two utility functions, evaluate, which associates a Point with its value, and its counter-part, valueOf, which extracts the value part of a Solution.<br>The rest of the code is really mechanics; I followed the algorithm notation from the Wikipedia page, rather than the MSDN article, because it was actually a bit easier to transcribe, built the search as a recursion (of course), which iteratively transforms an Amoeba for a given number of iterations. For good measure, I introduced another type, Domain, describing where the Amoeba should begin searching, and voila! We are done. In 91 lines of F#, we got a full implementation.<br><br>What I find nice about the algorithm is its relative simplicity. One nice benefit is that it doesn’t require a derivative. Quite often, search algorithms use a gradient to evaluate the slope and decide what direction to explore. The drawback is that first, computing gradients is not always fun, and second, there might not even be a properly defined gradient in the first place. By contrast, the Amoeba doesn’t require anything – just give it a function, and let it probe. In some respects, the algorithm looks to me like a very simple genetic algorithm, maintaining a population of solutions, breeding new ones and letting a form of natural selection operate.<br>Of course, the price to pay for this simplicity is that it is a heuristic, that is, there is no guarantee that the algorithm will find a good solution. From my limited experimentations with it, even in simple cases, failures were not that unusual. If I get time for this, I think it would be fun to try launching multiple searches, and stopping when, say, the algorithm has found the same Best solution a given number of times.<br>Also, note that in this implementation, 2 cases are not covered: the case where the function is not defined everywhere (some Points might throw an exception), and the case where the function doesn’t have a minimum. I will let the enterprising reader think about how that could be handled!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/amoeba-optimization-method-using-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Amoeba optimization method using FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:31 GMT</pubDate><enclosure url="https://mathias-brandewinder.github.io//assets/amoeba.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://mathias-brandewinder.github.io//assets/amoeba.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Baby steps with CNTK and FSharp]]></title><description><![CDATA[ 
 <br>So what have I been up to lately? Obsessing over <a data-tooltip-position="top" aria-label="https://www.microsoft.com/en-us/cognitive-toolkit/" rel="noopener" class="external-link" href="https://www.microsoft.com/en-us/cognitive-toolkit/" target="_blank">CNTK, the Microsoft deep-learning library</a>. Specifically, the team released a <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/cognitive-toolkit/cntk-library-managed-api" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/cognitive-toolkit/cntk-library-managed-api" target="_blank">.NET API</a>, which got me interested in exploring how usable this would be from the F# scripting environment. I started a <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp" target="_blank">repository to try out some ideas already</a>, but, before diving into that in later posts, I figure I could start by a simple introduction, to set some context.<br>First, what problem does CNTK solve?<br>Imagine that you are interested in predicting something, and that you have data available, both inputs you can observe (the features), and the values you are trying to predict (the labels). Imagine now that you have an idea of the type of relationship between the input and the output, something along the lines of:<br>labels ≈ function(features, parameters).<br>To make this more concrete, that function could be quite complex, and involve multiple layers of input transformation into the final output (“deep learning”), or it could be quite simple, for instance a traditional linear regression, something along the lines of:<br>car price ≈ car years * coefficient1 + car engine size * coefficient2 + constant.<br>In this particular case, we have 2 features (car years and car engine size), 1 label (car price), and 3 parameters (coefficient1, coefficient2 and constant) - and we would like to find “good” values for the 3 parameters so that the predicted value is in general close to the correct value.<br>The purpose of CNTK is to:<br>
<br>let you specify a function connecting input and output,
<br>let you specify how to read example data to learn from,
<br>learn good parameter values from the example data,
<br>let you learn parameters on CPU or GPU, for large datasets and complex functions.
<br>With that in mind, let’s take a look at a very basic example, a simple linear regression. Using CNTK here is complete overkill, and not worth the overhead; I would not use it for something that simple. Our goal here is simply to illustrate the basics of how CNTK works, from F#. In future posts, we will look into scenarios where CNTK is actually useful. As a secondary goal, I want to discuss some of the aspects that make building a nice F# API on top of the current .NET one tricky.<br><br>First order of business: let’s load this thing into VS Code.<br>CNTK has a few packages on Nuget, based on what environment you want to run on. In our case, we will focus on a <a data-tooltip-position="top" aria-label="https://www.nuget.org/packages/CNTK.CPUOnly/" rel="noopener" class="external-link" href="https://www.nuget.org/packages/CNTK.CPUOnly/" target="_blank">CPU-only scenario, using the CNTK.CPUOnly 2.3.1 package</a>.<br>We assume that the <a data-tooltip-position="top" aria-label="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-fsharp" rel="noopener" class="external-link" href="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-fsharp" target="_blank">Ionide-fsharp</a> and <a data-tooltip-position="top" aria-label="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-Paket" rel="noopener" class="external-link" href="https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-Paket" target="_blank">Ionide-Paket</a> extensions are installed in VS Code. Open the Folder where you want to work, and run the Paket: Init command (CTRL+SHIFT+P reveals the available commands). This will create a paket.dependencies file in the folder, where you can now specify what packages are needed, like this:<br>framework:net46
source https://www.nuget.org/api/v2
nuget CNTK.CPUOnly
Copy<br>Run Paket: Install next, and let Paket do its magic, and download the required packages. Once the operation completes, you should see a new folder, packages, with the following structure:<br>packages
  CNTK.CPUOnly
    lib
      net45
        x64
          Cntk.Core.Managed-2.3.1.dll
    support
      x64
        Debug
        Dependency
        Release
Copy<br>Let’s start creating the script we will be working with now, by adding an F# script file CNTK.fsx to our folder. Unfortunately, CNTK depends on a few native libraries to run properly. As a result, the setup is a bit more involved than the usual #r "path/to/library.dll. We’ll follow <a data-tooltip-position="top" aria-label="https://twitter.com/cdrnet" rel="noopener" class="external-link" href="https://twitter.com/cdrnet" target="_blank">@cdrnet</a> <a data-tooltip-position="top" aria-label="http://christoph.ruegg.name/blog/loading-native-dlls-in-fsharp-interactive.html" rel="noopener" class="external-link" href="http://christoph.ruegg.name/blog/loading-native-dlls-in-fsharp-interactive.html" target="_blank">approach to load native libraries described here</a>, and add to the PATH every folder that contains the dlls we need, so Cntk.Core.Managed-2.3.1.dll can find them:<br>
Note: I put the <a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" rel="noopener" class="external-link" href="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" target="_blank">full code used in the post on a gist here</a>
<br>open System
open System.IO

Environment.SetEnvironmentVariable("Path",
    Environment.GetEnvironmentVariable("Path") + ";" + __SOURCE_DIRECTORY__)

let dependencies = [
        "./packages/CNTK.CPUOnly/lib/net45/x64/"
        "./packages/CNTK.CPUOnly/support/x64/Dependency/"
        "./packages/CNTK.CPUOnly/support/x64/Dependency/Release/"
        "./packages/CNTK.CPUOnly/support/x64/Release/"    
    ]

dependencies 
|&gt; Seq.iter (fun dep -&gt; 
    let path = Path.Combine(__SOURCE_DIRECTORY__,dep)
    Environment.SetEnvironmentVariable("Path",
        Environment.GetEnvironmentVariable("Path") + ";" + path)
    )    

#I "./packages/CNTK.CPUOnly/lib/net45/x64/"
#I "./packages/CNTK.CPUOnly/support/x64/Dependency/"
#I "./packages/CNTK.CPUOnly/support/x64/Dependency/Release/"
#I "./packages/CNTK.CPUOnly/support/x64/Release/"

#r "./packages/CNTK.CPUOnly/lib/net45/x64/Cntk.Core.Managed-2.3.1.dll"
open CNTK
Copy<br><br>We can now start using CNTK in our script. Let’s build a function that takes 2 floats as input, and returns a float as an output, multiplying each of the inputs by a parameter.<br>A core element in CNTK is the NDShape, for n-dimensional shape. Think of an NDShape as an n-dimensional array. A vector of size 5 would be an NDShape of dimension [ 5 ] (rank 1), a 12x18 image a NDShape [ 12; 18 ] (rank 2), a 10 x 10 RGB image a NDShape [ 10; 10; 3 channels ] (rank 3), and so on. In our case, the input is an array of size 2, and the output an array of size 1:<br>let inputDim = 2
let outputDim = 1
let input = Variable.InputVariable(NDShape.CreateNDShape [inputDim], DataType.Double, "input")
let output = Variable.InputVariable(NDShape.CreateNDShape [outputDim], DataType.Double, "output")
Copy<br>Which produces the following output:<br>val inputDim : int = 2
val outputDim : int = 1
val input = Variable
val output = Variable
Copy<br>Note how the numeric type of the Variable, DataType.Double, is passed in as a argument, and not generic. Note also how the numeric types are aligned with the C# convention; that is, a DataType.Double is an F# float, and a DataType.Float is an F# single.<br>We can ask a Variable about its shape, for instance input.Shape:<br>val it : NDShape = CNTK.NDShape { Dimensions = seq [2]; (* more stuff *) Rank = 1; }
Copy<br>Let’s create our Function now:<br>let device = DeviceDescriptor.CPUDevice

let predictor =
    let dim = input.Shape.[0]
    let weights = new Parameter(NDShape.CreateNDShape [dim], DataType.Double, 0.0, device, "weights")
    // create an intermediate Function
    let product = CNTKLib.TransposeTimes(input, weights)    
    let constant = new Parameter(NDShape.CreateNDShape [ outputDim ], DataType.Double, 0.0, device, "constant") 
    CNTKLib.Plus(new Variable(product), constant)
Copy<br>val device : DeviceDescriptor
val predictor : Function
Copy<br>A couple of comments here. Our predictor creates a named Parameter weights of dimension and type matching the input Variable, with values initialized at 0.0. We multiply the two shapes together, by calling CNTKLib.TransposeTimes, computing x1 * w1 + x2 * w2, which returns a Function. We then create another Parameter for our constant, and sum them up, using CNTKLib.Plus.<br>Note how we have to explicitly convert product into a Variable in the final step, using new Variable(product). CNTKLib.Plus (and the other functions built in CNTKLib) expects 2 Variable arguments. Unfortunately, a Function is not a Variable, and they do not derive from a common class or interface. The .NET API supports implicit conversion between these 2 types, which works well in C#, where you could just sum these up directly, like this: CNTKLib.Plus(product, constant). F# doesn’t support implicit conversion, and as a result, this requires an annoying amount of explicit manual conversion to combine operations together.<br>Note also how we passed in device, a DeviceDescriptor, to the Parameter constructor. A CNTK Function is intended to run on a device, which must be specified. In this case, we could have omitted the device, in what case it would have picked up by default CPU.<br><br>Now that we have a Function - what can we do with it?<br>Unsuprisingly, we can pass input to a function, and compute the resulting value. We will do that next. However, before doing that, it’s perhaps useful to put things in perspective, to understand why this isn’t as straightforward as you might expect from something named a function. Once an F# function has been instantiated, its whole purpose is to transform an input value into an output value. The intent of a CNTK Function is subtly different: the objective here is to take a function, and modify its Parameters so that when passed in some input, the output it produces is close to some desired output, the Labels. In other words, we want a Function to be “trainable”: we want to be able to pass it known input/output pairs, and adjust the function parameters to fit the data better.<br>With that said, let’s evaluate our predictor function. To do that, we will need to do 3 things:<br>
<br>Supply values to fill in the “input” placeholder shape,
<br>Specify what values we want to observe - we might be interested in the output, but also the weights, for instance,
<br>Specify what device we want the function to run on.
<br>Let’s do that:<br>open System.Collections.Generic

let inputValue = Value.CreateBatch(NDShape.CreateNDShape [inputDim], [| 3.0; 5.0 |], device)
let inputMap = 
    let map = Dictionary&lt;Variable,Value&gt;()
    map.Add(input, inputValue)
    map

let predictedOutput = predictor.Output
let weights = 
    predictor.Parameters () 
    |&gt; Seq.find (fun p -&gt; p.Name = "weights")
let constant = 
    predictor.Parameters () 
    |&gt; Seq.find (fun p -&gt; p.Name = "constant")
let outputMap =
    let map = Dictionary&lt;Variable,Value&gt;()
    map.Add(predictedOutput, null)
    map.Add(weights, null)
    map.Add(constant, null)
    map

predictor.Evaluate(inputMap,outputMap,device)
Copy<br>To evaluate a Function, we pass it the input we care about, a Dictionary&lt;Variable,Value&gt;, which we fill in with input, the Variable we defined earlier. We provide (completely arbitrarily) a value of [3.0;5.0] as an input value. In a similar fashion, we specify what we want to observe: the predicted value, predictor.Output, as well as the 2 named parameters we created, “weights” and “constant”, which we also retrieve from the Function itself. In this case, we set the Value to null, because we have no input to supply. Finally, we run predictor.Evaluate, which will take the inputMap and fill in the missing values in the outputMap.<br>We can now review the outputs:<br>let currentPrediction = 
    outputMap.[predictedOutput].GetDenseData&lt;float&gt;(predictedOutput) 
    |&gt; Seq.map (fun x -&gt; x |&gt; Seq.toArray)
    |&gt; Seq.toArray

let currentWeights = 
    outputMap.[weights].GetDenseData&lt;float&gt;(weights) 
    |&gt; Seq.map (fun x -&gt; x |&gt; Seq.toArray)
    |&gt; Seq.toArray

let currentConstant = 
    outputMap.[constant].GetDenseData&lt;float&gt;(constant) 
    |&gt; Seq.map (fun x -&gt; x |&gt; Seq.toArray)
    |&gt; Seq.toArray
Copy<br>This is not pretty, but… we have values.<br>val currentPrediction : float [] [] = [| [| 0.0 |] |]
val currentWeights : float [] [] = [| [| 0.0; 0.0 |] |] 
val currentConstant : float [] [] = [| [| 0.0 |] |] 
Copy<br>The values we get back are pretty unexciting, but at least they are what we would expect to see. Given that both weights and constant were initialized at 0.0, the function should produce a currentPrediction of 0.0 * 3.0 + 0.0 * 5.0 + 0.0, which is indeed 0.0.<br>Two quick notes here. First, because a value could be of any DataType, we have to manually specify a type when retrieving the values, as in GetDenseData&lt;float&gt;. Then, this is a very stateful model: when we fill in values for the input in the inputMap, we pass in the input instance we initially created to construct the Function. In a similar fashion, we are retrieving values from the instances we passed into the outputMap.<br><br>This was pretty painful. So what is our reward for that pain?<br>As I stated earlier, one defining feature of a Function is that it can be trained. What we mean by that is the following: we can take a Function, supply it batches of input and desired output pairs, and progressively adjust the internal Parameter(s) of the Function so that the values computed by the Function become close(r) to the desired output.<br>Let’s start with a simple illustration. Suppose for a minute that, for our input [ 3.0; 5.0 ], we expected a result of 10.0. Currently, our weights and constant are set to 0.0. By modifying these 3 values, we should be able to tune our predictor to get an answer of 10.0.<br>This is, of course, a silly example. There are many ways I could change the parameters to produce 10.0 - I could set the constant to 10.0, or the second weight to 2.0, or infinitely many other combinations. To get something meaningful, I would need many different input/output pairs. However, we’ll start with this, strictly to illustrate the mechanics involved.<br>Training a Function involves 3 elements:<br>
<br>Supplying a batch of input / output pairs (features and labels),
<br>Defining a measure of fit, that is, how to measure if a value is close to the desired value,
<br>Specifying how parameters should be adjusted to improve the function.
<br>let batchInputValue = Value.CreateBatch(NDShape.CreateNDShape [inputDim], [| 3.0; 5.0 |], device)
let batchOutputValue = Value.CreateBatch(NDShape.CreateNDShape [outputDim], [| 10.0 |], device)

let batch =
    [
        input,batchInputValue
        output,batchOutputValue
    ]
    |&gt; dict

let loss = CNTKLib.SquaredError(new Variable(predictor), output, "loss")
let evaluation = CNTKLib.SquaredError(new Variable(predictor), output, "evaluation")

let learningRatePerSample = new TrainingParameterScheduleDouble(0.01, uint32 1)
let learners = 
    ResizeArray&lt;Learner&gt;(
        [
            Learner.SGDLearner(predictor.Parameters(), learningRatePerSample)
        ]
        )

let trainer = Trainer.CreateTrainer(predictor, loss, evaluation, learners)

for i in 0 .. 10 do
    let _ = trainer.TrainMinibatch(batch, true, device)
    trainer.PreviousMinibatchLossAverage () |&gt; printfn "Loss: %f"
    trainer.PreviousMinibatchEvaluationAverage () |&gt; printfn "Eval: %f"
Copy<br>First, we create a batch of input/output values ([ 3.0; 5.0 ] and [ 10.0 ]), and link them to the input and output Variable(s) we created. Then we define what measure we want to use to determine if a prediction is close or not from the target value. In this case, we use the built-in CNTKLib.SquaredError, which computes the square difference between the predicted value (new Variable(predictor)) and the target value (output). For instance, with the initial weights and constant, the predicted value will be 0.0, and we specified that the desired value was 10.0, so the loss function will evaluate to (0.0 - 10.0)^2, that is, 100.0 - and a perfect prediction of 10.0 would result in a loss of 0.0. Finally, without going into much detail, we specify in learners which strategy to apply when updating the function parameters. In this case, we use the built-in Stochastic Gradient Descent (SGD) strategy, with a learning rate of 0.01 (how aggressively to update the parameters) and a batch size of 1, using only one input/output pair at a time when performing adjustments.<br>We feed all that into a Trainer, and perform 10 updates (trainer.TrainMinibatch), using the same example input/output each time, and writing out the current value of the loss function:<br>Loss: 100.000000
Eval: 100.000000
Loss: 9.000000
Eval: 9.000000
// omitted intermediate results for brevity 
Loss: 0.000000
Eval: 0.000000
Loss: 0.000000
Eval: 0.000000
Copy<br>As you can observe, the prediction error decreases rapidly, from 100.0 initially (as expected), to basically 0.0 after only 10 steps.<br>Let’s make this a bit more interesting, by feeding different examples to the model:<br>let realModel (features:float[]) =
    3.0 * features.[0] - 2.0 * features.[1] + 5.0

let rng = Random(123456)
let batch () =        
    let batchSize = 32        
    let features = [| rng.NextDouble(); rng.NextDouble() |]
    let labels = [| realModel features |]
    let inputValues = Value.CreateBatch(NDShape.CreateNDShape [inputDim], features, device)
    let outputValues = Value.CreateBatch(NDShape.CreateNDShape [outputDim], labels, device)
    [
        input,inputValues
        output,outputValues
    ]
    |&gt; dict
Copy<br>Here we simply create a “true” function, realModel, which we use to generate synthetic data. We then modify our previous example, to feed 1,000 different examples for training:<br>#time "on"

for _ in 1 .. 1000 do
    
    let example = batch ()
    trainer.TrainMinibatch(example,true,device) |&gt; ignore
    trainer.PreviousMinibatchLossAverage () |&gt; printfn "Loss: %f"
Copy<br>On my machine, extracting the weights and constant from the Function after training yields 3.0019, -1.9978 and 4.9975 - pretty close to the correct values of 3.0, -2.0 and 5.0 that we used in realModel.<br>
Note: I put the <a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" rel="noopener" class="external-link" href="https://gist.github.com/mathias-brandewinder/d48abe4a571c53a4a70c709c3121a566" target="_blank">full code used in the post on a gist here</a>
<br><br>First, I want to re-iterate that the example we went through is not showcasing a good example of where and how to use CNTK. It is intended primarily as an illustration of CNTK’s building blocks and how they work together. For a trivial linear regression example like this one (shallow learning, if you will), you would be better served with a standard library such as <a data-tooltip-position="top" aria-label="http://accord-framework.net/" rel="noopener" class="external-link" href="http://accord-framework.net/" target="_blank">Accord.NET</a>. CNTK becomes interesting if you have a deeper, more complex model, and a larger dataset - we’ll explore this in later posts.<br>
As a side-note, my initial intent was to use real batches for the final example, passing in multiple examples at once, but for reasons I couldn’t figure out yet, the code kept crashing.
<br>My second goal was to explore the design of the current .NET API, as a preliminary step before trying to build an F#-scripting friendly layer on top of it.<br>In its current state, the CNTK .NET library is fairly low-level, and rather unpleasant to work with from F#. Ideally, one would like to be able to create re-usable blocks and compose them easily, along the lines of the Keras model, using a DSL to, for instance, define a network by stacking standard transformation layers on top of each other.<br>Such a DSL seems quite possible to achieve in F#, but requires taking into account a few design considerations. First, the choice to use implicit conversion between Variable and Function makes composition of functions in F# painful. This choice is reasonable for C#, but requires re-wrapping every Function into a Variable to string operations together on the F# side.<br>One aspect I am not a fan of in the library is how the DeviceDescriptor leaks all the way down. With the current model, I could create 2 parameters, one on CPU, one on GPU, and combine them together, which doesn’t make a lot of sense. In an ideal world, I would like to define a Function independently of any device, and only then decide whether I want to train that model on a CPU or a GPU.<br>Finally, the fact that a Variable or a Function cannot be named after it was instantiated, as far as I can tell, introduces complications in composing blocks together. If naming was separate from instantiation, we could create a function like named : string -&gt; Function -&gt; Function, which could be inserted anywhere.<br>I haven’t had much time yet to dig into the data readers; so far, most of my efforts have gone into exploring possible directions to address the questions above. If you are interested, the <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp" target="_blank">master branch of my repository</a> contains working, straight conversions of the <a data-tooltip-position="top" aria-label="https://github.com/Microsoft/CNTK/tree/master/Examples/TrainingCSharp/Common" rel="noopener" class="external-link" href="https://github.com/Microsoft/CNTK/tree/master/Examples/TrainingCSharp/Common" target="_blank">C# examples published by the CNTK team</a>; the results of my explorations can be found in the 3 branches <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-varorfun" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-varorfun" target="_blank">experiment-varorfun</a>, <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-interpreter" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-interpreter" target="_blank">experiment-interpreter</a> and <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-stacking" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/CNTK.FSharp/tree/experiment-stacking" target="_blank">experiment-stacking</a>.<br>I hope you found something of interest in this post! If you have feedback or suggestions, I would be quite interested to hear about them :) In the meanwhile, I will keep exploring - expect more on the topic in the near future!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/baby-steps-with-cntk-and-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Baby steps with CNTK and FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:32 GMT</pubDate></item><item><title><![CDATA[Basic Regression Tree]]></title><description><![CDATA[ 
 <br>In our previous installment, we <a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2016/08/06/gradient-boosting-part-1/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2016/08/06/gradient-boosting-part-1/" target="_blank">began exploring Gradient Boosting</a>, and outlined how by combining extremely crude regression models - stumps - we could iteratively create a decent prediction model for the quality of wine bottles, using one Feature, one of the chemical measurements we have available.<br>In and of itself, this is an interesting result: the approach allows us to aggregate mediocre indicators together into a predictor that is better than its individual parts. However, so far, we are using only a tiny subset of the information available. Why restrict ourselves to a single Feature, and not use all of them? And, if the approach works with something as weak as a stump, perhaps we can do better, by aggregating less trivial prediction models?<br>This will be our goal today: we will create a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Decision_tree_learning#Types" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Types" target="_blank">Regression Tree</a>, which we will in a future installment use in place of stumps in our Boosting procedure.<br><br><a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/05683d63bfa67c8b706ce458035c0b81#file-gradient-boosting-2-fsx" rel="noopener" class="external-link" href="https://gist.github.com/mathias-brandewinder/05683d63bfa67c8b706ce458035c0b81#file-gradient-boosting-2-fsx" target="_blank"><em></em></a>Full code for this post available here as a Gist<br>The Stump model is rather simple: we take a Feature and a split value, the threshold. If the input value is under that threshold, we predict the average output value computed across examples under the threshold, otherwise, we do the opposite:<br><img alt="Stump" src="https://mathias-brandewinder.github.io//assets/2016-08-14-stump.png" referrerpolicy="no-referrer"><br>Or, in code:<br>type Wine = CsvProvider&lt;"data/winequality-red.csv",";",InferRows=1500&gt;

type Observation = Wine.Row

type Feature = Observation -&gt; float

type Example = Observation * float

type Predictor = Observation -&gt; float

let learnStump (sample:Example seq) (feature:Feature) threshold =
    let under = 
        sample 
        |&gt; Seq.filter (fun (obs,lbl) -&gt; feature obs &lt;= threshold)
        |&gt; Seq.averageBy (fun (obs,lbl) -&gt; lbl)
    let over = 
        sample 
        |&gt; Seq.filter (fun (obs,lbl) -&gt; feature obs &gt; threshold)
        |&gt; Seq.averageBy (fun (obs,lbl) -&gt; lbl)
    fun obs -&gt;
        if (feature obs &lt;= threshold)
        then under
        else over
Copy<br>A regression tree extends the idea further. Instead of limiting ourselves to a single threshold, we can further divide each group, and create trees like this one for instance:<br><img alt="Simple Tree" src="https://mathias-brandewinder.github.io//assets/2016-08-14-simple-tree.png" referrerpolicy="no-referrer"><br>Nothing forces us to keep the tree symmetrical, or to use a single Feature, though. This would be a perfectly acceptable tree as well:<br><img alt="Complex Tree" src="https://mathias-brandewinder.github.io//assets/2016-08-14-complex-tree.png" referrerpolicy="no-referrer"><br>The nice thing about trees is, they are pretty flexible, and very easy to interpret. With a tree, we can incorporate multiple features and their interactions. In our example, we are really modelling Quality as a surface, instead of a simple line in the stump example:<br><img alt="Quality Surface" src="https://mathias-brandewinder.github.io//assets/2016-08-14-surface.png" referrerpolicy="no-referrer"><br>The resulting model can be expressed in a very understandable form:<br>
If the Alcohol Level is over 10.5, the Quality is 5.5; Otherwise, check the Volatile Acidity. If it is below 0.8, the Quality is 6.0, otherwise it is 3.0.
<br><br>How can we go about representing and learning a Tree?<br>As it turns out, the representation is fairly straightforward. A Tree can be seen as a recursive data structure: either we reached a terminal Leaf, which gives us a prediction, or we reach a Branch, where, based on a Feature and associated split value, we will find 2 new Trees, one for values under the split value, another for values above the split.<br>That is a match in heaven for a Discriminated Union:<br>type Tree =
    | Leaf of float
    | Branch of (Feature * float) * Tree * Tree
Copy<br>Creating manually the “complex” tree we described above can be done along these lines:<br>let exampleTree =
    // we start with a branch
    Branch(
        // we split on Alcohol level, 10.5
        (``Alcohol Level``, 10.5),
        // if alcohol level is under 10.5, 
        // we have another branch
        Branch(
            // we split on Volatile Acidity, 0.8
            (``Volatile Acidity``, 0.8),
            // if acidity is under 0.8, 
            // we predict 6.0
            Leaf(6.0),
            // otherwise we predict 3.0
            Leaf(3.0)
        ),
        // if alcohol is over 10.5,
        // we predict 5.5
        Leaf(5.5)
    )
Copy<br>How do we go about making predictions with a Tree? We simply walk it down recursively:<br>let rec predict (tree:Tree) (obs:Observation) =
    match tree with
    | Leaf(prediction) -&gt; prediction
    | Branch((feature,split),under,over) -&gt;
        let featureValue = feature obs
        if featureValue &lt;= split
        then predict under obs
        else predict over obs
Copy<br>Let’s try it out on our example:<br>predict exampleTree (reds.Rows |&gt; Seq.head)

&gt; val it : float = 6.0
Copy<br>Note that, if we use partial application:<br>let examplePredictor = predict exampleTree
Copy<br>… we get back a function, examplePredictor, which happens to have exactly the signature we defined earlier for a Predictor:<br>val examplePredictor : (Observation -&gt; float)
Copy<br>As a result, we can immediately re-use the sumOfSquares error function we wrote last time, and evaluate how good our tree is fitting the dataset:<br>let sumOfSquares (sample:Example seq) predictor = 
    sample
    |&gt; Seq.sumBy (fun (obs,lbl) -&gt; 
        pown (lbl - predictor obs) 2)

let redSample = 
    reds.Rows 
    |&gt; Seq.map (fun row -&gt; row, row.Quality |&gt; float)

sumOfSquares redSample examplePredictor 
Copy<br>val it : float = 1617.0
Copy<br>The result is pretty terrible - but then, I picked the tree values randomly. Can we automatically learn a “good” Tree?<br><br>If you recall, the approach we followed to learn a “good” stump was the following: for a given Feature, try out various possible split values, and pick the one that gives us the smallest error, defined as the sumOfSquares between the predicted and actual values.<br>We can use the same idea for a Tree. Instead of stopping once we found a good split, we will simply repeat the same process, and look for a good split in each of the two samples we got after the split. Also, instead of searching for a split on a single Feature, we will now consider all of them, and select the best split across all available Features.<br>That smells like recursion. As a first pass, we will re-use some of the code we wrote last time, the learnStump and evenSplits functions, and whip together a quick-and-dirty tree learning function, disregarding any performance consideration:<br>let rec learnTree (sample:Example seq) (features:Feature list) (depth:int) =
    
    if depth = 0
    then
        // we reached maximum depth, and
        // predict the sample average.
        let avg = sample |&gt; Seq.averageBy snd
        Leaf(avg)
    else
        let (bestFeature,bestSplit) = 
            // create all feature * split combinations
            seq {
                for feature in features do
                    let splits = evenSplits sample feature 10
                    for split in splits -&gt; feature,split
            }
            // find the split with the smallest error
            |&gt; Seq.minBy (fun (feature,split) -&gt; 
                let predictor = learnStump sample feature split
                sumOfSquares sample predictor)
        // split the sample following the split
        let under = 
            sample 
            |&gt; Seq.filter (fun (obs,_) -&gt; 
                bestFeature obs &lt;= bestSplit)
        let over = 
            sample 
            |&gt; Seq.filter (fun (obs,_) -&gt; 
                bestFeature obs &gt; bestSplit)
        // learn the corresponding trees
        let underTree = learnTree under features (depth - 1)
        let overTree =  learnTree over features (depth - 1)
        // and create the corresponding branch
        Branch((bestFeature,bestSplit),underTree,overTree)
Copy<br>Let’s try this out, with a Tree that should be equivalent to the first stump we created last time:<br>let originalStump = learnTree redSample [ ``Alcohol Level`` ] 1
sumOfSquares redSample (predict originalStump)
Copy<br>val it : float = 864.4309287
Copy<br>Good news - we get the same result. Now let’s crank it up a notch:<br>let deeperTree = learnTree redSample [``Alcohol Level``;``Volatile Acidity``] 4
sumOfSquares redSample (predict deeperTree)
Copy<br>val it : float = 680.1290569
Copy<br>This is significantly better that the best result we achieved by ensembling stumps, 811.4601191.<br><br>We have a decent-looking Tree learning algorithm. However, not everything is perfect. For instance, emboldened by our success, we could try to increase the depth a bit.<br>let explodingTree = learnTree redSample [``Alcohol Level``] 5
Copy<br>System.ArgumentException: The step of a range cannot be zero.
Parameter name: step
// long list of F# complaints follows
Copy<br>Uh-oh. What is happening here?<br>As we recurse deeper in the Tree, we split the samples further and further, and have less and less data to train our stump on. One thing which might happen for instance is that we are left only with examples sharing the same label. In that situation, generating even splits is going to cause issues, because the width in [ min + width .. width .. max - width ] (our evenly-spaced splits) will be 0.0.<br>This indicates a first problem, namely, that there might not be any good split to use for a given sample.<br>Beyond that, the design is also a bit problematic. The choice of 10 even splits is quite arbitrary; we might want to use 3, or 42 even splits, or use different strategies altogether (splits of same size, every possible distinct value, …). Our evenSplits function is hard-coded deep inside the algorithm - it would be much nicer if we could inject any split function as an argument.<br>In a similar vein, assuming we are comfortable with using stumps / binary splits, the choice of our error metric is also quite arbitrary. We might want to use something else that the sum of squared prediction errors (Manhattan distance, variance reduction, …). Again, that function is buried deep inside - we would like to use any reasonable cost function we think relevant to the problem.<br>Finally, we are picking the split that yields the best cost. However, that split is not guaranteed to be an improvement. As an example, every observation in the sample could have the same label, in which case no split will improve our predictions. If the resulting cost is the same as before, it is pointless to split, and we might as well spare the algorithm a useless deeper search.<br>In short,<br>
<br>we are not guaranteed to have splits for every sample,
<br>we should split only when strict cost improvements are found,
<br>we would like to decide what splits to use,
<br>we would like to decide what cost metric to use.
<br>We are probably going slightly overboard here; the only real problem we have is the first one. At the same time, why not have a bit of fun!<br>I am going to start with defining a couple of type aliases and utilities:<br>let underOver (sample:Example seq) (feat:Feature,split:float) =
    let under = sample |&gt; Seq.filter (fun (obs,_) -&gt; feat obs &lt;= split)
    let over =  sample |&gt; Seq.filter (fun (obs,_) -&gt; feat obs &gt; split)
    under,over

type Splitter = Example seq -&gt; Feature -&gt; float list

type Cost = Example seq -&gt; float
Copy<br>underOver simply takes a sample, and partitions it into 2 samples, based on a feature and a split value. Splitter is a function that, given a sample and a Feature, will produce a (potentially empty) list of values we could split on. Cost simply measures how good a sample is.<br>Given these elements, we can now rewrite our learnTree function along these lines:<br>let rec learnTree (splitter:Splitter,cost:Cost) (sample:Example seq) (features:Feature list) (depth:int) =
    
    if depth = 0
    then
        let avg = sample |&gt; Seq.averageBy snd
        Leaf(avg)
    else
        let initialCost = cost sample        
        let candidates = 
            // build up all the feature/split candidates,
            // and their associated sample splits
            seq {
                for feature in features do
                    let splits = splitter sample feature
                    for split in splits -&gt; 
                        let under,over = underOver sample (feature,split)  
                        (feature,split),(under,over)
            }
            // compute and append cost of split
            |&gt; Seq.map (fun (candidate,(under,over)) -&gt;
                candidate,(under,over), cost under + cost over)
            // retain only candidates with strict cost improvement
            |&gt; Seq.filter (fun (candidate,(under,over),splitCost) -&gt;
                splitCost &lt; initialCost)

        if (Seq.isEmpty candidates)
        then
            let avg = sample |&gt; Seq.averageBy snd
            Leaf(avg)
        else
            let ((bestFeature,bestSplit),(under,over),spliCost) = 
                candidates 
                |&gt; Seq.minBy (fun (_,_,splitCost) -&gt; splitCost)

            let underTree = learnTree (splitter,cost) under features (depth - 1)
            let overTree =  learnTree (splitter,cost) over features (depth - 1)

            Branch((bestFeature,bestSplit),underTree,overTree)
Copy<br><br>Does it work? Let’s try it out:<br>let evenSplitter n (sample:Example seq) (feature:Feature) = 
    let values = sample |&gt; Seq.map (fst &gt;&gt; feature)
    let min = values |&gt; Seq.min
    let max = values |&gt; Seq.max
    if min = max 
    then []
    else
        let width = (max-min) / (float (n + 1))
        [ min + width .. width .. max - width ]

let sumOfSquaresCost (sample:Example seq) = 
    let avg = sample |&gt; Seq.averageBy snd
    sample |&gt; Seq.sumBy (fun (_,lbl) -&gt; pown (lbl - avg) 2) 

let stableTree = learnTree (evenSplitter 10,sumOfSquaresCost) redSample [``Alcohol Level``;``Volatile Acidity``] 10

sumOfSquares redSample (predict stableTree)
Copy<br>This time, nothing explodes - and the value we get is<br>val it : float = 331.1456491
Copy<br>The nice thing here is that at that point, all it takes to create and try new trees is a specification for the cost and split functions, and a list of features. We can, for instance, create a Tree using every feature we have available:<br>let features = [
    ``Alcohol Level``
    ``Chlorides``
    ``Citric Acid``
    ``Density``
    ``Fixed Acidity``
    ``Free Sulfur Dioxide``
    ``PH``
    ``Residual Sugar``
    ``Total Sulfur Dioxide``
    ``Volatile Acidity``
]

let fullTree = learnTree (evenSplitter 5,sumOfSquaresCost) redSample features 10
Copy<br>The results are pretty decent, too:<br><img alt="Actual vs Predicted" src="https://mathias-brandewinder.github.io//assets/2016-08-14-actual-vs-predicted.PNG" referrerpolicy="no-referrer"><br>Out of curiosity, I also performed a crude training vs. testing analysis, to get a feel for potential over-fitting issues.<br><img alt="Over Fitting" src="https://mathias-brandewinder.github.io//assets/2016-08-14-overfitting.PNG" referrerpolicy="no-referrer"><br>The result we observe is typical of trees: as we increase depth, the error on the training sample steadily decreases, indicating that deeper tree fit the data better and better. However, the testing sample tells a different story: for a little while, error on the training and testing samples match fairly closely, but after we reach a certain depth (here, 3), they start diverging. While our tree fit the training sample better and better, that improvement doesn’t generalize to other samples, as we can see on the testing sample; at that point, we are over-fitting. In our particular case, this means that we shouldn’t put much trust in trees deeper than 3.<br><br>At that point, we have a working regression tree algorithm. It’s not perfect; in particular, we largely ignored any performance consideration. Or, stated more bluntly, performance is terrible ;) Still, the result has a couple nice features, the code is fairly simple, and… it works!<br>Trees are quite an interesting topic, which we only covered very superficially here. Still, we will leave it at that for now, and focus back on our initial goal, gradient boosting. All we needed was something a bit better than stumps to iteratively fit residuals. We have that now, with regression tree that allow us to learn a predictor using every feature we have available. In our next installments, we will look at replacing stumps with trees, and see where that leads us.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/basic-regression-tree.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Basic Regression Tree.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:34 GMT</pubDate><enclosure url="https://mathias-brandewinder.github.io//assets/2016-08-14-stump.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://mathias-brandewinder.github.io//assets/2016-08-14-stump.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Elmish.Snabbdom]]></title><description><![CDATA[ 
 <br>I've been recently playing with <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" rel="noopener" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" target="_blank">Feliz.Engine</a>, an attempt to take advantage of the great work done by Zaid Ajaj and contributors with <a data-tooltip-position="top" aria-label="https://zaid-ajaj.github.io/Feliz/" rel="noopener" class="external-link" href="https://zaid-ajaj.github.io/Feliz/" target="_blank">Feliz</a> when writing non-React applications. As part of this I wanted to check how easy was to adapt Feliz.Engine to an alternative Virtual-DOM implementation, and I read good things about <a data-tooltip-position="top" aria-label="https://github.com/snabbdom/snabbdom" rel="noopener" class="external-link" href="https://github.com/snabbdom/snabbdom" target="_blank">Snabbdom</a> so I gave it a go. This started just as an experiment but I've been pleasantly surprised by how simple yet powerful Snabbdom is, and more importantly, how well it fits with the <a data-tooltip-position="top" aria-label="https://elmish.github.io/" rel="noopener" class="external-link" href="https://elmish.github.io/" target="_blank">Elmish architecture</a>, so I want to share with you my findings hoping that you find them useful.<br>
我最近一直在玩 Feliz.Engine，试图利用 Zaid Ajaj 和 Feliz 贡献者在编写非 React 应用程序时所做的出色工作。作为其中的一部分，我想检查将 Feliz.Engine 适应替代 Virtual-DOM 实现有多容易，并且我阅读了有关 Snabbdom 的好文章，所以我尝试了一下。一开始只是一个实验，但我对 Snabbdom 的简单而强大感到惊喜，更重要的是，它与 Elmish 架构的契合程度，所以我想与您分享我的发现，希望您发现它们有用。<br>There was recently a discussion in Twitter about the <a data-tooltip-position="top" aria-label="https://twitter.com/7sharp9_/status/1365270255170428928" rel="noopener" class="external-link" href="https://twitter.com/7sharp9_/status/1365270255170428928" target="_blank">problems with Fable Elmish</a>. So far, Elmish in Fable apps has always used React as the view engine, including React native for mobile (there are also Elmish implementations for non-Fable platforms like <a data-tooltip-position="top" aria-label="https://github.com/elmish/Elmish.WPF" rel="noopener" class="external-link" href="https://github.com/elmish/Elmish.WPF" target="_blank">WPF</a>, <a data-tooltip-position="top" aria-label="https://fsprojects.github.io/Fabulous/" rel="noopener" class="external-link" href="https://fsprojects.github.io/Fabulous/" target="_blank">Xamarin</a> or <a data-tooltip-position="top" aria-label="https://fsbolero.io/docs/Elmish" rel="noopener" class="external-link" href="https://fsbolero.io/docs/Elmish" target="_blank">Blazor</a>), and there's been always friction between the concept of "component" in Elmish and React. This is a bit of technical discussion and I won't go into detail here, among other reasons because I've never managed to explain the difference in an understandable manner. Probably the easiest is to consider Elm/Elmish don't really have a notion of "component" as Dave Thomas explains. It's true the Fable Elmish community tends to "componentize" apps maybe under the influence of React, which sometimes leads to an excess of boilerplate to wire everything.<br>
最近 Twitter 上有一场关于《Fable Elmish》问题的讨论。到目前为止，《Fable》应用中的 Elmish 一直使用 React 作为视图引擎，包括适用于移动设备的 React Native（也有针对 WPF、Xamarin 或 Blazor 等非 Fable 平台的 Elmish 实现），并且“ Elmish 和 React 中的组件”。这是一些技术讨论，我不会在这里详细介绍，除其他原因外，因为我从未设法以可理解的方式解释其中的差异。也许最简单的方法是考虑 Elm/Elmish 并没有真正的“组件”概念，正如 Dave Thomas 所解释的那样。确实，Fable Elmish 社区可能在 React 的影响下倾向于“组件化”应用程序，这有时会导致过多的样板文件来连接所有内容。<br>It's possible to write an Elmish/React app with just a single view function, and some apps work well that way. But to take advantage of most of React features, like devtools, memoization or life-cycle events, you do need components, as React understands them. This is why some, myself guilty as charged, have been trying to drive towards more use of React components with Elmish. An important move for this has been the <a data-tooltip-position="top" aria-label="https://zaid-ajaj.github.io/Feliz/#/Hooks/UseElmish" rel="noopener" class="external-link" href="https://zaid-ajaj.github.io/Feliz/#/Hooks/UseElmish" target="_blank"><code></code> React hook</a>useElmish which many Fable devs have successfully adopted. But at this point Elmish gets reduced to manage the internal state of your components and your app gets eventually architected the React-way. This is not a bad thing if you already know React, but this post is about "rediscovering" the power of Elmish as I've been experiencing recently.<br>
可以仅使用单个视图函数编写 Elmish/React 应用程序，并且某些应用程序可以很好地工作。但要利用大多数 React 功能，例如开发工具、记忆或生命周期事件，您确实需要组件，正如 React 所理解的那样。这就是为什么一些人（我本人也有罪）一直在尝试推动更多地使用 Elmish 中的 React 组件。为此，一个重要的举措是 useElmish React hook，许多 Fable 开发人员已成功采用。但此时 Elmish 被简化为管理组件的内部状态，并且您的应用程序最终以 React 方式构建。如果您已经了解 React，这并不是一件坏事，但这篇文章是关于“重新发现”Elmish 的力量，正如我最近所经历的那样。<br>What if we try the other way around, that is, not worrying about "componentizing" our application? This is actually the original proposal of Elm/Elmish and what you get by using a low-level Virtual-DOM library like Snabbdom, instead of a full-fledged one like React. When I started trying to run Feliz.Engine with Snabbdom it was just about API ergonomics but being able to enjoy "pure" Elmish without giving up DOM control has been really freeing. Why I'm excited about Snabbdom? These are some of the reasons for it:<br>
如果我们尝试相反的方式，即不担心“组件化”我们的应用程序，会怎么样？这实际上是 Elm/Elmish 的最初提议，以及通过使用像 Snabbdom 这样的低级 Virtual-DOM 库而不是像 React 这样的成熟库所得到的结果。当我开始尝试使用 Snabbdom 运行 Feliz.Engine 时，它​​只是关于 API 人体工程学，但能够在不放弃 DOM 控制的情况下享受“纯粹的”Elmish 真的很自由。为什么我对 Snabbdom 感到兴奋？以下是一些原因：<br><br>There's no concept of component that clashes with Elmish, just composable functions from beginning to end. Again, you ca do the same with React but as soon as you need to deal with the DOM or some other features you need the components. This is not the case of Snabbdom, keep reading.<br>
没有与 Elmish 冲突的组件概念，只有从头到尾的可组合函数。同样，您可以对 React 执行相同的操作，但是一旦您需要处理 DOM 或其他一些功能，您就需要组件。 Snabbdom 的情况并非如此，请继续阅读。<br><br>内置 CSS 过渡#<br>Easy CSS transitions was one of biggest <a data-tooltip-position="top" aria-label="https://svelte.dev/" rel="noopener" class="external-link" href="https://svelte.dev/" target="_blank">Svelte</a> appeals for me, and I was very surprised to see Snabbdom has a similar mechanism. Together with the wonderful Feliz API (check <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/main/README.md" rel="noopener" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/main/README.md" target="_blank">the differences</a> in Feliz.Engine), we can get a nice zoom-in/zoom-out effect just by attaching some styles to a node.<br>
简单的 CSS 转换是 Svelte 对我最大的吸引力之一，我很惊讶地看到 Snabbdom 也有类似的机制。结合精彩的 Feliz API（查看 Feliz.Engine 中的差异），我们只需将一些样式附加到节点即可获得不错的放大/缩小效果。<br>Html.li [
    Attr.className "box"

    Css.opacity 0.
    Css.transformScale 1.5
    // Snabbdom doesn't support `all`, we need to list all the transitioning properties
    Css.transitionProperty(transitionProperty.opacity, transitionProperty.transform)
    Css.transitionDurationSeconds 0.5
    Css.delayed [
        Css.opacity 1.
        Css.transformScale 1.
    ]
    Css.remove [
        Css.opacity 0.
        Css.transformScale 0.1
    ]
Copy<br><img alt="Snabbdom CSS transitions" src="https://fable.io/static/img/blog/snabbdom-css-transitions.gif" referrerpolicy="no-referrer"><br>Learn more about Snabbdom CSS transitions <a data-tooltip-position="top" aria-label="https://github.com/snabbdom/snabbdom#delayed-properties" rel="noopener" class="external-link" href="https://github.com/snabbdom/snabbdom#delayed-properties" target="_blank">here</a>.<br>
在此处了解有关 Snabbdom CSS 过渡的更多信息。<br><br>In theory, given that a pure Elmish app fully recreates the whole virtual DOM for every tiny change it's important to be able to skip the parts of your app that don't need to change (in reality, this usually is not a performance issue thankfully). But memoization has been one of the biggest pain-points when writing Fable/React bindings (still is). Because of nuances of how JS/F# languages work and the way React expects you to declare a memoized component. a <a data-tooltip-position="top" aria-label="https://zaid-ajaj.github.io/Feliz/#/Feliz/React/CommonPitfalls" rel="noopener" class="external-link" href="https://zaid-ajaj.github.io/Feliz/#/Feliz/React/CommonPitfalls" target="_blank">common pitfall</a> is to recreate the component for every function call rendering memoization useless. With Feliz.Snabbdom we just need to wrap a call with the memoize helper. For example, if we are displaying a list of Todos:<br>
理论上，考虑到纯 Elmish 应用程序会针对每一个微小的更改完全重新创建整个虚拟 DOM，因此能够跳过应用程序中不需要更改的部分非常重要（实际上，幸运的是，这通常不是性能问题） ）。但记忆一直是编写 Fable/React 绑定时最大的痛点之一（仍然是）。由于 JS/F# 语言工作方式的细微差别以及 React 希望您声明记忆组件的方式。一个常见的陷阱是为每个函数调用重新创建组件，从而使记忆变得无用。对于 Feliz.Snabbdom，我们只需要使用 memoize 帮助器来包装调用。例如，如果我们要显示待办事项列表：<br>let renderTodo dispatch (todo: Todo, editing: string option) = ...

let renderTodoList (state: State) (dispatch: Msg -&gt; unit) =
    Html.ul (
        state.TodoList |&gt; List.map (fun todo -&gt;
            todo,
            state.Editing |&gt; Option.bind (fun (i, e) -&gt; if i = todo.Id then Some e else None))
        |&gt; List.map (renderTodo dispatch)
    )
Copy<br>We just need to wrap the renderTodo call (here also provide a way to get a unique id from the arguments). Note that we don't need to check dispatch for the memoization, so we can just partially apply it before the wrapping:<br>
我们只需要包装 renderTodo 调用（这里还提供了一种从参数中获取唯一 id 的方法）。请注意，我们不需要检查 dispatch 的记忆化，因此我们可以在包装之前部分应用它：<br>let renderTodoList (state: State) (dispatch: Msg -&gt; unit) =
    Html.ul (
        state.TodoList |&gt; List.map (fun todo -&gt;
            todo,
            state.Editing |&gt; Option.bind (fun (i, e) -&gt; if i = todo.Id then Some e else None))
        |&gt; List.map (memoizeWithId (renderTodo dispatch) (fun (t, _) -&gt; t.Id))
    )
Copy<br><br>Unlike React ones, <a data-tooltip-position="top" aria-label="https://github.com/snabbdom/snabbdom#hooks" rel="noopener" class="external-link" href="https://github.com/snabbdom/snabbdom#hooks" target="_blank">hooks in Snabbdom</a> are very easy to understand. They are just events fired at different points of the lifecycle of a virtual node, as when they get inserted into or removed from the actual DOM. Very conveniently, the virtual node holding a reference to the actual DOM element is passed as argument to the event handler so it's easy for example to get the actual height of an element.<br>
与 React 不同，Snabbdom 中的钩子非常容易理解。它们只是在虚拟节点生命周期的不同点触发的事件，就像它们插入实际 DOM 或从实际 DOM 中删除时一样。非常方便的是，保存对实际 DOM 元素的引用的虚拟节点作为参数传递给事件处理程序，因此可以轻松获取元素的实际高度。<br>React hooks allow you to do similar things, but they're designed in a way that forces you to translate your thinking into the React way of doing things. Let's say you want to turn some text into an input on double click, then select all the text and attach an event to the document so if you click outside the containing box you cancel the edit. For this, in React you need to (forgive me if there's a more clever way of doing this that I'm missing):<br>
React hooks 允许你做类似的事情，但它们的设计方式迫使你将你的想法转化为 React 的做事方式。假设您想通过双击将某些文本转换为输入，然后选择所有文本并将事件附加到文档，这样如果您在包含框之外单击，则会取消编辑。为此，在 React 中你需要（如果我缺少更聪明的方法来做到这一点，请原谅我）：<br>
<br>Make sure the function you are in is a component because this is required to use hooks.<br>
确保您所在的函数是一个组件，因为这是使用钩子所必需的。
<br>Declare a reference to hold the actual input element with useRef hook (beware! you don't have the actual element yet).<br>
使用 useRef 钩子声明一个引用来保存实际的输入元素（注意！您还没有实际的元素）。
<br>Pass the value returned by useRef to a ref prop on the input element so React fills it.<br>
将 useRef 返回的值传递给输入元素上的 ref 属性，以便 React 填充它。
<br>Declare an effect with useEffect hook. Because you want the effect to happen when the input appears, you need to pass an array with a flag like isEditable.<br>
使用 useEffect 钩子声明效果。因为您希望在输入出现时发生效果，所以您需要传递一个带有 isEditable 等标志的数组。
<br>The effect will happen when isEditable changes from false to true or from true to false, so make sure isEditable is true before running the effect.<br>
当 isEditable 从 false 变为 true 或从 true 变为 false 时，效果就会发生，因此在运行效果之前请确保 isEditable 为 true。
<br>Now get the input element from the value you declared in 2. Select the text and attach the event to the document body, return a disposable function to detach the event when isEditable changes to false.<br>
现在从 2 中声明的值获取输入元素。选择文本并将事件附加到文档正文，返回一次性函数以在 isEditable 更改为 false 时分离事件。
<br>On the other hand, in Snabbdom if you want to, when an input element appears, select all the text, attach an event to the html body and detach it when the input disappears you need to:<br>
另一方面，在 Snabbdom 中，如果您愿意，当输入元素出现时，选择所有文本，将事件附加到 html 正文，并在输入消失时将其分离，您需要：<br>
<br>Add an insert hook to the input, so when it appears, you can select all the text, attach an event to the html body and return a disposable to detach it when the input disappears.<br>
在输入中添加一个 insert 钩子，这样当它出现时，您可以选择所有文本，将事件附加到 html 主体，并返回一个一次性事件，以便在输入消失时将其分离。
<br>Well, I'm cheating a bit here, in "raw" Snabbdom keeping a reference to the disposable and disposing it when the element is destroyed is slightly more contrived, but luckily Feliz.Snabbdom provides an overload to Hook.insert so this is automatically done for you if the callback returns a disposable:<br>
好吧，我在这里有点作弊，在“原始”Snabbdom 中保留对一次性的引用并在元素被销毁时处理它，这有点做作，但幸运的是 Feliz.Snabbdom 为 Hook.insert 提供了重载因此，如果回调返回一次性值，则会自动为您完成此操作：<br>Html.input [
    Attr.classes [ "input"; "is-medium" ]
    Attr.value editing
    Ev.onTextChange (SetEditedDescription &gt;&gt; dispatch)
    onEnterOrEscape dispatch ApplyEdit CancelEdit

    Hook.insert(fun vnode -&gt;
        let el = vnode.elm.AsInputEl
        el.select() // Select all text

        let parentBox = findParentWithClass "box" el
        // This function attachs the event to the body
        // and returns a disposable to detach it
        BodyEv.onMouseDown(fun ev -&gt;
            if not (parentBox.contains(ev.target :?&gt; _)) then
                CancelEdit |&gt; dispatch)
    )
]
Copy<br>
Did you notice BodyEv.onMouseDown? This is another nice use-case of <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/cbf4b90de929d7202f941ef091436a8845634b80/src/Feliz.Snabbdom/Feliz.Snabbdom.fs#L163-L168" rel="noopener" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/blob/cbf4b90de929d7202f941ef091436a8845634b80/src/Feliz.Snabbdom/Feliz.Snabbdom.fs#L163-L168" target="_blank">Feliz.Engine abstract classes</a>, it implements EventEngine by making it return a disposable.<br>
你注意到 BodyEv.onMouseDown 了吗？这是 Feliz.Engine 抽象类的另一个很好的用例，它通过返回一次性对象来实现 EventEngine 。
<br>So Snabbdom is great, now what? Does this mean you need to ditch React for Fable apps? Of course not! React is still a great choice, with many useful tools and a gigantic ecosystem. It's true there are frictions with Elmish but thanks to the work of Zaid, Maxime Mangel and many others, together with the ReactComponent plugin in Fable 3 they've become more bearable. So if you already know React quirks and/or rely on some of its tools and libraries you can be sure will still be well supported by Fable. Just if you're mainly interested in Elmish and don't really care for the underlying renderer you may want to give Elmish.Snabbdom a try if you're looking for less complexity. Clone the repo and try out <a data-tooltip-position="top" aria-label="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" rel="noopener" class="external-link" href="https://github.com/alfonsogarciacaro/Feliz.Engine/tree/main/samples/Feliz.Snabbdom" target="_blank">this sample</a> to see how Elmish.Snabbdom can work for you.<br>
所以 Snabbdom 很棒，现在怎么办？这是否意味着您需要放弃 React 而使用 Fable 应用程序？当然不是！ React 仍然是一个不错的选择，拥有许多有用的工具和庞大的生态系统。确实与 Elmish 存在摩擦，但由于 Zaid、Maxime Mangel 和许多其他人的工作，再加上《神鬼寓言 3》中的 ReactComponent 插件，这些摩擦已经变得更容易忍受。因此，如果您已经了解 React 的怪癖和/或依赖它的一些工具和库，您可以肯定 Fable 仍然会提供良好的支持。如果您主要对 Elmish 感兴趣并且并不真正关心底层渲染器，如果您正在寻找较低的复杂性，您可能想尝试一下 Elmish.Snabbdom。克隆存储库并尝试此示例，看看 Elmish.Snabbdom 如何为您工作。<br>And! If you are really into a purer Fable/F# experience and want more control of the DOM, take also a look at the awesome work of David Dawkins with <a data-tooltip-position="top" aria-label="https://davedawkins.github.io/Sutil" rel="noopener" class="external-link" href="https://davedawkins.github.io/Sutil" target="_blank">Sutil</a>!<br>
和！如果您确实喜欢更纯粹的 Fable/F# 体验并希望更多地控制 DOM，还可以看看 David Dawkins 与 Sutil 的精彩作品！]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/elmish.snabbdom.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Elmish.Snabbdom.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:36 GMT</pubDate><enclosure url="https://fable.io/blog/2021/2021-03-02-Announcing-Elmish-Snabbdom.html#its-just-functions" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://fable.io/blog/2021/2021-03-02-Announcing-Elmish-Snabbdom.html#its-just-functions&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[FSharp Native AOT 的 JSON 库问题]]></title><description><![CDATA[ 
 <br>
<br>FSharp.SystemTextJson 不行
<br>FSharp.Json 不行
<br>用 FSharp.Data 的 JsonProvider 
<br>然后在 PropertyGroup 中设置 &lt;JsonSerializerIsReflectionEnabledByDefault&gt; true &lt;/JsonSerializerIsReflectionEnabledByDefault&gt;
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsharp-native-aot-的-json-库问题.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSharp Native AOT 的 JSON 库问题.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 26 May 2024 07:03:14 GMT</pubDate></item><item><title><![CDATA[FSharp Programming Scientific Models - A Step-By-Step Approach]]></title><description><![CDATA[ 
 <br>F Sharp programming offers unique advantages for scientific modeling, blending functional programming with .NET integration. This article delves into how F Sharp streamlines complex scientific computations, presenting practical examples and techniques.<br>F Sharp, a language well-suited for scientific modeling, offers unique features that enhance the development process. Its strong typing and functional-first approach streamline complex calculations and data manipulation. This article explores practical ways to leverage F Sharp in building robust scientific models, demonstrating its efficiency and effectiveness in tackling real-world problems.<br><img alt="F# Scientific Modeling Diagram" src="https://showme.redstarplugin.com/d/d:lgQImyCk" referrerpolicy="no-referrer"><br>
<br>[Fundamentals Of F Sharp In Scientific Modeling](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Fundamentals" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Fundamentals</a> Of F Sharp In Scientific Modeling)
<br>[Setting Up The F Sharp Environment For Scientific Computation](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Setting" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Setting</a> Up The F Sharp Environment For Scientific Computation)
<br>[Data Types And Structures In F Sharp For Modeling](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Data" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Data</a> Types And Structures In F Sharp For Modeling)
<br>[Functional Programming Concepts Applied In Scientific Models](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Functional" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Functional</a> Programming Concepts Applied In Scientific Models)
<br>[Building And Testing Simple Scientific Models With F Sharp](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Building" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Building</a> And Testing Simple Scientific Models With F Sharp)
<br>[Advanced Techniques In F Sharp For Complex Models](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced</a> Techniques In F Sharp For Complex Models)
<br>[Performance Optimization In F Sharp For Scientific Computing](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Performance" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Performance</a> Optimization In F Sharp For Scientific Computing)
<br>[Real-World Applications Of F Sharp In Science](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Real-World" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Real-World</a> Applications Of F Sharp In Science)
<br>[Frequently Asked Questions](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Frequently" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Frequently</a> Asked Questions)
<br><br>
<br>[Type Safety And Immutability](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type</a> Safety And Immutability)
<br>[Concise Syntax For Data Handling](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Concise" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Concise</a> Syntax For Data Handling)
<br>[Integration With .NET Libraries](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Integration" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Integration</a> With .NET Libraries)
<br>[Pattern Matching For Data Analysis](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pattern" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pattern</a> Matching For Data Analysis)
<br>[Interoperability With Other Languages](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Interoperability" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Interoperability</a> With Other Languages)
<br>F Sharp, a functional-first programming language, is particularly adept for scientific modeling. Its design emphasizes simplicity and expressiveness, crucial for dealing with complex scientific data and algorithms.<br><br>F Sharp's type safety and immutability are vital for scientific computations. Type safety prevents errors like mismatched data types, while immutability ensures data integrity throughout the modeling process.<br>let immutableValue = 5
// immutableValue &lt;- 10 // This line would cause a compilation error
Copy<br>📌<br>In this example, attempting to change immutableValue results in an error, showcasing immutability.<br><br>The language's concise syntax is beneficial for handling large datasets. F Sharp's list and array comprehensions provide a straightforward way to manipulate data sets.<br>let squaredNumbers = [1 .. 10] |&gt; List.map (fun x -&gt; x * x)
// This creates a list of squares from 1 to 10
Copy<br>📌<br>Here, we generate a list of squared numbers, demonstrating the language's ability to succinctly handle data operations.<br><br>F Sharp's seamless integration with .NET libraries extends its capabilities in scientific modeling. This allows access to a vast array of libraries for various computational tasks.<br>open System.Math
let logValue = Log(10.0) // Using Math library for logarithmic calculation
Copy<br>📌<br>This code snippet uses the .NET System.Math library to perform a logarithmic calculation, illustrating the ease of integrating external libraries.<br><br>Pattern matching in F Sharp is a powerful tool for data analysis. It simplifies the process of dissecting and understanding complex data structures.<br>let analyzeData data =
    match data with
    | "Temperature" -&gt; "Analyze temperature trends"
    | "Pressure" -&gt; "Analyze atmospheric pressure"
    | _ -&gt; "Data type not recognized"
Copy<br>📌<br>This function uses pattern matching to determine the type of data analysis needed, showcasing a structured approach to handling diverse data types.<br><br>F Sharp's interoperability with other programming languages, like C# and Python, is invaluable for scientific modeling, especially when integrating models or algorithms written in different languages.<br>// Example of calling a C# function from F Sharp
let result = CSharpLibrary.SomeFunction()
Copy<br>📌<br>This snippet demonstrates calling a function from a C# library, highlighting F Sharp's compatibility with other languages in the .NET ecosystem.<br>By leveraging these fundamental features of F Sharp, developers can efficiently and effectively tackle scientific modeling challenges.<br><br>
<br>[Installing F Sharp](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Installing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Installing</a> F Sharp)
<br>[Choosing An IDE](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Choosing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Choosing</a> An IDE)
<br>[Adding Necessary Libraries](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Adding" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Adding</a> Necessary Libraries)
<br>[Configuring The Environment](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Configuring" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Configuring</a> The Environment)
<br>[Testing The Setup](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing</a> The Setup)
<br>Setting up the F Sharp environment is the first step in leveraging its capabilities for scientific computation. The setup process is straightforward, ensuring a smooth start for developers.<br><br>Begin by installing F Sharp. It's typically included in the Visual Studio installation, but can also be installed separately for lighter IDEs or text editors.<br># For standalone installation, use the following command:
dotnet new console -lang "F#"
Copy<br>📌<br>This command initializes a new F Sharp project, setting up the necessary environment.<br><br>Select an Integrated Development Environment (IDE) or text editor. Visual Studio, Visual Studio Code, and JetBrains Rider are popular choices, each offering F Sharp support and tools for scientific computation.<br>- Visual Studio: Full-featured, ideal for large projects.
- Visual Studio Code: Lightweight, with essential features.
- JetBrains Rider: Offers cross-platform support.
Copy<br>📌<br>Each IDE has its strengths, so choose based on your project's complexity and your personal preference.<br><br>For scientific computation, add libraries like Math.NET Numerics or FSharp.Data. These libraries provide additional functions and data types useful in scientific modeling.<br>// Add Math.NET Numerics via NuGet
#r "nuget: MathNet.Numerics"
Copy<br>📌<br>This code snippet demonstrates how to reference the Math.NET Numerics library in an F Sharp script, enhancing mathematical computation capabilities.<br><br>Configure your environment for optimal performance. This includes setting up the .NET runtime and adjusting project settings for efficient execution.<br>// Example: Setting target framework in the project file
&lt;TargetFramework&gt;net5.0&lt;/TargetFramework&gt;
Copy<br>📌<br>In the project file, specify the .NET target framework to ensure compatibility and performance.<br><br>Finally, test your setup with a simple F Sharp script. This verifies that the environment is correctly configured and ready for more complex scientific computations.<br>// Test script: Calculate the square root of a number
let number = 16.0
let squareRoot = System.Math.Sqrt(number)
printfn "The square root of %f is %f" number squareRoot
Copy<br>📌<br>This script calculates the square root of a number, providing a basic test for your F Sharp setup.<br>By following these steps, you'll establish a solid foundation for developing scientific models in F Sharp, setting the stage for more advanced computations and analyses.<br><br>
<br>[Primitive Data Types](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Primitive" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Primitive</a> Data Types)
<br>[Tuples For Grouping Data](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Tuples" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Tuples</a> For Grouping Data)
<br>[Lists And Arrays](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lists" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lists</a> And Arrays)
<br>[Discriminated Unions For Complex Structures](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Discriminated" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Discriminated</a> Unions For Complex Structures)
<br>[Records For Structured Data](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Records" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Records</a> For Structured Data)
<br>Understanding Data Types And Structures in F Sharp is crucial for efficient and effective scientific modeling. F Sharp offers a range of types and structures that are particularly suited for handling complex scientific data.<br><br>At the core are primitive data types like integers, floats, and booleans. These are fundamental for any computation and data manipulation.<br>let intValue = 42 // Integer
let floatValue = 3.14 // Float
let boolValue = true // Boolean
Copy<br>📌<br>These examples illustrate the basic data types in F Sharp, forming the building blocks for more complex structures.<br><br>Tuples are used to group together values of possibly different types. They are particularly useful in scientific computations for representing complex data points.<br>let coordinates = (3.0, 4.0, 5.0) // A tuple representing 3D coordinates
Copy<br>📌<br>This tuple represents a point in 3D space, showcasing tuples' utility in grouping diverse data.<br><br>Lists and arrays are essential for handling sequences of data, common in scientific models. Lists are immutable, while arrays offer mutability and fixed size.<br>let numberList = [1; 2; 3; 4; 5] // List of numbers
let numberArray = [| 1; 2; 3; 4; 5 |] // Array of numbers
Copy<br>📌<br>These collections store sequences of numbers, illustrating their use in managing ordered data sets.<br><br>Discriminated unions provide a way to define types that can be one of several named cases, each potentially with different values and types. They are incredibly versatile for modeling complex scientific scenarios.<br>type Shape =
    | Circle of radius: float
    | Rectangle of width: float * height: float

let myShape = Circle(10.0) // Instance of a Circle
Copy<br>📌<br>Here, Shape can represent different geometric forms, demonstrating discriminated unions' flexibility in modeling diverse data types.<br><br>Records in F Sharp offer a way to define types that represent structured data, akin to classes in OOP but with immutable properties by default.<br>type ScientificData = { Temperature: float; Pressure: float }
let dataPoint = { Temperature = 23.5; Pressure = 1.01 }
Copy<br>📌<br>This record represents a scientific data point, highlighting records' utility in structuring and accessing data.<br>By leveraging these data types and structures, F Sharp enables developers to create sophisticated and efficient models for scientific computation, handling complex data with ease and clarity.<br><br>
<br>[Immutable Data Structures](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Immutable" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Immutable</a> Data Structures)
<br>[Pure Functions](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pure" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Pure</a> Functions)
<br>[Higher-Order Functions](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Higher-Order" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Higher-Order</a> Functions)
<br>[Recursion For Iterative Processes](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Recursion" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Recursion</a> For Iterative Processes)
<br>[Lazy Evaluation](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy</a> Evaluation)
<br>Incorporating Functional Programming Concepts into scientific models enhances readability, maintainability, and scalability of the code. F Sharp's functional nature makes it an ideal choice for scientific computations.<br><br>Emphasizing immutable data structures ensures data consistency and predictability. In scientific models, this aspect is crucial to maintain the integrity of data throughout computations.<br>let originalList = [1; 2; 3]
let newList = 0 :: originalList // Prepends '0' to the list
// originalList remains unchanged
Copy<br>📌<br>This example shows how immutability in F Sharp preserves the original data, preventing unintended modifications.<br><br>Utilizing pure functions that don’t have side effects and always produce the same output for the same input, leads to more predictable and testable code.<br>let square x = x * x // A pure function to square a number
Copy<br>📌<br>This square function is a typical example of a pure function, showcasing its simplicity and predictability.<br><br>Higher-order functions that take functions as parameters or return functions, allow for more abstract and powerful ways to manipulate data.<br>let applyFunction f x = f x // Applies a function 'f' to 'x'
let result = applyFunction square 5 // Passes 'square' as a parameter
Copy<br>📌<br>This demonstrates how higher-order functions can be used to apply different operations in a flexible manner.<br><br>In scientific modeling, recursive functions are often used instead of traditional loops. Recursion lends itself well to many mathematical operations and algorithms.<br>let rec factorial n = 
    if n &lt;= 1 then 1 else n * factorial (n - 1)
Copy<br>📌<br>The factorial function here uses recursion to calculate the factorial of a number, a common mathematical operation.<br><br>Lazy evaluation in F Sharp can be utilized to improve performance, especially when dealing with large datasets or complex calculations.<br>let lazyValue = lazy (System.Threading.Thread.Sleep(1000); "Computed")
// The computation is not executed until 'lazyValue' is actually used
Copy<br>📌<br>This lazy evaluation example defers the computation until the value is needed, enhancing efficiency in resource-intensive operations.<br>By applying these functional programming principles, scientific models in F Sharp become more robust, efficient, and easier to understand. These concepts are fundamental in handling the complexities and challenges of scientific computation.<br><br>
<br>[Creating A Basic Model](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Creating" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Creating</a> A Basic Model)
<br>[Implementing The Model](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Implementing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Implementing</a> The Model)
<br>[Testing The Model](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Testing</a> The Model)
<br>[Visualizing The Results](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Visualizing" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Visualizing</a> The Results)
<br>[Refining And Iterating](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Refining" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Refining</a> And Iterating)
<br>Building and testing simple scientific models in F Sharp involves creating models that are both easy to understand and effective in representing scientific concepts.<br><br>Start with defining a basic model. This could be a simple mathematical model, like a linear regression or a basic physical model representing real-world phenomena.<br>let linearRegression x = 2.0 * x + 5.0 // Simple linear regression model
Copy<br>📌<br>This linear regression model represents a basic scientific model, illustrating a relationship between variables.<br><br>Next, implement the model using F Sharp's functions and data structures. Ensure that your implementation is clear and concise.<br>let calculatePrediction x = linearRegression x
let prediction = calculatePrediction 10.0 // Predicts the value for x=10.0
Copy<br>📌<br>This code snippet uses the linear regression model to make a prediction, showcasing the implementation of the model.<br><br>Testing your model is crucial. Write test cases to verify the model's accuracy and reliability, especially for different input scenarios.<br>let testModel () =
    assert (calculatePrediction 0.0 = 5.0)
    assert (calculatePrediction 1.0 = 7.0)
Copy<br>📌<br>These tests validate the correctness of the linear regression model for given inputs, ensuring its reliability.<br><br>For better understanding and analysis, visualize the results. Utilize libraries like FSharp.Charting for creating graphs or charts.<br>open FSharp.Charting
let data = [for x in 1 .. 10 -&gt; (x, calculatePrediction (float x))]
Chart.Line(data)
Copy<br>📌<br>This visualization represents the output of the linear regression model across a range of values, aiding in analysis and interpretation.<br><br>Finally, refine and iterate your model based on test results and visualizations. Enhancing the model's accuracy and efficiency is a continuous process.<br>// Refine the model by adjusting the linear regression parameters
let refinedLinearRegression x = 2.5 * x + 4.5
Copy<br>📌<br>This adjusted model represents an iteration, improving upon the initial version based on insights gained from testing and visualization.<br>Building and testing simple scientific models in F Sharp is a process of continuous refinement, leveraging the language's capabilities for accurate and efficient scientific computation.<br><br>
<br>[Parallel Processing](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel</a> Processing)
<br>[Asynchronous Programming](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Asynchronous" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Asynchronous</a> Programming)
<br>[Type Providers](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Type</a> Providers)
<br><a data-tooltip-position="top" aria-label="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Meta-Programming" rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Meta-Programming" target="_blank">Meta-Programming</a>
<br>[Advanced Data Structures](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Advanced</a> Data Structures)
<br>Exploring Advanced Techniques In F Sharp can significantly enhance the capabilities of complex scientific models. These techniques allow for more intricate and efficient computation models.<br><br>Utilizing parallel processing capabilities in F Sharp can dramatically improve the performance of models that handle large datasets or complex calculations.<br>let computeInParallel data = 
    data |&gt; Array.Parallel.map (fun x -&gt; x * x)
// Processes each element in parallel
Copy<br>📌<br>This code demonstrates parallel mapping, where each element of the array is processed concurrently, showcasing enhanced performance.<br><br>Asynchronous programming is key for handling long-running operations without blocking the main thread, crucial in scientific simulations and data processing tasks.<br>let asyncOperation x = 
    async { return x * x }
// An asynchronous computation
Copy<br>📌<br>This asynchronous function allows for non-blocking operations, beneficial in complex models where multiple tasks run simultaneously.<br><br>F Sharp's type providers offer a way to access and manipulate different types of data seamlessly, ideal for scientific models that integrate various data sources.<br>type JsonProvider = FSharp.Data.JsonProvider&lt;Sample="sample.json"&gt;
let data = JsonProvider.GetSample()
// Parses JSON data using type provider
Copy<br>📌<br>This type provider example illustrates how to easily access and work with JSON data, reducing the complexity of data parsing and manipulation.<br><br>Meta-programming techniques, such as quotations and expression trees, allow for generating and manipulating code dynamically, enhancing the flexibility and power of models.<br>let expr = &lt;@ 2 + 2 @&gt;
// Represents an F# expression as data
Copy<br>📌<br>This expression tree can be analyzed or transformed at runtime, showcasing meta-programming's potential in building adaptable models.<br><br>Employing advanced data structures, such as trees and graphs, is essential for complex scientific models, especially in simulations and computational geometry.<br>type BinaryTree&lt;'T&gt; = 
    | Node of 'T * BinaryTree&lt;'T&gt; * BinaryTree&lt;'T&gt;
    | Leaf
// Represents a binary tree data structure
Copy<br>📌<br>This binary tree definition in F Sharp provides a foundation for building complex data structures necessary for sophisticated modeling tasks.<br>By integrating these advanced techniques, F Sharp becomes a powerful tool for constructing and managing complex scientific models, enabling deeper analysis and more robust simulations.<br><br>
<br>[Efficient Data Structures](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Efficient" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Efficient</a> Data Structures)
<br>[Lazy Computations](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Lazy</a> Computations)
<br>[Profiling And Benchmarking](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Profiling" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Profiling</a> And Benchmarking)
<br>[Parallel And Asynchronous Programming](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Parallel</a> And Asynchronous Programming)
<br>[Algorithm Optimization](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Algorithm" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Algorithm</a> Optimization)
<br>Optimizing performance in F Sharp for Scientific Computing is essential to handle large datasets and complex calculations efficiently. Several strategies can be employed to enhance the execution speed and resource usage of F Sharp programs.<br><br>Choosing the right data structures is crucial for performance. Immutable data structures are preferred for their safety, but mutable structures can be more efficient in some scenarios.<br>let mutableArray = Array.init 100000 (fun i -&gt; i * i)
// Mutable array for efficient in-place modifications
Copy<br>📌<br>This mutable array example is optimal for scenarios where in-place data modification is necessary, offering better performance than immutable structures.<br><br>Implementing lazy computations can save resources by deferring the execution of a computation until its result is actually needed.<br>let lazyValue = lazy (expensiveComputation())
// The computation is only executed when 'lazyValue' is accessed
Copy<br>📌<br>This lazy computation ensures that the resource-intensive operation is only performed when necessary.<br><br>Regularly profiling and benchmarking your code is important to identify performance bottlenecks. Tools like BenchmarkDotNet or the built-in Visual Studio profiler can be used.<br>- Profile CPU usage and memory allocation
- Identify slow functions or operations
Copy<br>📌<br>These practices help pinpoint inefficient code segments, allowing for targeted optimizations.<br><br>Leveraging parallel and asynchronous programming techniques can significantly improve the performance of CPU-bound and I/O-bound operations, respectively.<br>let computeInParallel data = 
    data |&gt; Array.Parallel.map expensiveComputation
// Parallel processing for CPU-bound tasks
Copy<br>📌<br>Parallel processing can dramatically speed up computations on large datasets by utilizing multiple CPU cores effectively.<br><br>Optimizing algorithms and logic is often the most effective way to enhance performance. Even small improvements in algorithm efficiency can lead to significant gains in large-scale computations.<br>// Optimize algorithm by reducing unnecessary computations
let optimizedCalculation x = 
    if x &lt; 10 then simpleCalculation x else complexCalculation x
Copy<br>📌<br>This example demonstrates conditional logic to choose the most efficient computation method based on the input, optimizing overall performance.<br>By employing these strategies, F Sharp's capabilities in scientific computing can be fully utilized, ensuring that models and computations are not only accurate but also efficient.<br><br>
<br><a data-tooltip-position="top" aria-label="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Bioinformatics" rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Bioinformatics" target="_blank">Bioinformatics</a>
<br>[Financial Modeling](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Financial" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Financial</a> Modeling)
<br>[Environmental Modeling](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Environmental" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Environmental</a> Modeling)
<br>[Physics Simulations](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Physics" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Physics</a> Simulations)
<br>[Astronomy And Astrophysics](<a rel="noopener" class="external-link" href="https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Astronomy" target="_blank">https://marketsplash.com/tutorials/f-sharp/f-sharp-programming-scientific-models/#Astronomy</a> And Astrophysics)
<br>F Sharp's application in science spans a wide range of fields, demonstrating its versatility and effectiveness in solving real-world problems.<br><br>In bioinformatics, F Sharp is used for processing and analyzing complex biological data. Its strong data handling capabilities make it ideal for tasks such as genome sequencing analysis.<br>let analyzeGenome sequence = 
    // Code for genome sequencing analysis
Copy<br>📌<br>This pseudocode represents the type of function you might find in bioinformatics, where F Sharp's data processing strengths are a significant asset.<br><br>F Sharp also finds extensive use in financial modeling. Its robustness and precision are essential for risk analysis, predictive modeling, and algorithmic trading.<br>let calculateRisk factors = 
    // Code for financial risk calculation
Copy<br>📌<br>In this financial model, F Sharp's ability to handle complex calculations and its precision are crucial.<br><br>In environmental science, F Sharp assists in creating models for climate change analysis, pollution tracking, and ecosystem simulations.<br>let simulateEcosystem conditions = 
    // Ecosystem simulation logic
Copy<br>📌<br>This code snippet would be typical in environmental modeling, where F Sharp's ability to process large data sets and run complex simulations is valuable.<br><br>F Sharp is also utilized in physics for simulations and calculations in areas like quantum mechanics, fluid dynamics, and materials science.<br>let modelQuantumSystem state = 
    // Quantum mechanics modeling
Copy<br>📌<br>This example represents F Sharp's application in physics, where its precision and performance are beneficial for complex simulations.<br><br>In astronomy and astrophysics, F Sharp helps in analyzing astronomical data, modeling celestial mechanics, and simulating cosmic phenomena.<br>let analyzeStarData data = 
    // Code for astronomical data analysis
Copy<br>📌<br>This pseudocode illustrates F Sharp's use in astronomy, where handling and analyzing vast amounts of data is critical.<br>Through these diverse applications, F Sharp proves to be a powerful tool in scientific computing, aiding researchers and scientists in various fields to model and analyze complex phenomena efficiently and effectively.<br>💡<br>**Case Study: Optimizing Meteorological Data Analysis with F Sharp**  <br>A leading meteorological research institute faced challenges in processing and analyzing vast amounts of weather data. Their existing system struggled with the complexity and scale of the datasets, leading to slow analysis times and less accurate weather predictions.  <br>The primary challenge was to handle large, complex datasets efficiently while maintaining high accuracy in weather prediction models. The system needed to process diverse data types, including temperature, humidity, and atmospheric pressure, and run complex simulations for forecasting.<br>🚩<br>**Solution:**  <br>The institute turned to F Sharp, a functional-first programming language known for its efficiency in handling large datasets and complex calculations. The key features of F Sharp utilized were:  <br>**Immutable Data Structures**: To maintain data integrity and facilitate safe parallel processing.<br>type WeatherData = { Temperature: float; Humidity: float; Pressure: float }
let historicalData = // Load and process historical weather data
Copy<br>🚩<br>**Parallel Processing**: For efficient handling of computationally intensive tasks like weather pattern simulation.<br>let simulateWeatherPattern data = 
    data |&gt; Array.Parallel.map analyzeWeatherData
Copy<br>😎<br>**Results:**  <br>The implementation of F Sharp led to remarkable improvements:  <br>**Increased Accuracy**: Enhanced data processing capabilities led to a 25% increase in the accuracy of weather predictions.  <br>**Reduced Processing Time**: Parallel processing decreased data analysis times by 40%, enabling faster weather forecasting.  <br>**Scalability**: The new system could scale effectively, handling larger datasets without a significant impact on performance.<br><br><br>**F Sharp** is used for a range of tasks in scientific computing, including data analysis, algorithm development, simulation, and predictive modeling. Its strong typing, functional programming features, and .NET ecosystem support make it well-suited for these applications.<br><br>**F Sharp** efficiently handles large datasets through its immutable data structures, lazy evaluation, and parallel processing capabilities. These features allow for effective management and manipulation of large volumes of data.<br><br>Yes, F Sharp can be effectively used for **machine learning and data science**. It can integrate with .NET libraries like ML.NET, and its concise syntax is suitable for data manipulation and statistical analysis.<br><br>F Sharp is highly **compatible** with other programming languages, especially those in the .NET ecosystem like C# and VB.NET. It can also interoperate with Python and other languages via libraries or APIs.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsharp-programming-scientific-models-a-step-by-step-approach.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSharp Programming Scientific Models - A Step-By-Step Approach.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:28 GMT</pubDate><enclosure url="https://showme.redstarplugin.com/d/d:lgQImyCk" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://showme.redstarplugin.com/d/d:lgQImyCk&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[FSharp Suave 跨域配置]]></title><description><![CDATA[ 
 <br>
<br><a data-tooltip-position="top" aria-label="https://www.fssnip.net/mL/title/CORS-response-with-Suave" rel="noopener" class="external-link" href="https://www.fssnip.net/mL/title/CORS-response-with-Suave" target="_blank">CORS response with Suave</a>
<br><a data-tooltip-position="top" aria-label="https://stackoverflow.com/questions/44359375/allow-multiple-headers-with-cors-in-suave" rel="noopener" class="external-link" href="https://stackoverflow.com/questions/44359375/allow-multiple-headers-with-cors-in-suave" target="_blank">Allow multiple headers with CORS in Suave</a>
<br>open Suave
open Suave.Writers
open Suave.Filters
open Suave.Operators
  
let CORS =
    addHeader "Access-Control-Allow-Origin" "*"
    &gt;=&gt; setHeader "Access-Control-Allow-Headers" "token"
    &gt;=&gt; addHeader "Access-Control-Allow-Headers" "Content-Type"
    &gt;=&gt; addHeader "Access-Control-Allow-Methods" "GET"
    
let Apps: WebPart =
    GET
    &gt;=&gt; fun context -&gt;
      context
      |&gt; (Server.CORS
          &gt;=&gt; choose
            [
              (* 需要跨域的 WebPart 放在这里 *) 
            ])

startWebServer defaultConfig Apps
Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fsharp-suave-跨域配置.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/FSharp Suave 跨域配置.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 26 May 2024 07:00:34 GMT</pubDate></item><item><title><![CDATA[Fun with L-Systems]]></title><description><![CDATA[ 
 <br>I had the great pleasure to speak at <a data-tooltip-position="top" aria-label="http://www.codemash.org/" rel="noopener" class="external-link" href="http://www.codemash.org/" target="_blank">CodeMash</a> this week, and, on my way back, ended up spending a couple of hours at the Atlanta airport waiting for my connecting flight back to the warmer climate of San Francisco – a perfect opportunity for some light-hearted coding fun. A couple of days earlier, I came across this really nice tweet, rendering the results of an L-system:<br>
{start:'FFPF',rules:{F:'PF++F[FF-F+PF+FPP][F]FFPF',P:''},'α':60} <a data-tooltip-position="top" aria-label="http://t.co/JZGDV4ghFy" rel="noopener" class="external-link" href="http://t.co/JZGDV4ghFy" target="_blank">pic.twitter.com/JZGDV4ghFy</a>
— LSystemBot 2.0 (@LSystemBot) <a data-tooltip-position="top" aria-label="https://twitter.com/LSystemBot/status/553954473694220288" rel="noopener" class="external-link" href="https://twitter.com/LSystemBot/status/553954473694220288" target="_blank">January 10, 2015</a>
<br>I ended up looking up <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/L-system" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/L-system" target="_blank">L-systems on Wikipedia</a>, and thought this would make for some fun coding exercise. In a nutshell, a L-system is a grammar. It starts with an alphabet of symbols, and a set of rules which govern how each symbol can be transformed into another chain of symbols. By applying these rules to a starting state (the initial axiom), one can evolve it into a succession of states, which can be seen as the growth of an organism. And by mapping each symbol to operations in a <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Logo_%28programming_language%29" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/Logo_%28programming_language%29" target="_blank">logo/turtle like language</a>, each generation can then be rendered as a graphic.<br>So how could we go about coding this in F#? If you are impatient, you can find the final result as a <a data-tooltip-position="top" aria-label="https://gist.github.com/mathias-brandewinder/bcbac9e92901af564055" rel="noopener" class="external-link" href="https://gist.github.com/mathias-brandewinder/bcbac9e92901af564055" target="_blank">gist here</a>.<br>First, I started with representing the core elements of an L-System with a couple of types:<br>type Symbol = | Sym of char

type State = Symbol list

type Rules = Map&lt;Symbol,State&gt;

type LSystem =
  { Axiom:State
    Rules:Rules }
Copy<br>A symbol is a char, wrapped in a single-case discriminated union, and a State is simply a list of Symbols. We define the Rules that govern the transformation of Symbols by a Map, which associates a particular Symbol with a State, and an L-System is then an Axiom (the initial State), with a collection of Rules.<br>Let’s illustrate this on the second example from the Wikipedia page, the Pythagoras tree. Our grammar contains 4 symbols, 0, 1, [ and ], we start with a 0, and we have 2 rules, (1 → 11), and (0 → 1[0]0). This can be encoded in a straightforward manner in our domain, like this:<br>let lSystem =
  { Axiom = [ Sym('0') ]
    Rules = [ Sym('1'), [ Sym('1'); Sym('1') ]
              Sym('0'), [ Sym('1'); Sym('['); Sym('0'); Sym(']'); Sym('0') ]]
            |&gt; Map.ofList }
Copy<br>Growing the organism by applying the rules is fairly straightforward: given a State, we traverse the list of Symbols, look up for each of them if there is a matching rule, and perform a substitution if it is found, leaving it unchanged otherwise:<br>(*
Growing from the original axiom
by applying the rules
*)

let applyRules (rs:Rules) (s:Symbol) =
  match (rs.TryFind s) with
  | None -&gt; [s]
  | Some(x) -&gt; x

let evolve (rs:Rules) (s:State) =
  [ for sym in s do yield! (applyRules rs sym) ]

let forward (g:LSystem) =
  let init = g.Axiom
  let gen = evolve g.Rules
  init |&gt; Seq.unfold (fun state -&gt; Some(state, gen state))

// compute nth generation of lSystem
let generation gen lSystem =
  lSystem
  |&gt; forward
  |&gt; Seq.nth gen
  |&gt; Seq.toList
Copy<br>What does this give us on the Pythagoras Tree?<br>&gt; lSystem |&gt; generation 1;;
val it : Symbol list = [Sym '1'; Sym '['; Sym '0'; Sym ']'; Sym '0']
Copy<br>Nice and crisp – that part is done. Next up, rendering. The idea here is that for each Symbol in a State, we will perform a substitution with a sequence of instructions, either a Move, drawing a line of a certain length, or a Turn of a certain Angle. We will also have a Stack, where we can Push or Pop the current position of the Turtle, so that we can for instance store the current position and direction on the stack, perform a couple of moves with a Push, and then return to the previous position by a Pop, which will reset the turtle to the previous position. Again, that lends itself to a very natural model:<br>(*
Modelling the Turtle/Logo instructions
*)

type Length = | Len of float
type Angle = | Deg of float

// override operator later
let add (a1:Angle) (a2:Angle) =
  let d1 = match a1 with Deg(x) -&gt; x
  let d2 = match a2 with Deg(x) -&gt; x
  Deg(d1+d2)

type Inst =
  | Move of Length
  | Turn of Angle
  | Push
  | Pop

let Fwd x = Move(Len(x))
let Lft x = Turn(Deg(x))
let Rgt x = Turn(Deg(-x))
Copy<br>We can now transform our L-system state into a list of instructions, and convert them into a sequence of Operations, in that case Drawing lines between 2 points:<br>type Pos = { X:float; Y:float; }
type Dir = { L:Length; A:Angle }

type Turtle = { Pos:Pos; Dir:Dir }
type ProgState = { Curr:Turtle; Stack:Turtle list }

let turn angle turtle =
  let a = turtle.Dir.A |&gt; add angle
  { turtle with Dir = { turtle.Dir with A = a } }

type Translation = Map&lt;Symbol,Inst list&gt;

type Ops = | Draw of Pos * Pos

let pi = System.Math.PI

let line (pos:Pos) (len:Length) (ang:Angle) =
  let l = match len with | Len(l) -&gt; l
  let a = match ang with | Deg(a) -&gt; (a * pi / 180.)
  { X = pos.X + l * cos a ; Y = pos.Y + l * sin a }

let execute (inst:Inst) (state:ProgState) =
  match inst with
  | Push -&gt; None, { state with Stack = state.Curr :: state.Stack }
  | Pop -&gt;
    let head::tail = state.Stack // assumes more Push than Pop
    None, { state with Curr = head; Stack = tail }
  | Turn(angle) -&gt;
    None, { state with Curr =  state.Curr |&gt; turn angle }
  | Move(len) -&gt;
    let startPoint = state.Curr.Pos
    let endPoint = line startPoint len state.Curr.Dir.A
    Some(Draw(startPoint,endPoint)), { state with Curr = { state.Curr with Pos = endPoint } }

let toTurtle (T:Translation) (xs:Symbol list) =

  let startPos = { X = 400.; Y = 400. }
  let startDir = { L = Len(0.); A = Deg(0.) }
  let init =
    { Curr = { Pos = startPos; Dir = startDir }
      Stack = [] }
  xs
  |&gt; List.map (fun sym -&gt; T.[sym])
  |&gt; List.concat
  |&gt; Seq.scan (fun (op,state) inst -&gt; execute inst state) (None,init)
  |&gt; Seq.map fst
  |&gt; Seq.choose id
Copy<br>We simply map each Symbol to a List of instructions, transform the list of symbols into a list of instructions, and maintain at each step the current position and direction, as well as a Stack (represented as a list) of positions and directions. How does it play out on our Pythagoras Tree? First, we define the mapping from Symbols to Instructions:<br>let l = 1.
let T =
  [ Sym('0'), [ Fwd l; ]
    Sym('1'), [ Fwd l; ]
    Sym('['), [ Push; Lft 45.; ]
    Sym(']'), [ Pop; Rgt 45.; ] ]
  |&gt; Map.ofList
Copy<br>… and we simply send that toTurtle, which produces a list of Draw instructions:<br>&gt; lSystem |&gt; generation 1 |&gt; toTurtle T;;
val it : seq&lt;Ops&gt; =
  seq
  [ Draw ({X = 400.0; Y = 400.0;},{X = 401.0; Y = 400.0;});
    Draw ({X = 401.0; Y = 400.0;},{X = 401.7071068; Y = 400.7071068;});
    Draw ({X = 401.0; Y = 400.0;},{X = 401.7071068; Y = 399.2928932;})]
Copy<br>Last step – some pretty pictures. We’ll simply generate a html document, rendering the image using SVG, by creating one SVG line per Draw instruction:<br>let header = """
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;body&gt;
&lt;svg height="800" width="800"&gt;"""

let footer = """
&lt;/svg&gt;
&lt;/body&gt;
&lt;/html&gt;
"""

let toSvg (ops:Ops seq) =
  let asString (op:Ops) =
    match op with
    | Draw(p1,p2) -&gt;
      sprintf """&lt;line x1="%f" y1="%f" x2="%f" y2="%f" style="stroke:rgb(0,0,0);stroke-width:1" /&gt;""" p1.X p1.Y p2.X p2.Y

  [ yield header
    for op in ops -&gt; asString op
    yield footer ]
  |&gt; String.concat "\n"

open System.IO

let path = "C:/users/mathias/desktop/lsystem.html"
let save template = File.WriteAllText(path,template)
Copy<br>And we are pretty much done:<br>&gt; lSystem |&gt; generation 8 |&gt; toTurtle T |&gt; toSvg |&gt; save;;
val it : unit = ()
Copy<br>… which produces the following graphic:<br><img alt="Pythagoras tree" src="https://mathias-brandewinder.github.io//assets/pythagoras-tree.png" referrerpolicy="no-referrer"><br>Pretty neat! Just for fun, I replicated the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/L-system#Example_5:_Sierpinski_triangle" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/L-system#Example_5:_Sierpinski_triangle" target="_blank">Sierpinski Triangle</a> example as well:<br>let sierpinski () =

  let lSystem =
    { Axiom = [ Sym('A') ]
      Rules =
        [ Sym('A'), [ Sym('B'); Sym('&gt;'); Sym('A'); Sym('&gt;'); Sym('B') ]
          Sym('B'), [ Sym('A'); Sym('&lt;'); Sym('B'); Sym('&lt;'); Sym('A') ]]
        |&gt; Map.ofList }

  let l = 1.
  let T =
    [ Sym('A'), [ Fwd l; ]
      Sym('B'), [ Fwd l; ]
      Sym('&gt;'), [ Lft 60.; ]
      Sym('&lt;'), [ Rgt 60.; ] ]
    |&gt; Map.ofList

  lSystem
  |&gt; generation 9
  |&gt; toTurtle T
  |&gt; toSvg
  |&gt; save
Copy<br>… which results in the following picture:<br><img alt="Sierpinski triangle" src="https://mathias-brandewinder.github.io//assets/sierpinski-triangle.png" referrerpolicy="no-referrer"><br>That’s it for tonight! I had a lot of fun coding this (it certainly made the flight less boring), and found the idea of converting code to turtle instructions, with a stack, pretty interesting. Hope you enjoyed it, and if you end up playing with this, share your creations on Twitter and ping me!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/fun-with-l-systems.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Fun with L-Systems.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:35 GMT</pubDate><enclosure url="https://mathias-brandewinder.github.io//assets/pythagoras-tree.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://mathias-brandewinder.github.io//assets/pythagoras-tree.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Functional Reactive Programming]]></title><description><![CDATA[ 
 <br>Events are everywhere. Almost every program has to handle events, whether it be button clicks in the user interface, listening to sockets in a server, or even a system shutdown notification.<br>And events are the basis of one of the most common OO design patterns: the “Observer” pattern.<br>But as we know, event handling, like concurrency in general, can be tricky to implement. Simple event logic is straightforward, but what about logic like “do something if two events happen in a row but do something different if only one event happens” or “do something if two events happen at roughly the same time”. And how easy is it to combine these requirements in other, more complex ways?<br>Even if you can successfully implement these requirements, the code tends to be spaghetti like and hard to understand, even with the best intentions.<br>Is there an approach that can make event handling easier?<br>We saw in the previous post on message queues that one of the advantages of that approach was that the requests were “serialized” making it conceptually easier to deal with.<br>There is a similar approach that can be used with events. The idea is to turn a series of events into an “event stream”. Event streams then become quite like IEnumerables, and so the obvious next step is to treat them in much the the same way that LINQ handles collections, so that they can be filtered, mapped, split and combined.<br>F# has built in support for this model, as well as for the more traditional approach.<br><br>Let’s start with a simple example to compare the two approaches. We’ll implement the classic event handler approach first.<br>First, we define a utility function that will:<br>
<br>create a timer
<br>register a handler for the Elapsed event
<br>run the timer for five seconds and then stop it
<br>Here’s the code:<br>open System
open System.Threading

/// create a timer and register an event handler,
/// then run the timer for five seconds
let createTimer timerInterval eventHandler =
    // setup a timer
    let timer = new System.Timers.Timer(float timerInterval)
    timer.AutoReset &lt;- true

    // add an event handler
    timer.Elapsed.Add eventHandler

    // return an async task
    async {
        // start timer...
        timer.Start()
        // ...run for five seconds...
        do! Async.Sleep 5000
        // ... and stop
        timer.Stop()
        }
Copy<br>Now test it interactively:<br>// create a handler. The event args are ignored
let basicHandler _ = printfn "tick %A" DateTime.Now

// register the handler
let basicTimer1 = createTimer 1000 basicHandler

// run the task now
Async.RunSynchronously basicTimer1
Copy<br>Now let’s create a similar utility method to create a timer, but this time it will return an “observable” as well, which is the stream of events.<br>let createTimerAndObservable timerInterval =
    // setup a timer
    let timer = new System.Timers.Timer(float timerInterval)
    timer.AutoReset &lt;- true

    // events are automatically IObservable
    let observable = timer.Elapsed

    // return an async task
    let task = async {
        timer.Start()
        do! Async.Sleep 5000
        timer.Stop()
        }

    // return a async task and the observable
    (task,observable)
Copy<br>And again test it interactively:<br>// create the timer and the corresponding observable
let basicTimer2 , timerEventStream = createTimerAndObservable 1000

// register that every time something happens on the
// event stream, print the time.
timerEventStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "tick %A" DateTime.Now)

// run the task now
Async.RunSynchronously basicTimer2
Copy<br>The difference is that instead of registering a handler directly with an event, we are “subscribing” to an event stream. Subtly different, and important.<br><br>In this next example, we’ll have a slightly more complex requirement:<br>Create a timer that ticks every 500ms.
At each tick, print the number of ticks so far and the current time.
Copy<br>To do this in a classic imperative way, we would probably create a class with a mutable counter, as below:<br>type ImperativeTimerCount() =

    let mutable count = 0

    // the event handler. The event args are ignored
    member this.handleEvent _ =
      count &lt;- count + 1
      printfn "timer ticked with count %i" count
Copy<br>We can reuse the utility functions we created earlier to test it:<br>// create a handler class
let handler = new ImperativeTimerCount()

// register the handler method
let timerCount1 = createTimer 500 handler.handleEvent

// run the task now
Async.RunSynchronously timerCount1
Copy<br>Let’s see how we would do this same thing in a functional way:<br>// create the timer and the corresponding observable
let timerCount2, timerEventStream = createTimerAndObservable 500

// set up the transformations on the event stream
timerEventStream
|&gt; Observable.scan (fun count _ -&gt; count + 1) 0
|&gt; Observable.subscribe (fun count -&gt; printfn "timer ticked with count %i" count)

// run the task now
Async.RunSynchronously timerCount2
Copy<br>Here we see how you can build up layers of event transformations, just as you do with list transformations in LINQ.<br>The first transformation is scan, which accumulates state for each event. It is roughly equivalent to the List.fold function that we have seen used with lists. In this case, the accumulated state is just a counter.<br>And then, for each event, the count is printed out.<br>Note that in this functional approach, we didn’t have any mutable state, and we didn’t need to create any special classes.<br><br>For a final example, we’ll look at merging multiple event streams.<br>Let’s make a requirement based on the well-known “FizzBuzz” problem:<br>Create two timers, called '3' and '5'. The '3' timer ticks every 300ms and the '5' timer ticks
every 500ms.

Handle the events as follows:
a) for all events, print the id of the time and the time
b) when a tick is simultaneous with a previous tick, print 'FizzBuzz'
otherwise:
c) when the '3' timer ticks on its own, print 'Fizz'
d) when the '5' timer ticks on its own, print 'Buzz'
Copy<br>First let’s create some code that both implementations can use.<br>We’ll want a generic event type that captures the timer id and the time of the tick.<br>type FizzBuzzEvent = {label:int; time: DateTime}
Copy<br>And then we need a utility function to see if two events are simultaneous. We’ll be generous and allow a time difference of up to 50ms.<br>let areSimultaneous (earlierEvent,laterEvent) =
    let {label=_;time=t1} = earlierEvent
    let {label=_;time=t2} = laterEvent
    t2.Subtract(t1).Milliseconds &lt; 50
Copy<br>In the imperative design, we’ll need to keep track of the previous event, so we can compare them. And we’ll need special case code for the first time, when the previous event doesn’t exist<br>type ImperativeFizzBuzzHandler() =

    let mutable previousEvent: FizzBuzzEvent option = None

    let printEvent thisEvent  =
      let {label=id; time=t} = thisEvent
      printf "[%i] %i.%03i " id t.Second t.Millisecond
      let simultaneous = previousEvent.IsSome &amp;&amp; areSimultaneous (previousEvent.Value,thisEvent)
      if simultaneous then printfn "FizzBuzz"
      elif id = 3 then printfn "Fizz"
      elif id = 5 then printfn "Buzz"

    member this.handleEvent3 eventArgs =
      let event = {label=3; time=DateTime.Now}
      printEvent event
      previousEvent &lt;- Some event

    member this.handleEvent5 eventArgs =
      let event = {label=5; time=DateTime.Now}
      printEvent event
      previousEvent &lt;- Some event
Copy<br>Now the code is beginning to get ugly fast! Already we have mutable state, complex conditional logic, and special cases, just for such a simple requirement.<br>Let’s test it:<br>// create the class
let handler = new ImperativeFizzBuzzHandler()

// create the two timers and register the two handlers
let timer3 = createTimer 300 handler.handleEvent3
let timer5 = createTimer 500 handler.handleEvent5

// run the two timers at the same time
[timer3;timer5]
|&gt; Async.Parallel
|&gt; Async.RunSynchronously
Copy<br>It does work, but are you sure the code is not buggy? Are you likely to accidentally break something if you change it?<br>The problem with this imperative code is that it has a lot of noise that obscures the the requirements.<br>Can the functional version do better? Let’s see!<br>First, we create two event streams, one for each timer:<br>let timer3, timerEventStream3 = createTimerAndObservable 300
let timer5, timerEventStream5 = createTimerAndObservable 500
Copy<br>Next, we convert each event on the “raw” event streams into our FizzBuzz event type:<br>// convert the time events into FizzBuzz events with the appropriate id
let eventStream3  =
   timerEventStream3
   |&gt; Observable.map (fun _ -&gt; {label=3; time=DateTime.Now})

let eventStream5  =
   timerEventStream5
   |&gt; Observable.map (fun _ -&gt; {label=5; time=DateTime.Now})
Copy<br>Now, to see if two events are simultaneous, we need to compare them from the two different streams somehow.<br>It’s actually easier than it sounds, because we can:<br>
<br>combine the two streams into a single stream:
<br>then create pairs of sequential events
<br>then test the pairs to see if they are simultaneous
<br>then split the input stream into two new output streams based on that test
<br>Here’s the actual code to do this:<br>// combine the two streams
let combinedStream =
    Observable.merge eventStream3 eventStream5

// make pairs of events
let pairwiseStream =
   combinedStream |&gt; Observable.pairwise

// split the stream based on whether the pairs are simultaneous
let simultaneousStream, nonSimultaneousStream =
    pairwiseStream |&gt; Observable.partition areSimultaneous
Copy<br>Finally, we can split the nonSimultaneousStream again, based on the event id:<br>// split the non-simultaneous stream based on the id
let fizzStream, buzzStream  =
    nonSimultaneousStream
    // convert pair of events to the first event
    |&gt; Observable.map (fun (ev1,_) -&gt; ev1)
    // split on whether the event id is three
    |&gt; Observable.partition (fun {label=id} -&gt; id=3)
Copy<br>Let’s review so far. We have started with the two original event streams and from them created four new ones:<br>
<br>combinedStream contains all the events
<br>simultaneousStream contains only the simultaneous events
<br>fizzStream contains only the non-simultaneous events with id=3
<br>buzzStream contains only the non-simultaneous events with id=5
<br>Now all we need to do is attach behavior to each stream:<br>//print events from the combinedStream
combinedStream
|&gt; Observable.subscribe (fun {label=id;time=t} -&gt;
                              printf "[%i] %i.%03i " id t.Second t.Millisecond)

//print events from the simultaneous stream
simultaneousStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "FizzBuzz")

//print events from the nonSimultaneous streams
fizzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Fizz")

buzzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Buzz")
Copy<br>Let’s test it:<br>// run the two timers at the same time
[timer3;timer5]
|&gt; Async.Parallel
|&gt; Async.RunSynchronously
Copy<br>Here’s all the code in one complete set:<br>// create the event streams and raw observables
let timer3, timerEventStream3 = createTimerAndObservable 300
let timer5, timerEventStream5 = createTimerAndObservable 500

// convert the time events into FizzBuzz events with the appropriate id
let eventStream3  = timerEventStream3
                    |&gt; Observable.map (fun _ -&gt; {label=3; time=DateTime.Now})
let eventStream5  = timerEventStream5
                    |&gt; Observable.map (fun _ -&gt; {label=5; time=DateTime.Now})

// combine the two streams
let combinedStream =
   Observable.merge eventStream3 eventStream5

// make pairs of events
let pairwiseStream =
   combinedStream |&gt; Observable.pairwise

// split the stream based on whether the pairs are simultaneous
let simultaneousStream, nonSimultaneousStream =
   pairwiseStream |&gt; Observable.partition areSimultaneous

// split the non-simultaneous stream based on the id
let fizzStream, buzzStream  =
    nonSimultaneousStream
    // convert pair of events to the first event
    |&gt; Observable.map (fun (ev1,_) -&gt; ev1)
    // split on whether the event id is three
    |&gt; Observable.partition (fun {label=id} -&gt; id=3)

//print events from the combinedStream
combinedStream
|&gt; Observable.subscribe (fun {label=id;time=t} -&gt;
                              printf "[%i] %i.%03i " id t.Second t.Millisecond)

//print events from the simultaneous stream
simultaneousStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "FizzBuzz")

//print events from the nonSimultaneous streams
fizzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Fizz")

buzzStream
|&gt; Observable.subscribe (fun _ -&gt; printfn "Buzz")

// run the two timers at the same time
[timer3;timer5]
|&gt; Async.Parallel
|&gt; Async.RunSynchronously
Copy<br>The code might seem a bit long winded, but this kind of incremental, step-wise approach is very clear and self-documenting.<br>Some of the benefits of this style are:<br>
<br>I can see that it meets the requirements just by looking at it, without even running it. Not so with the imperative version.
<br>From a design point of view, each final “output” stream follows the single responsibility principle – it only does one thing – so it is very easy to associate behavior with it.
<br>This code has no conditionals, no mutable state, no edge cases. It would be easy to maintain or change, I hope.
<br>It is easy to debug. For example, I could easily “tap” the output of the simultaneousStream to see if it contains what I think it contains:
<br>// debugging code
//simultaneousStream |&gt; Observable.subscribe (fun e -&gt; printfn "sim %A" e)
//nonSimultaneousStream |&gt; Observable.subscribe (fun e -&gt; printfn "non-sim %A" e)
Copy<br>This would be much harder in the imperative version.<br><br>Functional Reactive Programming (known as FRP) is a big topic, and we’ve only just touched on it here. I hope this introduction has given you a glimpse of the usefulness of this way of doing things.<br>If you want to learn more, see the documentation for the F# <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/ee370313%28v=vs.100%29?redirectedfrom=MSDN" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/ee370313%28v=vs.100%29?redirectedfrom=MSDN" target="_blank">Observable module</a>, which has the basic transformations used above. And there is also the <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/previous-versions/dotnet/reactive-extensions/hh242985%28v=vs.103%29" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/previous-versions/dotnet/reactive-extensions/hh242985%28v=vs.103%29" target="_blank">Reactive Extensions (Rx)</a> library which shipped as part of .NET 4. That contains many other additional transformations.<br>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/functional-reactive-programming.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Functional Reactive Programming.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:32 GMT</pubDate></item><item><title><![CDATA[K-Means clustering in FSharp]]></title><description><![CDATA[ 
 <br>Machine Learning in Action, in F#<br>Porting <a data-tooltip-position="top" aria-label="http://www.manning.com/pharrington/" rel="noopener" class="external-link" href="http://www.manning.com/pharrington/" target="_blank">Machine Learning in Action</a> from Python to F#<br>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/07/29/Nearest-Neighbor-Classification-part-1/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/07/29/Nearest-Neighbor-Classification-part-1/" target="_blank">KNN classification (1)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/08/01/Nearest-Neighbor-Classification-part-2/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/08/01/Nearest-Neighbor-Classification-part-2/" target="_blank">KNN classification (2)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/08/05/Decision-Tree-classification/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/08/05/Decision-Tree-classification/" target="_blank">Decision Tree classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/08/18/Naive-Bayes-classification/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/08/18/Naive-Bayes-classification/" target="_blank">Naive Bayes classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/09/30/Logistic-Regression/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/09/30/Logistic-Regression/" target="_blank">Logistic Regression classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/11/25/Support-Vector-Machine-in-FSharp-work-in-progress/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/11/25/Support-Vector-Machine-in-FSharp-work-in-progress/" target="_blank">SVM classification (1)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/12/26/Support-Vector-Machine-in-FSharp/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/12/26/Support-Vector-Machine-in-FSharp/" target="_blank">SVM classification (2)</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2012/12/29/AdaBoost-classifier-in-FSharp/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2012/12/29/AdaBoost-classifier-in-FSharp/" target="_blank">AdaBoost classification</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2013/02/10/K-Means-Clustering-in-FSharp/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2013/02/10/K-Means-Clustering-in-FSharp/" target="_blank">K-Means clustering</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2013/03/25/Simplify-data-with-SVD-and-MathNET-in-FSharp/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2013/03/25/Simplify-data-with-SVD-and-MathNET-in-FSharp/" target="_blank">SVD</a>
<br><a data-tooltip-position="top" aria-label="https://mathias-brandewinder.github.io//2013/04/28/Recommendation-Engine-with-SVD-and-MathNET-in-FSharp/" rel="noopener" class="external-link" href="https://mathias-brandewinder.github.io//2013/04/28/Recommendation-Engine-with-SVD-and-MathNET-in-FSharp/" target="_blank">Recommendation engine</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action" target="_blank">Code on GitHub</a>
<br>And the Journey converting “<a data-tooltip-position="top" aria-label="http://www.manning.com/pharrington/" rel="noopener" class="external-link" href="http://www.manning.com/pharrington/" target="_blank">Machine Learning in Action</a>” from Python to F# continues! Rather than following the order of the book, I decided to skip chapters 8 and 9, dedicated to regression methods (regression is something I spent a bit too much time doing in the past to be excited about it just right now), and go straight to Unsupervised Learning, which begins with the K-means clustering algorithm. So what is clustering about? In a nutshell, clustering focuses on the following question: given a set of observations, can the computer figure out a way to classify them into “meaningful groups”? The major difference with Classification methods is that in clustering, the Categories / Groups are initially unknown: it’s the algorithm’s job to figure out sensible ways to group items into Clusters, all by itself (hence the word “unsupervised”). Chapter 10 covers 2 clustering algorithms, k-means , and bisecting k-means. We’ll discuss only the first one today. The underlying idea behind the k-means algorithm is to identify k “representative archetypes” (k being a user input), the Centroids. The algorithm proceeds iteratively:<br>
Starting from k random Centroids,<br>
Observations are assigned to the closest Centroid, and constitute a Cluster,<br>
Centroids are updated, by taking the average of their Cluster,<br>
Until the allocation of Observation to Clusters doesn’t change any more.
<br>When things go well, we end up with k stable Centroids (minimal modification of Centroids do not change the Clusters), and Clusters contain Observations that are similar, because they are all close to the same Centroid (The <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" target="_blank">wikipedia page</a> for the algorithm provides a nice graphical representation).<br><br>The Python implementation proposed in the book is both very procedural and deals with Observations that are vectors. I thought it would be interesting to take a different approach, focused on functions instead. The current implementation is likely to change when I get into bisecting k-means, but should remain similar in spirit. Note also that I have given no focus to performance – this is my take on the easiest thing that would work. The entire code can be found <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/KMeansClustering.fs" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/KMeansClustering.fs" target="_blank">here on GitHub</a>. Here is how I approached the problem. First, rather than restricting ourselves to vectors, suppose we want to deal with any generic type. Looking at the pseudo-code above, we need a few functions to implement the algorithm:<br>
<br>to assign Observations of type 'a to the closest Centroid 'a, we need a notion of Distance,
<br>we need to create an initial collection of k Centroids of type 'a, given a dataset of 'as,
<br>to update the Centroids based on a Cluster of 'as, we need some aggregation function.
<br>Let’s create these 3 functions:<br>// the Distance between 2 observations 'a is a float
// It also better be positive - left to the implementer
type Distance&lt;'a&gt; = 'a -&gt; 'a -&gt; float
// CentroidsFactory, given a dataset, 
// should generate n Centroids
type CentroidsFactory&lt;'a&gt; = 'a seq -&gt; int -&gt; 'a seq
// Given a Centroid and observations in a Cluster,
// create an updated Centroid
type ToCentroid&lt;'a&gt; = 'a -&gt; 'a seq -&gt; 'a
Copy<br>We can now define a function which, given a set of Centroids, will return the index of the closest Centroid to an Observation, as well as the distance from the Centroid to the Observation:<br>// Returns the index of and distance to the 
// Centroid closest to observation
let closest (dist: Distance&lt;'a&gt;) centroids (obs: 'a) =
    centroids
    |&gt; Seq.mapi (fun i c -&gt; (i, dist c obs)) 
    |&gt; Seq.minBy (fun (i, d) -&gt; d)
Copy<br>Finally, we’ll go for the laziest possible way to generate k initial Centroids, by picking up k random observations from our dataset:<br>// Picks k random observations as initial centroids
// (this is very lazy, even tolerates duplicates)
let randomCentroids&lt;'a&gt; (rng: System.Random) 
                        (sample: 'a seq) 
                        k =
    let size = Seq.length sample
    seq { for i in 1 .. k do 
            let pick = Seq.nth (rng.Next(size)) sample
            yield pick }
Copy<br>We have all we need – we can now write the algorithm itself:<br>// Given a distance, centroid factory and
// centroid aggregation function, identify
// the k centroids of a dataset
let kmeans (dist: Distance&lt;'a&gt;) 
           (factory: CentroidsFactory&lt;'a&gt;) 
           (aggregator: ToCentroid&lt;'a&gt;)
           (dataset: 'a seq) 
           k =
    // Recursively update Centroids and
    // the assignment of observations to Centroids
    let rec update (centroids, assignment) =
        // Assign each point to the closest centroid
        let next = 
            dataset 
            |&gt; Seq.map (fun obs -&gt; closest dist centroids obs)
            |&gt; Seq.toList
        // Check if any assignment changed
        let change =
            match assignment with
            | Some(previous) -&gt; 
                Seq.zip previous next    
                |&gt; Seq.exists (fun ((i, _), (j, _)) -&gt; not (i = j))
            | None -&gt; true // initially we have no assignment
        if change 
        then 
            // Update each Centroid position:
            // extract cluster of points assigned to each Centroid
            // and compute the new Centroid by aggregating cluster
            let updatedCentroids =
                let assignedDataset = Seq.zip dataset next
                centroids 
                |&gt; Seq.mapi (fun i centroid -&gt; 
                    assignedDataset 
                    |&gt; Seq.filter (fun (_, (ci, _)) -&gt; ci = i)
                    |&gt; Seq.map (fun (obs, _) -&gt; obs)
                    |&gt; aggregator centroid)
            // Perform another round of updates
            update (updatedCentroids, Some(next))
        // No assignment changed, we are done
        else (centroids, next)

    let initialCentroids = factory dataset k
    let centroids = update (initialCentroids, None) |&gt; fst |&gt; Seq.toList        
    let classifier = fun datapoint -&gt; 
        centroids 
        |&gt; List.minBy (fun centroid -&gt; dist centroid datapoint)
    centroids, classifier
Copy<br>The meat of the algorithm is the update function. It takes in a set of current Centroids, and an optional Assignment of Observations to Centroids, represented as a list, mapping each Observation to Centroid indexes and corresponding distance. Note that we could drop the distance for the assignment – it’s never used afterwards, I added it prematurely because it is needed in the bissecting k-means algorithm.<br>The update function is recursive – it computes what Centroid / Cluster each observation will be assigned to next, checks whether any Observation has been assigned to a different Cluster than before (or if there is an assignment at all, to cover the initial case when no assignment has been computed yet). If a change occurred, new Centroids are computed and we go for another round, and otherwise we are done.<br>The outer function calls update, and once it terminates, returns the Centroids that have been identified, as well as a Classifier function, which will return the closest Centroid to an Observation.<br><br>I created two small examples illustrating the algorithm in action: one classic, with numeric observations, and one “just for kicks”, attempting to cluster a collection of strings. Both can be found in the file <a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/Chapter10.fsx" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/blob/463cc43a5870cc8253bbf8b608800cb8380404b6/MachineLearningInAction/MachineLearningInAction/Chapter10.fsx" target="_blank">Chapter10.fsx</a>.<br>The classic case operates on an artificially created dataset: we generate 3 points in 3 dimensions, and a collection of 50 points randomly generated in spheres around these 3 points:<br>let rng = new System.Random()
let centroids = [ [| 0.; 0.; 0. |]; [| 20.; 30.; 40. |]; [| -40.; -50.; -60. |] ]
// Create 50 points centered around each Centroid
let data = [ 
    for centroid in centroids do
        for i in 1 .. 50 -&gt; 
            Array.map (fun x -&gt; x + 5. * (rng.NextDouble() - 0.5)) centroid ]
Copy<br>If everything works correctly, we expect the algorithm to identify 3 Centroids close to the 3 points we used as anchor points for our data sample. We need to define 2 functions, which are included in the main module: a Distance, and a function to compute a Centroid from a Cluster of Observations:<br>// Euclidean distance between 2 points, represented as float []
let euclidean x y = 
    Array.fold2 (fun d e1 e2 -&gt; d + pown (e1 - e2) 2) 0. x y 
    |&gt; sqrt

// Recompute Centroid as average of given sample
let avgCentroid (current: float []) (sample: float [] seq) =
    let size = Seq.length sample
    match size with
    | 0 -&gt; current
    | _ -&gt;
        sample
        |&gt; Seq.reduce (fun v1 v2 -&gt; 
               Array.map2 (fun v1x v2x -&gt; v1x + v2x) v1 v2)
        |&gt; Array.map (fun e -&gt; e / (float)size)
Copy<br>Armed with this, we can run the algorithm:<br>let factory = randomCentroids&lt;float[]&gt; rng
let identifiedCentroids, classifier = kmeans euclidean factory avgCentroid data 3
printfn "Centroids identified"
identifiedCentroids 
|&gt; List.iter (fun c -&gt; 
    printfn ""
    printf "Centroid: "
    Array.iter (fun x -&gt; printf "%.2f " x) c)
Copy<br>On my machine, this produces the following:<br>Centroids identified  
Centroid: 19.93 30.32 39.89  
Centroid: -39.98 -50.10 -59.69  
Centroid: -0.28 0.43 -0.01
Copy<br>The 3 centroids are exactly what we expect – 3 points close to {20; 30; 40}, {-40; –50; -60} and {0; 0; 0}. Things seem to be working.<br>Now I was curious to see if this would be usable on something completely different, like strings. As usual, in order to make that work, we need a Distance, and a way to reduce a Cluster to a Centroid. The most obvious choice for a Distance between strings is the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/Levenshtein_distance" target="_blank">Levenshtein distance</a>, which measures how many edits are required to transform a string into another. Fortunately for me, someone already provided an <a data-tooltip-position="top" aria-label="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" rel="noopener" class="external-link" href="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" target="_blank">implementation in F#</a>, which I shamelessly lifted.<br>The Centroid update question required a bit of thinking. Obviously, computing the average of strings isn’t going to work – so how could we find a good “representative string” from a Cluster? I decided to go for something fairly simple: pick the string in the Cluster which has the least worst-case distance to all the others (as an alternative, I also tried picking the string with the lowest sum of squares distance, which produced similar results).<br>Finally, I created a sample, using a collection of 53 words sharing three different roots: “GRAPH”, “SCRIPT” and “GRAM”. Results vary from run to run (not surprisingly, the algorithm often struggles to separate GRAPH and GRAM words), but overall I was pleasantly surprised by the results:<br>Words identified
TELEGRAPHIC
RADIOGRAM
PRESCRIPTIVE

Classification of sample words
AUTOBIOGRAPHER -&gt; TELEGRAPHIC
AUTOBIOGRAPHICAL -&gt; TELEGRAPHIC
AUTOBIOGRAPHY -&gt; TELEGRAPHIC
AUTOGRAPH -&gt; RADIOGRAM
BIBLIOGRAPHIC -&gt; TELEGRAPHIC
BIBLIOGRAPHY -&gt; TELEGRAPHIC
CALLIGRAPHY -&gt; TELEGRAPHIC
CARTOGRAPHY -&gt; RADIOGRAM
CRYPTOGRAPHY -&gt; RADIOGRAM
GRAPH -&gt; TELEGRAPHIC
HISTORIOGRAPHY -&gt; TELEGRAPHIC
PARAGRAPH -&gt; TELEGRAPHIC
SEISMOGRAPH -&gt; TELEGRAPHIC
STENOGRAPHER -&gt; TELEGRAPHIC
TELEGRAPH -&gt; TELEGRAPHIC
TELEGRAPHIC -&gt; TELEGRAPHIC
BIBLIOGRAPHICAL -&gt; TELEGRAPHIC
STEREOGRAPH -&gt; TELEGRAPHIC
DESCRIBABLE -&gt; PRESCRIPTIVE
DESCRIBE -&gt; PRESCRIPTIVE
DESCRIBER -&gt; PRESCRIPTIVE
DESCRIPTION -&gt; PRESCRIPTIVE
DESCRIPTIVE -&gt; PRESCRIPTIVE
INDESCRIBABLE -&gt; PRESCRIPTIVE
INSCRIBE -&gt; PRESCRIPTIVE
INSCRIPTION -&gt; PRESCRIPTIVE
POSTSCRIPT -&gt; PRESCRIPTIVE
PRESCRIBE -&gt; PRESCRIPTIVE
PRESCRIPTION -&gt; PRESCRIPTIVE
PRESCRIPTIVE -&gt; PRESCRIPTIVE
SCRIBAL -&gt; RADIOGRAM
SCRIBBLE -&gt; PRESCRIPTIVE
SCRIBE -&gt; PRESCRIPTIVE
SCRIBBLER -&gt; RADIOGRAM
SCRIPT -&gt; PRESCRIPTIVE
SCRIPTURE -&gt; PRESCRIPTIVE
SCRIPTWRITER -&gt; PRESCRIPTIVE
SUPERSCRIPT -&gt; PRESCRIPTIVE
TRANSCRIBE -&gt; PRESCRIPTIVE
TYPESCRIPT -&gt; PRESCRIPTIVE
TRANSCRIPTION -&gt; PRESCRIPTIVE
DESCRIPTOR -&gt; PRESCRIPTIVE
ANAGRAM -&gt; RADIOGRAM
CABLEGRAM -&gt; RADIOGRAM
CRYPTOGRAM -&gt; RADIOGRAM
GRAMMAR -&gt; RADIOGRAM
GRAMMARIAN -&gt; RADIOGRAM
GRAMMATICAL -&gt; RADIOGRAM
MONOGRAM -&gt; RADIOGRAM
RADIOGRAM -&gt; RADIOGRAM
TELEGRAM -&gt; TELEGRAPHIC
UNGRAMMATICAL -&gt; TELEGRAPHIC
AEROGRAM -&gt; RADIOGRAM
Copy<br>That’s it for today! In our next “ML in Action” episode, we’ll look into the bissecting k-means algorithm, which is a variation on today’s algorithm, and probably revisit the implementation. In the meanwhile, feel free to leave comments or feedback!<br><br><a data-tooltip-position="top" aria-label="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/tree/463cc43a5870cc8253bbf8b608800cb8380404b6" rel="noopener" class="external-link" href="https://github.com/mathias-brandewinder/Machine-Learning-In-Action/tree/463cc43a5870cc8253bbf8b608800cb8380404b6" target="_blank">Source code on GitHub</a>: the relevant code is in the files KMeansClustering.fs and Chapter10.fsx.<br><a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm" target="_blank">K-means algorithm on Wikipedia</a>.<br><a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/Levenshtein_distance" target="_blank">Levenshtein distance on Wikipedia</a>, and an <a data-tooltip-position="top" aria-label="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" rel="noopener" class="external-link" href="http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance#F.23" target="_blank">F# implementation of Levenshtein distance</a>.<br><a data-tooltip-position="top" aria-label="http://richardminerich.com/2012/09/levenshtein-distance-and-the-triangle-inequality/" rel="noopener" class="external-link" href="http://richardminerich.com/2012/09/levenshtein-distance-and-the-triangle-inequality/" target="_blank">Interesting discussion on the Levenshtein distance</a> on <a data-tooltip-position="top" aria-label="https://twitter.com/rickasaurus" rel="noopener" class="external-link" href="https://twitter.com/rickasaurus" target="_blank">@Rickasaurus</a>’ blog.<br>Another <a data-tooltip-position="top" aria-label="http://tech.blinemedical.com/k-means-step-by-step-in-f/" rel="noopener" class="external-link" href="http://tech.blinemedical.com/k-means-step-by-step-in-f/" target="_blank">K-means implementation in F#,</a> from <a data-tooltip-position="top" aria-label="https://twitter.com/devshorts" rel="noopener" class="external-link" href="https://twitter.com/devshorts" target="_blank">@DevShorts</a>.<br><a data-tooltip-position="top" aria-label="http://www.learnthat.org/pages/view/roots.html" rel="noopener" class="external-link" href="http://www.learnthat.org/pages/view/roots.html" target="_blank">Root Words</a>: an intriguing web page, providing help to learn words and vocabulary, which contains a list of words roots. It has one incredibly annoying feature – you can’t copy paste text from the page.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/k-means-clustering-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/K-Means clustering in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:33 GMT</pubDate></item><item><title><![CDATA[Learning from mistakes - Winnow algorithm in FSharp]]></title><description><![CDATA[ 
 <br>During some recent meanderings through the confines of the internet, I ended up discovering the <a data-tooltip-position="top" aria-label="http://www.cc.gatech.edu/~ninamf/ML11/lect0906.pdf" rel="noopener" class="external-link" href="http://www.cc.gatech.edu/~ninamf/ML11/lect0906.pdf" target="_blank">Winnow Algorithm</a>. The simplicity of the approach intrigued me, so I thought it would be interesting to try and implement it in F# and see how well it worked.<br>The purpose of the algorithm is to train a binary classifier, based on binary features. In other words, the goal is to predict one of two states, using a collection of features which are all binary. The prediction model assigns weights to each feature; to predict the state of an observation, it checks all the features that are “active” (true), and sums up the weights assigned to these features. If the total is above a certain threshold, the result is true, otherwise it’s false. Dead simple – and so is the corresponding F# code:<br>type Observation = bool []
type Label = bool
type Example = Label * Observation
type Weights = float []
 
let predict (theta:float) (w:Weights) (obs:Observation) =
    (obs,w) ||&gt; Seq.zip
    |&gt; Seq.filter fst
    |&gt; Seq.sumBy snd
    |&gt; ((&lt;) theta)
Copy<br>We create some type aliases for convenience, and write a predict function which takes in theta (the threshold), weights and and observation; we zip together the features and the weights, exclude the pairs where the feature is not active, sum the weights, check whether the threshold is lower that the total, and we are done.<br>In a nutshell, the learning process feeds examples (observations with known label), and progressively updates the weights when the model makes mistakes. If the current model predicts the output correctly, don’t change anything. If it predicts true but should predict false, it is over-shooting, so weights that were used in the prediction (i.e. the weights attached to active features) are reduced. Conversely, if the prediction is false but the correct result should be true, the active features are not used enough to reach the threshold, so they should be bumped up.<br>And that’s pretty much it – the algorithm starts with arbitrary initial weights of 1 for every feature, and either doubles or halves them based on the mistakes. Again, the F# implementation is completely straightforward. The weights update can be written as follows:<br>let update (theta:float) (alpha:float) (w:Weights) (ex:Example) =
    let real,obs = ex
    match (real,predict theta w obs) with
    | (true,false) -&gt; w |&gt; Array.mapi (fun i x -&gt; if obs.[i] then alpha * x else x)
    | (false,true) -&gt; w |&gt; Array.mapi (fun i x -&gt; if obs.[i] then x / alpha else x)
    | _ -&gt; w
Copy<br>Let’s check that the update mechanism works:<br>&gt; update 0.5 2. [|1.;1.;|] (false,[|false;true;|]);;
val it : float [] = [|1.0; 0.5|]
Copy<br>The threshold is 0.5, the adjustment multiplier is 2, and each feature is currently weighted at 1. The state of our example is [| false; true; |], so only the second feature is active, which means that the predicted value will be 1. (the weight of that feature). This is above the threshold 0.5, so the predicted value is true. However, because the correct value attached to that example is false, our prediction is incorrect, and the weight of the second feature is reduced, while the first one, which was not active, remains unchanged.<br>Let’s wrap this up in a convenience function which will learn from a sequence of examples, and give us directly a function that will classify observations:<br>let learn (theta:float) (alpha:float) (fs:int) (xs:Example seq) =
    let updater = update theta alpha
    let w0 = [| for f in 1 .. fs -&gt; 1. |]   
    let w = Seq.fold (fun w x -&gt; updater w x) w0 xs
    fun (obs:Observation) -&gt; predict theta w obs
Copy<br>We pass in the number of features, fs, to initialize the weights at the correct size, and use a fold to update the weights for each example in the sequence. Finally, we create and return a function that, given an observation, will predict the label, based on the weights we just learnt.<br>And that’s it – in 20 lines of code, we are done, the Winnow is implemented.<br>But… does it work? An example doesn’t prove anything, of course, but I was curious, and cooked up the following idea. Let’s use the Winnow to predict if the next character in a piece of text is going to be a letter, or something else (space, punctuation…), based on the previous characters. In other words, let’s try to predict if we reached the end of a word.<br>To simplify the coding part a bit, I will ignore case, and convert every character to upper case. Obviously, whether a character is upper or lower case is relevant to where we are in a word, but my goal here is just to satisfy my curiosity, so I will ignore that and be lazy. The letters A to Z correspond to char 65 to 90 (that’s an alphabet of 26 characters), and I also want to catch everything that isn’t a letter. One way we can then encode a character so that it fits our requirement of binary features is the following: create an array of 27 slots, and mark with true the slot corresponding to the letter, reserving the last slot for the case “not a letter”.<br>I will readily admit it, the following code is a bit ugly (there is probably a cleaner way to do that), but gets the work done:<br>let letter (c:char) = int c &gt;= 65 &amp;&amp; int c &lt;= 90
 
let encode (c:char) =
    let vec = Array.create (90-65+2) false   
    let x = int c
    if (x &gt;= 65 &amp;&amp; x &lt;= 90)
    then vec.[x-65] &lt;- true
    else vec.[90-65+1] &lt;- true
vec
 
let prepare (cs:char[]) =
    cs |&gt; Seq.map encode |&gt; Array.concat
Copy<br>letter simply recognizes if a char is an uppercase letter, encode creates a vector representing a character, and prepare takes in an array of chars, and returns an array which puts side-by-side each of the encoded characters. As an example,<br>&gt; encode 'B';;
val it : bool [] =
[|false; true; false; false; false; false; false; false; false; false; false;
false; false; false; false; false; false; false; false; false; false;
false; false; false; false; false; false|]
Copy<br>This returns an array of 27 booleans – all of them false, except the second position, which corresponds to B’s position in the alphabet.<br>We will try to predict the next character based not only on the previous one, but rather on the preceding sequence, the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/N-gram" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/N-gram" target="_blank">N-gram</a>. Let’s write a quick and dirty function to transform a string into N-grams, and whether the character that immediately follows is the end of a word, or any letter:<br>let ngrams n (text:string) =
text.ToUpperInvariant()
|&gt; Seq.windowed (n+1)
|&gt; Seq.map (fun x -&gt; x.[n],x.[0..(n-1)])
|&gt; Seq.map (fun (c,cs) -&gt; letter c |&gt; not, prepare cs)
Copy<br>And we are ready to go. What I am really interested in here is not that much how good or bad the classifier is, but whether it actually improves as it gets feds more data. To observe that, let’s do the following: we’ll use a body of text for training, and another one for validation; we will train the classifier on a larger and larger portion of the training text, and measure the quality of the various models by applying it to the validation text.<br>We will train the model on a paragraph by Borges, and validate on some Cicero, both lifted from the <a data-tooltip-position="top" aria-label="http://en.wikipedia.org/wiki/Infinite_monkey_theorem#Origins_and_.22The_Total_Library.22" rel="noopener" class="external-link" href="http://en.wikipedia.org/wiki/Infinite_monkey_theorem#Origins_and_.22The_Total_Library.22" target="_blank">Total Library section in the Infinite Monkey wikipedia page</a>.<br>Training:<br>
Everything would be in its blind volumes. Everything: the detailed history of the future, Aeschylus’ The Egyptians, the exact number of times that the waters of the Ganges have reflected the flight of a falcon, the secret and true nature of Rome, the encyclopedia Novalis would have constructed, my dreams and half-dreams at dawn on August 14, 1934, the proof of Pierre Fermat’s theorem, the unwritten chapters of Edwin Drood, those same chapters translated into the language spoken by the Garamantes, the paradoxes Berkeley invented concerning Time but didn’t publish, Urizen’s books of iron, the premature epiphanies ofStephen Dedalus, which would be meaningless before a cycle of a thousand years, the Gnostic Gospel of Basilides, the song the sirens sang, the complete catalog of the Library, the proof of the inaccuracy of that catalog. Everything: but for every sensible line or accurate fact there would be millions of meaningless cacophonies, verbal farragoes, and babblings. Everything: but all the generations of mankind could pass before the dizzying shelves—shelves that obliterate the day and on which chaos lies—ever reward them with a tolerable page.
<br>Validation:<br>
He who believes this may as well believe that if a great quantity of the one-and-twenty letters, composed either of gold or any other matter, were thrown upon the ground, they would fall into such order as legibly to form the Annals of Ennius. I doubt whether fortune could make a single verse of them.
<br>Here is how one might go about coding that experiment:<br>let training = ngrams 3 borges
let validation = ngrams 3 cicero
 
let len = Seq.length training
    for l in 25 .. 25 .. (len - 1) do
    let sample = training |&gt; Seq.take l
    let model = learn 0.5 2. (3*(92-65)) sample
    validation
    |&gt; Seq.averageBy (fun (l,o) -&gt;
    if l = model o then 1. else 0.)
    |&gt; printfn "Sample: %i, correct: %.4f" l
Copy<br>Running that code will produce some rather unexciting output:<br>Sample: 25, correct: 0.2168 
Sample: 50, correct: 0.4434 
Sample: 75, correct: 0.5049 
Sample: 100, correct: 0.5955 
Sample: 125, correct: 0.5081 
Sample: 150, correct: 0.6861

// snipped because more of the same

Sample: 1125, correct: 0.7476 
Sample: 1150, correct: 0.6278 
Sample: 1175, correct: 0.6893 
Sample: 1200, correct: 0.7314
Copy<br>Visibly, the quality starts pretty low, with around 21% correct predictions for the smallest sample, climbs up as the sample increases, and ends up oscillating in the 65% – 75% range. This isn’t a proof of anything, of course, but it seems to indicate that the model is “learning”, getting better and better at recognizing word endings as it is fed more 3-grams.<br>And that’s as far as I’ll go on the Winnow. I thought this was an interesting algorithm, if only for its simplicity. It is also suitable for online learning: you don’t need to train your model on a dataset before using it - it can progressively learn on the fly as data is arriving, and the only state you need to maintain is the latest set of weights. The biggest limitation is that it is a linear classifier, which assumes that the data can be cleanly separated along a plane.<br>In any case, I definitely had fun playing with this – I hope you did, too!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/learning-from-mistakes-winnow-algorithm-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Learning from mistakes - Winnow algorithm in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:29 GMT</pubDate></item><item><title><![CDATA[Messages and Agents]]></title><description><![CDATA[ 
 <br>In this post, we’ll look at the message-based (or actor-based) approach to concurrency.<br>In this approach, when one task wants to communicate with another, it sends it a message, rather than contacting it directly. The messages are put on a queue, and the receiving task (known as an “actor” or “agent”) pulls the messages off the queue one at a time to process them.<br>This message-based approach has been applied to many situations, from low-level network sockets (built on TCP/IP) to enterprise wide application integration systems (for example RabbitMQ or IBM WebSphere MQ).<br>From a software design point of view, a message-based approach has a number of benefits:<br>
<br>You can manage shared data and resources without locks.
<br>You can easily follow the “single responsibility principle”, because each agent can be designed to do only one thing.
<br>It encourages a “pipeline” model of programming with “producers” sending messages to decoupled “consumers”, which has additional benefits:

<br>The queue acts as a buffer, eliminating waiting on the client side.
<br>It is straightforward to scale up one side or the other of the queue as needed in order to maximize throughput.
<br>Errors can be handled gracefully, because the decoupling means that agents can be created and destroyed without affecting their clients.


<br>From a practical developer’s point of view, what I find most appealing about the message-based approach is that when writing the code for any given actor, you don’t have to hurt your brain by thinking about concurrency. The message queue forces a “serialization” of operations that otherwise might occur concurrently. And this in turn makes it much easier to think about (and write code for) the logic for processing a message, because you can be sure that your code will be isolated from other events that might interrupt your flow.<br>With these advantages, it is not surprising that when a team inside Ericsson wanted to design a programming language for writing highly-concurrent telephony applications, they created one with a message-based approach, namely Erlang. Erlang has now become the poster child for the whole topic, and has created a lot of interest in implementing the same approach in other languages.<br><br>F# has a built-in agent class called MailboxProcessor. These agents are very lightweight compared with threads - you can instantiate tens of thousands of them at the same time.<br>These are similar to the agents in Erlang, but unlike the Erlang ones, they do not work across process boundaries, only in the same process. And unlike a heavyweight queueing system such as RabbitMQ, the messages are not persistent. If your app crashes, the messages are lost.<br>But these are minor issues, and can be worked around. In a future series, I will go into alternative implementations of message queues. The fundamental approach is the same in all cases.<br>Let’s see a simple agent implementation in F#:<br>
let printerAgent = MailboxProcessor.Start(fun inbox-&gt;

    // the message processing function
    let rec messageLoop() = async{

        // read a message
        let! msg = inbox.Receive()

        // process a message
        printfn "message is: %s" msg

        // loop to top
        return! messageLoop()
        }

    // start the loop
    messageLoop()
    )

Copy<br>The MailboxProcessor.Start function takes a simple function parameter. That function loops forever, reading messages from the queue (or “inbox”) and processing them.<br>Here’s the example in use:<br>// test it
printerAgent.Post "hello"
printerAgent.Post "hello again"
printerAgent.Post "hello a third time"
Copy<br>In the rest of this post we’ll look at two slightly more useful examples:<br>
<br>Managing shared state without locks
<br>Serialized and buffered access to shared IO
<br>In both of these cases, a message based approach to concurrency is elegant, efficient, and easy to program.<br><br>Let’s look at the shared state problem first.<br>A common scenario is that you have some state that needs to be accessed and changed by multiple concurrent tasks or threads. We’ll use a very simple case, and say that the requirements are:<br>
<br>A shared “counter” and “sum” that can be incremented by multiple tasks concurrently.
<br>Changes to the counter and sum must be atomic – we must guarantee that they will both be updated at the same time.
<br><br>Using locks or mutexes is a common solution for these requirements, so let’s write some code using a lock, and see how it performs.<br>First let’s write a static LockedCounter class that protects the state with locks.<br>open System
open System.Threading
open System.Diagnostics

// a utility function
type Utility() =
    static let rand = Random()

    static member RandomSleep() =
        let ms = rand.Next(1,10)
        Thread.Sleep ms

// an implementation of a shared counter using locks
type LockedCounter () =

    static let _lock = Object()

    static let mutable count = 0
    static let mutable sum = 0

    static let updateState i =
        // increment the counters and...
        sum &lt;- sum + i
        count &lt;- count + 1
        printfn "Count is: %i. Sum is: %i" count sum

        // ...emulate a short delay
        Utility.RandomSleep()


    // public interface to hide the state
    static member Add i =
        // see how long a client has to wait
        let stopwatch = Stopwatch()
        stopwatch.Start()

        // start lock. Same as C# lock{...}
        lock _lock (fun () -&gt;

            // see how long the wait was
            stopwatch.Stop()
            printfn "Client waited %i" stopwatch.ElapsedMilliseconds

            // do the core logic
            updateState i
            )
        // release lock
Copy<br>Some notes on this code:<br>
<br>This code is written using a very imperative approach, with mutable variables and locks
<br>The public Add method has explicit Monitor.Enter and Monitor.Exit expressions to get and release the lock. This is the same as the lock{...} statement in C#.
<br>We’ve also added a stopwatch to measure how long a client has to wait to get the lock.
<br>The core “business logic” is the updateState method, which not only updates the state, but adds a small random wait as well to emulate the time taken to do the processing.
<br>Let’s test it in isolation:<br>// test in isolation
LockedCounter.Add 4
LockedCounter.Add 5
Copy<br>Next, we’ll create a task that will try to access the counter:<br>let makeCountingTask addFunction taskId  = async {
    let name = sprintf "Task%i" taskId
    for i in [1..3] do
        addFunction i
    }

// test in isolation
let task = makeCountingTask LockedCounter.Add 1
Async.RunSynchronously task
Copy<br>In this case, when there is no contention at all, the wait times are all 0.<br>But what happens when we create 10 child tasks that all try to access the counter at once:<br>let lockedExample5 =
    [1..10]
        |&gt; List.map (fun i -&gt; makeCountingTask LockedCounter.Add i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
Copy<br>Oh dear! Most tasks are now waiting quite a while. If two tasks want to update the state at the same time, one must wait for the other’s work to complete before it can do its own work, which affects performance.<br>And if we add more and more tasks, the contention will increase, and the tasks will spend more and more time waiting rather than working.<br><br>Let’s see how a message queue might help us. Here’s the message based version:<br>type MessageBasedCounter () =

    static let updateState (count,sum) msg =

        // increment the counters and...
        let newSum = sum + msg
        let newCount = count + 1
        printfn "Count is: %i. Sum is: %i" newCount newSum

        // ...emulate a short delay
        Utility.RandomSleep()

        // return the new state
        (newCount,newSum)

    // create the agent
    static let agent = MailboxProcessor.Start(fun inbox -&gt;

        // the message processing function
        let rec messageLoop oldState = async{

            // read a message
            let! msg = inbox.Receive()

            // do the core logic
            let newState = updateState oldState msg

            // loop to top
            return! messageLoop newState
            }

        // start the loop
        messageLoop (0,0)
        )

    // public interface to hide the implementation
    static member Add i = agent.Post i
Copy<br>Some notes on this code:<br>
<br>The core “business logic” is again in the updateState method, which has almost the same implementation as the earlier example, except the state is immutable, so that a new state is created and returned to the main loop.
<br>The agent reads messages (simple ints in this case) and then calls updateState method
<br>The public method Add posts a message to the agent, rather than calling the updateState method directly
<br>This code is written in a more functional way; there are no mutable variables and no locks anywhere. In fact, there is no code dealing with concurrency at all! The code only has to focus on the business logic, and is consequently much easier to understand.
<br>Let’s test it in isolation:<br>// test in isolation
MessageBasedCounter.Add 4
MessageBasedCounter.Add 5
Copy<br>Next, we’ll reuse a task we defined earlier, but calling MessageBasedCounter.Add instead:<br>let task = makeCountingTask MessageBasedCounter.Add 1
Async.RunSynchronously task
Copy<br>Finally let’s create 5 child tasks that try to access the counter at once.<br>let messageExample5 =
    [1..5]
        |&gt; List.map (fun i -&gt; makeCountingTask MessageBasedCounter.Add i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
Copy<br>We can’t measure the waiting time for the clients, because there is none!<br><br>A similar concurrency problem occurs when accessing a shared IO resource such as a file:<br>
<br>If the IO is slow, the clients can spend a lot of time waiting, even without locks.
<br>If multiple threads write to the resource at the same time, you can get corrupted data.
<br>Both problems can be solved by using asynchronous calls combined with buffering – exactly what a message queue does.<br>In this next example, we’ll consider the example of a logging service that many clients will write to concurrently. (In this trivial case, we’ll just write directly to the Console.)<br>We’ll first look at an implementation without concurrency control, and then at an implementation that uses message queues to serialize all requests.<br><br>In order to make the corruption very obvious and repeatable, let’s first create a “slow” console that writes each individual character in the log message and pauses for a millisecond between each character. During that millisecond, another thread could be writing as well, causing an undesirable interleaving of messages.<br>let slowConsoleWrite msg =
    msg |&gt; String.iter (fun ch-&gt;
        System.Threading.Thread.Sleep(1)
        System.Console.Write ch
        )

// test in isolation
slowConsoleWrite "abc"
Copy<br>Next, we will create a simple task that loops a few times, writing its name each time to the logger:<br>let makeTask logger taskId = async {
    let name = sprintf "Task%i" taskId
    for i in [1..3] do
        let msg = sprintf "-%s:Loop%i-" name i
        logger msg
    }

// test in isolation
let task = makeTask slowConsoleWrite 1
Async.RunSynchronously task
Copy<br>Next, we write a logging class that encapsulates access to the slow console. It has no locking or serialization, and is basically not thread-safe:<br>type UnserializedLogger() =
    // interface
    member this.Log msg = slowConsoleWrite msg

// test in isolation
let unserializedLogger = UnserializedLogger()
unserializedLogger.Log "hello"
Copy<br>Now let’s combine all these into a real example. We will create five child tasks and run them in parallel, all trying to write to the slow console.<br>let unserializedExample =
    let logger = UnserializedLogger()
    [1..5]
        |&gt; List.map (fun i -&gt; makeTask logger.Log i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
Copy<br>Ouch! The output is very garbled!<br><br>So what happens when we replace UnserializedLogger with a SerializedLogger class that encapsulates a message queue.<br>The agent inside SerializedLogger simply reads a message from its input queue and writes it to the slow console. Again there is no code dealing with concurrency and no locks are used.<br>type SerializedLogger() =

    // create the mailbox processor
    let agent = MailboxProcessor.Start(fun inbox -&gt;

        // the message processing function
        let rec messageLoop () = async{

            // read a message
            let! msg = inbox.Receive()

            // write it to the log
            slowConsoleWrite msg

            // loop to top
            return! messageLoop ()
            }

        // start the loop
        messageLoop ()
        )

    // public interface
    member this.Log msg = agent.Post msg

// test in isolation
let serializedLogger = SerializedLogger()
serializedLogger.Log "hello"
Copy<br>So now we can repeat the earlier unserialized example but using the SerializedLogger instead. Again, we create five child tasks and run them in parallel:<br>let serializedExample =
    let logger = SerializedLogger()
    [1..5]
        |&gt; List.map (fun i -&gt; makeTask logger.Log i)
        |&gt; Async.Parallel
        |&gt; Async.RunSynchronously
        |&gt; ignore
Copy<br>What a difference! This time the output is perfect.<br><br>There is much more to say about this message based approach. In a future series, I hope to go into much more detail, including discussion of topics such as:<br>
<br>alternative implementations of message queues with RabbitMQ and TPL Dataflow.
<br>cancellation and out of band messages.
<br>error handling and retries, and handling exceptions in general.
<br>how to scale up and down by creating or removing child agents.
<br>avoiding buffer overruns and detecting starvation or inactivity.
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/messages-and-agents.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Messages and Agents.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:35 GMT</pubDate></item><item><title><![CDATA[Tips for working with Elmish]]></title><description><![CDATA[ 
 <br>his article is intended for people who are already familiar with the MVU architecture. You can learn more about it on the <a data-tooltip-position="top" aria-label="https://elmish.github.io/elmish/" rel="noopener" class="external-link" href="https://elmish.github.io/elmish/" target="_blank"><em></em></a>Elmish website.<br>
本文面向已经熟悉 MVU 架构的人员。您可以在 Elmish 网站上了解更多相关信息。<br>I am one of the maintainer of Elmish and I created several <a data-tooltip-position="top" aria-label="http://fable.io" rel="noopener" class="external-link" href="http://fable.io" target="_blank">Fable</a> / Elmish libraries and applications. When doing so, I needed to understand Elmish deeply and I want to take the opportunity of <a data-tooltip-position="top" aria-label="https://sergeytihon.com/2018/10/22/f-advent-calendar-in-english-2018/" rel="noopener" class="external-link" href="https://sergeytihon.com/2018/10/22/f-advent-calendar-in-english-2018/" target="_blank"><em></em></a>F# Advent Calendar to share with you my tips and knowledge.<br>
我是 Elmish 的维护者之一，我创建了几个 Fable / Elmish 库和应用程序。这样做时，我需要深入了解 Elmish，我想利用 F# Advent Calendar 的机会与大家分享我的技巧和知识。<br>I will be covering a lot in this article, so grab yourself a cup of tea 🍵, take a deep breath 💨 and let’s get started 🏁.<br>
我将在这篇文章中介绍很多内容，所以给自己喝杯茶🍵，深呼吸💨，让我们开始吧🏁。<br><br>A command is a container for a function that Elmish executes immediately but, may schedule dispatch of message at any time.<br>
命令是 Elmish 立即执行的函数的容器，但可以随时安排消息的发送。<br>For example:&nbsp;例如：<br>
<br>Issue immediately a new Message<br>
立即发出新消息
<br>Make an HTTP call and then return the result in your application via a Message<br>
进行 HTTP 调用，然后通过消息将结果返回到您的应用程序中
<br>Save data in your storage<br>
将数据保存在您的存储中
<br><br>In this section, I will cover all the default functions offered by Elmish and show an example of it.<br>
在本节中，我将介绍 Elmish 提供的所有默认函数并展示它的示例。<br>Use Cmd.none to schedule no Commands.<br>
使用 Cmd.none 不安排任何命令。<br>type Model =
    { Tick : int }
    
type Msg =
    | Tick
    
let private update msg model =
    match msg with
    | Tick -&gt;
        { model with Tick = model.Tick + 1 }, Cmd.none
Copy<br>Use Cmd.ofMsg to schedules another message directly, it can be seen as a way to chain messages.<br>
使用 Cmd.ofMsg 直接调度另一条消息，可以看作是一种链接消息的方式。<br>type Model =
    { Value : string }

type Msg =
    | ChangeValue of string
    | ValidateData
    
let private update msg model =
    match msg with
    | ChangeValue newValue -&gt;
        { model with Value = newValue }, Cmd.ofMsg ValidateData
Copy<br>UseCmd.ofAsync to evaluate an async block and map the result into success or error (of exception).<br>
使用 Cmd.ofAsync 评估 async 块并将结果映射为成功或错误（异常）。<br>type Model =
    { InputValue : string
      UpperValue : string }
      
type Msg =
    | ToUpper of string
    | OnUpperResult of string
    | OnUpperError of exn

let private update (msg : Msg) (model : Model) =
    match msg with
    | ToUpper txt -&gt;
        let asyncUpper (txt : string) =
            async {
                do! Async.Sleep 1000

                return txt.ToUpper()
            }

        model, Cmd.ofAsync asyncUpper txt OnUpperResult OnUpperError

    | OnUpperResult result -&gt;
        { model with UpperValue = result }, Cmd.none

    | OnUpperError error -&gt;
        Browser.console.error error
        model, Cmd.none
Copy<br>Use Cmd.ofFunc to evaluate a simple function and map the result into success or error (of exception).<br>
使用 Cmd.ofFunc 评估一个简单的函数并将结果映射为成功或错误（异常）。<br>type Model =
    { CurrentRoute : Route
      Value : string
      Tick : int
      UpperValue : string }

type Msg =
    | Save of string
    | OnSaveSuccess of bool
    | OnSaveError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | Save txt -&gt;
        let save (txt : string) =         
            // If there is an error, an exception will be thrown and captured by Cmd.ofFunc
            Browser.localStorage.setItem("my-app.input", txt)
            true

        model, Cmd.ofFunc save txt OnSaveSuccess OnSaveError

    | OnSaveSuccess result -&gt;
        // Here we can notify the user that the save succeeded
        model, Cmd.none

    | OnSaveError error -&gt;
        // Here we can notifu the user that the save failed
        Browser.console.error error
        model, Cmd.none
Copy<br>UseCmd.performFunc to evaluate a simple function and map the success to a message discarding any possible error.<br>
使用 Cmd.performFunc 评估一个简单的函数，并将成功映射到一条消息，丢弃任何可能的错误。<br>type Model =
    { IsLoading : bool
      Value : string }

type Msg =
    | Load
    | OnLoadSuccess of bool
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | Load -&gt;
        // This function can never fail so we can use Cmd.performFunc
        let load () =
            let storedValue : string = Browser.localStorage.getItem("my-app.input") :?&gt; string
            if isNull storedValue then
                ""
            else
                storedValue

        { model with IsLoading = true }, Cmd.performFunc load () OnLoadSuccess

    | OnLoadSuccess value -&gt;
        { model with IsLoading = false
                     Value = value }, Cmd.none
Copy<br>UseCmd.attemptFunc to evaluate a simple function and map the error (in case of exception)<br>
使用 Cmd.attemptFunc 计算一个简单的函数并映射错误（如果出现异常）<br>type Model =
    { Value : string }

type Msg =
    | ChangeValue of string
    | OnLogError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | ChangeValue newValue -&gt;
        let log (msg : string) =
            // Send a msg to your logger
            // If there is an error it will throw
            Log.send msg

        let msg = sprintf "Value changed from %s to %s" model.Value newValue        

        { model with Value = newValue }, Cmd.attemptFunc log msg OnLogError

    | OnLogError value -&gt;
        // There was an error during logging, should we do something ?
        model, Cmd.none
Copy<br>Use Cmd.ofPromise to call promise block and map the results.<br>
使用 Cmd.ofPromise 调用 promise 块并映射结果。<br>type Model =
    { Value : int }

type Msg =
    | Submit
    | OnPromiseSuccess of int
    | OnPromiseError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | Submit -&gt;
        let myPromise () =
            promise {
                do! Promise.sleep 1000
                return 10
            }

        model, Cmd.ofPromise myPromise () OnPromiseSuccess OnPromiseError

    | OnPromiseSuccess value -&gt;
        { model with Value = value }, Cmd.none

    | OnPromiseError error -&gt;
        Browser.console.error error
        model, Cmd.none
Copy<br>Use Cmd.ofSub to call the subscriber. This is useful when you are dealing with an API which use callbacks or to listen to an event.<br>
使用 Cmd.ofSub 呼叫订阅者。当您处理使用回调或侦听事件的 API 时，这非常有用。<br>type Model =
    { Position : Browser.Position option
      ErrorMessage : string }

type Msg =
    | GetPosition
    | GetPositionSuccess of Browser.Position
    | GetPositionError of Browser.PositionError

module Sub =
    // Subscriber used to reach gealocation API
    let getPosition onSuccess onError dispatch =
        Browser.navigator.geolocation.getCurrentPosition(
          onSuccess &gt;&gt; dispatch,
          onError &gt;&gt; dispatch
        )

let update (msg:Msg) (model:Model) =
    match msg with
    | GetPosition -&gt;
        model, Cmd.ofSub (Sub.getPosition GetPositionSuccess GetPositionError)
    
    | GetPositionSuccess posision -&gt;
        { model with Position = Some position
                     ErrorMessage = "" }, Cmd.none
    
    | GetPositionError error -&gt;
        { model with Position = None
                     ErrorMessage = error.message }, Cmd.none
Copy<br>Use Cmd.batch to aggregate multiple commands. You can aggregate any of the previous commands together.<br>
使用 Cmd.batch 聚合多个命令。您可以将前面的任何命令聚合在一起。<br>type Model =
    { Data1 : int
      Data2 : int }

type Msg =
    | FetchDatas
    | OnFetchData1 of int
    | OnFetchData2 of int
    | OnPromiseError of exn
    | OnLogError of exn
    
let private update (msg : Msg) (model : Model) =
    match msg with
    | FetchDatas -&gt;
        model, Cmd.batch [ Cmd.ofPromise fetchData1 () OnFetchData1 OnPromiseError 
                           Cmd.ofPromise fetchData2 () OnFetchData2 OnPromiseError 
                           Cmd.performFunc log "FetchDatas message has been treated" OnLogError ]

    | OnFetchData1 data1 -&gt;
        { model with Data1 = data1 }, Cmd.performFunc log "FetchData1 data has been received" OnLogError

    | OnFetchData2 data2 -&gt;
        { model with Data2 = data2 }, Cmd.performFunc log "FetchData2 data has been received" OnLogError

    | OnPromiseError error -&gt;
        Browser.console.error("An error occured when fetching data", error)
        model, Cmd.none

    | OnLogError error -&gt;
        Browser.console.error("An error occured when logging", error)
        model, Cmd.none
Copy<br>The functions provided out of the box by Elmish are enough to cover most of the needs of your application. However, when working on a library or with a specific API you could want to create your own commands. We will take a look at it later.<br>
Elmish 提供的开箱即用的功能足以满足您应用程序的大部分需求。但是，当使用库或特定 API 时，您可能想要创建自己的命令。我们稍后会看一下。<br><br>根据您的需求对模型进行建模<br>When working with a Single Page Application, you will have to deal with navigation and need to reflect it in your models.<br>
使用单页应用程序时，您将必须处理导航并需要将其反映在模型中。<br>In general, when I am working with at a page level, I use Discrimination Union so I can store only the active page state.<br>
一般来说，当我在页面级别工作时，我使用歧视联盟，这样我就可以只存储活动页面状态。<br>
For example, if in your application you have a login page you can’t store your authenticated page state in your application because you can’t fetch the resource to display it.<br>
例如，如果您的应用程序中有一个登录页面，您无法在应用程序中存储经过身份验证的页面状态，因为您无法获取资源来显示它。<br>In general my main Model looks like this:<br>
一般来说，我的主要 Model 看起来像这样：<br>
[&lt;RequireQualifiedAccess&gt;]
type AuthPage =
    // Section for the website related to Messages management
    // For example
    // - Display the list of Messages
    // - Create a message
    | Messages of Messages.Model
    // Administration section of the website
    | Administration of Administration.Model

[&lt;RequireQualifiedAccess&gt;]
type Page =
    // The application is loading, display a loader
    | Loading
    // Login page
    | Login of Login.Model
    // From here, the user need to be authenticated
    | AuthPage of AuthPage
    // Global error page like:
    // - Generic error
    // - You need to be connected
    | Errored of Errored.Reason
    // A page that can be accessed when the user is not logged in
    // For example, to reset it's password from an URL
    | ResetAccount of ResetAccount.Model

type Model =
    { // Store the user session information
      // If Some xxx, then the user is signed-in
      // If None, the user isn't connected
      Session : User option
      // Store the active page state
      ActivePage : Page
      // Store the current route information, this is useful if your
      // route have parameters than you need to pass to child later
      CurrentRoute : Router.Route option }```

Using DUs to represent your pages state helps you isolate your logics. For me, the biggest benefit compared to storing all page states in a big record, is that you are not caching your page when navigating.  
使用 DU 表示页面状态可以帮助您隔离逻辑。对我来说，与将所有页面状态存储在大记录中相比，最大的好处是您在导航时不会缓存页面。

Of course sometimes your application needs to maintain state between navigation. For example, if you have a form in several steps you could modelize it like this:  
当然，有时您的应用程序需要在导航之间维护状态。例如，如果您有一个分几个步骤的表单，您可以像这样对其进行建模：
```F#
type Rank =
    | One
    | Two

type Step1 =
    { Login : string
      Password : string }

type Step2 =
    { Age : int 
      Surname : string 
      Firstname : string }

type Model =
    { CurrentRank : Rank
      Step1 : Step1
      Step2 : Step2 }

let private view (model : Model) (dispatch : Msg -&gt; unit) =
    match model.CurrentRank with
    | Rank.One -&gt; // render step n°1 form
    | Rank.Two -&gt; // render step n°2 form
Copy<br><br>One thing we tend to forget when working with Elmish is that everything is a function. The main reason for this forgetfulness is that most of the examples only show you this code:<br>
使用 Elmish 时我们容易忘记的一件事是一切都是函数。造成这种遗忘的主要原因是大多数示例仅向您展示以下代码：<br>type Model =
    { Value : int }

type Msg =
    | SomeAction

let init =
    { Value = 0 }, Cmd.none

let update (msg : Msg) (model : Model) : Model * Cmd&lt;Msg&gt; =
    match msg with
    | SomeAction -&gt;
        model, Cmd.none

let view (model : Model) (dispatch : Msg -&gt; unit) : React.ReactElement =
    div [ ]
        [ str "This is my view" ]
Copy<br>So in our mind update takes two arguments when in fact we should think update takes at least two arguments. The same applies to view and init functions.<br>
因此，在我们看来 update 需要两个参数，而实际上我们应该认为 update 至少需要两个参数。这同样适用于 view 和 init 函数。<br>This idea is linked to the fact that a component’s Model should have all the information needed by the component, but this is not always true.<br>
这个想法与组件的 Model 应该拥有组件所需的所有信息有关，但这并不总是正确的。<br><br>将数据作为参数传递<br>For example, if you have a session in your application in order to make HTTP calls, should you store this session in each component’s Model ?<br>
例如，如果您的应用程序中有一个会话用于进行 HTTP 调用，您是否应该将此会话存储在每个组件的 Model 中？<br>
No, you should store it at one place and then pass your session to the functions that need it.<br>
不，您应该将其存储在一个位置，然后将会话传递给需要它的函数。<br>The next code illustrates this situation, we request a session argument in our init function because we need to make an authenticated http request using Http.Auth.* (custom module used, includes the session info in a request).<br>
下一个代码说明了这种情况，我们在 init 函数中请求 session 参数，因为我们需要使用 Http.Auth.* 发出经过身份验证的 http 请求（使用的自定义模块，包括会话信息要求）。<br>open Elmish
open Fable.Import
open Fable.PowerPack

type Session = 
    { Token : string
      UserId : int }

type Model =
    | Loading
    | Loaded of string
    | Errored

type Msg =
    | OnFetchDataSuccess of string
    | OnFetchDataError of exn

let private fetchData (session : Session) : JS.Promise&lt;string&gt; =
    promise {
        let! res = Http.Auth.postRecord "/api/get-data" session
        return! res.text()
    }

let init (session : Session) =
    Loading, Cmd.ofPromise (fetchData session) OnFetchDataSuccess OnFetchDataError

let private update (msg : Msg) (model : Model) =
    match msg with
    | OnFetchDataSuccess value -&gt;
        Loaded value, Cmd.none

    | OnFetchDataError error -&gt;
        Browser.console.error error
        Errored, Cmd.none
Copy<br><br>将记录作为参数传递<br>If your view function takes several arguments you can use a record as an argument. It will force you to name the arguments and by doing so make your code easier to read and maintain over time.<br>
如果您的视图函数采用多个参数，您可以使用记录作为参数。它将迫使您命名参数，并通过这样做使您的代码随着时间的推移更易于阅读和维护。<br>type private SectionProps =
    { Icon : Fa.IconOption
      Count : int
      Label : string
      IsActive : bool
      ZoneColor : string }
  
let private renderSection (props : SectionProps) =
    // Render the view
    
let view (model : Model) =
    renderSection 
        { Icon = Fa.Solid.Cog
          Count = model.ZoneA.Value + model.ZoneB.Value
          Label = model.Zone.Name
          IsActive = model.IsActive
          ZoneColor = findColor model.Zone.ColorIndex }
Copy<br>It will also make it easier to optimize your code for react using memoBuilder.<br>
它还将使使用 memoBuilder 更轻松地优化代码以进行反应<br>type private SectionProps =
    { Icon : Fa.IconOption
      Count : int
      Label : string
      IsActive : bool
      ZoneColor : string }
  
let private renderSection = 
    memoBuilder "Section" (fun (props : SectionProps) -&gt;
        // Render the view
    )
        
let view (model : Model) =
    renderSection 
        { Icon = Fa.Solid.Cog
          Count = model.ZoneA.Value + model.ZoneB.Value
          Label = model.Zone.Name
          IsActive = model.IsActive
          ZoneColor = findColor model.Zone.ColorIndex }
Copy<br><br>使用辅助函数来处理您的域<br>For example, when an elmish component needs to fetch data from the server often, I like to show a loading animation. Here is how I handle it:<br>
例如，当一个精灵组件需要经常从服务器获取数据时，我喜欢显示加载动画。我是这样处理的：]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/tips-for-working-with-elmish.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Tips for working with Elmish.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:33 GMT</pubDate></item><item><title><![CDATA[UI programming with Elmish in FSharp]]></title><description><![CDATA[ 
 <br><br>The "Model View Update" (MVU) architecture was made famous by the front-end programming language <a data-tooltip-position="top" aria-label="http://elm-lang.org/" rel="noopener" class="external-link" href="http://elm-lang.org/" target="_blank">Elm</a> and can be found in many popular environments like <a data-tooltip-position="top" aria-label="http://redux.js.org/" rel="noopener" class="external-link" href="http://redux.js.org/" target="_blank">Redux</a>. Today it's probably the most famous UI pattern in functional programming. The reason for this is that it clearly separates state changes from UI controls and provides a nice and easily testable way to create modular UIs. Especially for developers that are already familiar with concepts like CQRS or event sourcing many of the things in the MVU architecture will feel natural. Here's what the MVU pattern looks like:<br>
“模型视图更新”（MVU）架构因前端编程语言 Elm 而闻名，并且可以在 Redux 等许多流行环境中找到。如今，它可能是函数式编程中最著名的 UI 模式。原因是它清楚地将状态更改与 UI 控件分开，并提供了一种很好且易于测试的方法来创建模块化 UI。特别是对于已经熟悉 CQRS 或事件源等概念的开发人员来说，MVU 架构中的许多内容都会感觉很自然。 MVU 模式如下所示：<br><img src="https://www.compositional-it.com/wp-content/uploads/2019/09/elmish-1.png" referrerpolicy="no-referrer"><br>Elmish for F# is a project that provides the capability for client-side apps written in F# to follow the MVU architecture. It comes in multiple flavors, so that you can choose between a <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/react/" rel="noopener" class="external-link" href="https://fable-elmish.github.io/react/" target="_blank">React renderer</a> for HTML, <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/react/react-native.html" rel="noopener" class="external-link" href="https://fable-elmish.github.io/react/react-native.html" target="_blank">React Native renderer</a> for mobile apps and a <a data-tooltip-position="top" aria-label="https://github.com/Prolucid/Elmish.WPF" rel="noopener" class="external-link" href="https://github.com/Prolucid/Elmish.WPF" target="_blank">WPF renderer</a> for desktop applications. In this article we will explore its power and modularity by working through a simple example.<br>
Elmish for F# 是一个为用 F# 编写的客户端应用程序提供遵循 MVU 架构的功能的项目。它有多种风格，因此您可以在用于 HTML 的 React 渲染器、用于移动应用程序的 React Native 渲染器和用于桌面应用程序的 WPF 渲染器之间进行选择。在本文中，我们将通过一个简单的示例来探索它的强大功能和模块化性。<br><br>Let's start to illustrate Elmish and the MVU architecture with the very common HTML button counter sample. The following code shows a counter implemented in HTML and JavaScript.<br>
让我们开始用非常常见的 HTML 按钮计数器示例来说明 Elmish 和 MVU 架构。以下代码显示了用 HTML 和 JavaScript 实现的计数器。<br>&lt;html&gt;
    &lt;body&gt;
    &lt;button onclick="--counter; update();"&gt;-&lt;/button&gt;
    &lt;div id="counter"&gt;&lt;/div&gt;
    &lt;button onclick="++counter; update();"&gt;+&lt;/button&gt;
    &lt;script&gt;
        var counter = 0;

        function update() { 
        document.getElementById("counter").textContent = "" + counter;
        }

        update();
    &lt;/script&gt;
    &lt;/body&gt;
&lt;/html&gt;
Copy<br>The code is very straight forward and works as intended, but it has a number of issues:<br>
该代码非常简单并且按预期工作，但它有许多问题：<br>
<br>We are mutating the global variable counter - almost always a dangerous thing to do<br>
我们正在改变全局变量 counter - 几乎总是一件危险的事情
<br>We are directly mutating a DOM element, coupling our "business logic" to the UI.<br>
我们直接改变 DOM 元素，将我们的“业务逻辑”耦合到 UI。
<br>We are referencing the DOM element with its name via a string, which is fragile and can lead to costly knock-on effects.<br>
我们通过字符串引用 DOM 元素及其名称，这是脆弱的，可能会导致代价高昂的连锁反应。
<br>We've embedded some "domain logic" directly in the onclick event<br>
我们直接在 onclick 事件中嵌入了一些“领域逻辑”
<br>These issues prevent us from using this in a modular way. For example, if we wanted to create a list with counters, we could not reuse this code. Instead, we could copy &amp; paste the code a couple of times to create a fixed number of counters, but even then we would need to be very careful that we fix all the references to the corresponding global variables. In the object-oriented world, there are number of patterns that allow you encapsulate this problem of shared mutable state - let's see how to do it in a manner that promotes some FP core practices using the three parts of the MVU pattern:<br>
这些问题阻止我们以模块化的方式使用它。例如，如果我们想创建一个带有计数器的列表，我们就不能重用这段代码。相反，我们可以复制并粘贴代码几次来创建固定数量的计数器，但即使如此，我们也需要非常小心地修复对相应全局变量的所有引用。在面向对象的世界中，有许多模式允许您封装共享可变状态的问题 - 让我们看看如何使用 MVU 模式的三个部分来促进一些 FP 核心实践：<br><br>Let's start with the Model.<br>
让我们从模型开始。<br>type Model = int

type Msg =
| Increment
| Decrement

let init() : Model = 0
Copy<br>In this F# code we capture the current value of the counter in a domain type , before creating a message type which can signal that we want to increment or decrement the counter. We also implement an init function that allows us to create the initial model for our application.<br>
在此 F# 代码中，我们在创建消息类型之前捕获域类型中计数器的当前值，该消息类型可以表明我们想要递增或递减计数器。我们还实现了一个 init 函数，它允许我们为应用程序创建初始模型。<br><br>The View part deals with the question of displaying controls on the screen. This is where we need to decide on a UI framework; in our case we've decided to stick with HTML, and so we will use the React renderer.<br>
View 部分处理在屏幕上显示控件的问题。这是我们需要决定 UI 框架的地方；在我们的例子中，我们决定坚持使用 HTML，因此我们将使用 React 渲染器。<br>let view model dispatch =
    div []
        [ button [ OnClick (fun _ -&gt; dispatch Decrement) ] [ "-" ]
          div [] [ model.ToString() ]
          button [ OnClick (fun _ -&gt; dispatch Increment) ] [ "+" ] ]
Copy<br>This is valid F# code that uses the excellent <a data-tooltip-position="top" aria-label="https://github.com/fable-compiler/fable-react" rel="noopener" class="external-link" href="https://github.com/fable-compiler/fable-react" target="_blank">Fable.React bindings</a> to convert from F# into React JS. The syntax is still similar to our HTML version from the beginning, but instead of &lt; and &gt;, we are using [ and ]. This syntax is easy to learn and IDE tools like <a data-tooltip-position="top" aria-label="http://ionide.io/" rel="noopener" class="external-link" href="http://ionide.io/" target="_blank">Ionide</a> provide code completion for it, so it's a natural fit for existing F# developers. An important observation is that we are no longer mutating state from within the OnClick handlers; instead we simply dispatch one of the earlier defined messages into the system. This decouples our model from the view.<br>
这是有效的 F# 代码，它使用出色的 Fable.React 绑定从 F# 转换为 React JS。语法从一开始仍然与我们的 HTML 版本类似，但我们使用 [ 和 ] 代替 &lt; 和 &gt; 。 。这种语法很容易学习，并且 Ionide 等 IDE 工具为其提供了代码补全，因此它非常适合现有的 F# 开发人员。一个重要的观察结果是，我们不再从 OnClick 处理程序内部改变状态；相反，我们只是将先前定义的消息之一发送到系统中。这将我们的模型与视图分离。<br><br>In the Update part, we define a state machine that represents our domain logic (or at least hooks into it).<br>
在更新部分，我们定义一个状态机来表示我们的域逻辑（或至少挂钩它）。<br>let update (msg:Msg) (model:Model) : Model =
    match msg with
    | Increment -&gt; model + 1
    | Decrement -&gt; model - 1
Copy<br>Here, we have defined an update function that will be called by Elmish whenever a message is received. In this very basic scenario there are only two cases to handle, and we can't really see the power of F#'s pattern matching yet. The most interesting observation is that we don't rely on any mutable state. Instead the update function takes a message and the current model, and returns a completely new version of the model. Since all the data is simply inside the model, this is very well testable - we can easily write a set of unit tests around the update function.<br>
在这里，我们定义了一个 update 函数，每当收到消息时 Elmish 都会调用该函数。在这个非常基本的场景中，只有两种情况需要处理，而且我们还不能真正看到 F# 模式匹配的强大功能。最有趣的观察是我们不依赖任何可变状态。相反，更新函数接受消息和当前模型，并返回模型的全新版本。由于所有数据都位于模型内部，因此非常容易测试 - 我们可以轻松地围绕更新函数编写一组单元测试。<br><br>So far, we've not used any functionality from Elmish at all. The Model and Update parts are pure F# code, whilst the View part uses the <a data-tooltip-position="top" aria-label="https://github.com/fable-compiler/fable-react" rel="noopener" class="external-link" href="https://github.com/fable-compiler/fable-react" target="_blank">Fable.React</a> package. At this point all three parts are completely independent from each other - now Elmish will "wire" these up into an application:<br>
到目前为止，我们还没有使用 Elmish 的任何功能。模型和更新部分是纯 F# 代码，而视图部分使用 Fable.React 包。此时，所有三个部分完全相互独立 - 现在 Elmish 将把它们“连接”到一个应用程序中：<br>Program.mkSimple Counter.init Counter.update Counter.view
|&gt; Program.withConsoleTrace
|&gt; Program.withDebugger
|&gt; Program.withReact "counter-app"
|&gt; Program.run
Copy<br>Elmish's <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/elmish/program.html" rel="noopener" class="external-link" href="https://fable-elmish.github.io/elmish/program.html" target="_blank">Program</a> abstraction provides "glue" functions like mkSimple to bind the three parts together into an application. Internally, Elmish gives us a message loop and takes care of dispatching messages between the view and the update function. Since the complete state is captured in the model and every state change is explicit by processing a message, this opens a whole new world of debugging features like <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/debugger/" rel="noopener" class="external-link" href="https://fable-elmish.github.io/debugger/" target="_blank">time travelling</a>.<br>
Elmish 的程序抽象提供了“粘合”函数，例如 mkSimple 将这三个部分绑定到一个应用程序中。在内部，Elmish 为我们提供了一个消息循环，并负责在视图和更新函数之间调度消息。由于模型中捕获了完整的状态，并且通过处理消息来明确每个状态更改，因此这打开了时间旅行等调试功能的全新世界。<br><br>亲子作文<br>In the last section we saw how to build a very basic sample app in the MVU architecture. For this minimal example it's not very clear what the benefit is compared to the original implementation that was using mutation. So, in the following example we'll use the counter as a module and create a list of counters.<br>
在上一节中，我们了解了如何在 MVU 架构中构建一个非常基本的示例应用程序。对于这个最小的示例，与使用突变的原始实现相比，并不清楚有什么好处。因此，在下面的示例中，我们将使用计数器作为模块并创建计数器列表。<br><br>As before let's start with the model:<br>
和之前一样，让我们​​从模型开始：<br>module CounterList
type Model = Counter.Model list

type Msg = 
| Insert
| Remove
| Modify of int * Counter.Msg

let init() : Model =
    [ Counter.init() ]
Copy<br>In this case we have a list of counter models and a new message type. We can signal to:<br>
在本例中，我们有一个计数器模型列表和一个新消息类型。我们可以向以下人员发出信号：<br>
<br>Insert a new counter<br>
插入新计数器
<br>Remove the latest counter<br>
删除最新的计数器
<br>Dispatch a counter message to the corresponding counter in the list.<br>
向列表中相应的计数器发送计数器消息。
<br>When the application starts we will start with one counter as provided by the init function. As you can see, every part of the model refers to the corresponding part of the submodel. CounterList.Model is a collection of Counter.Model, the CounterList.Msg uses the Counter.Msg as payload and the CounterList.init function calls the Counter.init function.<br>
当应用程序启动时，我们将从 init 函数提供的一个计数器开始。正如您所看到的，模型的每个部分都引用子模型的相应部分。 CounterList.Model 是 Counter.Model 的集合， CounterList.Msg 使用 Counter.Msg 作为负载， CounterList.init 函数调用 Counter.init 函数。<br><br>Let's take a look at the View code:<br>
我们看一下 View 代码：<br>let view model dispatch =
    let counters =
        model
        |&gt; List.mapi (fun pos counterModel -&gt; 
            Counter.view
                counterModel
                (fun msg -&gt; dispatch (Modify (pos, msg)))) 

    div [] [ 
        yield button [ OnClick (fun _ -&gt; dispatch Remove) ] [ "Remove" ]
        yield button [ OnClick (fun _ -&gt; dispatch Insert) ] [ "Add" ]
        yield! counters ]
Copy<br>It's not that much different to the view code of the counter itself, except now we render a list of counters and wrap the messages with position information. This code may look a bit unfamiliar at this point, but the F# compiler is helping us here. There is only one way to get it to compile - and that's the correct way! Once you've done this step a few times, it becomes second nature - just like any task that you become familiar with.<br>
它与计数器本身的视图代码没有太大区别，只是现在我们渲染计数器列表并用位置信息包装消息。这段代码此时看起来可能有点陌生，但 F# 编译器在这里为我们提供了帮助。只有一种方法可以编译它 - 这就是正确的方法！一旦您完成此步骤几次，它就会成为第二天性 - 就像您熟悉的任何任务一样。<br><br>As with the Model, we also see a nice symmetry in the Views. But what about the Update part?<br>
与模型一样，我们在视图中也看到了很好的对称性。但是更新部分呢？<br>let update (msg:Msg) (model:Model) =
    match msg with
    | Insert -&gt;
        Counter.init() :: model // append to list
    | Remove -&gt;
        match model with
        | [] -&gt; []              // list is already empty
        | x :: rest -&gt; rest     // remove from list
    | Modify (pos, counterMsg) -&gt;
        model
        |&gt; List.mapi (fun i counterModel -&gt;
            if i = pos then
                Counter.update counterMsg counterModel
            else
                counterModel) }
Copy<br>Now this may come as no surprise, but we have the same situation here: CounterList.update forwards to Counter.update of the corresponding counter! We end up with something really beautiful, since now we have a CounterList component which exposes exactly the same elements as the Counter itself, namely Model, View and Update. This allows us to use the CounterList itself as a component!<br>
现在这可能并不奇怪，但我们这里有同样的情况： CounterList.update 转发到相应计数器的 Counter.update ！我们最终得到了一些非常漂亮的东西，因为现在我们有一个 CounterList 组件，它公开了与 Counter 本身完全相同的元素，即模型、视图和更新。这允许我们使用 CounterList 本身作为组件！<br><br>F# - 非常适合 MVU 架构<br>The MVU architecture was made famous by the Elm language, but can be used in many languages, even in vanilla <a data-tooltip-position="top" aria-label="https://github.com/ccorcos/elmish" rel="noopener" class="external-link" href="https://github.com/ccorcos/elmish" target="_blank">JavaScript</a>. F# - like Elm - is a language in the ML family of programming language, and provides features such as pattern matching that are extremely powerful and particularly useful for modelling state machines. In conjunction with F# features such as <a data-tooltip-position="top" aria-label="https://fsharpforfunandprofit.com/posts/discriminated-unions/" rel="noopener" class="external-link" href="https://fsharpforfunandprofit.com/posts/discriminated-unions/" target="_blank">Discriminated Unions</a>, this allows us to create applications in a type-safe manner that cater for all possibilities. The compiler provides us with guidance when corner cases are not dealt with, leading to quicker development time, whilst allowing F# developers to rapidly create UIs in a typesafe manner. The larger and more complex an application becomes, the greater the benefit; the F# compiler (like the Elm compiler) emits warnings for us when we forget to handle all possibilites - even for more complicated patterns - meaning less time spent debugging or writing unit tests, and more time delivering business value.<br>
MVU 架构因 Elm 语言而闻名，但可以在多种语言中使用，甚至可以在普通 JavaScript 中使用。 F# 与 Elm 一样，是 ML 编程语言系列中的一种语言，提供模式匹配等功能，这些功能非常强大，对于状态机建模特别有用。与受歧视联合等 F# 功能相结合，这使我们能够以类型安全的方式创建应用程序，以满足所有可能性。当未处理极端情况时，编译器为我们提供指导，从而缩短开发时间，同时允许 F# 开发人员以类型安全的方式快速创建 UI。应用程序变得越大、越复杂，好处就越大；当我们忘记处理所有可能性时（即使是更复杂的模式），F# 编译器（如 Elm 编译器）会向我们发出警告，这意味着花在调试或编写单元测试上的时间更少，而有更多时间提供业务价值。<br><br>For more information about Elmish and the MVU architecture please see the following resources:<br>
有关 Elmish 和 MVU 架构的更多信息，请参阅以下资源：<br>
<br>"Modern app development with Fable and React Native" from NDC Oslo 2017 - <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=fmaPeUBWZuM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=fmaPeUBWZuM" target="_blank">Video</a><br>
“使用 Fable 和 React Native 进行现代应用程序开发”来自 NDC Oslo 2017 - 视频
<br>"The Elm Architecture" in the <a data-tooltip-position="top" aria-label="https://guide.elm-lang.org/architecture/" rel="noopener" class="external-link" href="https://guide.elm-lang.org/architecture/" target="_blank">elm docs</a><br>
elm 文档中的“Elm 架构”
<br>Elmish for F# <a data-tooltip-position="top" aria-label="https://fable-elmish.github.io/" rel="noopener" class="external-link" href="https://fable-elmish.github.io/" target="_blank">docs</a>&nbsp;F# 文档的 Elmish
<br><a data-tooltip-position="top" aria-label="https://github.com/ccorcos/elmish" rel="noopener" class="external-link" href="https://github.com/ccorcos/elmish" target="_blank">Elmish for vanilla JavaScript
用于原生 JavaScript 的 Elmish</a><br>
<br>Fable compiler <a data-tooltip-position="top" aria-label="http://fable.io/" rel="noopener" class="external-link" href="http://fable.io/" target="_blank">docs</a>&nbsp;Fable 编译器文档
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/ui-programming-with-elmish-in-fsharp.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/UI programming with Elmish in FSharp.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:32 GMT</pubDate><enclosure url="https://www.compositional-it.com/wp-content/uploads/2019/09/elmish-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://www.compositional-it.com/wp-content/uploads/2019/09/elmish-1.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Unlocking FSharp Potential -- Tips, Tricks, and Tactics]]></title><description><![CDATA[ 
 <br><a rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=8aBmGUNFBQI" target="_blank">https://www.youtube.com/watch?v=8aBmGUNFBQI</a><br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=8aBmGUNFBQI#t=28.795104190734865" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=8aBmGUNFBQI#t=28.795104190734865" target="_blank">00:28</a>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/unlocking-fsharp-potential-tips,-tricks,-and-tactics.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Unlocking FSharp Potential -- Tips, Tricks, and Tactics.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:29 GMT</pubDate></item><item><title><![CDATA[Writing high performance Fsharp code]]></title><description><![CDATA[ 
 <br>While this post is addressed to F# .NET developers, it introduces much wider concepts starting from hardware architecture to overall .NET runtime and JIT compiler optimizations. It shouldn't be a surprise - optimizing the application performance requires us to understand the relationships between our high level code and what actually happens on the hardware.<br>There's a popular opinion that F# code must be slower than equivalent C# code. This opinion is mostly false, however it comes with some rationale. Usually comparison doesn't use equivalent code in both languages, and F# is generally more high level and declarative in nature. "Idiomatic" F# code doesn't always play well with .NET virtual machine. Writing code that is high level, declarative and fast on .NET platform is not an easy task.<br>In the examples below we'll use some common tools that will help us get better insight into nature of F# code:<br>
<br><a data-tooltip-position="top" aria-label="https://sharplab.io/?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://sharplab.io/?ref=bartoszsypytkowski.com" target="_blank">Sharplab</a> allows us to easily inspect generated JIT intermediate representation, assembly or even equivalent C# code (which sometimes is approximate, since not all IL idioms are representable in C#) for a given F# snippet. For assembly code usually some extra mangling with params may be necessary for code to be generated as SharpLab sometimes cannot introspect F# core lib code.
<br><a data-tooltip-position="top" aria-label="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" target="_blank">Object Layout Inspector</a> lets us see how structs and classes will actually be represented in memory.
<br><a data-tooltip-position="top" aria-label="https://benchmarkdotnet.org/articles/overview.html?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://benchmarkdotnet.org/articles/overview.html?ref=bartoszsypytkowski.com" target="_blank">BenchmarkDotNet</a> is very popular library for writing micro benchmarks. We'll use it to show heap allocations and execution times of our code.
<br>A correct profiling of the executing application is crucial before starting any work on optimizing the code - there's no sense in shaving last possible CPU cycles out of the function that's executed for 0.1% of the time.<br>Keep in mind that for most day-to-day business applications, first way to solve performance problems is to reduce obvious mistakes (eg. replacing multiple I/O requests with one, writing more efficient database query etc.). If that was not the case, next step for satisfactory solution can be simply writing more imperative code - to make it easier to reason about for the compiler rather than human - or picking better-suited data structures. This is especially prevalent in F#, where we can observe pervasive usage of types like 't list (tip: if you're looking which collection to use and want good performance, F# list is almost never a good answer). Here we're about to go deeper, into area where we're about to compete with the prefabricated data types and algorithms.<br><br>One of the big performance gains, that .NET runtime uses to take advantage over other managed virtual machines (like JVM) in race for ultimate performance, often comes from using value types. So if we're about to go fast, we first need to understand how they work.<br>.NET structs represent types, which are not allocated separately on the managed memory heap, but rather inlined within the containing scope (instance of the class in case of fields, thread stack for variables, etc.). This means that usually they are cheaper and easier to access in high-allocation scenarios.<br><img alt="class-vs-struct-layout" src="https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-layout.png" referrerpolicy="no-referrer"><br>Historically, F# code was not very promising, when it comes to utilizing value types. Nowadays we got things like struct tuples - struct('a * 'b) which unfortunately are not widely used in F# even though in practice they should be preferable choice when working with tuples - and [&lt;Struct&gt;] attribute, that can be used on records and discriminated unions (we'll return to them later), making use of them became much more feasible.<br>However this doesn't necessarily mean, that replacing all reference types with value types will make our code magically go faster. In fact, this may be quite opposite. Why? Imagine what happens when we want to pass a record as a parameter. How is it done? Usually passing object to a function happens by copying reference to that object, which is either 4B or 8B depending on our OS being x86 or x64, and therefore fits perfectly into standard CPU register.<br>type A() =
  class
    [&lt;DefaultValue&gt;] val mutable x: int
    [&lt;DefaultValue&gt;] val mutable y: int
    [&lt;DefaultValue&gt;] val mutable z: int
  end

let print (value: 't) = System.Console.WriteLine(value.ToString())

let a = A() // sub rsp, 0x28
            // mov rcx, 0x7ff91b23d1a8
            // call 0x00007ff9730aade0 ; allocate A on the heap
print a     // mov rdx, rax            ; copy reference to a to the stack
            // mov rcx, 0x7ff91b23d5f8
            // call _.print[[System.__Canon, System.Private.CoreLib]](System.__Canon)
Copy<br>Now what if we're using structs? For reference types we copy object's reference on the stack - since reference is just a single address, it always can fit into register and be done within a single operation. For value types, we copy entire value instead. If they don't fit into register, we'll have to copy them over in multiple steps.<br>type B =
  struct
    [&lt;DefaultValue&gt;] val mutable x: int
    [&lt;DefaultValue&gt;] val mutable y: int
    [&lt;DefaultValue&gt;] val mutable z: int
  end

let b = B() // sub rsp, 0x38
            // xor eax, eax       ; zero field b.x
            // xor ecx, ecx       ; zero field b.y
            // xor edx, edx       ; zero field b.z
print b     // lea r8, [rsp+0x28]
            // mov [r8], ecx      ; copy field b.x to the stack
            // mov [r8+4], eax    ; copy field b.y to the stack
            // mov [r8+8], edx    ; copy field b.z to the stack
            // lea rcx, [rsp+0x28]
            // call _.print[[_+B, _]](B)
Copy<br>Each of these steps is a machine instruction that takes time to execute. However, sometimes .NET can optimize that - pointer-sized registers are not only ones available in modern machines. We also have a special purpose SIMD (Single Instruction Multiple Data) ones, that are much bigger and can be used as long as passed data fits into them perfectly.<br>type C =
  struct
    [&lt;DefaultValue&gt;] val mutable x: int
    [&lt;DefaultValue&gt;] val mutable y: int
    [&lt;DefaultValue&gt;] val mutable z: int
    [&lt;DefaultValue&gt;] val mutable zz: int
  end
  
let c = C() // sub rsp, 0x48
            // xor eax, eax             ; zero register
            // mov [rsp+0x38], rax      ; init fields b.x and b.y together with zero'ed register
            // mov [rsp+0x40], rax      ; init fields b.z and b.zz together with zero'ed register
print c     // vmovupd xmm0, [rsp+0x38] ; copy all 4 fields together on the stack using SIMD registers
            // vmovupd [rsp+0x28], xmm0
            // lea rcx, [rsp+0x28]
            // call _.print[[_+C, _]](C)
Copy<br>Another thing available in .NET, that allows us addressing inefficiencies of passing structs as arguments are so called by-ref parameters. There are 3 types of these, marked using 't inref, 't outref and 't byref:<br>let print(value: 't inref) = ...
let c = C()
print &amp;c // lea rcx, [rsp+0x28] ; copy address of the struct head onto the stack
         // call _.print[[_+C, _]](C ByRef)
Copy<br>Please, don't confuse by-ref parameters with ref data type:<br>
<br>'a ref is actually an alias for Ref&lt;'a&gt; class, therefore allocated on the heap and passed by reference. In general, using this class in F# very rarely has sense (outside of writing exemplar, idiomatic code).
<br>'a byref is equivalent to C# ref parameter tag - it means that we're passing reference (memory address) to an object or struct. It expects it to be initialized and can be used to change the contents of the underlying value. For this reason F# requires fields and variables passed as byref to be declared with mutable keyword.
<br>'a outref is equivalent to C# out parameter tag - it always must be initialized by the end of the function body. This may sound a bit tricky as F# doesn't put that requirement explicitly. If we didn't make that assignment in any of the code branches, F# compiler will simply initialize it for us with default value (just like using Unchecked.defaultof&lt;_&gt;), which sometimes may lead to null reference exceptions.
<br>'a inref is the youngest of these and is equivalent of C# in parameter - while in C# structs passed as arguments for that parameters don't have to be tagged, F# will always require to mark passing by ref (using &amp; prefix for passed argument) for any parameter marked with byref/inref/outref. inref is basically an optimization technique for what we saw above - it allows us to pass struct into a function using only its memory address, without copying entire struct contents. Additionally inref says that parameter is treated as read only, so it cannot be modified inside of function body. .NET JIT can utilize this information in some cases to reduce number of safety checks, therefore reducing number of instructions to be executed.
<br>While using by-ref parameters is usually good idea when it comes to writing code targeting complex value types, there are several limitations to it.<br>One is that arguments passed using by-ref params cannot be captured by closures/lambdas/anonymous functions, which prevents them from being used in more abstract code:<br>// WRONG!
let doSomething (a: int inref) =
  [1..10]
  |&gt; List.map (fun x -&gt; x + a) // `a` is captured by closure, which is compilation error
  
// RIGHT
let doSomething (a: int inref) =
  let mutable result = []
  for x=10 downto 1 do
    result &lt;- (x + a)::result
  result
Copy<br>This includes problems even for common inlined functions like eg. pipe operator |&gt;. That's the price, we have to pay for speed (at least for now).<br>Second issue is that, at the moment by-ref parameters cannot be involved in building nested functions (regardless if they capture the values from function in outer scope or not). This again makes very inconvenient to use them in cases like tail recursive loop pattern:<br>// WRONG!
let doSomething (a: 'a) =
  let rec loop n (x: 'a inref) = // this nested function won't compile
    if n = 0 then ()
    else loop (n-1) &amp;x
  loop 100 &amp;a

// RIGHT
let rec loop n (x: 'a inref) =
  if n = 0 then ()
  else loop (n-1) &amp;x
  
let doSomething (a: 'a) = loop 100 &amp;a
Copy<br><br>While we talked about by-ref params, .NET (and latest F#) enable us to do something more - we can define so called by-ref structs and readonly structs:<br>[&lt;Struct; IsByRefLike; IsReadOnly&gt;]
type BufWriter&lt;'a&gt; =
  // since BufWriter is by-ref struct it can have by-ref types as fields
  // otherwise it would result in compilation error
  [&lt;DefaultValue&gt;] val buffer: ReadOnlySpan&lt;'a&gt;
  
/// F# records and discriminated unions are marked with IsReadOnly by default
[&lt;Struct; IsByRefLike&gt;]
type BufWriter&lt;'a&gt; = { Buffer: ReadOnlySpan&lt;'a&gt; }
Copy<br>[&lt;IsReadOnly&gt;] attribute let's us define given structure as being readonly. For obvious reasons this also means, that corresponding data type cannot contain any mutable fields within.<br>It's used as a slight optimization technique - sometimes .NET JIT compiler must guarantee that structs contents will not be modified. To do so, it will conservatively copy that structure, even when it has been passed into function using inref parameter. If struct has been marked with [&lt;IsReadOnly&gt;] attribute, compiler can skip this step and avoid building defensive copies. You can read more about it <a data-tooltip-position="top" aria-label="https://devblogs.microsoft.com/premier-developer/avoiding-struct-and-readonly-reference-performance-pitfalls-with-errorprone-net/?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://devblogs.microsoft.com/premier-developer/avoiding-struct-and-readonly-reference-performance-pitfalls-with-errorprone-net/?ref=bartoszsypytkowski.com" target="_blank">here</a>.<br>[&lt;IsByRefLike&gt;] is another attribute. We are talking a lot about passing value types using memory location addresses instead of doing deep copies. Marking struct using this attribute is basically saying "I always want to pass this value by reference". This of course comes with severe limitations: it cannot be boxed (moved to managed heap) and for this reason it can never be captured by closures, implement interfaces or be used as field in classes or other non-by-ref structs.<br>In terms of F# this basically means that this kind of structs are used mostly for code that is executed right away within the function body, with no computation expressions or other indirections. This usually qualifies them to hot paths in our code, where CPU intensive work is expected and allocations are not welcome, like:<br>
<br>for .. in loops - in fact many moderns .NET structures have special variants of GetEnumerator that doesn't allocate any memory and is implemented as by-ref struct. F# also understands that pattern - in fact you can define custom GetEnumerator(): MyEnumerator method for your collection, with MyEnumerator - which can even be a ref struct - having two methods: Current: 'item and MoveNext: unit -&gt; bool, and F# will automatically understand how to use it in loops. You can see an example implementation of it <a data-tooltip-position="top" aria-label="https://github.com/Horusiath/fsharp.core.extensions/blob/62b102e84325e89b0a6c4065b973936c11adee55/src/FSharp.Core.Extensions/Vec.fs?ref=bartoszsypytkowski.com#L147" rel="noopener" class="external-link" href="https://github.com/Horusiath/fsharp.core.extensions/blob/62b102e84325e89b0a6c4065b973936c11adee55/src/FSharp.Core.Extensions/Vec.fs?ref=bartoszsypytkowski.com#L147" target="_blank">here</a> - it's a part of implementation of persistent vector data type, similar to <a data-tooltip-position="top" aria-label="https://fsprojects.github.io/FSharpx.Collections/PersistentVector.html?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://fsprojects.github.io/FSharpx.Collections/PersistentVector.html?ref=bartoszsypytkowski.com" target="_blank">FSharpX persistent vector</a>, but it's 4.5 times faster and not allocating anything on heap when executed in loops.
<br>Contextual data around byte-shaving operations. All things related to parsing/formatting can make use of that technique to optimize speed and reduce allocations. It's also used inside of all kinds of drivers working with I/O.
<br>While we're using explicit class/struct type definition here, from .NET runtime point of view memory layout for records and struct records is exactly the same (it differs for discriminated unions thou, but we'll cover that soon).<br><br>Another point worth noticing is that .NET have it's own assumptions regarding data size of classes. Let's see that on an example:<br>type A = { x: int; y: int }
type B = { x: int; y: int; z: int }
Copy<br>How do you think, what's the size of A and B? Naively, we could assume that B instance would be 4 bytes bigger than instance of type A. However that's not always true. Let's inspect memory layout of both classes:<br><img alt="padding-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/padding-1.png" referrerpolicy="no-referrer"><br>As you can see both classes start with 16 byte object header and vtable pointer: it's mandatory for every class (and boxed structs). They make things like method overriding or lock calls on objects possible. Then we have actual class content: 2 * sizeof(int) = 8 bytes in case of A and 3 * sizeof(int) = 12 bytes in case of B. However that's not the end. In case of B you can also see 4 extra bytes of padding. Where does it comes from?<br>When managing heap size, .NET GC/allocator makes some simplifications. Namely it assigns blocks of memory that are multiplications of a standard pointer size, which is 4 bytes on 32-bit OS'es and and 8 bytes on 64-bit ones. So, when instantiating objects, GC will always assign them as much space as it's necessary to encapsulate all fields and fit into 4-/8-bytes ceiling: since most servers operate on 64-bits nowadays, we're talking about buckets of 16+8 bytes, 16+16 bytes, 16+24 bytes etc.<br>What's interesting, this padding requirement doesn't concern unmanaged structs (value types consisting only of other value types). If we modify our record B to be a struct:<br>[&lt;Struct&gt;] type B = { x: int; y: int; z: int }
Copy<br>, we'll see that it takes only 12 bytes. If we take into account object header, that's over 2.5 less space than in case of class-based record, with no heap allocations and therefore no need to GC it later. Keep in mind that adding a reference type (eg. string) as struct field will cause it to add padding again. In that case the space saving comes from lack of object header/vtable pointer.<br>Now, if necessary we could also apply padding to structs manually. While in eg. Java you need to add redundant extra fields to do that, in .NET we can hint the runtime about the expected struct size:<br>[&lt;Struct; StructLayout(LayoutKind.Auto, Size=16)&gt;] 
type B = { x: int; y: int; z: int }
Copy<br>StructLayout has many useful properties i.e. it opens the door to manually define the position of each record field within the type. It also exposes the Size property, which we can use to manually say what's the expected size of our struct - in that case when creating it, runtime will explicitly add extra bytes for padding. But what would we need it for? We answer that <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#falsesharing" rel="noopener" class="external-link" href="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#falsesharing" target="_blank">shortly</a>.<br><br>We need to go a little bit deeper and step into hardware territory. Junior programmers often are taught to think about computer memory as a single homogenous block. That's a convenient lie, especially since languages - even as low level like C - rarely expose any primitives to operate on it. From computer architecture classes, you could learn that memory is split into several layers - from RAM to L1-L3 caches.<br><img alt="CPU-architecture" src="https://www.bartoszsypytkowski.com/content/images/2021/02/CPU-architecture.svg" referrerpolicy="no-referrer"><br>Thing is that, access time to L1 can be several dozens times faster than to main memory (RAM). For this reason, when data residing in main memory is about to be used by the CPU, it's first loaded into cache. Hardware does a little bet here: it comes into assumption that most of the data used together resides in the main memory closely to each other. For that reason it doesn't just load a single object reference - it doesn't even know what is it - but instead an entire following block of data, so called cache line. On modern hardware, cache lines are usually 64 bytes long.<br>One of the reasons, why we talked about structs for so long is that collections of entities like A[] behave very differently depending on A being a class or a struct:<br>
<br>If A is a class, it means that A[] contains only references to objects, which actual contents may reside in totally different places of memory. Given nature of .NET GC, when they are created on different threads, you may be pretty sure they will not be placed together. This means that when iterating over them, you may need to load them many times from different places in memory.
<br>If A is a struct, then A[] will contain inlined values of A, with all their contents stored sequentially next to each other.
<br>There's one thing about the cache lines, that can cause misleading results during microbenchmarking of the code. Consider simple operation like sum of the list values: intList |&gt; List.sum. Let's run it twice and check the results:<br><br>In both cases we're talking about the exactly same code over preallocated lists (so list initialization is not part of the benchmark), yet second example takes almost 3 times longer to execute. What has changed then? When setting up the test case I added extra allocation of an object in between appending nodes of the list like so:<br>caseA &lt;- [1..1024] // dataset for TestA
for i = 1024 downto 1 do
    caseB &lt;- i::caseB // dataset for TestB
    unused &lt;- { x = i; y = i; z = i }::unused
Copy<br>Since F# list is implemented as a linked list, it means that its nodes are allocated on a heap and linked together. In first case, even thou suboptimal, those list nodes were still allocated in continuous space in memory, making more efficient use of cache line loads. In second case, our list was fragmented over much bigger space of memory. If elements of our list are value types, we can squash them together by using List. operations over it or just map it into an array. This however won't work for reference types, as we'll only move pointers alone, while objects themselves will stay in their old place.<br>Another way to improve performance of some operations is to revert the field order of the elements stored in collection, eg:<br>[&lt;Struct&gt;] type Point3D = { x: int; y: int; z: int}

type ContainerA(input: Point3D[]) =
  member this.SumX = input |&gt; Array.sumBy (fun a -&gt; a.x)
  member this.Item index = input.[index]

type ContainerB(input: Point3D[]) =
  let x = input |&gt; Array.map (fun a -&gt; a.x)
  let y = input |&gt; Array.map (fun a -&gt; a.y)
  let z = input |&gt; Array.map (fun a -&gt; a.z)
  member this.SumX = Array.sum x
  member this.Item index = { x = x.[index]; y = y.[index]; z = z.[index] }
Copy<br>Now, depending on which operation we care about more - accessing a single element, or computing sum of X coordinates - one or another implementation will have more sense. This approach is even more prevalent if we look into world of databases - a big contributor to performance difference between OLTP databases (oriented towards standard transactional workloads) and OLAP databases (oriented towards analytical data processing) comes exactly from laying out data on a disk by rows vs by columns.<br>PS: In case of ContainerB we can add even better optimization techniques in form of <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#makeuseofvectorization" rel="noopener" class="external-link" href="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#makeuseofvectorization" target="_blank">vectorization</a>, which we'll cover further down the blog post.<br><br>Now the next thing is that L1-L2 caches are residing closely to CPU cores. In fact, as we've shown, every core has it's own cache. This comes with it's own problems: since every CPU has it's own copy of a value, they occasionally need to synchronize and invalidate their caches when that value is accessed from different cores. This is an expensive operation, which we want to avoid.<br>When such accidental sharing may happen? It's not easy to detect in micro benchmarks, and usually needs a dose of profiling and good old fashion trial and error of actual application code. IMO that's why optimizations in this area are not applied so often. Some tips to help build our intuition are:<br>
<br>This can happen when two adjacent fields of the same object are concurrently accessed and modified from different threads. Thing is that unless you configure your types with [&lt;StructLayout(LayoutKind.Explicit)&gt;] you won't know if two fields defined in code one after another will be placed in adjacent memory cells by .NET runtime. Using <a data-tooltip-position="top" aria-label="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://github.com/SergeyTeplyakov/ObjectLayoutInspector?ref=bartoszsypytkowski.com" target="_blank">Object Layout Inspector</a> can help you validate your assumptions here.
<br>It can also happen that two different objects/structs will be placed closely inside of collection. This is mostly common for array-backed collections (as they keep elements continuously in memory block) and with small structs (as you may fit more of them inside of single cache line).
<br>We can sometimes reduce risk of false sharing in 2nd case and making it more predictable, by defining type size explicitly to fit exactly into boundaries of cache lines eg. 64B (it's enough to have a class with 6 references/12 int fields or a struct with 8 references/16 int fields). If you know that your objects can be accessed concurrently, but don't fit nicely into into cache lines, you may add extra padding by using [&lt;StructLayout(LayoutKind.Auto, Size=64)&gt;] in your struct definition. While memory usage increases, the overall application performance may improve.<br><br>So far, we only talked about structs in terms of singular elements - when talking about collections, we got pretty much used to the fact, that we have to allocate. This however is not always the case. .NET has a long history of allowing users to allocate collections on stack rather than heap - in C# it's related with stackalloc keyword, in F# it's bit more verbose:<br>open FSharp.NativeInterop

let inline stackalloc&lt;'a when 'a: unmanaged&gt; (length: int): Span&lt;'a&gt; =
  let p = NativePtr.stackalloc&lt;'a&gt; length |&gt; NativePtr.toVoidPtr
  Span&lt;'a&gt;(p, length)
Copy<br>What we returned here is a Span&lt;'a&gt; - a by-ref struct type (meaning: it cannot be used in closures or as a field in most types), that allows us to address its elements just like they existed on the heap. In general, you should avoid allocating too much memory on the stack (in .NET stacks have fixed size that by default is limited to 1MB per thread, allocating over it will cause irrecoverable StackOverflowException). Most common case for using these are short parsing methods, that can be used on the hot paths without producing garbage to be collected later:<br>/// Parse Protocol Buffers style variable length uint32.
let readVarUInt32 (reader: Reader) : uint32 =
  // var int for 32 bit values is never encoded on more than 5 bytes
  let buffer = stackalloc&lt;byte&gt; 5 
  let read = reader.Read buffer
  if read = 0
  then failwith "trying to read var int from empty stream"
  else
    let buffer = buffer.Slice(0, read)
    let mutable decoded = 0u
    let mutable i = 0
    let mutable cont = true
    while cont &amp;&amp; i &lt; buffer.Length do
      let b: byte = buffer.[i]
      decoded &lt;- decoded ||| ((uint32 (b &amp;&amp;&amp; 0x7Fuy)) &lt;&lt;&lt; i * 7)
      i &lt;- i + 1
      if b &lt; 0x80uy then  // check if most significant bit is set
        cont &lt;- false // stop condition reached    
    reader.Advance i
    decoded
Copy<br>Even though we did create a buffer (to avoid cost of multiple virtual calls to reader.Read method), in practice we didn't allocate anything that has to be later collected by the GC.<br>Unfortunately we cannot use spans everywhere eg. as fields of ordinary classes, but there are still situations where we might want to have collections without GC. This often desirable in case of huge number of collections, that most of the time are very small (eg. <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/the-state-of-a-state-based-crdts/#noteaboutvectorclocks" rel="noopener" class="external-link" href="https://www.bartoszsypytkowski.com/the-state-of-a-state-based-crdts/#noteaboutvectorclocks" target="_blank">vector clocks</a>). We can imagine such non-allocating collection like:<br>[&lt;IsReadOnly&gt;]
type HybridMap&lt;'k, 'v&gt; =
    struct
       let count: int  // size: 4B
       // null by default, initialized once we pass over 3 entries
       // size: 8B (reference)
       let inner: Map&lt;'k,'v&gt;
       // inline first 3 entries. size: 48B = 3 * (8B+8B) (assume reference type)
       let entry0: KeyValuePair&lt;'k,'v&gt;   
       let entry1: KeyValuePair&lt;'k,'v&gt;
       let entry2: KeyValuePair&lt;'k,'v&gt;
    end
Copy<br>With map like this, adding first 3 elements produce no garbage. Why only 3? Just like mentioned previously, we prefer our structs to fit into cache lines and this way (assuming both 'k and 'v types are classes) we'll still not surpass 64B (on 64bit OS) or 32B (on 32bit one).<br><br>Discriminated unions are somewhat special citizens, in a sense they have to be represented in terms of .NET reference types (classes) or value types (structs). It means that depending on how they are defined (with or without [&lt;Struct&gt;] attribute), their in memory representation may be very different.<br><img alt="class-vs-struct-union-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-union-1.png" referrerpolicy="no-referrer"><br>A thing you can see in both situations is that fields order in both cases doesn't reflect order in which they were defined. That's because .NET runtime can reorder them in any type unless explicitly forbidden by using [&lt;StructLayout(LayoutKind.Explicit)&gt;] attribute. Moreover, using this attribute is forbidden in case of discriminated unions.<br>Another thing, that you may have noticed is how DUs are represented. A class-based discriminated union is basically equivalent to an empty abstract class with each case being a sealed class inheriting from it. Struct based DU is more complicated. Many other languages with algebraic data types optimize the size according to formula sizeof(tag field) + max([sizeof(ADT case)]). But not F# - what we see here is sizeof(int) + sum([sizeof(ADT case)]):<br><img alt="rust-vs-fsharp-adt-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/rust-vs-fsharp-adt-1.png" referrerpolicy="no-referrer"><br>The reason behind this is a limtation of .NET platform - while technically it's possible to use <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.layoutkind?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.interopservices.layoutkind?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">LayoutKind.Explicit</a> to implement F# in similar manner to i.e. Rust, it works only as long as we don't try to use it together with generics: .NET cannot make safe guesses in that situation. In result, used memory space for struct-based DU is a sum of all fields defined all cases. For this reason good candidates for struct DU's are usually the ones with very small overall number of fields. F# standard library makes good use of these in form of Result&lt;'t,'e&gt; and 't voption types.<br>Options have somewhat special treatment when it comes to F#. While value options work pretty much in ordinary way, class-based ones have some special magic in them. We could imagine them as:<br>[&lt;Struct&gt;]
type ValueOption&lt;'t&gt; =
  | ValueNone
  | ValueSome of value:'t
  
type Option&lt;'t&gt; = 
  { Value: 't }
  static member Some value = { Value = value }
  static member None = Unchecked.defaultof&lt;Option&lt;'t&gt;&gt; // null
Copy<br>This means, that value option will always have to be initialized and use sizeof(int) + sizeof('t) bytes in memory, while standard option may get optimized away into uninitialized instance (null), which size is always sizeof(IntPtr). So, while you still need to pay for allocating new object for cases where option indeed has value, in some cases where you're working with collections of mostly None values, it may turn out that using standard options is actually more effective approach.<br>A common scenario, where option types are used is when we're dealing with failable operations like trying to find element in a map (which may not be there) or parsing an int. In that case keep in mind:<br>
<br>Using option is the most expensive approach, as eventual success means allocating an extra intermediate object.
<br>Using voption is much cheaper, but at the moment .NET doesn't really know how to pass value types using registers alone, so returning voption may mean copying it through stack in multiple steps, even though we avoided GC allocations.
<br>In practice, the best performing solution is using straight old pattern - popular in C# - of try function definition ending up with 't outref -&gt; bool signature. It can be found in most .NET collections (both F# and C#), as well as parsing methods. In fact this pattern is so popular, that F# can automatically derive tuple out of it eg. let (ok, value) = map.TryGetValue(key). Under normal circumstances this tuple would mean heap allocations, however if you won't capture it and propagate further, but instead use its contents right away like i.e. in match expression, it will let F# compiler to skip allocating an object.
<br>One of the patterns sometimes used by F# programmers is to add more type info to value by wrapping it into DU:<br>type Mileage = Mileage of int 
Copy<br>Patterns like this one are generally devastating for performance - we're allocating 24B of garbage on the heap for every int used. Using units of measure or even type aliases is much better option, since both of them have only compile time representation and are erased by the compiler, they never introduce a runtime overhead.<br>[&lt;Measure&gt;] type miles
type Mileage = int&lt;miles&gt;
Copy<br><br>Did you even wonder, when function is about to call a method on interface parameter, how does it actually know, where to find the method of the underlying object implementing that interface? Runtime resolves actual method to be called by jumping to virtual table of that object (pointer to vtable is part of object header), finding the address of corresponding method (function can have pointers too!) and calling it. As you may imagine all of that indirections can take time. If you think, you're safe because you're not doing object method calls but using module functions instead, check twice - in practice many of them are being inlined as non-static methods.<br><img alt="virtual-call-dispatch-1" src="https://www.bartoszsypytkowski.com/content/images/2021/02/virtual-call-dispatch-1.png" referrerpolicy="no-referrer"><br>But how much longer does it actually take? Let's take an example code:<br>/// interface we want to test
type Stub = abstract member DoNothing: unit -&gt; unit
/// class implementing an interface
type A() =
  member _.DoNothing() = ()
  interface Stub with member this.DoNothing() = this.DoNothing()
  
/// struct implementing an interface
[&lt;Struct&gt;]
type B =
  member _.DoNothing() = ()
  interface Stub with member this.DoNothing() = this.DoNothing()

[&lt;MemoryDiagnoser&gt;]
type Benchmark() =
  [&lt;DefaultValue&gt;] val mutable a: A
  [&lt;DefaultValue&gt;] val mutable b: B
  
  static let execute (x: Stub) = x.DoNothing()
  static let executeGeneric (x: #Stub) = x.DoNothing()  
  static let executeDirect (x: A) = x.DoNothing()  
  static let executeDirect2 (x: B) = x.DoNothing()  
  
  [&lt;GlobalSetup&gt;]
  member this.Setup() =
    this.a &lt;- A()
    this.b &lt;- B()        
  [&lt;Benchmark(Baseline=true)&gt;] member this.ExecuteClassDirect() = executeDirect this.a
  [&lt;Benchmark&gt;] member this.ExecuteClass() = execute this.a
  [&lt;Benchmark&gt;] member this.ExecuteClassGeneric() = executeGeneric this.a
  [&lt;Benchmark&gt;] member this.ExecuteStructDirect() = executeDirect2 this.b
  [&lt;Benchmark&gt;] member this.ExecuteStruct() = execute this.b
  [&lt;Benchmark&gt;] member this.ExecuteStructGeneric() = executeGeneric this.b
Copy<br>Example run from BenchmarkDotNet could give us following results:<br><br>There are couple of interesting observations to be made here:<br>
<br>In both cases when either struct or class type parameter is known exactly, the call itself is almost instantaneous (in fact for ExecuteStructDirect method you should receive warning about entire call being optimized away). That's because runtime can say directly which version of the method is going to be called and skip the dispatch to type's virtual table. We call this devirtualization - a process, when runtime is able to replace virtual call dispatch with a direct function call. This is also the reason why you should not pass objects by interface parameters i.e. 't seq instead of 't[] any time when performance matters.
<br>While there exists a small difference between a class instance being called via interface or as generic argument, this is not what we're after. In both cases we can observe similar results - it's because .NET 5 runtime doesn't really specialize generic method calls for reference types. At the moment it can happen sometimes for sealed classes. This means, that usually calling by interface (either explicitly or by using generic type parameter) will require virtual table dispatch anyway.
<br>Passing struct as parameter into a function that expects interface requires boxing - it means that this structure is copied onto heap (hence we can see allocations), prefixed with a header that includes a vtable pointer. From here struct methods look very similar to classes. All of this extra work causes the entire call to be even more expensive. Sometimes when we know that we're about to pass a struct into some callback expecting a generic object (<a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.threading.timer.-ctor?view=net-5.0&amp;ref=bartoszsypytkowski.com#System_Threading_Timer__ctor_System_Threading_TimerCallback_System_Object_System_Int32_System_Int32_" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.timer.-ctor?view=net-5.0&amp;ref=bartoszsypytkowski.com#System_Threading_Timer__ctor_System_Threading_TimerCallback_System_Object_System_Int32_System_Int32_" target="_blank">Timer</a> can be such example) multiple times, it may we worth to eagerly box it ahead and pass boxed version instead, reducing number of allocations. This may also be a sign that reference type is preferred over struct.
<br>An interesting thing happens, once we pass a struct as generic argument. As you can see, this call is several times faster than generic call on the class instance. This is because for structs, .NET JIT uses a generic code specialization - it basically emits machine code for this method call, dedicated for handling this particular type of struct when it's passed as a generic type argument. Since this is specialized branch of code, we don't need to check for vtable every time, as we already can say what function implementation is going to be called.
<br>Generic function specialization may sound a little similar to SRTP (statically resolved type parameters - generics which are erased by F# compiler at compile time), but it's performed by .NET runtime itself. Unlike SRTP, it doesn't prolong our compile times (and .NET JIT is really fast at machine code generation) and can be composed in much better way - you can pass generic functions as parameters themselves over many levels of function calls and let .NET runtime optimize them in a wider context.<br>In some cases we can leverage .NET behavior to introduce something aligned to zero-cost abstractions in our code - it's a term forged by Rust programming language, meaning that we can write abstract code that's as fast as equivalent code written by hand. While .NET and F# offer it in limited scope, we can still use it. Example:<br>type Hasher&lt;'t&gt; = abstract Hash: 't -&gt; int
let inline hash&lt;'h, 't when 'h: struct and 'h :&gt; Hasher&lt;'t&gt;&gt; (value: 't) =
  Unchecked.defaultof&lt;'h&gt;.Hash value

// we should introduce interface for equality check as well

type HashSet&lt;'t, 'h when 'h: struct and 'h :&gt; Hasher&lt;'t&gt;&gt;() =
  member this.Add(item: 't) =
    let h = hash&lt;'h, _&gt; item // 'h cannot be inferred as it has no input parameter
    // ... rest of the implementation

module StringHashers =
  
  [&lt;Struct&gt;]
  type InvariantCultureIgnoreCase =
    interface Hasher&lt;string&gt; with
      [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;]
      member this.Hash(key: string) = StringComparer.InvariantCultureIgnoreCase.GetHashCode key
  
open StringHashers  
let map = HashSet&lt;string, InvariantCultureIgnoreCase&gt;()
Copy<br>Here we managed to introduce two improvements:<br>
<br>Our collection is safer that ordinary .NET HashSet&lt;'t&gt;, as we included information about what hashing method we use. Traditional HashSet&lt;'t&gt; can take IEqualityComparer&lt;'t&gt; as a parameter, but doesn't expose it at type level. That means, that a.Union(b) operation may yield different result than b.Union(a) when a and b use different comparers and we don't get any warnings.
<br>Our collection is also faster, because we provided a precise definition of what hashing method we use at the type level. As mentioned before, this allows .NET runtime to specialize HashSet&lt;'t,'h&gt; methods over 'h, since it's a struct type. Unchecked.defaultof&lt;'h&gt; used here is a pattern that we can use, as we cannot provide static classes (or F# modules) as generic type parameters. However we can provide a struct with no fields instead - it's in-memory representation is 0 bytes, and since it's not boxed (we're providing it as a generic value), .NET will elide its existence completely and compile it to the exact method call (no virtual dispatch is necessary here).
<br>If you're interested more with this approach, I can recommend <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=UybGH0xL5ns&amp;ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=UybGH0xL5ns&amp;ref=bartoszsypytkowski.com" target="_blank">this presentation</a> by Frederico Lois.<br><br>We already mentioned registers. As you may (or may not) know, modern day processors offer general purpose registers up to 64bit size. But that's not end of the story. You might have stumbled upon term SIMD (Single Instruction Multiple Data), which was already used here - it's a technique that allows to apply the same operation to multiple values at once. It's a basis for efficient graphical processing and number crunching - hence it's a core building block for GPU programs (eg. shaders).<br>However pretty much every modern day CPU also have dedicated registers - varying in size from 128, 256 to 512 bits atm. of writing this post - that can also be used for this purpose. Their API has been wrapped and exposed in .NET, and it's sometimes used for common operations i.e. finding substrings in provided text or copying structs of certain sizes (we saw that already). Sometimes we call the process of making code use these specialized registers, vectorization.<br>You can also use them on your own. Let's a simple function that's supposed to check if value can be found within given array:<br>
<br>Since vectorized operations can work only over vectors, we first need to create a vector filled in all places with value we try to find.
<br>Next, instead of checking array elements one by one, we load entire chunk of it (as much as we can fit into the vector), and compare it with a previously constructed comparator. This comparison is done over all pairwise vector elements in one instruction.
<br>Since we have to compare all vectors contents or none at all, we need to fallback to standard comparison of elements one by one, once an array remainder is smaller than vector's capacity.
<br>#r "nuget: System.Numerics.Vectors"
open System.Numerics

let inline contains (value: 't) (array: 't[]) =
  let chunkSize = Vector&lt;'t&gt;.Count
  /// ' Use SIMD registers to compare chunks of array at once
  let rec fast (cmp: Vector&lt;'t&gt;) (array: 't[]) (i: int) =
    if i &gt; array.Length - chunkSize then slow value array i
    elif Vector.EqualsAny(cmp, Vector(array, i)) then true // compare entire chunk at once
    else fast cmp array (i+chunkSize)
    
  /// if array remainer size doesn't fit into SIMD register
  /// fallback to check array items one by one
  and slow (value: 't) (array: 't[]) (i: int) =
    if i &gt;= array.Length then false
    elif array.[i] = value then true
    else slow value array (i+1)
    
  // create vector of 't filled with value copies on all positions
  let cmp = Vector(value)
  fast cmp array 0
Copy<br>Limitation here is that this operation works only on numbers - the smaller they are in size, the more of them can we compare at once. But is it fast? Let's check it and compare against standard F# Array.contains 900 [|1..1000|]:<br><br>It's 4 times performance improvement simply by using vectorized operations. There are <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.numerics.vector?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.numerics.vector?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">dozens of operations</a> defined in high-level Vector API. Knowledge on how to make an advantage of SIMD is basis for modern day design and implementation of data structures and algorithms.<br><br>Immutable data structures are prevalent technique of writing programs in functional paradigm and F# is no exception here. Thing is, that immutable structures, be it records or collections, introduce an extra overhead related to copying parts of the code in use. This can be reduced by writing structures that take advantage of <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Persistent_data_structure?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Persistent_data_structure?ref=bartoszsypytkowski.com" target="_blank">structural sharing</a> - which deserves its own blog post - but in general .NET doesn't provide optimizations that could make executing single-threaded, large scale immutable code faster than its mutable equivalent.<br>Often advantage of having immutable data types is not performance related - they just offer simpler way to analyze and debug program, which has fewer moving parts, as you can compare snapshots of previous/current/expected states with each other.<br>One of the bigger advantages - which we're also going to use soon - it's natural idempotency of such structures. Aside of being useful in testing (some of the model checkers and property-based tests make extensive use of it), we can also use it to improve performance. It's not usually visible in simple code microbenchmarking, but rather when we need to combine several different operations at once.<br>Example: imagine, that you have a dictionary, that needs to be updated concurrently, but also checked for size from time to time. In .NET we could simply implement it by using ConcurrentDictionary&lt;'k,'v&gt;, but there's a catch - have you ever wondered, how mutable concurrent dictionary ensures, that during counting process a dictionary size has not changed? Well it does it in the simplest way - it locks entire dictionary until counting completes.<br>open System
open System.Collections.Concurrent

let map = ConcurrentDictionary&lt;_,_&gt;()

// 1st set of workers tries can try to add value to a map
let write k v = map.AddOrUpdate(k, Func&lt;_,_&gt;(fun _ -&gt; v), Func&lt;_,_,_&gt;(fun _ _ -&gt; v))

// another worker performs operation over the elements of the map
let count () = map.Count
Copy<br>This issue doesn't really exists in immutable collections, since there's no risk of changing the collection as it's being iterated - we can simply iterate over old (possibly outdated) snapshot of data, but we won't stop the field/variable from being updated.<br>// shared mutable field
let map = ref Map.empty

let write k v = map := Map.add k v !map

let count () = Map.count !map
Copy<br>Now, the question would be - is it safe? Well... no. Modifying contents of F# ref cells (or even static mutable members) is not threadsafe operation. But we can make it so. How? Old school way would be to fall back to OS-level primitives like semaphores and mutexes, but we still can actually make them faster, thanks to the idempotency of immutable collections.<br><br>We'll again fallback to hardware intrinsic operations, this time exposed as part of <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.threading.volatile?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.volatile?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">Volatile</a> and <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.threading.interlocked?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.interlocked?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">Interlocked</a> classes. We can use first to make sure that our reads and writes are invalidating field values that might be accidentally reordered or cached by other CPU cores. Just like .NET runtime feels free to reorder our fields in defined types, it - of even CPU itself - can decide to reorder our operations if it deems it to be more efficient. The latter provides a set of operations, which can be executed within single processor instruction - without worrying that OS may decide to switch threads in between leaving our shared variable in undefined state.<br>The most powerful method in that toolbox is compare-and-swap in .NET known as Interlocked.CompareExchange, which enables to atomically swap register-sized value or reference to a new one, but only if the existing reference at the moment of swap is equal to expected one. What's important here, we're doing reference-based comparison (basically comparing pointer addresses), not structural one that we know as default in F#.<br>Now, here's how can we use that operation to replace locks:<br>let mutable map = Map.empty

let rec write k v =
  let prev = Volatile.Read &amp;map // read most up-to-date value
  let next = Map.add k v prev   // update operation
  if obj.ReferenceEquals(prev, Interlocked.CompareExchange(&amp;map, next, prev)) 
  then () // we successfully updated the map
  else write k v // retry
Copy<br>What we basically try to do is to retrieve the most recent value, update it and store back using Interlocked.CompareExchange. This operation returns a previously stored value, which should be the same reference as the one, we got prior to making an update. If it's different, it means that another thread concurrently swapped it while we were making an update. It's very rare situation, even when lock contention is fairly high, but if it happens, we just retry a whole operation.<br>Here, we're using Map.add but in practice this could be any function 't -&gt; 't, as long as it satisfies few conditions:<br>
<br>A reference returned by update function must be different than the input - otherwise our if expression will never be able to reach the stop condition. It's one reason, why we prefer immutable data types here.
<br>Update must be idempotent. In case of concurrent conflict, only one of the sides will win, while other will have to repeat, and we don't want to i.e. insert the same item to our list multiple times. That's another reason for using immutable collections.
<br>Update action should be fairly fast. No I/O operations, no number crunching. The longer it takes to execute, the less useful this pattern becomes, as risk of retries grows and their cost may outweigh the cost of acquiring the lock.
<br>All of this puts some strong restrictions on the code that can be used with this pattern, nonetheless it's still used pretty often - especially in combination with standard locking mechanism, where we first try to optimistically use Interlocked primitives to acquire fast locks and on failure fallback to heavier ones. This is how "slim" locks work as well as thread safe queues and unbounded channels (eg. BlockingQueue).<br><br>Inlining is a popular optimization technique, were instead of making function call, we directly emit function body in the outer function. This allows us to avoid costs like putting invocation arguments on the stack or jumps related to returning from function. In F# and .NET, there are several situations, where that happens:<br>
<br>F# function itself can be marked using inline attribute. This will trigger F# code to literally imprint the function body at the callsite. This means that encapsulation rules of such functions must respect .NET encapsulation (eg. you cannot have public inline function calling private function in its body). Like in many other languages, F# inline is optimistic - in some cases when function cannot be inlined eg. because it has been passed as parameter to another non-inlined function, inlining won't occur.
<br>Any function can be forced to be inlined at runtime level by using [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;] attribute. It will tell JIT to emit machine code directly at callsite. This option does not have limitations of F# inline keyword, however it's not always able to introduce some of the optimizations, F# compiler is capable of.
<br>Most of the time inlining happens without our precise knowledge. It can be done by the JIT compiler itself over any function not marked with [&lt;MethodImpl(MethodImplOptions.NoInlining)&gt;] attribute. It's based on a set of heuristic rules, one of which being size of the calling and called functions body - the smaller they are, the higher chance for inlining to happen. Additionally at the current moment, code that explicitly throws an exception is prevented from being inlined at JIT level, so using NoInlining option can also improve speed of your code in some cases - most popular being exception-driven input assertions inside of functions.
<br>As I mentioned, F# inline sometimes can apply optimizations outside of the scope of JIT optimizer alone. Let's take an example:<br>type AtomicRef&lt;'t when 't: not struct&gt;(initValue: 't) =
  let mutable value = initValue
  member this.Value with [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;] get () = Volatile.Read &amp;value
    
  [&lt;MethodImpl(MethodImplOptions.AggressiveInlining)&gt;]
  member this.CompareAndSwap(comparand: 't, newValue: 't): bool =
    obj.ReferenceEquals(comparand, Interlocked.CompareExchange(&amp;value, newValue, comparand))
Copy<br>This type is going to present behavior similar to F# ref cell, with the difference that its operations are going to be thread safe in the same sense we described in <a data-tooltip-position="top" aria-label="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#atomiccompareandswap" rel="noopener" class="external-link" href="https://www.bartoszsypytkowski.com/writing-high-performance-f-code/#atomiccompareandswap" target="_blank">atomic compare-and-swap section</a>. Now imagine, that we'd like to have a generic updating mechanism:<br>module Atomic

let update (modify: 't -&gt; 't) (atom: AtomicRef&lt;'t&gt;) =
  let mutable old = atom.Value
  let mutable newValue = modify old
  while not (atom.CompareAndSwap(old, newValue)) do
    old &lt;- atom.Value
    newValue &lt;- modify old
  newValue
Copy<br>With this we can atomically modify a value within the cell. We could leave it like this, but if you'll benchmark an exemplar snippet like this:<br>// benchmark setup
val a = AtomicRef "hello"
val value = "world"

// benchmarked function body
a |&gt; Atomic.update (fun _ -&gt; value)
Copy<br>You'd discover that this call allocates - It's a result of passing a function argument (in .NET these are realized as objects), that captures value field. Now, we could try to mark Atomic.update function using [&lt;MethodImpl&gt;] attribute or use inline keyword. If we'd try to benchmark these however, the results would be slightly different:<br><br>You may notice, that using attribute might slightly improve speed, but didn't change anything in terms of allocations around capturing lambda parameter. However using F# inline keyword indeed helped here: a lambda argument has been erased, as its behavior was aggressively printed together with inlined function body.<br><br>It's been a long post, but we just touched a tip of an iceberg here. We're didn't really mention optimizations in the area of I/O operations, different flavors of async code execution or <a data-tooltip-position="top" aria-label="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe?view=net-5.0&amp;ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://docs.microsoft.com/en-us/dotnet/api/system.runtime.compilerservices.unsafe?view=net-5.0&amp;ref=bartoszsypytkowski.com" target="_blank">dropping .NET safety belt</a> in cases when we wish to omit safe checks even when .NET compiler alone could not. There are also many tricky situations in which one small, seemingly insignificant change in code can throw .NET runtime into pit of deoptimized code. If you're curious about these, you can follow <a data-tooltip-position="top" aria-label="https://twitter.com/badamczewski01?ref=bartoszsypytkowski.com" rel="noopener" class="external-link" href="https://twitter.com/badamczewski01?ref=bartoszsypytkowski.com" target="_blank">Bartosz Adamczewski</a> on twitter.<br>Ultimately, while many of these tips and behaviors may stay with us for years to come, remember that compilers are still actively developed and just like some of these optimizations are not applied on older runtimes like .NET Full Framework, new ones (like smarter escape analysis, new devirtualization rules etc.) may turn some of the warnings presented here obsolete and shift the optimization techniques to enable us writing code that's both fast and high-level.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/fsharp/writing-high-performance-fsharp-code.html</link><guid isPermaLink="false">Computer Science/Programming Language/FSharp/Writing high performance Fsharp code.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 May 2024 13:31:40 GMT</pubDate><enclosure url="https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-layout.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://www.bartoszsypytkowski.com/content/images/2021/02/class-vs-struct-layout.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[OCaml News 2023 - 3]]></title><description><![CDATA[ 
 <br>o-新鲜事儿<br>
<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ocaml-5-1-0-released/13021" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ocaml-5-1-0-released/13021" target="_blank">OCaml 5.1.0 released</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-dune-3-14/14096" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-dune-3-14/14096" target="_blank">[ANN] dune 3.14</a>

<br>
<a data-tooltip-position="top" aria-label="https://ocaml.codes/search/" rel="noopener" class="external-link" href="https://ocaml.codes/search/" target="_blank">code search</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-ocaml-codes-code-search-for-opam-packages/14092" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-ocaml-codes-code-search-for-opam-packages/14092" target="_blank">[ANN] ocaml.codes, code search for OPAM packages</a><br>


<br>
用 livegrep 基于 opam 包的源码做的代码搜索，还挺方便的。

<br>
<a data-tooltip-position="top" aria-label="https://melange.re/blog/posts/announcing-melange-3" rel="noopener" class="external-link" href="https://melange.re/blog/posts/announcing-melange-3" target="_blank">Announcing Melange 3</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-melange-3-0/14102" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-melange-3-0/14102" target="_blank">[ANN] Melange 3.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/learn-ocaml-1-0-is-out/14100" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/learn-ocaml-1-0-is-out/14100" target="_blank">Learn-OCaml 1.0 is out!</a><br>


<br>
<a rel="noopener" class="external-link" href="https://ocaml-sf.org/learn-ocaml-public/" target="_blank">https://ocaml-sf.org/learn-ocaml-public/</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/owlbarn/owl" rel="noopener" class="external-link" href="https://github.com/owlbarn/owl" target="_blank">GitHub - owlbarn/owl: Owl - OCaml Scientific Computing @ http://ocaml.xyz</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/owl-project-concluding/14117" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/owl-project-concluding/14117" target="_blank">Owl project concluding</a><br>


<br>
经过八年的维护，Owl项目即将终止<br>


<br><br>o-视频<br>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=jvQ7fj9LlVA" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=jvQ7fj9LlVA" target="_blank">Inferring Locality in OCaml | OCaml Unboxed</a><br>

<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=AGu4AO5zO8o" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=AGu4AO5zO8o" target="_blank">OCaml Locals Save Allocations | OCaml Unboxed</a><br>

<br><a data-tooltip-position="top" aria-label="https://watch.ocaml.org/w/qQzb94X9WM7zLif7FynPyN" rel="noopener" class="external-link" href="https://watch.ocaml.org/w/qQzb94X9WM7zLif7FynPyN" target="_blank">Ocsigen: Developing Web and mobile applications in OCaml – Jérôme Vouillon &amp; Vincent Balat</a><br>

<br><a data-tooltip-position="top" aria-label="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" rel="noopener" class="external-link" href="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" target="_blank">Verifying an Effect-Based Cooperative Concurrency Scheduler in Iris by Adrian Dapprich</a><br>

<br><br>o-博客 / 文章 / 帖子<br>
<br>
<a data-tooltip-position="top" aria-label="https://priver.dev/blog/dbcaml/dbcaml/" rel="noopener" class="external-link" href="https://priver.dev/blog/dbcaml/dbcaml/" target="_blank">Introducing DBCaml, Database toolkit for OCaml</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://priver.dev/blog/dbcaml/building-a-connnection-pool/" rel="noopener" class="external-link" href="https://priver.dev/blog/dbcaml/building-a-connnection-pool/" target="_blank">Building a Connnection Pool for DBCaml on top of riot</a>

<br>
<a data-tooltip-position="top" aria-label="https://ocamlpro.com/blog/2021_09_02_generating_static_and_portable_executables_with_ocaml/" rel="noopener" class="external-link" href="https://ocamlpro.com/blog/2021_09_02_generating_static_and_portable_executables_with_ocaml/" target="_blank">Generating static and portable executables with OCaml</a><br>


<br>
OCaml编译器没有内置生成静态可移植可执行文件的特性，这里提到了一些技巧

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/analog-of-promise-any-for-multicore-ocaml/14145" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/analog-of-promise-any-for-multicore-ocaml/14145" target="_blank">Analog of Promise.any() for Multicore OCaml</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/printf-vs-format/14130" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/printf-vs-format/14130" target="_blank">Printf vs. Format?</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/how-to-represent-tuples-in-ast/14095" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/how-to-represent-tuples-in-ast/14095" target="_blank">How to represent tuples in AST?</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/how-do-i-pass-an-unsigned-char-an-array-of-bytes-representing-binary-data-from-c-to-ocaml/14074" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/how-do-i-pass-an-unsigned-char-an-array-of-bytes-representing-binary-data-from-c-to-ocaml/14074" target="_blank">How do I pass an unsigned char * (an array of bytes representing binary data) from C to OCaml?</a><br>


<br><br>o-未来<br>
<br>
<a data-tooltip-position="top" aria-label="https://docs.google.com/forms/d/e/1FAIpQLSe1U_5KanTeKt1h9t5vjYohYXepXDhPCru4tsms4OcI5k0Fkw/viewform?pli=1" rel="noopener" class="external-link" href="https://docs.google.com/forms/d/e/1FAIpQLSe1U_5KanTeKt1h9t5vjYohYXepXDhPCru4tsms4OcI5k0Fkw/viewform?pli=1" target="_blank">How do we want to present OCaml to the World on OCaml.org?</a><br>


<br>
一个问卷，用于更好的改进 <a data-tooltip-position="top" aria-label="http://ocaml.org/" rel="noopener" class="external-link" href="http://ocaml.org/" target="_blank">ocaml.org</a> 有关学术和工业应用板块的内容。

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/feedback-help-wanted-upcoming-ocaml-org-cookbook-feature/14127" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/feedback-help-wanted-upcoming-ocaml-org-cookbook-feature/14127" target="_blank">Feedback / Help Wanted: Upcoming OCaml.org Cookbook Feature</a><br>


<br>
<a data-tooltip-position="top" aria-label="http://ocaml.org/" rel="noopener" class="external-link" href="http://ocaml.org/" target="_blank">ocaml.org</a> 准备上线一个cookbook页面，放一些如何用OCaml的生态解决常见需求的资源

<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/state-of-compaction-in-ocaml-5/14121/1" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/state-of-compaction-in-ocaml-5/14121/1" target="_blank">State of compaction in OCaml 5?</a><br>


<br>
OCaml 5.2 的 compact heap 会将未使用的内存返回给操作系统。在 OCaml 5 的 GC 中，小于 128byte 的块用大小隔离池进行管理，比如有一个池，处理大小为 3byte 的分配，另一个池处理大小为 4byte 的分配等等，这样的池在每个Domain里都有。用这个方法分配速度很快，因为不用找合适的内存间隙了，只要找正确的池大小就行。<br>


<br><br>o-值得被注意的项目<br>
<br>
<a data-tooltip-position="top" aria-label="https://github.com/mbarbin/vcs" rel="noopener" class="external-link" href="https://github.com/mbarbin/vcs" target="_blank">GitHub - mbarbin/vcs: A versatile OCaml library for Git interaction</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/a-versatile-ocaml-library-for-git-interaction-seeking-community-feedback/14155" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/a-versatile-ocaml-library-for-git-interaction-seeking-community-feedback/14155" target="_blank">A Versatile OCaml Library for Git Interaction - Seeking Community Feedback</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/dbcaml/dbcaml" rel="noopener" class="external-link" href="https://github.com/dbcaml/dbcaml" target="_blank">GitHub - dbcaml/dbcaml: DBCaml is a database library for OCaml</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/dbcaml-a-new-database-toolkit-built-on-riot/14150" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/dbcaml-a-new-database-toolkit-built-on-riot/14150" target="_blank">DBcaml, a new database toolkit built on Riot</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/c-cube/fuseau" rel="noopener" class="external-link" href="https://github.com/c-cube/fuseau" target="_blank">GitHub - c-cube/fuseau: [alpha] lightweight fiber library for OCaml 5</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-fuseau-0-1/14157" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-fuseau-0-1/14157" target="_blank">[ANN] fuseau 0.1</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/issuu/ocaml-protoc-plugin" rel="noopener" class="external-link" href="https://github.com/issuu/ocaml-protoc-plugin" target="_blank">GitHub - issuu/ocaml-protoc-plugin: ocaml-protoc-plugin</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/taking-over-maintanence-of-a-stale-project/14156" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/taking-over-maintanence-of-a-stale-project/14156" target="_blank">Taking over maintanence of a stale project</a>

<br>
<a data-tooltip-position="top" aria-label="https://github.com/darrenldl/docfd" rel="noopener" class="external-link" href="https://github.com/darrenldl/docfd" target="_blank">GitHub - darrenldl/docfd: TUI multiline fuzzy document finder</a><br>


<br>
<a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-docfd-tui-multiline-fuzzy-document-finder-2-2-0/14109/1" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-docfd-tui-multiline-fuzzy-document-finder-2-2-0/14109/1" target="_blank">[ANN] Docfd: TUI multiline fuzzy document finder 2.2.0</a>

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2023-3.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2023 - 3.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 23 Mar 2024 03:28:01 GMT</pubDate></item><item><title><![CDATA[OCaml News 2024 - 1]]></title><description><![CDATA[ 
 <br><br>
<br><a data-tooltip-position="top" aria-label="https://tarides.com/blog/2023-12-20-ocaml-survey-developers-perception-interest-and-perceived-barriers/" rel="noopener" class="external-link" href="https://tarides.com/blog/2023-12-20-ocaml-survey-developers-perception-interest-and-perceived-barriers/" target="_blank">OCaml Survey: Developers' Perception, Interest, and Perceived Barriers</a>
<br><a data-tooltip-position="top" aria-label="https://tarides.com/blog/2023-12-29-announcing-the-orchide-project-powering-satellite-innovation/" rel="noopener" class="external-link" href="https://tarides.com/blog/2023-12-29-announcing-the-orchide-project-powering-satellite-innovation/" target="_blank">Announcing the ORCHIDE Project: Powering Satellite Innovation</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12885" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/12885" target="_blank">Dynarrays, unboxed (with local dummies) #12885</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12871" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/12871" target="_blank">Stdlib priority queues #12871</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12596" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/12596" target="_blank">Compile recursive bindings in Lambda #12596</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12828" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/12828" target="_blank">Add short syntax for dependent functor types #12828</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12508" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/12508" target="_blank">[shapes] Add support for project-wide occurrences #12508</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/1802" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/1802" target="_blank">Make the character set for OCaml source code officially UTF-8. #1802</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/12719" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/12719" target="_blank">Add thread local storage #12719</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=zG7JejHlQoM" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=zG7JejHlQoM" target="_blank">"Melange: The next frontier in type-safe web development" by Dillon Mulroy - RVAJS 2023</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=R-XJzUrP7bQ" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=R-XJzUrP7bQ" target="_blank">Trying out OCaml TUI framework Mint Tea!</a>
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=XyDbG9FGR1o" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=XyDbG9FGR1o" target="_blank">A TUI chat in OCaml 🐫</a>
<br>[Building a Game Engine... with OCaml ?! [Part 1]](<a rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=1XpUaTnssQE" target="_blank">https://www.youtube.com/watch?v=1XpUaTnssQE</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://sancho.dev/blog/whats-possible-with-melange" rel="noopener" class="external-link" href="https://sancho.dev/blog/whats-possible-with-melange" target="_blank">What's possible with Melange</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/access-inferred-types/13805" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/access-inferred-types/13805" target="_blank">Access inferred types</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/using-menhir-to-parse-into-idiomatic-js-typescript-structures/13809" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/using-menhir-to-parse-into-idiomatic-js-typescript-structures/13809" target="_blank">Using Menhir to parse into idiomatic JS (TypeScript) structures</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/why-constructors-are-not-curried/13792" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/why-constructors-are-not-curried/13792" target="_blank">Why constructors are not curried?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/practical-example-of-applicative-vs-generative-functors/13777" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/practical-example-of-applicative-vs-generative-functors/13777" target="_blank">Practical example of applicative vs generative functors?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/compiler-optimization-on-flattening-adt-for-less-boxing/13764" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/compiler-optimization-on-flattening-adt-for-less-boxing/13764" target="_blank">Compiler optimization on flattening ADT for less boxing?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/how-to-express-koka-home-page-example/13748" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/how-to-express-koka-home-page-example/13748" target="_blank">How to express Koka home page example?</a>
<br><a data-tooltip-position="top" aria-label="https://practicalocaml.com/parsing-with-binary-string-pattern-matching/" rel="noopener" class="external-link" href="https://practicalocaml.com/parsing-with-binary-string-pattern-matching/" target="_blank">Parsing with Binary String Pattern Matching</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/backtracking/grid" rel="noopener" class="external-link" href="https://github.com/backtracking/grid" target="_blank">grid: A tiny library for two-dimensional arrays</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/minttea" rel="noopener" class="external-link" href="https://github.com/leostera/minttea" target="_blank">A fun little TUI framework for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/terrateamio/ocaml-ts-mode" rel="noopener" class="external-link" href="https://github.com/terrateamio/ocaml-ts-mode" target="_blank">ocaml-ts-mode: Ocaml mode for emacs using treesitter</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/dmmulroy/create-melange-app" rel="noopener" class="external-link" href="https://github.com/dmmulroy/create-melange-app" target="_blank">create-melange-app: An example app created by create-melange-app</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/andersfugmann/ppx_protocol_conv" rel="noopener" class="external-link" href="https://github.com/andersfugmann/ppx_protocol_conv" target="_blank">ppx_protocol_conv: Pluggable serialization and deserialization of ocaml data strucures based on type_conv</a>
<br><a data-tooltip-position="top" aria-label="https://spatial-shell.app/" rel="noopener" class="external-link" href="https://spatial-shell.app/" target="_blank">spatial-shell: Spatial Shell is a daemon implementing a spatial model inspired by Material Shell, for i3 and sway. More precisely, it organizes your windows within a grid whose rows are the workspaces of your favorite WM.</a>
<br><a data-tooltip-position="top" aria-label="https://codeberg.org/marcc/fixgen" rel="noopener" class="external-link" href="https://codeberg.org/marcc/fixgen" target="_blank">Fixgen: A language agnostic fixture generator</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/tweag/opam-nix" rel="noopener" class="external-link" href="https://github.com/tweag/opam-nix" target="_blank">opam-nix: Turn opam-based OCaml projects into Nix derivations</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/blink" rel="noopener" class="external-link" href="https://github.com/leostera/blink" target="_blank">Blink: A pure OCaml HTTP client for Riot</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/colors" rel="noopener" class="external-link" href="https://github.com/leostera/colors" target="_blank">colors: A pure OCaml library for manipulating colors in different color spaces.</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/nationalarchives/miiify" rel="noopener" class="external-link" href="https://github.com/nationalarchives/miiify" target="_blank">miilfy: A web annotation server built with the same principles as Git</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/riot" rel="noopener" class="external-link" href="https://github.com/leostera/riot" target="_blank">Roit v0.0.7: An actor-model multi-core scheduler for OCaml 5 🐫</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/leostera/castore" rel="noopener" class="external-link" href="https://github.com/leostera/castore" target="_blank">CAStore: A portable pure OCaml CA Store, with no dependencies, inspired by Elixir's [:castore](https://github.com/elixir-mint/castore).</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/learn-ocaml-1-0-approaching-call-for-testers/13621" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/learn-ocaml-1-0-approaching-call-for-testers/13621" target="_blank">Lean-OCaml 1.0: A Web Application for Learning OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/cmarkit" rel="noopener" class="external-link" href="https://erratique.ch/software/cmarkit" target="_blank">cmarkit 0.3.0" CommonMark parser and renderer for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://ocaml.org/p/dream-html/latest" rel="noopener" class="external-link" href="https://ocaml.org/p/dream-html/latest" target="_blank">dream-html 2.0.0: A library for generating HTML</a>
<br><a data-tooltip-position="top" aria-label="https://git.frama-c.com/pub/caisar/" rel="noopener" class="external-link" href="https://git.frama-c.com/pub/caisar/" target="_blank">Caisar: A platform under active development at CEA LIST, aiming to provide a wide range of features to characterize the safety and robustness of artificial intelligence based software.</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/jserot/rfsm" rel="noopener" class="external-link" href="https://github.com/jserot/rfsm" target="_blank">RFSM 2.0: A toolset for describing and simulating StateChart-like state diagrams.</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/chshersh/zbg/tree/main" rel="noopener" class="external-link" href="https://github.com/chshersh/zbg/tree/main" target="_blank">Zbg 2.0: <code></code> (short for <strong></strong>ero <strong></strong>ullshit <strong></strong>it) is a CLI tool for using <code></code> efficiently.</a>zbgZBGgit
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2024-1.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2024 - 1.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 08 Jan 2024 10:28:31 GMT</pubDate></item><item><title><![CDATA[OCaml News 2024 - 2]]></title><description><![CDATA[ 
 <br><br>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-preview-play-with-project-wide-occurrences-for-ocaml/13814" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-preview-play-with-project-wide-occurrences-for-ocaml/13814" target="_blank">[ANN][PREVIEW] Play with project-wide occurrences for OCaml!</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/dune/pull/8784" rel="noopener" class="external-link" href="https://github.com/ocaml/dune/pull/8784" target="_blank">[Dune]: Add link flags ocamlmklib when using ctypes stubs. #8784</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ocaml-software-foundation-january-2024-update/13828" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ocaml-software-foundation-january-2024-update/13828" target="_blank">OCaml Software Foundation: January 2024 update</a>
<br>[Apprendre à programmer avec OCaml](Apprendre à programmer avec OCaml)
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/call-for-speakers-for-the-2024-carolina-code-conference-is-open-until-april-15th/13827" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/call-for-speakers-for-the-2024-carolina-code-conference-is-open-until-april-15th/13827" target="_blank">[Call for Speakers for the 2024 Carolina Code Conference is open until April 15th](https://discuss.ocaml.org/t/call-for-speakers-for-the-2024-carolina-code-conference-is-open-until-april-15th/13827)</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/discussions/11924" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/discussions/11924" target="_blank">[OCaml]: feature request: better errors #11924</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ocsigen-summary-of-recent-releases/13817" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ocsigen-summary-of-recent-releases/13817" target="_blank">Ocsigen: summary of recent releases</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/benchmark-between-open-addressing-and-closed-addressing-hashtbl/13882" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/benchmark-between-open-addressing-and-closed-addressing-hashtbl/13882" target="_blank">Benchmark between open-addressing and closed-addressing hashtbl</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/examples-of-caqti-infix/13878" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/examples-of-caqti-infix/13878" target="_blank">Examples of Caqti infix?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/why-can-t-i-create-a-project-with-non-ascii-characters/13865" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/why-can-t-i-create-a-project-with-non-ascii-characters/13865" target="_blank">Why can’t I create a project with non-ASCII characters? </a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/old-self-taught-vs-uni-debate-landscape-for-former-jane-street/13851" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/old-self-taught-vs-uni-debate-landscape-for-former-jane-street/13851" target="_blank">Old self-taught vs. uni debate; Landscape for former; Jane Street</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/toml-file-parser/13854" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/toml-file-parser/13854" target="_blank">Is there an easy way to read the values from a toml file?</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/generate-typed-ast-fragments/13824" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/generate-typed-ast-fragments/13824" target="_blank">Generate typed AST fragments</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/partially-apply-function-accepting-multiple-modules-and-keep-polymorphic-types/13823" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/partially-apply-function-accepting-multiple-modules-and-keep-polymorphic-types/13823" target="_blank">Partially apply function accepting multiple modules and keep polymorphic types</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/printing-unicode-characters-on-different-platforms/13813" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/printing-unicode-characters-on-different-platforms/13813" target="_blank">Printing Unicode Characters on Different Platforms</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" rel="noopener" class="external-link" href="https://watch.ocaml.org/w/iQNqZzA8gVmd4RQaycAwx4" target="_blank">Verifying an Effect-Based Cooperative Concurrency Scheduler in Iris by Adrian Dapprich</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/advice-for-combining-multiple-monads/10409" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/advice-for-combining-multiple-monads/10409" target="_blank">Advice for combining multiple monads</a>
<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/writing-ctypes-bindings-to-system-shared-libraries-for-bytecode-targets-via-the-dune-ctypes-stanza/13844" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/writing-ctypes-bindings-to-system-shared-libraries-for-bytecode-targets-via-the-dune-ctypes-stanza/13844" target="_blank">Writing ctypes bindings to system shared libraries for bytecode targets via the dune ctypes stanza</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/johnyob/grace" rel="noopener" class="external-link" href="https://github.com/johnyob/grace" target="_blank">Grace: A fancy diagnostics library that allows your compilers to exit with grace</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/thierry-martinez/metapp" rel="noopener" class="external-link" href="https://github.com/thierry-martinez/metapp" target="_blank">meta-pp: Meta-preprocessor for OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/stedolan/ppx_stage" rel="noopener" class="external-link" href="https://github.com/stedolan/ppx_stage" target="_blank">ppx_stage: Staged metaprogramming in stock OCaml</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/cmarkit" rel="noopener" class="external-link" href="https://erratique.ch/software/cmarkit" target="_blank">Cmarkit is an [OCaml](http://ocaml.org) libary for parsing the [CommonMark](https://spec.commonmark.org/) specification.</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/brr" rel="noopener" class="external-link" href="https://erratique.ch/software/brr" target="_blank">Brr is a toolkit for programming browsers in [OCaml](http://ocaml.org) with the [js_of_ocaml](http://ocsigen.org/js_of_ocaml/) compiler. It provides:</a>
<br><a data-tooltip-position="top" aria-label="https://erratique.ch/software/zipc" rel="noopener" class="external-link" href="https://erratique.ch/software/zipc" target="_blank">Zipc is an in-memory [ZIP](https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT) archive and [deflate](https://www.rfc-editor.org/rfc/rfc1951) compression codec for [OCaml](http://ocaml.org).</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/lukstafi/ocaml-gccjit" rel="noopener" class="external-link" href="https://github.com/lukstafi/ocaml-gccjit" target="_blank">ocaml-gccjit: OCaml bindings for libgccjit</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/whitequark/ocaml-m17n" rel="noopener" class="external-link" href="https://github.com/whitequark/ocaml-m17n" target="_blank">ocaml-m17n: Multilingualization for the OCaml source code</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/hackwaly/ocamlearlybird" rel="noopener" class="external-link" href="https://github.com/hackwaly/ocamlearlybird" target="_blank">ocamlearlybird: OCaml debug adapter</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/dmbaturin/otoml" rel="noopener" class="external-link" href="https://github.com/dmbaturin/otoml" target="_blank">otoml: TOML parsing, manipulation, and pretty-printing library for OCaml (fully 1.0.0-compliant)</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/mattjbray/ocaml-decoders" rel="noopener" class="external-link" href="https://github.com/mattjbray/ocaml-decoders" target="_blank">ocaml-decoders: Elm-inspired decoders for Ocaml</a>
<br><a data-tooltip-position="top" aria-label="https://github.com/OCamlPro/ez_toml" rel="noopener" class="external-link" href="https://github.com/OCamlPro/ez_toml" target="_blank">ez_toml: A library to parse and print TOML files</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="http://cambium.inria.fr/~fpottier/oma/doc/oma/Oma/index.html" rel="noopener" class="external-link" href="http://cambium.inria.fr/~fpottier/oma/doc/oma/Oma/index.html" target="_blank">Oma: OCaml implementation of 'Two Simplified Algorithms for Maintaining Order in a List'</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-first-release-of-oma/13845" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-first-release-of-oma/13845" target="_blank">[ANN] First release of oma</a>
<br><a data-tooltip-position="top" aria-label="https://erikdemaine.org/papers/DietzSleator_ESA2002/paper.pdf" rel="noopener" class="external-link" href="https://erikdemaine.org/papers/DietzSleator_ESA2002/paper.pdf" target="_blank">Two Simplified Algorithms for Maintaining Order in a List</a>


<br><a data-tooltip-position="top" aria-label="https://github.com/hackwaly/ocamlearlybird" rel="noopener" class="external-link" href="https://github.com/hackwaly/ocamlearlybird" target="_blank">ocamlearlybird: v1.3.0</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-ocamlearlybird-just-got-ability-to-inspect-opaque-abstract-values/13852" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-ocamlearlybird-just-got-ability-to-inspect-opaque-abstract-values/13852" target="_blank">[ANN] Ocamlearlybird just got ability to inspect opaque/abstract values</a>


<br><a data-tooltip-position="top" aria-label="https://github.com/Axot017/validate" rel="noopener" class="external-link" href="https://github.com/Axot017/validate" target="_blank">validate: OCaml Data Validation library</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-validate-a-new-library-for-data-validation/13861" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-validate-a-new-library-for-data-validation/13861" target="_blank">[ANN] Validate - A New Library for Data Validation</a>


<br><a data-tooltip-position="top" aria-label="https://github.com/gborough/sarif" rel="noopener" class="external-link" href="https://github.com/gborough/sarif" target="_blank">sarif v2.1.0: Static Analysis Results Interchange Format (SARIF) For OCaml</a>

<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-sarif-0-1-0-static-analysis-results-interchange-format-sarif-for-ocaml/13821" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-sarif-0-1-0-static-analysis-results-interchange-format-sarif-for-ocaml/13821" target="_blank">[ANN] sarif 0.1.0 - Static Analysis Results Interchange Format (SARIF) For OCaml</a>


<br><a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/ann-new-release-of-menhir-20231231/13816" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/ann-new-release-of-menhir-20231231/13816" target="_blank">[ANN] New release of Menhir (20231231)</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2024-2.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2024 - 2.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Thu, 18 Jan 2024 03:39:20 GMT</pubDate></item><item><title><![CDATA[OCaml News 2024 - 6]]></title><description><![CDATA[ 
 <br>       ^o3
~/\_/\_|)
|/=_=\|
"     "
Copy<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13275" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13275" target="_blank">Modular explicits #13275</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12828" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12828" target="_blank">Add short syntax for dependent functor types #12828</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13310" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13310" target="_blank">Add Pair module to standard library #13310</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13272" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13272" target="_blank">Allow maximum number of domains to be specified as a OCAMLRUNPARAM parameter #13272</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13097" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13097" target="_blank">Immutable arrays #13097</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13161" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13161" target="_blank">Restore native armv7 support for NetBSD 10.0 #13161</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12309" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12309" target="_blank">Add effect syntax #12309</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12114" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12114" target="_blank">Add ThreadSanitizer support #12114</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13195" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13195" target="_blank">A new abstract data type of enumerations in Set.Make(Ord).Enum #13195</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12871" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12871" target="_blank">Stdlib priority queues #12871</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/9080" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/9080" target="_blank">Aliasing == and != with explicit names #9080</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12964" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12964" target="_blank">Memory cleanup at exit #12964</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13169" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13169" target="_blank">A document type for error messages #13169</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13318" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13318" target="_blank">Fix GC alarm regression #13318</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13326" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13326" target="_blank">Implement O_APPEND on windows #13326</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13296" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/13296" target="_blank">Add missing functions from Array to Dynarray #13296</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12182%23discussion_r1678134261" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12182%23discussion_r1678134261" target="_blank">Improve the type clash error message #12182</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12298" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/ocaml/ocaml/pull/12298" target="_blank">Emphasize that Bigarray.int refers to the OCaml int type, and not the C int type #12298</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/effects-with-lwt-a-dead-end-for-now/15002" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/effects-with-lwt-a-dead-end-for-now/15002" target="_blank">Effects with Lwt, a dead end for now?</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/changes-in-handling-of-gc-parameters-and-alarms-in-5-2-0/14986" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/changes-in-handling-of-gc-parameters-and-alarms-in-5-2-0/14986" target="_blank">Changes in handling of Gc parameters and alarms in 5.2.0</a>

<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mopsa-1-0-modular-open-platform-for-static-analysis/15013" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-mopsa-1-0-modular-open-platform-for-static-analysis/15013" target="_blank">[ANN] Mopsa 1.0 -- Modular Open Platform for Static Analysis</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-a-small-extension-of-bigarray-genarray-adding-iteration-mapping-and-folding/15005" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-a-small-extension-of-bigarray-genarray-adding-iteration-mapping-and-folding/15005" target="_blank">[ANN] A small extension of Bigarray.Genarray adding iteration, mapping and folding</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-cudajit-bindings-to-the-cuda-and-nvrtc-libraries/15010" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-cudajit-bindings-to-the-cuda-and-nvrtc-libraries/15010" target="_blank">[ANN] cudajit: Bindings to the <code></code> and <code></code> libraries</a>cudanvrtc

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ocaml-lsp-1-18-0/14952" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ocaml-lsp-1-18-0/14952" target="_blank">[ANN] OCaml LSP 1.18.0</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ortac-0-3-0-dynamic-formal-verification-made-easy/14936" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/ann-ortac-0-3-0-dynamic-formal-verification-made-easy/14936" target="_blank">[ANN] Ortac 0.3.0 Dynamic formal verification made easy</a>

<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//hal.sorbonne-universite.fr/hal-02890500v1/document" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//hal.sorbonne-universite.fr/hal-02890500v1/document" target="_blank">Combinations of Reusable Abstract Domains for a Multilingual Static Analyzer</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-21-fixing-living/" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-21-fixing-living/" target="_blank">Fighting Mutation with Mutation in Living</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" target="_blank">Type system and polymorphic let's</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/using-docusaurus-to-document-an-ocaml-project/13359" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/using-docusaurus-to-document-an-ocaml-project/13359" target="_blank">Using Docusaurus to document an OCaml project</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" target="_blank">Exploring the Docusaurus+Odoc combo</a><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/type-system-and-polymorphic-lets/14990" target="_blank">Type system and polymorphic let's</a><a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//discuss.ocaml.org/t/exploring-the-docusaurus-odoc-combo/15012" target="_blank">Exploring the Docusaurus+Odoc combo</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-11-a-possibly-safer-interface-to-the-ctypes-ffi/" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-11-a-possibly-safer-interface-to-the-ctypes-ffi/" target="_blank">A (Possibly) Safer Interface to the Ctypes FFI</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-09-ocaml-ffi-sharp-edges-and-how-to-avoid-them/" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//fizzixnerd.com/blog/2024-07-09-ocaml-ffi-sharp-edges-and-how-to-avoid-them/" target="_blank">OCaml FFI Sharp Edges -- and How to Avoid them!</a>

<br><br>
<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//ocaml.libvirt.org/" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//ocaml.libvirt.org/" target="_blank">ocaml-libvirt OCaml bindings for libvirt</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//gitlab.com/mopsa/mopsa-analyzer/" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//gitlab.com/mopsa/mopsa-analyzer/" target="_blank">Gitlab - MOPSA/MOPSA analyzer: stands for Modular and Open Platform for Static Analysis.</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/Heyji2/GenArrayIter" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/Heyji2/GenArrayIter" target="_blank">GitHub - Heyji2/GenArrayIter: Adding iteration, mapping and folding to the ocaml BigArray.Genarrays module which provides arrays of arbitrary dimensions</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/mbarbin/bopkit" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/mbarbin/bopkit" target="_blank">GitHub - mbarbin/bopkit: An educational project for digital circuits programming</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/dx3mod/rpmfile" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/dx3mod/rpmfile" target="_blank">GitHub - dx3mod/rpmfile: A library for reading metadata from RPM packages.</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/gildor478/ocaml-fileutils" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/gildor478/ocaml-fileutils" target="_blank">Github - gildor478/ocaml-fileutils: OCaml API to manipulate real files (POSIX like) and filenames</a>

<br>
<a data-tooltip-position="top" aria-label="https://link.zhihu.com/?target=https%3A//github.com/NathanReb/ocaml-api-watch" rel="noopener" class="external-link" href="https://link.zhihu.com/?target=https%3A//github.com/NathanReb/ocaml-api-watch" target="_blank">Github - NathanReb/ocaml-api-watch: Libraries and tools to keep watch on you OCaml lib's API changes</a>

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/ocaml-news/ocaml-news-2024-6.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/OCaml News/OCaml News 2024 - 6.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Tue, 23 Jul 2024 09:35:59 GMT</pubDate></item><item><title><![CDATA[A hack to implement efficient TLS (thread-local-storage)]]></title><description><![CDATA[ 
 <br>Currently OCaml 5 provides a <a data-tooltip-position="top" aria-label="https://v2.ocaml.org/api/Domain.DLS.html" rel="noopener" class="external-link" href="https://v2.ocaml.org/api/Domain.DLS.html" target="_blank"><code></code> 2</a>Domain.DLS module for domain-local storage.<br>Unfortunately,<br>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/11770" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/issues/11770" target="_blank">there is no corresponding <code></code> 7</a>Thread.TLS for (sys)thread-local storage, and
<br>the current implementation of Domain.DLS is not thread-safe.
<br>I don’t want to spend time to motivate this topic, but for many of the use cases of Domain.DLS, what you actually want, is to use a Thread.TLS. IOW, many of the uses of Domain.DLS are probably “wrong” and should actually use a Thread.TLS, because, when using Domain.DLS, the implicit assumption is often that you don’t have multiple threads on the domain, but that is typically decided at a higher level in the application and so making such an assumption is typically not safe.<br><br>I mentioned that the current implementation of Domain.DLS is not thread-safe. What I mean by that is that <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/12677" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/issues/12677" target="_blank">the current implementation is literally not thread-safe at all</a> in the sense that unrelated concurrent Domain.DLS accesses can actually break the DLS. That is because the state updates performed by Domain.DLS contain safe-points during which the OCaml runtime may switch between (sys)threads.<br>Consider <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L120-L127" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L120-L127" target="_blank">the implementation of <code></code> 1</a>Domain.DLS.get:<br>  let get (idx, init) =
    let st = maybe_grow idx in
    let v = st.(idx) in
    if v == unique_value then
      let v' = Obj.repr (init ()) in
      st.(idx) &lt;- (Sys.opaque_identity v');
      Obj.magic v'
    else Obj.magic v
Copy<br>If there are two (or more) threads on a single domain that concurrently call get before init has been called initially, then what might happen is that init gets called twice (or more) and the threads get different values which could e.g. be pointers to two different mutable objects.<br>Consider <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L98-L111" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/stdlib/domain.ml#L98-L111" target="_blank">the implementation of <code></code></a>maybe_grow:<br>  let maybe_grow idx =
    let st = get_dls_state () in
    let sz = Array.length st in
    if idx &lt; sz then st
    else begin
      let rec compute_new_size s =
        if idx &lt; s then s else compute_new_size (2 * s)
      in
      let new_sz = compute_new_size sz in
      let new_st = Array.make new_sz unique_value in
      Array.blit st 0 new_st 0 sz;
      set_dls_state new_st;
      new_st
    end

Copy<br>Imagine calling get (which calls maybe_grow) with two different keys from two different threads concurrently. The end result might be that two different arrays are allocated and only one of them “wins”. What this means, for example, is that effects of set calls may effectively be undone by concurrent calls of get.<br>In other words, the Domain.DLS, as it is currently implemented, is not thread-safe.<br><br>If you dig into the implementation of threads, you will notice that the opaque <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/thread.mli#L18" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/thread.mli#L18" target="_blank"><code></code> type</a>Thread.t is actually a heap block (record) of three fields. You can see the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L66-L68" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L66-L68" target="_blank"><code></code> accessors</a>Thread.t:<br>#define Ident(v) Field(v, 0)
#define Start_closure(v) Field(v, 1)
#define Terminated(v) Field(v, 2)
Copy<br>and the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L335-L346" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L335-L346" target="_blank"><code></code> allocation 1</a>Thread.t:<br>static value caml_thread_new_descriptor(value clos)
{
  CAMLparam1(clos);
  CAMLlocal1(mu);
  value descr;
  /* Create and initialize the termination semaphore */
  mu = caml_threadstatus_new();
  /* Create a descriptor for the new thread */
  descr = caml_alloc_3(0, Val_long(atomic_fetch_add(&amp;thread_next_id, +1)),
                       clos, mu);
  CAMLreturn(descr);
}
Copy<br>The second field, Start_closure, is used to pass the closure to the thread start:<br>static void * caml_thread_start(void * v)
{
  caml_thread_t th = (caml_thread_t) v;
  int dom_id = th-&gt;domain_id;
  value clos;
  void * signal_stack;

  caml_init_domain_self(dom_id);

  st_tls_set(caml_thread_key, th);

  thread_lock_acquire(dom_id);
  restore_runtime_state(th);
  signal_stack = caml_init_signal_stack();

  clos = Start_closure(Active_thread-&gt;descr);
  caml_modify(&amp;(Start_closure(Active_thread-&gt;descr)), Val_unit);
  caml_callback_exn(clos, Val_unit);
  caml_thread_stop();
  caml_free_signal_stack(signal_stack);
  return 0;
}

Copy<br>and, as seen above, <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L575" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/blob/e397ed28bcef85fdc1f0f007af481ef201fb1fd7/otherlibs/systhreads/st_stubs.c#L575" target="_blank">it is overwritten with the unit value</a> before the closure is called.<br>What this means is that when you call Thread.self () and get a reference to the current Thread.t, the Start_closure field of that heap block will be the unit value:<br>assert (Obj.field (Obj.repr (Thread.self ())) 1 = Obj.repr ())
Copy<br>Let’s hijack that field for the purpose of implementing an efficient TLS!<br>Here is the full hack:<br>module TLS : sig
  type 'a key
  val new_key : (unit -&gt; 'a) -&gt; 'a key
  val get : 'a key -&gt; 'a
  val set : 'a key -&gt; 'a -&gt; unit
end = struct
  type 'a key = { index : int; compute : unit -&gt; 'a }

  let counter = Atomic.make 0
  let unique () = Obj.repr counter

  let new_key compute =
    let index = Atomic.fetch_and_add counter 1 in
    { index; compute }

  type t = { _id : int; mutable tls : Obj.t }

  let ceil_pow_2_minus_1 n =
    let n = n lor (n lsr 1) in
    let n = n lor (n lsr 2) in
    let n = n lor (n lsr 4) in
    let n = n lor (n lsr 8) in
    let n = n lor (n lsr 16) in
    if Sys.int_size &gt; 32 then n lor (n lsr 32) else n

  let[@inline never] grow_tls t before index =
    let new_length = ceil_pow_2_minus_1 (index + 1) in
    let after = Array.make new_length (unique ()) in
    Array.blit before 0 after 0 (Array.length before);
    t.tls &lt;- Obj.repr after;
    after

  let[@inline] get_tls index =
    let t = Obj.magic (Thread.self ()) in
    let tls = t.tls in
    if Obj.is_int tls then grow_tls t [||] index
    else
      let tls = (Obj.magic tls : Obj.t array) in
      if index &lt; Array.length tls then tls else grow_tls t tls index

  let get key =
    let tls = get_tls key.index in
    let value = Array.unsafe_get tls key.index in
    if value != unique () then Obj.magic value
    else
      let value = key.compute () in
      Array.unsafe_set tls key.index (Obj.repr (Sys.opaque_identity value));
      value

  let set key value =
    let tls = get_tls key.index in
    Array.unsafe_set tls key.index (Obj.repr (Sys.opaque_identity value))
end
Copy<br>The above achieves about 80% of the performance of Domain.DLS allowing roughly 241M TLS.gets/s (vs 296M Domain.DLS.gets/s) on my laptop.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/a-hack-to-implement-efficient-tls-(thread-local-storage).html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/A hack to implement efficient TLS (thread-local-storage).md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:29 GMT</pubDate></item><item><title><![CDATA[Generalised signature]]></title><description><![CDATA[ 
 <br>
This post presents a technique for defining more reusable OCaml signatures, helping to maintain consistent APIs with minimal boilerplate. We'll work through a few examples, which you can check out <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/generalised-signatures" rel="noopener" class="external-link" href="https://github.com/CraigFe/generalised-signatures" target="_blank">on GitHub</a>.
<br><br>Consider the following definition of an iter function for some container type t:<br>let iter f t =
  for i = 0 to length t - 1 do
    f (get t i)
  done
Copy<br>iter requires only that t comes with functions get and length. Many useful operations can be derived in terms of such indexing functions. To take advantage of this, let's move iter into a functor and provide some other useful operations too:<br>module type Indexable1 = sig
  type 'a t

  val get    : 'a t -&gt; int -&gt; 'a
  val length : _ t -&gt; int
end

module Foldable_of_indexable1 (I : Indexable1) : sig
  open I

  val iter      :        ('a -&gt; unit) -&gt; 'a t -&gt; unit
  val iteri     : (int -&gt; 'a -&gt; unit) -&gt; 'a t -&gt; unit
  val fold_left : ('acc -&gt; 'a -&gt; 'acc) -&gt; 'acc -&gt; 'a t -&gt; 'acc
  val exists    : ('a -&gt; bool) -&gt; 'a t -&gt; bool
  val for_all   : ('a -&gt; bool) -&gt; 'a t -&gt; bool
  val is_empty  : _ t -&gt; bool
  (* ... *)
end
Copy<br>For many types, including array, the get-based definitions are identical to their hand-optimised equivalents (modulo functor application). We can imagine avoiding a lot of standard-library boilerplate – and potential for API inconsistency – by using many such functors <a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fn-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fn-1" target="_blank">1</a>. We'd end up defining exactly one iter function that suffices for all Indexable types.<br>All good so far. Now, let's consider the string type.<br>A string is also an indexable container with length and get functions, albeit one that can only contain char values. It's natural to expect to be able to re-use Foldable_of_indexable1 in some way: indeed, our definition of iter above is exactly equal to the one in Stdlib.String.iter. Unfortunately, our Indexable1 module type can only describe parametric containers:<br>module _ : (Indexable1 with type 'a t := string) = Stdlib.String
Copy<br>Error: Signature mismatch:
       ...
       Values do not match:
         val get : t -&gt; int -&gt; char
       is not included in
         val get : t -&gt; int -&gt; 'a
       File "string.mli", line 52, characters 0-57: Actual declaration
Copy<br>We're unable to tell the type system something like<br>
'a t = string &nbsp;&nbsp; implies &nbsp;&nbsp; 'a = char
<br>as part of our substitution. This means that many types – including string, bytes, unboxed arrays and unboxed vectors – can't benefit from our Foldable_of_iterable1 definitions, even though their own definitions will be identical!<br>When we wrapped our code in the Foldable_of_indexable1 functor, we needed to give it specific input and output module types, and the ones we picked artificially limited its usefulness. This is a hazard of functorising highly-generic code. As ever, we could solve the problem with copy-paste: a new Indexable0 module type for non-parametric containers, and a new functor Foldable_of_indexable0 with exactly the same implementations as our previous one.<br>(* Non-parametric indexable types *)
module type Indexable0 = sig
  type t
  type elt

  val get    : t -&gt; int -&gt; elt
  val length : t -&gt; int
end

module Foldable_of_indexable0 (I : Indexable0) : sig
  (* All with the same implementation as before... *)
end
Copy<br>This definition suffers from the dual problem when we try to apply it to parameterised containers like 'a array:<br>module _ : (Indexable0 with type t := 'a array) = Stdlib.Array
Copy<br>Error: The type variable 'a is unbound in this type declaration.
Copy<br>This time, we wanted to be able to say something like<br>
elt = 'a &nbsp;&nbsp; implies &nbsp;&nbsp; t = 'a array &nbsp;&nbsp; (where 'a is universally quantified),
<br>which is even more nonsensical than our previous attempt. Neither Indexable0 nor Indexable1 can be expressed in terms of the other. We need something more general.<br><br>Interestingly, it's possible to generalise Indexable0 and Indexable1 with <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" target="_blank">another layer of indirection</a> by making elt a type operator:<br>module type IndexableN = sig
  type 'a t
  type 'a elt

  val get    : 'a t -&gt; int -&gt; 'a elt
  val length : _ t -&gt; int
end
Copy<br>elt carries the type equalities needed for the Indexable1 case, without forbidding the non-parametric implementation needed for the Indexable0 case. Arrays can set 'a elt := 'a, and strings can set 'a elt := char. Indeed, we can do this in the general case:<br>(** [Indexable0] is a special-case of [IndexableN] *)
module Indexable0_to_N = functor
  (T : Indexable0) -&gt;
  (T : IndexableN with type 'a t := T.t and type 'a elt := elt)

(** [Indexable1] is a special-case of [IndexableN] *)
module Indexable1_to_N = functor
  (T : Indexable1) -&gt;
  (T : IndexableN with type 'a t := 'a T.t and type 'a elt := 'a)
Copy<br>Now we can define a single Foldable_of_indexableN functor (with exactly the same implementations as before), and it will work for polymorphic and monomorphic containers. Neat!<br><img alt="A lattice showing Indexable0 and Indexable1 being generalised by IndexableN." src="https://www.craigfe.io/posts/generalised-signatures/dag-indexable.png" referrerpolicy="no-referrer"><br>In the general case, when you notice that different signatures are sharing common functions, it's often possible to unify them under a common interface with the following two steps:<br>
<br>generalise. Convert pure type variables into type operators (as in 'a → 'a elt), to support use-cases like instantiating those variables to fixed types. Add type parameters to existing types to carry type equalities between them (as in 'a t / 'a elt), to support use-cases where these types depend on each other.
<br>specialise. Use destructive substitution (:=) to eliminate those types and type parameters when they're not needed. We're taking advantage of the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/pull/792" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/pull/792" target="_blank">more powerful destructive substitution</a> offered by OCaml 4.06, which allows us to freely undo our generalisation step.
<br>The truly magical part of this trick is that – with better support for destructive type substitutions recently added to Odoc – it can be made completely invisible<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fn-2" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fn-2" target="_blank">2</a> in documentation!<br>module type Indexable1 = sig
  type _ t

  val get : 'a t -&gt; int -&gt; 'a
  val length : _ t -&gt; int
end

(** This module gets identical documentation to the one above! *)
module type Indexable1' = sig
  include IndexableN with type 'a elt := 'a (** @inline *)
end
Copy<br><br>One unavoidable limitation is in what sort of operations we can put in the Foldable_of_indexable functor. Suppose our initial attempt at generalising containers included a sum function:<br>let sum : int t -&gt; t = fold_left ( + ) 0
Copy<br>sum requires a container that can hold int values, which is clearly not possible for strings as the type system will happily tell us:<br>   |   let sum = fold_left ( + ) 0
                           ^^^^^
Error: This expression has type int -&gt; int -&gt; int
       but an expression was expected of type int -&gt; 'a elt -&gt; int
       Type int is not compatible with type 'a elt
Copy<br>To state the obvious, we can't rely on parametricity in our container functions if we want them to work on non-parametric containers. The natural solution here would be to define such parametric-only functions in a separate functor.<br><br>Indexable containers aren't the only example of generalised signatures in the real world. Indeed, many other data-structures and design patterns have APIs that can be unified in this way. Consider the case of hashtables, which have a huge space of possible implementations:<br>
<br>key types can be left polymorphic by using a magic hash function like caml_hash (as in <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.html" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.html" target="_blank"><code></code></a>Stdlib.Hashtbl), or fixed by a user-specified hash function (as in <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.Make.html" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/libref/Hashtbl.Make.html" target="_blank"><code></code></a>Stdlib.Hashtbl.Make).<br>

<br>value types can be left polymorphic, fixed by the user (as in persistent hashtables like <a data-tooltip-position="top" aria-label="https://mirage.github.io/index/index/Index/Make/index.html" rel="noopener" class="external-link" href="https://mirage.github.io/index/index/Index/Make/index.html" target="_blank"><code></code></a>Index), or even determined by the keys used to index them (as in universal maps like <a data-tooltip-position="top" aria-label="https://erratique.ch/software/hmap/doc/Hmap" rel="noopener" class="external-link" href="https://erratique.ch/software/hmap/doc/Hmap" target="_blank"><code></code></a>Hmap).<br>

<br>Initially, it looks like these different hashtables will each require their own hand-written signature (and this is what the standard library does with its hashtables). However, with enough type parameters, these different implementations can all be unified under a single Hashtbl_generalised module type:<br>module type Hashtbl_generalised = sig
  (** We have three types ([t], [key] and [value]) and three type variables:

      - ['k]/['v] allow the hashtable to determine key/value types;
      - ['a] is carried from keys to corresponding values, allowing the key to
        determine the types of values. *)

  type ('k, 'v) t
  type ('k, 'a) key
  type ('v, 'a) value

  val create : int -&gt; (_, _) t
  val replace : ('k, 'v) t -&gt; ('k, 'a) key -&gt; ('v, 'a) value -&gt; unit
  val remove : ('k, _) t -&gt; ('k, _) key -&gt; unit
  val find_opt : ('k, 'v) t -&gt; ('k, 'a) key -&gt; ('v, 'a) value option
  (* ... *)
end
Copy<br>We can then implement our different hashtable signatures as specialisations:<br><img alt="A lattice showing four different `Hashtbl` module types being generalised by `Hashtbl_generalised`." src="https://www.craigfe.io/posts/generalised-signatures/dag-hashtables.png" referrerpolicy="no-referrer"><br>For instance, for the regular polymorphic hashtable:<br>module type Poly_hash = sig
  include Hashtbl_generalised
    with type ('k, _) key := 'k
     and type ('v, _) value := 'v (** @inline **)
end
Copy<br>The other specialisations are very similar (see <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/generalised-signatures/blob/main/examples/hashtbl.ml" rel="noopener" class="external-link" href="https://github.com/CraigFe/generalised-signatures/blob/main/examples/hashtbl.ml" target="_blank">here</a> for the specifics).<br>What is it that makes Hashtable_generalised a good parent interface for these four flavours of hashtable? To get some insight, we can notice that each of the type parameters ('k, 'v, and 'a) connects its own pair of types:<br>hashtbl_generalised<br><img src="https://www.craigfe.io/posts/generalised-signatures/dep-hashtbl_generalised.png" referrerpolicy="no-referrer"><br>Framed this way, the type parameter 'k exists solely to carry type information between hashtables and their keys (using a type equality at call sites). Similarly, 'v bridges between hashtables and values, and 'a between keys and values. From here, each of our hashtable variants uses destructive subsitution (:=) to prune away unnecessary bridges and express some sort of dependency relation between the types:<br><br>In this case, it's not feasible for all these data structures to share the same implementation, but it's still valuable for them to implement a common core API: it ensures consistency of the user-facing functions, allows sharing of documentation, and may even allow these implementations to share a common test suite.<br><br>The full code for our Indexable and Hashtbl examples, including explicit definitions of each of the module types, can be found in the <a data-tooltip-position="top" aria-label="https://github.(com/CraigFe/generalised-signatures)" rel="noopener" class="external-link" href="https://github.(com/CraigFe/generalised-signatures)" target="_blank"><code></code> repository</a>generalised-signatures. This repository also contains and a <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/generalised-signatures/blob/main/examples/monads.ml" rel="noopener" class="external-link" href="https://github.com/CraigFe/generalised-signatures/blob/main/examples/monads.ml" target="_blank">third demonstration</a> of this technique being used to express monad-like signatures. The auto-generated documentation for these examples can be <a data-tooltip-position="top" aria-label="https://craigfe.github.io/generalised-signatures/generalised_signatures/Generalised_signatures/index.html" rel="noopener" class="external-link" href="https://craigfe.github.io/generalised-signatures/generalised_signatures/Generalised_signatures/index.html" target="_blank">viewed online</a>.x<br>Thanks for making it to the end; I hope you picked up something useful. If you think it would help others in your network, I'd appreciate it if you <a data-tooltip-position="top" aria-label="https://twitter.com/share?url=NaN&amp;text=%E2%80%9CGeneralised%20signatures%E2%80%9D%2C%20a%20post%20by%20Craig%20Ferguson.%20&amp;via=_craigfe" rel="noopener" class="external-link" href="https://twitter.com/share?url=NaN&amp;text=%E2%80%9CGeneralised%20signatures%E2%80%9D%2C%20a%20post%20by%20Craig%20Ferguson.%20&amp;via=_craigfe" target="_blank">shared it</a> with them.<br><br><br>The typeclasses in Haskell's <a data-tooltip-position="top" aria-label="https://hackage.haskell.org/package/base-4.14.0.0/docs/Data-Foldable.html" rel="noopener" class="external-link" href="https://hackage.haskell.org/package/base-4.14.0.0/docs/Data-Foldable.html" target="_blank">base</a> have the same "polymorphic-instances-only" property as our Indexable1 signature (unsurprising, since it doesn't provide any unboxed container types).<br>class Indexable1 f where        -- Polymorphic instances only
  get    :: f a -&gt; Int -&gt; a
  length :: f a -&gt; Int
Copy<br>A similar trick can be performed there to generalise the typeclass instances for monomorphic containers like Text:<br>{-# LANGUAGE TypeFamilies #-}

type family   Elt container     -- Relate containers to their element type
type instance Elt [a]  = a
type instance Elt Text = Char

class IndexableN c where
  get    :: c -&gt; Int -&gt; Elt c
  length :: c -&gt; Int

instance IndexableN [a] where   -- Polymorphic instance
  get    = (!!)
  length = Prelude.length

instance IndexableN Text where  -- Monomorphic instance
  get    = Text.index
  length = Text.length
Copy<br>As in the OCaml version, we use an Elt type operator to carry the equality needed for the monomorphic case. This time we used type families to specify the relations explicitly, but we could have used multi-parameter type classes for something more akin to the OCaml functor implementation. See the <a data-tooltip-position="top" aria-label="https://hackage.haskell.org/package/mono-traversable" rel="noopener" class="external-link" href="https://hackage.haskell.org/package/mono-traversable" target="_blank">mono-traversable</a> package for more of this sort of trickery in Haskell.<br><br>
<br>This is the approach taken by Jane Street's <a data-tooltip-position="top" aria-label="https://github.com/janestreet/base" rel="noopener" class="external-link" href="https://github.com/janestreet/base" target="_blank">base</a>, and is very similar to the Haskell notion of building standard libraries from type-class instances.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fnref-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fnref-1" target="_blank">↩</a>
<br>This example uses the (** @inline *) tag to ensure that Odoc doesn't leak that Indexable1' is implemented in terms of IndexableN.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/generalised-signatures#fnref-2" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/generalised-signatures#fnref-2" target="_blank">↩</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/generalised-signature.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Generalised signature.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:34 GMT</pubDate><enclosure url="https://www.craigfe.io/posts/generalised-signatures/dag-indexable.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://www.craigfe.io/posts/generalised-signatures/dag-indexable.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Interfacing OCaml and PostgreSQL with Caqti]]></title><description><![CDATA[ 
 <br>
On dealing with dependencies in your Dune-powered OCaml app and interfacing with the most popular DBMS in town.
<br>This article is part of Hands-on OCaml, a series of articles that I’m working on that is focusing on doing web app development with OCaml.<br>The project we will be building throughout the series is a To-Do List app, which connects to a PostgreSQL database as its datastore. In the previous article, we have covered initializing and bootstrapping our project with Dune; if you haven’t seen it, check out the link below:<br>This tutorial will build upon the foundation we have laid out in the previous article in the series. While you can of course follow along without actually doing the tutorial, it is recommended to give the article a read first to be sure that we have the required knowledge in place.<br>In this second article of Hands-on OCaml series, we will explore how to manage dependencies in OCaml project; in particular, we will bring in and use a third-party library to deal with DB operations.<br><br>Assuming you followed the previous article, you should have both opam and jbuilder installed. Verify their installation as follows:<br>$ opam --version  
1.2.2
$ jbuilder --version  
1.0+beta20
Copy<br>We would also need to have a local PostgreSQL instance up and running. I’m assuming readers have had prior experience with PostgreSQL, so I’m not going to expand about it here, but you should be able to install it via your OS package manager (e.g. apt on Ubuntu or brew on MacOS) or via a Docker container. Verify that it is running and and you can connect to it via psql. At the time of this writing, I am using locally installed PostgreSQL 10.4.<br><br>In this section we’ll see how we will prepare our project. Again, this tutorial assumes that you have done the initial <a data-tooltip-position="top" aria-label="https://medium.com/@bobbypriambodo/starting-an-ocaml-app-project-using-dune-d4f74e291de8" rel="noopener" class="external-link" href="https://medium.com/@bobbypriambodo/starting-an-ocaml-app-project-using-dune-d4f74e291de8" target="_blank">Dune setup tutorial</a>. If you haven’t, do check it out. You might also want to remove the previously created .mli and .ml files from bin and lib since we won’t need them anymore.<br><br>First off, we will fetch some dependencies! Unlike the previous tutorial where we install packages directly, we are going to use a different and cleaner method of installing dependencies: through opam files.<br>Make sure you’re in the todolist directory, create a file named todolist.opam with the following contents:<br>opam-version: "1.2"
name: "todolist"
version: "1.0.0"
maintainer: "Your Name &lt;email@example.com&gt;"

depends: [
  "jbuilder" {build}
  "lwt"
  "lwt_ppx"
  "caqti"
  "caqti-lwt"
  "caqti-driver-postgresql"
]
Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/interfacing-ocaml-and-postgresql-with-caqti.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Interfacing OCaml and PostgreSQL with Caqti.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:31 GMT</pubDate></item><item><title><![CDATA[Off to the Races Using ThreadSanitizer in OCaml]]></title><description><![CDATA[ 
 <br>OCaml Multicore opened up a new world of performance for developers, something that <a data-tooltip-position="top" aria-label="https://tarides.com/blog/2022-12-20-how-nomadic-labs-used-multicore-processing-to-create-a-faster-blockchain/" rel="noopener" class="external-link" href="https://tarides.com/blog/2022-12-20-how-nomadic-labs-used-multicore-processing-to-create-a-faster-blockchain/" target="_blank">Nomadic Labs has tested with great results.</a> Rather than relying on one core to do everything, the program can take advantage of multiple cores simultaneously for a significant performance boost.<br>With new programming possibilities come new classes of bugs, which require updated detection methods. One of these types of bugs is called a data race. A data race is a race condition that occurs when two accesses are made to the same memory location, at least one is a write, and no order is enforced between them.<br>Data races can be dangerous as they are easy to miss and capable of yielding unexpected results. Consequently, integrating a tool to detect data races has been a high priority for the teams working on OCaml 5.0 with Multicore support. Whilst data races in OCaml are less problematic than in many other languages (for example, data races in OCaml do not cause crashes and do not constitute undefined behaviour), developers still want to be made aware of possible data races so that they can remove them from their programs. More about this below.<br><br><br><a data-tooltip-position="top" aria-label="https://clang.llvm.org/docs/ThreadSanitizer.html" rel="noopener" class="external-link" href="https://clang.llvm.org/docs/ThreadSanitizer.html" target="_blank">ThreadSanitizer</a>, or TSan, is an open-source tool that reliably detects data races at runtime. It consists of instrumenting programs with calls to a dedicated runtime that performs the detection.<br>Support for TSan will officially be part of the OCaml 5.2 release, and there is already a backport for OCaml 5.1.<br>This blog post will demonstrate the benefits of using TSan, offer insight into how TSan works, and outline the challenges of integrating it with OCaml. For a more practically oriented guide on how to use TSan in your own projects, the <a data-tooltip-position="top" aria-label="https://ocaml.org/docs/multicore-transition" rel="noopener" class="external-link" href="https://ocaml.org/docs/multicore-transition" target="_blank">tutorial on using TSan with OCaml Multicore</a> is a great place to start.<br>We will begin by examining what a data race looks like, both before and after using TSan.<br><br><br>Let us consider how a data race might occur. Say an OCaml programmer writes code to populate a table of clients from several sources. They decide to make it Multicore to improve performance by using two Domains for two data sources:<br>let clients = Hashtbl.create 16
let free_id = Atomic.make 0

let clients1 = (* Some data source *)

let clients2 = (* Some data source *)

let record_clients =
  Seq.iter
    (fun c -&gt; Hashtbl.add clients (Atomic.fetch_and_add free_id 1) c)

let () =
  let d = Domain.spawn (fun () -&gt; record_clients clients1) in
  record_clients clients2;
  Domain.join d
Copy<br>As we can tell, each incoming client is bound to a unique ID. The programmer correctly used the Atomic module for ID generation, ensuring the IDs are truly unique. However, they have failed to use a domain-safe module designed for concurrency, instead opting for Hashtbl. Unfortunately, this module is unsafe for concurrent use: using Hashtbl.t in parallel can cause data races and lead to surprising results.<br>For example, when two domains add elements in parallel it may cause some elements to be silently dropped. To make matters worse, the resulting bugs would be non-deterministic and as such be hard to detect and track down. Furthermore, if the programmer's project depends on libraries that use Hashtbl, it would make them unsafe to use in parallel without it necessarily being clear from their documentation.<br>If, however, the programmer were to build their program on a special opam switch with a TSan-enabled compiler like this:<br>$ opam switch create 5.1.0+tsan
$ opam install dune
$ dune exec ./clients.exe
Copy<br>(Side note: the 5.1.0+tsan switch is the most convenient way to use TSan with OCaml at the time of writing. Once OCaml 5.2 is released, the blessed command will be opam switch create &lt;switch name&gt; ocaml-option-tsan.)<br>All memory accesses would be instrumented with calls to the TSan runtime, and TSan would detect the data race condition and output a data race report:<br>==================
WARNING: ThreadSanitizer: data race (pid=790576)
  Write of size 8 at 0x7f42b37f57e0 by main thread (mutexes: write M86):
    #0 caml_modify runtime/memory.c:166 (clients.exe+0x58b87d)
    #1 camlStdlib__Hashtbl.resize_749 stdlib/hashtbl.ml:152 (clients.exe+0x536766)
    #2 camlStdlib__Seq.iter_329 stdlib/seq.ml:76 (clients.exe+0x4c8a87)
    #3 camlDune__exe__Clients.entry /workspace_root/clients.ml:9 (clients.exe+0x4650ef)
    #4 caml_program &lt;null&gt; (clients.exe+0x45fefe)
    #5 caml_start_program &lt;null&gt; (clients.exe+0x5a0ae7)

  Previous read of size 8 at 0x7f42b37f57e0 by thread T1 (mutexes: write M90):
    #0 camlStdlib__Hashtbl.key_index_1308 stdlib/hashtbl.ml:507 (clients.exe+0x53a625)
    #1 camlStdlib__Hashtbl.add_1312 stdlib/hashtbl.ml:511 (clients.exe+0x53a6f8)
    #2 camlStdlib__Seq.iter_329 stdlib/seq.ml:76 (clients.exe+0x4c8a87)
    #3 camlStdlib__Domain.body_703 stdlib/domain.ml:202 (clients.exe+0x50bf60)
    #4 caml_start_program &lt;null&gt; (clients.exe+0x5a0ae7)
    #5 caml_callback_exn runtime/callback.c:197 (clients.exe+0x56917b)
    #6 caml_callback runtime/callback.c:293 (clients.exe+0x569cb0)
    #7 domain_thread_func runtime/domain.c:1100 (clients.exe+0x56d37f)
    [...]

SUMMARY: ThreadSanitizer: data race runtime/memory.c:166 in caml_modify
==================
[...]
ThreadSanitizer: reported 2 warnings
Copy<br>Above is a truncated view of what the TSan report, warning of a data race, looks like in this case. TSan has detected two memory accesses, a write and a read, made to one memory location. As they are also unordered, this constitutes a data race and TSan reports it along with the backtraces of both accesses.<br>In this case it would be evident that something has gone wrong with Hashtbl.add – a big hint to the programmer.<br><br><br>Now that we know what TSan is used for, it's time to explore how it works. Compiling a program with TSan enabled causes the executable to be instrumented with calls to the TSan runtime library. The runtime library tracks memory accesses and ordering relations between these accesses.<br>Internally, the TSan runtime assigns a vector clock to each OCaml domain or system thread. Each thread holds a vector clock – a vector clock being an array of n integers, where n is the number of threads – and increments its clock upon each event (memory access, mutex operation, etc.). Certain operations like mutex locks, atomic reads, and so on, will synchronise clocks between threads.<br><a data-tooltip-position="top" aria-label="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/798d4/vector-clocks.png" rel="noopener" class="external-link" href="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/798d4/vector-clocks.png" target="_blank"></a><img alt="A mutex lock synchronising the clock between two threads." src="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/c5bb3/vector-clocks.png" referrerpolicy="no-referrer"><br>Comparing vector clocks allows TSan to establish an order between events, so-called <a data-tooltip-position="top" aria-label="https://jameshfisher.com/2017/02/10/happened-before/" rel="noopener" class="external-link" href="https://jameshfisher.com/2017/02/10/happened-before/" target="_blank">happens-before relations.</a> TSan reports a data race every time two memory accesses are made to overlapping memory regions, if:<br>
<br>At least one of them is a write, and
<br>There is no established happens-before relation between them.
<br><br><br>Let us look at this process in more detail. Each word of application memory is associated with one or more 'shadow words'. Each shadow word contains information about a recent memory access to that word. This information points to the vector clock's state at the moment the access was performed.<br><a data-tooltip-position="top" aria-label="https://tarides.com/static/805211c514eae2a5f8a8410fd26f476a/d125e/shadow-state.png" rel="noopener" class="external-link" href="https://tarides.com/static/805211c514eae2a5f8a8410fd26f476a/d125e/shadow-state.png" target="_blank"></a><img alt="A box labelled application with an arrow to a box labeled shadow state." src="https://tarides.com/static/805211c514eae2a5f8a8410fd26f476a/c5bb3/shadow-state.png" referrerpolicy="no-referrer"><br>This information (called the 'shadow state') is updated at every instrumented memory access: TSan compares the accessor's clock with each existing shadow word, and checks the following:<br>
<br>Do the accesses overlap?
<br>Is one of them a write?
<br>Are the thread IDs different?
<br>Are they unordered by happens-before?
<br>If these conditions are met, TSan detects and reports a data race.<br>In addition to memory access, operations like Domain.spawn and Domain.join (as well as mutex operations) are relevant for operation ordering. As such, TSan also instruments these operations.<br><br><br>The core of TSan support is instrumentation of memory acceses with calls to the TSan runtime. The OCaml compiler performs this instrumentation in a dedicated pass.<br><br><br>For TSan to show a backtrace of past events, function entries and exits must also be instrumented. This is done as part of the instrumentation pass.<br>However, in OCaml, a function can also be exited by an <a data-tooltip-position="top" aria-label="https://github.com/fabbing/obts_exn" rel="noopener" class="external-link" href="https://github.com/fabbing/obts_exn" target="_blank">exception</a>, bypassing part of the instrumentation. When that happens, for TSan’s view of the backtrace to remain up-to-date, the OCaml runtime informs TSan about every exited function.<br><br><br><a data-tooltip-position="top" aria-label="https://v2.ocaml.org/manual/effects.html" rel="noopener" class="external-link" href="https://v2.ocaml.org/manual/effects.html" target="_blank">Effect handlers</a> are a generalisation of exception handlers. Performing an effect results in a jump to the associated effect handler, and then a delimited continuation makes it possible to resume the computation. In the same way as with exceptions, the OCaml runtime must signal to TSan which functions are exited when an effect is performed and re-entered when a continuation is resumed.<br><br><br>Each language specifies how memory behaves in parallel programs using what is known as a memory model. Incidentally, what counts as a data race in a given language also depends on its memory model.<br>TSan can detect data races in programs that follow the C memory model. OCaml 5's memory model is different from the C model, however, and it offers more guarantees: data races in C and C++ cause undefined behaviour (i.e., anything can happen), which is not the case in OCaml. OCaml's semantics are “fully defined” (see the <a data-tooltip-position="top" aria-label="https://v2.ocaml.org/manual/memorymodel.html" rel="noopener" class="external-link" href="https://v2.ocaml.org/manual/memorymodel.html" target="_blank">manual page</a> about the memory model). In particular, a program with data races in OCaml will not crash, unlike in C++. In addition, there can be no <a data-tooltip-position="top" aria-label="https://www.hboehm.info/c++mm/thin_air.html" rel="noopener" class="external-link" href="https://www.hboehm.info/c++mm/thin_air.html" target="_blank">out-of-thin-air values</a>: the only values that can be observed are values that are previously written to that location. The OCaml memory model guarantees that even for programs with data races, memory safety is preserved.<br>Data races in OCaml can still result in unexpected surprises for the OCaml programmer. A multi-threaded execution may produce behaviours that cannot be explained by the mere interleaving of actions from different threads. The only way such behaviours can be explained is through a reordering of actions in the same thread. Such reasoning is quite unintuitive for a programmer who will be more used to thinking about program behaviour as being an interleaving of actions from different threads.<br>However, if the program is data-race free, then the observed behaviour can be explained by a simple interleaving of operations from different threads (a property known as sequential consistency). Eliminating data races reduces non-determinism in the program and hence it is beneficial to remove data races whenever possible. Note that we do not completely eliminate non-determinism from a parallel program.<br>In essence, because of the differences between the C and OCaml memory models, in order for TSan to detect data races in OCaml the instrumentation of memory accesses must conceptually map OCaml programs to C programs. During development, the team took care to ensure that this mapping preserved the detection of data races (in the OCaml sense) and did not introduce false positives.<br>You can find more details about the inner workings of TSan and its OCaml support in this <a data-tooltip-position="top" aria-label="https://github.com/fabbing/ocaml_tsan_icfp/blob/master/presentation/presentation.pdf" rel="noopener" class="external-link" href="https://github.com/fabbing/ocaml_tsan_icfp/blob/master/presentation/presentation.pdf" target="_blank">OCaml Workshop 2023 talk</a>.<br><br><br>In terms of the cost of running TSan, currently, it affects memory and performance in the following ways:<br>
<br>Performance cost: about a 2-7x slowdown (compared to 5-15x for C/C++)
<br>Memory consumption: increased by about 4-7x (compared to 5-10x for C++)
<br>As with all tools, TSan has some limitations. These are due to how TSan is built and are unlikely to change. With TSan, data races are only detected on visited code paths. In addition, TSan only remembers a finite amount of memory accesses for space-saving reasons, which can in principle cause TSan to miss some races. TSan also does not currently support Windows.<br>TSan support for OCaml is currently only implemented for x86-64 Linux and macOS, but will hopefully be extended to include more architectures such as arm64.<br><br><br>Knowing the limitations, let us explore TSan's use cases. So far, TSan has helped by unearthing data races in several OCaml libraries:<br>
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/saturn" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/saturn" target="_blank">Saturn (formerly known as Lockfree):</a> TSan <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/saturn/issues/39" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/saturn/issues/39" target="_blank">found a benign data race</a>, as well as a data race occuring from the use of <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/saturn/pull/40" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/saturn/pull/40" target="_blank">semaphores</a>.
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/domainslib" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/domainslib" target="_blank">Domainslib:</a> TSan found benign data races in Chan, not just <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/domainslib/issues/72" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/domainslib/issues/72" target="_blank">once</a> but <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/domainslib/pull/103" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/domainslib/pull/103" target="_blank">twice</a>.
<br><a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml" target="_blank">The OCaml runtime system</a> itself: TSan warned about a <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/11040" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/issues/11040" target="_blank">number of race conditions</a> in the OCaml runtime.
<br>In addition, TSan has been a great help in transitioning the effects-based I/O library <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/eio" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/eio" target="_blank">Eio</a> and the distributed database <a data-tooltip-position="top" aria-label="https://github.com/mirage/irmin" rel="noopener" class="external-link" href="https://github.com/mirage/irmin" target="_blank">Irmin</a> to Multicore. It allowed teams to detect potential data races and fix them as required.<br><br><br>We want to hear from you – are you using TSan for your OCaml projects? Please get in touch and let us know about your experience, whether you have encountered any problems, and if you have any suggestions for how it could be improved.<br>You can share your thoughts on the <a data-tooltip-position="top" aria-label="https://discuss.ocaml.org" rel="noopener" class="external-link" href="https://discuss.ocaml.org" target="_blank">OCaml Discuss Forum</a> or contact Tarides directly <a data-tooltip-position="top" aria-label="https://tarides.com/contact/" rel="noopener" class="external-link" href="https://tarides.com/contact/" target="_blank">on our website</a>. Don't forget to check out the <a data-tooltip-position="top" aria-label="https://ocaml.org/docs/multicore-transition" rel="noopener" class="external-link" href="https://ocaml.org/docs/multicore-transition" target="_blank">TSan tutorial</a> as well. Happy hacking!]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/off-to-the-races-using-threadsanitizer-in-ocaml.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Off to the Races Using ThreadSanitizer in OCaml.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:30 GMT</pubDate><enclosure url="https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/c5bb3/vector-clocks.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://tarides.com/static/0bf64db362ac972b1285ab93f569b53a/c5bb3/vector-clocks.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Pitfalls of polymorphic ignore]]></title><description><![CDATA[ 
 <br>In OCaml, we can ignore the return value of a side-effecting computation by either naming it _ or using Stdlib.ignore. These are commonly used to test functions:<br>let test_eval =
  (* we only care that [eval] doesn't raise an exception *)
  let _ = eval (`Int 5) in
  ignore (eval (`Add (`Int 1, `Int 2)))
Copy<br>There's a bug waiting to happen here. Suppose we later refactor eval to take another variable:<br>val eval : expr -&gt; context -&gt; expr
Copy<br>Suddenly, the test is ignoring partially-applied functions of type context -&gt; expr: it silently became useless! For this reason, one often sees code that explicitly asserts the type of the value being ignored:<br>let ignore_expr (_ : expr) = ()

let test_eval =
  (* require the ignored value to have type [expr] *)
  let (_ : expr) = eval (`Int 5) in
  ignore_expr (eval (`Add (`Int 1, `Int 2)))
Copy<br>The type-checker now catches our partial application bug:<br>File "test_eval.ml", line 9, characters 19-25:
9 |   let (_ : expr) = eval (`Int 5) in
                       ^^^^^^^^^^^^^
Error: This expression has type context -&gt; expr
       but an expression was expected of type expr
Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/pitfalls-of-polymorphic-ignore.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Pitfalls of polymorphic ignore.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:33 GMT</pubDate></item><item><title><![CDATA[Polymorphic type constraints]]></title><description><![CDATA[ 
 <br>
In this post, I explain a common mistake when writing constraints of polymorphic functions in OCaml programs, then show how to correct it.
<br><br>One of the earliest lessons of any functional programming tutorial is how to write polymorphic functions and their signatures:<br>val id  : 'a -&gt; 'a
val fst : ('a * 'b) -&gt; 'a
val map : ('a -&gt; 'b) -&gt; 'a list -&gt; 'b list
Copy<br>A typical explanation of these type signatures goes along the lines of:<br>
Types of the form 'a, 'b, ..., known as type variables, stand for an unknown type. They allow us to describe functions that work uniformly over many possible input types. This is known as "parametric polymorphism".
— Hypothetical education resource<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-1" target="_blank">1</a>
<br>As is often the case with introductory explanations, this is just specific enough to be technically correct without introducing too many new concepts, letting us hurry on to demonstrating useful examples before the student gets bored. Unfortunately, we've laid a trap: when our reader learns about type constraints, they naturally try to combine these two "intuitive" features and get bitten:<br>ᐅ let id1 : 'a -&gt; 'a = (fun x -&gt; x) ;;        (* Accepted. So far so good... *)
  val id1 : 'a -&gt; 'a = &lt;fun&gt;

ᐅ let id2 : 'a -&gt; 'a = (fun x -&gt; x + 1) ;;    (* Also accepted. Uh oh... *)
  val id2 : int -&gt; int = &lt;fun&gt;
Copy<br>In this case, the student finds that 'a -&gt; 'a is a valid constraint for a function of type int -&gt; int, and their mental model is broken almost immediately. It's quite natural to expect id2 to be rejected as a non-polymorphic function, particularly given our vague explanation of what 'a actually means.<br>Our hypothetical student's mistake stems from the fact that type variables in signatures are implicitly universally-quantified – that is, they stand for all types – whereas type variables in constraints are not. To understand what this means, let's try to pin down a more precise idea of what type variables are. If you're already &nbsp;indoctrinated&nbsp; comfortable with type variables, you may wish to <a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#true-polymorphic-constraints" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#true-polymorphic-constraints" target="_blank">cut to the chase</a>.<br><br>Type variables in constraints are referred to as being "unbound" (or "free"), meaning that they stand for some type that is not yet known to the type-checker: they are placeholders that can later be filled by a particular type. Without going into <a data-tooltip-position="top" aria-label="http://dev.stephendiehl.com/fun/006_hindley_milner.html" rel="noopener" class="external-link" href="http://dev.stephendiehl.com/fun/006_hindley_milner.html" target="_blank">the details</a>, these placeholders are gradually determined as the type-checker resolves constraints. For instance, in our id2 example, the type-checker decides that 'a equals int by first reconciling the user-supplied constraint 'a -&gt; 'a with the constraint int -&gt; int that it inferred from the implementation.<br>To a theorist (or type-system developer), who regularly has to worry about types that are not yet fully known, the notion of a "placeholder" is a sensible default meaning of an unbound type variable. Such people also tend to use explicit syntax to disambiguate the alternative case, type variables that are bound:<br>
∀ a. a -&gt; a (read as: "For all a, a -&gt; a")
<br>We call "∀ a" a universal quantifier because it introduces a variable a, bound inside the quantifier, that can stand for any type in the universe of OCaml types. It's this flavour of type variable that enables parametric polymorphism and – although the OCaml syntax often tries to hide it from you – these quantifiers exist everywhere in your programs. As I already mentioned, all unbound variables in signatures are implicitly quantified in this way:<br>
val length : 'a list -&gt; int
... secretly means ...
val length : ∀ a. a list -&gt; int
<br>On the implementation side of length, the compiler will check to see if there are any placeholder variables left after type-checking the definition and wrap them in universal quantifiers (if it's sure that it's safe to do so<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-2" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-2" target="_blank">2</a>). When this happens, we say that those type variables have been generalised. Once length has been given its polymorphic type, the user gets to pick a specific type a at each call-site by passing it a list of any element type they want. This idea of choosing the instantiation of a at each call-site is what is "parametric" about "parametric polymorphism".<br>Taking a step back, we can now see what went wrong with our hypothetical introduction to type variables above: it led our student to think of all type variables as being implicitly universally-quantified, when this is not true in constraints<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-3" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-3" target="_blank">3</a>. So, given that we can't rely on implicit generalisation in constraints, what can we do to declare that our code is polymorphic within the implementation itself?<br><br>The punchline is that OCaml actually does have syntax for expressing polymorphic constraints – and it even involves an explicit quantifier – but sadly it's not often taught to beginners:<br>let id : 'a. 'a -&gt; 'a = (fun x -&gt; x + 1)
Copy<br>The syntax 'a. 'a -&gt; 'a denotes an <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/types.html#poly-typexpr" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/types.html#poly-typexpr" target="_blank">explicitly-polymorphic type</a>, where 'a. corresponds directly with the ∀ a. quantifier we've been using so far. Applying it here gives us a satisfyingly readable error message:<br>Error: This definition has type int -&gt; int which is less general than
         'a. 'a -&gt; 'a
Copy<br>The caveat of polymorphic constraints is that we can only apply them directly to let-bindings, not to function bodies or other forms of expression:<br>let panic : 'a. unit -&gt; 'a = (fun () -&gt; raise Crisis)  (* Works fine... *)

let panic () : 'a. 'a = raise Crisis                   (* Uh oh... *)
(*               ^
 *  Error: Syntax error  *)
Copy<br>This somewhat unhelpful error message arises because OCaml will never infer a polymorphic type for a value that is not let-bound. Trying to make your type inference algorithm cleverer than this quickly runs into certain <a data-tooltip-position="top" aria-label="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf" rel="noopener" class="external-link" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf" target="_blank">undecidable problems</a>; the parser knows that the type-checker is afraid of undecidable problems, and so rejects the program straight away<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-4" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fn-4" target="_blank">4</a>.<br>In spite of their limitations, explicitly-polymorphic type constraints are a great way to express polymorphic intent in your OCaml programs, either as internal documentation or to have more productive conversations with the type-checker when debugging. I recommend using them frequently and teaching them to beginners as soon as possible.<br>At this point, if you suffered through my explanation of type variables in the previous section, you may be thinking the following:<br>
"If introducing type variables properly requires so many paragraphs of jargon, we shouldn't burden beginners with the details right away."
— Straw-man argument
<br>Personally, I find that introducing these terms early on in the learning process is easily worthwhile in avoiding early roadblocks, but that discussion can wait for another time. In the spirit of functional programming for the masses, let's summarise with a less jargon-heavy attempt at redrafting our hypothetical education resource:<br>
Types of the form 'a, 'b, ..., known as type variables, are placeholders for an undetermined type. When bound by for-all quantifiers (of the form 'a.), they can be used to describe values that can take on many possible types. For instance, we can write the type of (fun x -&gt; x) as 'a. 'a -&gt; 'a, meaning:

"For any type 'a, this function can take on type 'a -&gt; 'a."

The OCaml type-checker will infer polymorphic types wherever it is safe, but we can also explicitly specify a polymorphic type for a let-binding:
ᐅ let fst : 'a 'b. ('a * 'b) -&gt; 'a = (* "For all ['a] and ['b], ..." *)
    fun (x, _) -&gt; x ;;

val fst : 'a * 'b -&gt; 'a = &lt;fun&gt;
Copy
Note that all type variables in signatures are implicitly universally-quantified: it's not necessary (or even possible) to write 'a 'b. before the type.
<br>The explanation is undeniably still longer and more technical than the one we started with, but crucially it uses the extra space to give the reader a clue as to how to debug their polymorphic functions.<br>The story doesn't end here. We haven't discussed <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/gadts.html" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/gadts.html" target="_blank">existential quantifiers</a>, the other type of type variable binding; or <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s:polymorphic-recursion" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s:polymorphic-recursion" target="_blank">polymorphic recursion</a>, where polymorphic annotations become compulsory; or <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/locallyabstract.html" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/locallyabstract.html" target="_blank">locally-abstract types</a>, which offer other useful syntaxes for constraining your OCaml programs to be polymorphic. These will all have to wait for future posts. For now, thanks for reading!<br><br>
<br>
Very similar equivalents of this explanation exist in <a data-tooltip-position="top" aria-label="http://dev.realworldocaml.org/guided-tour.html" rel="noopener" class="external-link" href="http://dev.realworldocaml.org/guided-tour.html" target="_blank">Real World OCaml</a>, <a data-tooltip-position="top" aria-label="https://www.cs.cornell.edu/courses/cs3110/2020sp/textbook/" rel="noopener" class="external-link" href="https://www.cs.cornell.edu/courses/cs3110/2020sp/textbook/" target="_blank">Cornell's OCaml course</a>, and <a data-tooltip-position="top" aria-label="https://www.cl.cam.ac.uk/teaching/1920/FoundsCS/focs-201920-v1.1.pdf" rel="noopener" class="external-link" href="https://www.cl.cam.ac.uk/teaching/1920/FoundsCS/focs-201920-v1.1.pdf" target="_blank">Cambridge's OCaml course</a>. Type variables are variously described as representing "any type", "an unknown type" or "a generic type"; explanations that are all as different as they are vague.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-1" target="_blank">↩</a>

<br>
The most famous example of a type variable that is unsafe to generalise is one that has been captured in mutable state:
ᐅ let state = ref [] ;;
ᐅ let sneaky_id x = (state := x :: !state); x ;;

val sneaky_id : '_weak1 -&gt; '_weak1 = &lt;fun&gt;
Copy
In this case, it's not possible to give sneaky_id the type ∀ a. a -&gt; a because different choices of the type a are not independent: passing a string to sneaky_id, followed by an integer, would build a list containing both strings and integers, violating type safety. Instead, sneaky_id is given a type containing a "<a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:weak-types" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:weak-types" target="_blank">weak type variable</a>" which represents a single, unknown type. This meaning of type variables should be familiar to you; it's exactly the same as the "unbound" type variables we've been discussing!
In general, it's not easy to decide if it's safe to generalise a particular type variable. OCaml makes a quick under-approximation called the <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:valuerestriction" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#ss:valuerestriction" target="_blank">(relaxed) value restriction</a>.
<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-2" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-2" target="_blank">↩</a>

<br>
As an aside, there's no profound reason why constraints must behave differently with respect to implicit quantification. Both SML and Haskell choose to generalise variables in constraints:
val id1 : 'a -&gt; 'a = (fn x =&gt; x + 1);
(*  Error: pattern and expression in val dec do not agree

pattern:    'a -&gt; 'a
expression: 'Z[INT] -&gt; 'Z[INT] *)
Copy

(Note: val f : t = e in SML is analogous to let f : t = e in OCaml.)

I suspect that constraints having the same quantification behaviour as signatures is more intuitive, at least for simple examples. In complex cases, the exact point at which type variables are implicitly quantified can be surprising, and so SML '97 provides an explicit quantification syntax for taking control of this behaviour. See <a data-tooltip-position="top" aria-label="https://www.smlnj.org/doc/Conversion/types.html#Explicit" rel="noopener" class="external-link" href="https://www.smlnj.org/doc/Conversion/types.html#Explicit" target="_blank">the SML/NJ guide (§ 1.1.3)</a> for much more detail.
The advantage of OCaml's approach is that it enables constraining subcomponents of types without needing to specify the entire thing (as in (1, x) : (int * _)), which can be useful when quickly constraining types as a sanity check or for code clarity. As far as I'm aware, SML has no equivalent feature.
<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-3" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-3" target="_blank">↩</a>

<br>
This limitation of the type-checker can artificially limit the polymorphism that can be extracted from your programs. If you want to take polymorphism to its limits – as God intended – it's sometimes necessary to exploit another point where explicitly-polymorphic types can appear: <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s%3Ahigher-rank-poly" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/docs/manual-ocaml/polymorphism.html#s%3Ahigher-rank-poly" target="_blank">record and object fields</a>.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-4" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/polymorphic-type-constraints#fnref-4" target="_blank">↩</a>

]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/polymorphic-type-constraints.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Polymorphic type constraints.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:34 GMT</pubDate></item><item><title><![CDATA[Polymorphic Variants]]></title><description><![CDATA[ 
 <br><br>This tutorial teaches you how to use polymorphic variants. This includes starting to use them, maintaining a project already using them, deciding when to use them or not, and balancing their unique benefits against their drawbacks.<br>Product types and data types such as option and list are variants and polymorphic. In this tutorial, they are called simple variants to distinguish them from the polymorphic variants presented here. Simple variants and polymorphic variants are close siblings. Their values are both introduced using labels that may carry data. Both can be recursive and have type parameters. By the way, don't trust a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Large_language_model" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">LLM</a> chatbot if it tells you polymorphic variants are dynamically typed; it is a hallucination. Like simple variants, polymorphic variants are type-checked statically.<br>However, they are type-checked using different algorithms, which results in a different programming experience. The relationship between value and type (written with the colon symbol :) is changed with polymorphic variants. Usually, values are thought of as inhabitants of the type, which is regarded as a set-like thing. Rather, polymorphic variant values should be considered as pieces of data that several functions can accept. Polymorphic variants types are a way to express compatibility relationships between those functions. The approach in this tutorial is to build sense from experience using features of polymorphic variants.<br>Prerequisites: This is an intermediate-level tutorial. It is required to have completed tutorials on <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/functions-and-values" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/functions-and-values" target="_blank">Functions and Values</a>, <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/basic-data-types" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/basic-data-types" target="_blank">Basic Data Types</a>, and <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/lists" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/lists" target="_blank">Lists</a> to begin this one.<br><br>Polymorphic variants originate from Jacques Garrigue's work on Objective Label, which was <a data-tooltip-position="top" aria-label="https://caml.inria.fr/pub/old_caml_site/caml-list-ar/0533.html" rel="noopener" class="external-link" href="https://caml.inria.fr/pub/old_caml_site/caml-list-ar/0533.html" target="_blank">first published in 1996</a>. It became part of standard Objective Caml with <a data-tooltip-position="top" aria-label="https://caml.inria.fr/distrib/ocaml-3.00/" rel="noopener" class="external-link" href="https://caml.inria.fr/distrib/ocaml-3.00/" target="_blank">release 3.0</a> in 2000, along with labelled and optional function arguments. They were introduced to give more precise types in <a data-tooltip-position="top" aria-label="https://garrigue.github.io/labltk/" rel="noopener" class="external-link" href="https://garrigue.github.io/labltk/" target="_blank">LablTk</a>.<br>The core type system of OCaml follows a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Nominal_type_system" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Nominal_type_system" target="_blank"><em></em></a>nominal discipline. Variants must be explicitly declared before being used. The typing discipline used for polymorphic variants and classes is different, as it is <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Structural_type_system" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Structural_type_system" target="_blank"><em></em></a>structural.<br>In the nominal approach of typing, types are first defined; later, when the type of an expression is inferred, three outcomes are possible:<br>
<br>If a matching type is found, it becomes the inferred type.
<br>If any type can be applied, a type parameter is created.
<br>If typing inconsistencies are found, an error is raised.
<br>This is very similar to solving an equation in mathematics. Equations accept either zero, exactly one, several, or infinitely many numbers as solutions. Nominal type-checking finds that either zero, exactly one, or any type can be used in an expression.<br>In the structural approach of typing, type definitions are optional, so they can be omitted. Type-checking an expression constructs a data structure that represents the types that are compatible with it. These data structures are displayed as type expressions sharing a resemblance with simple variants.<br><br>The type expression 'a list does not designate a single type; it designates a family of types, basically all the types that can be created by substituting an actual type to type parameter 'a. The type expressions int list, bool option list, or (float -&gt; float) list list are real types. They're actual members of the type family 'a list. Types are intended to have inhabitants, type families don't.<br>The identifiers list, option, and others are type operators. Just like functions, they take parameters. Although these parameters are not values, they are types. Their results aren't values but types, too.<br>Simple variants are polymorphic, but not in the same sense as polymorphic variants.<br>
<br>Simple variants have <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Parametric_polymorphism" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Parametric_polymorphism" target="_blank">parametric polymorphism</a>
<br>Polymorphic variants have a form of <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Structural_type_system" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Structural_type_system" target="_blank">structural polymorphism</a>
<br><br><br>The visual signature of the polymorphic variants is the back quote. Pattern matching on them looks just the same as with simple variants, except for back quotes (and capitals, which are no longer required).<br># let f = function `Broccoli -&gt; "Broccoli" | `Fruit name -&gt; name;;
val f : [&lt; `Broccoli | `Fruit of string ] -&gt; string = &lt;fun&gt;
Copy<br>Here `Broccoli and `Fruit play a role similar to the one played by the constructors Broccoli and Fruit in a variant declared as type t = Broccoli | Fruit of string. Except, and most importantly, that the definition doesn't need to be written. The tokens `Broccoli and `Fruit are called tags instead of constructors. A tag is defined by a name and a list of parameter types.<br>The expression [&lt; `Broccoli | `Fruit of string ] plays the role of a type. However, it does not represent a single type, it represents three different types.<br>
<br>The type that only has `Broccoli as an inhabitant, its translation into a simple variant is type t0 = Broccoli
<br>The type that only has `Fruit as an inhabitant, its translation into a simple variant is type t1 = Fruit of string
<br>Type type that has both `Broccoli and `Fruit inhabitants, its translation into a simple variant is type t2 = Broccoli | Fruit of string
<br>Note each of the above translations into simple variants is correct. However, entering them as-is into the environment would lead to constructor shadowing (unless type annotation is used at pattern or expression level, see section on <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#Shared-Constructors" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#Shared-Constructors" target="_blank">Shared Constructors</a>).<br>This also illustrates the other striking feature of polymorphic variants: values can be attached to several types. For instance, the tag `Broccoli inhabits [ `Broccoli ] and[ `Broccoli | `Fruit of String ], but it also inhabits any type that contains it.<br>What is displayed by the type-checker, for instance [&lt; `Broccoli | `Fruit of string ], isn't a single type. It is a type expression that designates a constrained set of types. For instance, all the types that are defined by a group of tags containing either `Broccoli or `Fruit of string and nothing more. This is the meaning of the &lt; sign in this type expression. This is a bit similar to what happens with 'a list, which isn't a single type either, but a type expression that designates the set of list types of something, i.e. the set of types where 'a has been replaced by some other type. It is also meant to indicate that the exact types are subsets of the indicated ones (this will be explained in the section on <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#Subtyping-of-Polymorphic-Variants" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#Subtyping-of-Polymorphic-Variants" target="_blank">Subtyping</a>).<br>This is the sense of the polymorphism of polymorphic variants. Polymorphic variants types are type expressions. The structural typing algorithm used for polymorphic variants creates type expressions that designate sets of types (here the three types above), which are defined by constraints on sets of tags (the inequality symbols). The polymorphism of polymorphic variants is different.<br>In the rest of this tutorial, the following terminology is used:<br>
<br>“simple variants”: product types and variants such as list, option
<br>polymorphic variant: type expressions displayed by the OCaml type-checker such as [&lt; `Broccoli | `Fruit of string ]
<br><br>Here is another function using pattern matching on a polymorphic variant.<br># let g = function `Edible name -&gt; name | `Broccoli -&gt; "Brassica oleracea";;
val g : [&lt; `Broccoli | `Edible of string ] -&gt; string = &lt;fun&gt;

# f `Broccoli;;
- : string = "Broccoli"

# g `Broccoli;;
- : string = "Brassica oleracea"
Copy<br>Both f and g accept the `Broccoli tag as input because they both have code for it. They do not have the same domain because f also accepts `Fruit of string whilst g also accepts `Edible of string. The domains of f and g express this. The tag `Broccoli satisfies both the constraints of the domain of f: [&lt; `Broccoli | `Fruit of string ] and the constraints of the domain of g: [&lt; `Broccoli | `Fruit of string ]. That type is [ `Broccoli ]. The value defined by a tag belongs to several types, similarly, a tag accepting functions belongs to several types.<br>Polymorphic variant tags are meant to be used as stand-alone values, wherever it makes sense. As long as used consistently, with a single implicit type per tag, the type checker will accept any combination of them in pattern-matching expressions.<br><br>Type-checking of polymorphic variants is static. No information on tag types is available at runtime.<br># [ `Fruit "Banana"; `Fruit true ];;
Error: This expression has type bool but an expression was expected of type
         string
Copy<br>When a tag is used inconsistently, the type-checker raises an error.<br><br>When building an expression from sub-expressions, the type-checker assembles types from sub-expressions to create the type of the whole expression. This is why the type discipline used for polymorphic variants is said to be structural, it follows the structure of the expressions.<br># let brocco = `Broccoli;;

# let pepe = `Peperone;;

# [ brocco; pepe; brocco ];;
- : [&gt; `Broccoli | `Peperone ] list = [`Broccoli; `Peperone; `Broccoli]
Copy<br><br>Polymorphic variant type expressions can have three forms:<br>
<br>Exact: [ `Broccoli | `Gherkin | `Fruit of string ]<br>
This only designates the type inhabited by the values introduced by these tags.
<br>Closed: [&lt; `Broccoli | `Gherkin | `Fruit of string ]<br>
This designates a set of exact types. Each exact type is inhabited by the values introduced by a subset of the tags from 1. For instance, there are 7 exact types as such:
<br>
<br>[ `Broccoli ]
<br>[ `Gherkin ]
<br>[ `Fruit of string ]
<br>[ `Broccoli | `Gherkin ]
<br>[ `Broccoli | `Fruit of string ]
<br>[ `Gherkin | `Fruit of string ]
<br>[ `Broccoli | `Gherkin | `Fruit of string ]
<br>
<br>Open: [&gt; `Broccoli | `Gherkin | `Fruit of string ]<br>
This designates a set of exact types. Each exact type is inhabited by the values introduced by supersets of the tags from 1.
<br>Note: This syntax works like if the set of natural numbers greater or equal than three was written { &gt; 3 } and the set { 0, 1, 2, 3 } was written { &lt; 3 }.<br>Tip: To distinguish closed and open type expressions, you can remember that the less-than sign &lt; is oriented the same way as a capital C letter, as in closed.<br>An exact form is inferred by the type-checker when naming a type defined by a set of tags:<br># type t = [ `Broccoli | `Gherkin | `Fruit of string ]
type t = [ `Broccoli | `Fruit of string | `Gherkin ]
Copy<br>Exact variants correspond closely to simple variants.<br>The closed form is introduced when performing pattern matching over an explicit set of tags<br># let f = function
    | `Broccoli -&gt; "Broccoli"
    | `Gherkin -&gt; "Gherkin"
    | `Fruit Fruit -&gt; Fruit;;
val upcast : [&lt; `Broccoli | `Fruit of string | `Gherkin ] -&gt; string = &lt;fun&gt;
Copy<br>The function f can be used with any exact type that has these three tags or less. When applied to a type with fewer tags, branches associated with removed tags turn safely into dead code. The type is closed because the function can't accept more than what is listed.<br>The open form can be introduced in two different ways.<br>Open polymorphic variants appear when using a catch-all pattern, either the underscore _ symbol or a name:<br># let g = function
    | `Broccoli -&gt; "Broccoli"
    | `Gherkin -&gt; "Gherkin"
    | `Fruit Fruit -&gt; Fruit
    | _ -&gt; "Edible plant";;
val g : [&gt; `Broccoli | `Fruit of string | `Gherkin ] -&gt; string = &lt;fun&gt;
Copy<br>The function g can be used with any exact type that has these three tags or more. Because of the catch-all pattern, if g is passed a value introduced by a tag that is not part of the list, it will be accepted, and "Edible plant" is returned. The type is open because it can accept more than what is listed in its expression.<br>Note: The type of g is also meant to disallow exact types with tags removed or changed in type. OCaml is a statically typed language, which means no type information is available at runtime. As a consequence pattern matching only relies on tag names. If g was assigned to a type with removed tags, such as [&gt; `Broccoli | `Gherkin ], then passing `Fruit to g would be allowed, but since dispatch is based on names, it would execute the `Fruit of string branch and crash because no string is available. Therefore, an open polymorphic variant must include all the tags from pattern matching.<br>Open polymorphic variants also appear when type-checking tags as values.<br># `Gherkin
- : [&gt; `Gherkin ] = `Gherkin

# [ `Broccoli; `Gherkin; `Broccoli ];;
- : [&gt; `Broccoli | `Gherkin ] list = [ `Broccoli; `Gherkin; `Broccoli ]
Copy<br>Setting the type of tag to the open polymorphic variant which only contains it enables:<br>
<br>To use it in all the contexts where applicable code is available.
<br>To avoid using it in contexts that can't deal with it.
<br>The build of sums of polymorphic variants. This is shown in the second example.
<br>This also applies to functions returning polymorphic variant values.<br># let f b = if b then `Up else `Down;;
val f : bool -&gt; [&gt; `Down | `Up ] = &lt;fun&gt;
Copy<br>The codomain of f is the open type [&gt; `Down | `Up ]. This makes sense when having a look at function composition.<br># let g = function
    | `Up -&gt; 1
    | `Down -&gt; 2
    | `Broken -&gt; 3;;

# fun x -&gt; x |&gt; f |&gt; g;;
- : bool -&gt; int = &lt;fun&gt;
Copy<br>Functions g and f can be composed because g accepts more than what f can return. The value `Broken will never pass from f into g, but it is safe to write it as no unexpected value can make its way through.<br>Closed and open cases are polymorphic because they do not designate a single type but families of types. An exact variant type is (structurally) monomorphic, it corresponds to a single variant type, not a set of variant types.<br><br>A closed variant type may also have an additional constraint preventing some tags from being removed.<br># let is_red = function `Clubs -&gt; false | `Diamonds -&gt; true | `Hearts -&gt; true | `Spades -&gt; false;;
val is_red : [&lt; `Clubs | `Diamonds | `Hearts | `Spades ] -&gt; bool = &lt;fun&gt;

# let h = fun u -&gt; List.map is_red (`Hearts :: u);;
val h : [&lt; `Clubs | `Diamonds | `Hearts | `Spades &gt; `Hearts ] list -&gt; bool list = &lt;fun&gt;
Copy<br>Function is_red accepts values from any subtype of [ `Clubs | `Diamonds | `Hearts | `Spades ]. However, function h excludes the exact types [ `Clubs ], [ `Diamonds ] and [ `Spades ]. The list passed to List.map includes a `Hearts tag. Types that do not include it are not allowed.<br>The domain of h is:<br>
<br>closed by [ `Clubs | `Diamonds | `Hearts | `Spades ] and,
<br>open by [ `Hearts ].
<br><br>Simple variants have a form of polymorphism called parametric polymorphism. Together with predefined types, they are type-checked using the nominal typing discipline. In this discipline, a value has a unique type.<br>In the structural type-checking discipline used for polymorphic variants, a value may have several types. Equivalently it can be said to inhabit several types:<br># let gherkin = `Gherkin;;
val Gherkin : [&gt; `Gherkin ] = `Gherkin

# let (gherkin : [ `Gherkin ]) = `Gherkin;;
val Gherkin : [ `Gherkin ] = `Gherkin

# let (gherkin : [ `Gherkin | `Avocado ]) = `Gherkin;;
val Gherkin : [ `Gherkin | `Avocado ] = `Gherkin
Copy<br>
<br>By default, the type assigned to the `Gherkin tag is [&gt; `Gherkin]. It means any variant type that includes that tag.
<br>Using an annotation, the type can be restrained to [ `Gherkin ], the exact variant type only containing `Gherkin
<br>Using another annotation allows assigning the `Gherkin value to a type containing more tags.
<br>This entails a partial ordering relation between exact variant types. The type [ `Gherkin ] is smaller than the type [ `Gherkin | `Avocado ]. The types `[ `Gherkin ] and [ `Avocado ] do not compare. The type `[ `Gherkin ] is the smallest possible type for the tag `Gherkin.<br>The order between the exact variants derives from the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_order" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Partially_ordered_set#Partial_order" target="_blank">partial order</a> on <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Subset" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Subset" target="_blank">subsets</a> of tags. The sets considered are the tags, with names and types. This order is said to be partial because some sets can't be compared. This order is called the subtyping order.<br>The OCaml syntax does not allow expression of the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/10687" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/issues/10687" target="_blank">empty</a> polymorphic variant type. If it was, it would be the least element in the subtyping order.<br>OCaml has a cast operator, it allows to raise the type of an expression into any larger type, with respect to the subtyping order. It is written :&gt;<br># (gherkin :&gt; [ `Avocado | `Gherkin | `Tomato ]);;
- : [ `Avocado | `Gherkin | `Tomato ] = `Gherkin
Copy<br>It means the type of gherkin is raised from [ `Gherkin | `Tomato ] into [ `Avocado | `Gherkin | `Tomato ]. It is admissible because [ `Gherkin | `Tomato ] is smaller than [ `Avocado | `Gherkin | `Tomato ] in the subtyping order.<br>When casting, it is also possible to indicate the subtype from which the value is upcast.<br># (gherkin : [ `Gherkin | `Tomato ] :&gt; [ `Avocado | `Gherkin | `Tomato ]);;
- : [ `Avocado | `Gherkin | `Tomato ] = `Gherkin
Copy<br><br><br>Exact polymorphic variant types can be given names.<br># type exotic = [ `Guayaba | `Maracuya | `Papaya ];;
Copy<br>Named polymorphic variants are always exact, thus they are equivalent to simple variants. It is not possible to give names to closed or open polymorphic variants.<br><br>Named polymorphic variants can be used to create extended types.<br># type mexican = [ exotic | `Pitahaya | `Sapodilla ];;
type mexican = [ `Guayaba | `Maracuya | `Papaya | `Pitahaya | `Sapodilla ]
Copy<br><br>Named polymorphic variants can be used as patterns.<br># let f = function
    | #exotic -&gt; "Exotic Fruit"
    | `Mango -&gt; "Mango";;
val f : [&lt; `Guayaba | `Mango | `Maracuya | `Papaya ] -&gt; string = &lt;fun&gt;
Copy<br>This is not a dynamic type check. The #exotic pattern acts like a macro, it is a shortcut to avoid writing all the corresponding patterns.<br><br><br># function `Avocado -&gt; `Cilantro | plant -&gt; plant;;
- : ([&gt; `Cilantro | `Avocado ] as 'a) -&gt; 'a = &lt;fun&gt;
Copy<br>The meaning of the type of this function is twofold.<br>
<br>Any exact variant type which is a super set of [&gt; `Cilantro | `Avocado ] is a domain
<br>The very same type is also a codomain
<br>The meaning of 'a is not the same as with simple variants. It is not a type parameter meant to be replaced by another type. The as 'a part in the type expression [&gt; `Cilantro | `Avocado ] as 'a means [&gt; `Cilantro | `Avocado ] is bound to the local name 'a in the overall expression. It allows expressing that the same variant type is used as both domain and codomain. This is a consequence of the plant -&gt; plant clause that forces domain and codomain types to be the same.<br><br>It is possible to combine polymorphic variants and parametric polymorphism. Here is a duplication of the 'a option type, translated as a polymorphic variant parametrized with a type parameter.<br># let map f = function
    | `Some x -&gt; `Some (f x)
    | `None -&gt; `None;;
val map : ('a -&gt; 'b) -&gt; [&lt; `None | `Some of 'a ] -&gt; [&gt; `None | `Some of 'b ] =
  &lt;fun&gt;
Copy<br>This map function has two type parameters: 'a and 'b. They are used to type its f parameter.<br><br>The inferred type of a polymorphic variant may be recursive.<br># let rec map f = function
    | `Nil -&gt; `Nil
    | `Cons (x, u) -&gt; `Cons (f x, map f u);;
val map :
  ('a -&gt; 'b) -&gt;
  ([&lt; `Cons of 'a * 'c | `Nil ] as 'c) -&gt; ([&gt; `Cons of 'b * 'd | `Nil ] as 'd) =
  &lt;fun&gt;
Copy<br>The aliasing mechanism is used to express type recursion. In the type expression [&lt; `Cons of 'a * 'c | `Nil ] as 'c, the local name 'c occurs inside the defined thing: [&lt; `Cons of 'a * 'c | `Nil ]. Therefore, it is a recursive type definition.<br><br>A tag `Night inhabits any type with additional tags, for instance [ `Night | `Day ] or [`Morning | `Afternoon | `Evening | `Night]. This is summarized by the subtyping order where [ `Night ] is smaller to both [ `Night | `Day ] and [`Morning | `Afternoon | `Evening | `Night ].<br>This can be seen by defining a function upcast in the following way:<br># let upcast (x : [ `Night ]) = (x :&gt; [ `Night | `Day ]);;
val upcast : [ `Night ] -&gt; [ `Night | `Day ] = &lt;fun&gt;
Copy<br>This function is an identity function it returns its parameter unchanged. It illustrates a value of type [ `Night ] can be cast into the type [ `Night | `Day ]. Casting goes from subtype to supertype.<br><br>The subtyping order extends to simple variants. Functions upcast_opt upcast_list and upcast_snd are doing the same thing as upcast except they are taking parameters from variants parametrized by polymorphic variants.<br># let upcast_opt (x : [ `Night ] option) = (x :&gt; [ `Night | `Day ] option);;
val upcast_opt : [ `Night ] option -&gt; [ `Night | `Day ] option = &lt;fun&gt;

# let upcast_list (x : [ `Night ] list) = (x :&gt; [ `Night | `Day ] list);;
val upcast_list : [ `Night ] list -&gt; [ `Night | `Day ] list = &lt;fun&gt;

let upcast_snd (x : [ `Night ] * int) = (x :&gt; [ `Night | `Day ] * int);;
val upcast_snd : [ `Night ] * int -&gt; [ `Night | `Day ] * int = &lt;fun&gt;
Copy<br>The product type and the types option, list are said to be covariant. Subtyping on parametrized variants goes “in the same direction” as on polymorphic variants parameters:<br>
<br>[ `Night ] is a subtype of [ `Night | `Day ]
<br>[ `Night ] list is a subtype of [ `Night | `Day ] list
<br>Note that these types are covariant because of the way their type parameters appear in the type of their constructors. This is detailed in <a data-tooltip-position="top" aria-label="https://v2.ocaml.org/manual/typedecl.html#ss:typedefs" rel="noopener" class="external-link" href="https://v2.ocaml.org/manual/typedecl.html#ss:typedefs" target="_blank">Chapter 11, Section 8.1</a> of the OCaml Manual. This has nothing to do with being predefined types.<br><br>The function type is covariant on codomains. Casting a function is allowed if the target codomain is larger.<br># let upcast_dom (f : int -&gt; [ `Night ]) = (x :&gt; int -&gt; [ `Night | `Day ]);;
val f : (int -&gt; [ `Night ]) -&gt; int -&gt; [ `Night | `Day ] = &lt;fun&gt;
Copy<br>Covariance means subtyping “goes in the same direction”:<br>
<br>On codomain: [ `Night ] is a subtype of [ `Night | `Day ]
<br>On function type: int -&gt; [ `Night ] is a subtype of int -&gt; [ `Night | `Day ]
<br>Adding tags to a polymorphic variant codomain of a function is harmless. Extending a function's codomain is pretending it can return something that is never returned. It is a false promise, and the precision of the type is reduced, but it is safe, no unexpected data will ever be returned by the function.<br><br>The function type is contravariant on domains. Casting a function is allowed if the target domain is smaller.<br># let upcast_cod (f : [ `Night | `Day ] -&gt; int) = (x :&gt; [ `Night ] -&gt; int);;
val f : ([ `Night | `Day ] -&gt; int) -&gt; [ `Night ] -&gt; int = &lt;fun&gt;
Copy<br>Contravariance means subtyping “is reversed”:<br>
<br>On domain: [ `Night ] is a subtype of [ `Night | `Day ]
<br>On function type: [ `Night | `Day ] -&gt; int is a subtype of [ `Night ] -&gt; int
<br>At first, it may seem counterintuitive. However, removing tags from a polymorphic variant domain is also harmless. The code in charge of the removed tags is turned into dead paths. Implemented generality of the function is lost, but it is safe, no data will be passed that the function can't handle.<br><br># let ingredient = function 0 -&gt; `Flour | _ -&gt; `Masa;;
val ingredient : int -&gt; [&gt; `Masa | `Flour ] = &lt;fun&gt;

# let chef = function
    | `Flour -&gt; `Bread
    | `Egg -&gt; `Tortilla
    | `Masa -&gt; `Tortilla;;
val chef : [&lt; `Masa | `Flour | `Egg ] -&gt; [&gt; `Bread | `Tortilla ] =
  &lt;fun&gt;

# let taste = function `Tortilla | `Bread -&gt; "Nutritious" | `Cake -&gt; "Yummy";;
val taste : [&lt; `Bread | `Tortilla | `Cake ] -&gt; string = &lt;fun&gt;

# fun n -&gt; n |&gt; ingredient |&gt; chef |&gt; taste;;
- : int -&gt; string = &lt;fun&gt;

# let upcasted_chef =
    (chef :&gt; [&lt; `Flour | `Masa ] -&gt; [&gt; `Bread | `Tortilla | `Cake ]);;
val upcasted_chef : [&lt; `Flour | `Masa ] -&gt; [&gt; `Bread | `Tortilla | `Cake ] =
  &lt;fun&gt;

# fun n -&gt; n |&gt; ingredient |&gt; upcasted_chef |&gt; taste;;
- : int -&gt; string = &lt;fun&gt;
Copy<br>The type of chef is a subtype of the type of upcasted_chef. The function upcasted_chef has a reduced domain and enlarged codomain. However, upcasted_chef has a domain that remains larger than the codomain of ingredient and, a codomain that remains smaller than the domain of taste. Therefore, it is safely possible to consider chef as an inhabitant of the type of upcasted_chef in the composition pipe where it is used.<br>It is also possible to refactor chef into a new function that can be used safely at the same place.<br># let refactored_chef = function
    | `Flour -&gt; `Bread
    | `Masa -&gt; (`Tortilla : [&gt; `Bread | `Tortilla | `Cake ]);;

# fun n -&gt; n |&gt; ingredient |&gt; refactored_chef |&gt; taste;;
- : int -&gt; string = &lt;fun&gt;
Copy<br><br><br>Not having to explicitly declare polymorphic variant types is beneficial in several cases.<br>
<br>When few functions use the type
<br>When many types would have to be declared
<br>When reading a pattern-matching expression using polymorphic variant tags, understanding is local. Since polymorphic variant types are anonymous or aliases, there is no need to search for the meaning of the tags somewhere else. The meaning arises from the expression itself.<br><br>When several simple variants use the same constructor name, shadowing takes place. To pattern match over a shadowed variant, type annotation must be added, either to the whole pattern-matching expression or to some patterns.<br># type a = A;;
type a = A
# type b = A;;
type b = A
# function A -&gt; true;;
- : b -&gt; bool = &lt;fun&gt;
# function (A : a) -&gt; true;;
- : a -&gt; bool = &lt;fun&gt;
# (function A -&gt; true : a -&gt; bool);;
- : a -&gt; bool = &lt;fun&gt;
Copy<br>Without this, previously entered ones are no longer reachable. This can be worked around using modules. This is explained in <a data-tooltip-position="top" aria-label="https://dev.realworldocaml.org/records.html#scrollNav-3" rel="noopener" class="external-link" href="https://dev.realworldocaml.org/records.html#scrollNav-3" target="_blank">Reusing Field Names</a> ) of the Real World OCaml book written by Yaron Minsky and Anil Madhavapeddy.<br>This problem never happens with polymorphic variants. When a tag appears several times in an expression, it must be with the same type, that's the only restriction. This makes polymorphic variants very handy when dealing with multiple sum types sharing constructors.<br><br>Using the same type in two different modules can be done in several ways:<br>
<br>Having a dependency, either direct or shared
<br>Turn the dependent module into a functor and inject the dependence as a parameter
<br>The polymorphic variants provide an additional alternative. This was proposed by Jacques Garrigue in his seminal paper “Programming with Polymorphic Variants” (ACM SIGPLAN Workshop on ML, October 1998):<br>
You [...] define the same [polymorphic variant] type in both [modules], and since these are only type abbreviations, the two definitions are compatible. The type system checks the structural equality when you pass a value from one [module] to the other.
<br><br>The <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/error-handling" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/error-handling" target="_blank">Error Handling</a> guide details possible ways to handle errors in OCaml. Among various mechanisms, the result type, when used as a monad, provides a powerful means to handle errors. Refer to the guide and documentation on this type to learn how to use it. In this section, we discuss why using polymorphic variants to carry error values can be beneficial.<br>Let's consider this exception-raising code:<br># let f_exn m n =
    let open List in
    let u = init m Fun.id |&gt; map (fun n -&gt; n * n) in
    nth u n;;
val f_exn : int -&gt; int -&gt; int = &lt;fun&gt;
Copy<br>The following is an attempt at translating f_exn with result values instead of exceptions. It is using the Result.bind instead of |&gt;:<br># type init_error = Negative_length;;

# let init n f = try Ok (List.init n f) with
    | Invalid_argument _ -&gt; Error Negative_length;;
val init : int -&gt; (int -&gt; 'a) -&gt; ('a list, init_error) result = &lt;fun&gt;

# type nth_error = Too_short of int * int | Negative_index of int;;
type nth_error = Too_short of int * int | Negative_index of int

# let nth u i = try Ok (List.nth u i) with
    | Invalid_argument _ -&gt; Error (Negative_index i)
    | Failure _ -&gt; Error (Too_short (List.length u, i))
val nth : 'a list -&gt; int -&gt; ('a, nth_error) result = &lt;fun&gt;

# let f_res m n =
    let* u = init m Fun.id in
    let u = List.map (fun n -&gt; n * n) u in
    let* x = nth u n in
    Ok x;;
Error: This expression has type (int, nth_error) result
       but an expression was expected of type (int, init_error) result
       Type nth_error is not compatible with type init_error
Copy<br>This does not work because of the type of Result.bind.<br># Result.bind;;
- : ('a, 'e) result -&gt; ('a -&gt; ('b, 'e) result) -&gt; ('b, 'e) result = &lt;fun&gt;
Copy<br>Binding can't change the 'e type, while this code needs it to change throughout the pipe.<br>Here is an equivalent version using polymorphic variants:<br># let ( let* ) = Result.bind;;
val ( let* ) : ('a, 'b) result -&gt; ('a -&gt; ('c, 'b) result) -&gt; ('c, 'b) result =
  &lt;fun&gt;

# let init n f = try Ok (List.init n f) with
    | Invalid_argument _ -&gt; Error `Negative_length;;
val init : int -&gt; (int -&gt; 'a) -&gt; ('a list, [&gt; `Negative_length ]) result =

# let nth u i = try Ok (List.nth u i) with
    | Invalid_argument _ -&gt; Error (`Negative_index i)
    | Failure _ -&gt; Error (`Too_short (List.length u, i));;
val nth :
  'a list -&gt;
  int -&gt; ('a, [&gt; `Negative_index of int | `Too_short of int * int ]) result =
  &lt;fun&gt;

# let f_res m n =
    let* u = init m Fun.id in
    let u = List.map (fun n -&gt; n * n) u in
    let* x = nth u n in
    Ok x;;
val f_res :
  int -&gt;
  int -&gt;
  (int,
   [&gt; `Negative_index of int | `Negative_length | `Too_short of int * int ])
  result = &lt;fun&gt;

Copy<br>Using polymorphic variants, the type-checker generates a unique type for the whole pipe. The constraints coming from calling init and nth_error are merged into a single type.<br><br><br>By default, polymorphic variant types aren't declared with a name before being used. The compiler will generate type expressions corresponding to each function dealing with polymorphic variants. Some of those types are hard to read. When several such functions are composed together, inferred types can become very large and difficult to understand.<br># let rec fold_left f y = function `Nil -&gt; y | `Cons (x, u) -&gt; fold_left f (f x y) u;;
val fold_left :
  ('a -&gt; 'b -&gt; 'b) -&gt; 'b -&gt; ([&lt; `Cons of 'a * 'c | `Nil ] as 'c) -&gt; 'b = &lt;fun&gt;
Copy<br><br>In some circumstances, combining sets of constraints will artificially reduce the domain of a function. This happens when the conjunction of constraints must be taken. Functions f and g used here are those defined in the <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#a-first-example" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#a-first-example" target="_blank">first section</a>.<br># f;;
- : [&lt; `Broccoli | `Fruit of string ] -&gt; string = &lt;fun&gt;

# g;;
- : [&lt; `Broccoli | `Edible of string ] -&gt; string = &lt;fun&gt;

# let u = [f; g];;
u : ([&lt; `Broccoli ] -&gt; string) list = [&lt;fun&gt;; &lt;fun&gt;]

# f (`Fruit "Pitahaya");;
- : string = "Pitahaya"

# (List.hd u) (`Fruit "Pitahaya");;
Error: This expression has type [&gt; `Fruit of string ]
       but an expression was expected of type [&lt; `Broccoli ]
       The second variant type does not allow tag(s) `Fruit
Copy<br>Function f accepts tags `Broccoli and `Fruit whilst g accepts `Broccoli and `Edible. But if f and g are stored in a list, they must have the same type. That forces their domain to be restricted to a common subtype. Although f can handle the tag `Fruit, it no longer accepts that parameter when f is extracted from the list.<br><br>This is adapted from the section <a data-tooltip-position="top" aria-label="https://dev.realworldocaml.org/variants.html#scrollNav-4-2" rel="noopener" class="external-link" href="https://dev.realworldocaml.org/variants.html#scrollNav-4-2" target="_blank">Example: Terminal Colors Redux</a> from the “Real World OCaml” book.<br>Tags are used to store color representations:<br>
<br>`RGB contains a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/RGB_color_model" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/RGB_color_model" target="_blank">red, green and blue</a> triplet
<br>`Gray contains a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Grayscale" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Grayscale" target="_blank">grayscale</a> value
<br>`RGBA contains a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/RGBA_color_model" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/RGBA_color_model" target="_blank">red, green, blue, alpha</a> quadruplet
<br>The polymorphic variant type color groups the tags `RGB and `Gray whilst the type extended_color has three tags by extending color with `RGBA. Functions are defined to convert colors into integers.<br># type color = [ `Gray of int | `RGB of int * int * int ];;
type color = [ `Gray of int | `RGB of int * int * int ]

# type extended_color = [ color | `RGBA of int * int * int * int ];;
type extended_color =
    [ `Gray of int | `RGB of int * int * int | `RGBA of int * int * int * int ]

# let color_to_int = function
  | `RGB (r, g, b) -&gt; 36 * r + 6 * g + b + 16
  | `Gray i -&gt; i + 232;;
val color_to_int : [&lt; `Gray of int | `RGB of int * int * int ] -&gt; int = &lt;fun&gt;

# let extended_color_to_int = function
  | `RGBA (r, g, b, a) -&gt; 216 * r + 36 * g + 6 * b + a + 16
  | `Grey i -&gt; i + 2000
  | #color as color -&gt; color_to_int color;;
val extended_color_to_int :
  [&lt; `Gray of int
   | `Grey of int
   | `RGB of int * int * int
   | `RGBA of int * int * int * int ] -&gt;
  int = &lt;fun&gt;

Copy<br>The function color_to_int can convert `RGB or `Gray values. The function extended_color_to_int is intended to convert `RGB, `Gray or `RGBA values; but it is supposed to apply a different conversion formula for gray scales. However, a typo was made, it is spelled `Gray. The type-checker accepts this definition of extended_color_to_int as a function accepting four tags.<br><br>The following function was presented in the <a data-tooltip-position="top" aria-label="https://staging.ocaml.org/docs/polymorphic-variants#Inferred-Type-Aliases" rel="noopener" class="external-link" href="https://staging.ocaml.org/docs/polymorphic-variants#Inferred-Type-Aliases" target="_blank">Inferred Type Aliases</a> section.<br># function `Avocado -&gt; `Cilantro | plant -&gt; plant;;
- : ([&gt; `Cilantro | `Avocado ] as 'a) -&gt; 'a = &lt;fun&gt;
Copy<br>Because of the plant -&gt; plant pattern, the types inferred as domain and codomain are the same. As `Avocado is accepted, it must be part of the domain; and as `Cilantro is returned, it must be part of the codomain. Both end up being part of the common type. However, `Avocado should not be part of the codomain, as this function can't possibly return this value. A finer type-checker would infer more precise types. Type-checking is an approximation and a trade-off, some valid programs are rejected, and some types are too coarse.<br><br>TODO: Expand this section<br>
There is one more downside: the runtime cost. A value Pair (x, y) occupies 3 words in memory, while a value `Pair (x, y) occupies 6 words.
<br>Guillaume Melquiond<br><br>Only using polymorphic variants is over-engineering, it should be avoided. Polymorphic variants aren't an extension of simple variants. From the data perspective, polymorphic variants and simple variants are equivalent. They both are sum types in the algebraic data types sense, both with parametric polymorphism and recursion. Back quote isn't a difference that matters. The only meaningful difference is the algorithm. Therefore, deciding when to use polymorphic variants boils down to another question:<br>
Are nominally or structurally type-checked variants needed?
<br>Answering this isn't significantly easier, but it helps to narrow what to consider. The key difference lies in the way pattern matching is type-checked. Polymorphic variant induces functions that intrinsically accept more data than simple variants. This is a consequence of the subtyping relation between polymorphic variant types.<br>In a precautionary approach (inspired by <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/KISS_principle" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/KISS_principle" target="_blank">KISS</a> or <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it" target="_blank">YAGNI</a> design principles), using simple variants should be the default. However, they can feel too tight. Here are clues indicating this:<br>
<br>Many variant declarations look alike
<br>Equivalent constructor duplicated in several variants
<br>Variant declared for a single-purpose
<br>Variant declaration feeling made up or artificial
<br>Having difficulties finding names for simple variants or constructors may be smell for one of the above. When such discomfort is felt, polymorphic variants may not be the solution, but they can be considered. Keep in mind they may ease some parts but at the expense of understandability, precision and performance. Hopefully, very soon, LLM bots will be able to refactor variants of some sort into the other. This will ease experimentation.<br>To conclude, remember a simple variant that naturally encodes some data should remain a simple variant.<br><br>Although polymorphic variants share a lot with simple variants, they are substantially different. This comes from the structural type-checking algorithm used for polymorphic variants.<br>
<br>Type declaration is optional
<br>A type is a set of constraints over values
<br>A Value satisfies its type rather than inhabits it
<br>Type expression designates sets of types
<br>There is a subtyping relation between types
<br>Polymorphic variants should not be considered as an improvement over simple variants. In some regards, they are more flexible and lightweight, but they also have harder-to-read types and slightly weaker type-checking assurances. Choosing when to prefer polymorphic variants over simple variants is a subtle decision to make. It is safe to prefer simple variants as the default and go for polymorphic variants when there is a solid case for them.<br>It is important to be comfortable with polymorphic variants, many projects are using them. Most often, only a fraction of their expressive strength is used. However, refactoring code using them requires being able to understand more than what's used. Otherwise, one may quickly end up stalled by indecipherable type error messages.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/polymorphic-variants.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Polymorphic Variants.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:35 GMT</pubDate></item><item><title><![CDATA[Rethinking OCaml abstract signatures]]></title><description><![CDATA[ 
 <br>Abstract module types are one of the less understood features of the OCaml module system. They have been one of the obstacles in the on-going effort to specify, and eventually redesign, the module system.<br>In this blog post, I (Clément Blaudeau) present an explanation of what are those abstract module types, and propose a slightly restricted version that might be easier to understand and specify while remaining pretty expressive.<br>For the past 2 years, I’ve been working on building a new specification for the OCaml module system based on Fω. The goal, besides the theoretical interest, is to eventually redo (the module part of) the typechecker with this approach, which would have several benefits:<br>
<br>fix some soundness issues and edge-cases that have appeared and built up over the years, due to unforeseen interactions between features
<br>simplify the (notoriously hard) code of the typechecker by removing ad-hoc techniques and hacks (such as the strengthening or the treatment of aliases for instance)
<br>provide a clean base to add new and awaited features. Notably, transparent ascription and modular implicits are proposals for OCaml modules stalled by the lack of specification of the module system.
<br>Yet, a key aspect in OCaml development culture is to ensure backward compatibility. Therefore, the new Fω approach I’ve been building should not only subsumes the current typechecker in normal use cases, but actually support all of the features of the module system. For long, abstract signatures (also called abstract module types) were believed to be, at least, problematic for Fω. Hopefully, we found out that a slightly restricted version of the feature was encodable in Fω, and, in passing, made the semantics of abstract signatures much simpler. Thus, only one question remains: does this restricted form actually covers all use cases, i.e., is the restriction backward compatible ?<br>Here, we aim at presenting the current state of abstract signatures and our proposed simplification purely from an OCaml user point of view, not from the theoretical one. We welcome any feedback, specifically, use cases or potential use cases that significantly differ from our examples.<br>We start by introducing abstract signatures through examples. Then, we present the current state of abstract signatures in OCaml: we explain the syntactic approach and the issues associated with it. We argue that it has surprising behaviors and, in its current unrestricted form, it is actually too powerful for its own good. Then, we propose a restriction to make the system predicative which, by decreasing its expressiveness, actually makes it more usable. (Our actual proposal is given in 3.3). We finish by other aspect related to usability (syntax, inference).<br><br>The art of modularity is all about controlling abstraction and interfaces. ML languages offer this control via a module system, which contains a signature language to describe interfaces. Signatures contain declarations (fields): values val x : t, types type t = int, modules module X : S, and module types module type T = S. Type and module type declarations can also be abstract type t, module type T, which serves both to hide implementation details via sealing and to have polymorphic interfaces, using functors.<br>Here, we focus on the construct module type T, called abstract module type or abstract signature. We start with examples adapted from <a data-tooltip-position="top" aria-label="https://discuss.ocaml.org/t/what-are-abstract-module-types-useful-for/10121/3" rel="noopener" class="external-link" href="https://discuss.ocaml.org/t/what-are-abstract-module-types-useful-for/10121/3" target="_blank">this forum discussion</a>.<br><br>Let’s consider the following scenario. Two modules providing an implementation of UDP (UDP1 and UDP2) are developed with different design trade-offs. They both implement a signature with basic send and receive operations. Then, functors are added as layers on top: taking a udp library as input, they return another udp library as an output.<br>
<br>Reliable adds sequence numbers to the packets and re-sends missing packets;
<br>CongestionControl tracks the rate of missing packets to adapt the throughput to network congestion situations;
<br>Encryption encrypts the content of all messages.
<br>A project might need different combinations of the basic libraries and functors, while requiring that all combinations use encryption. To enforce this, the solution is to use the module-level sealing of abstract signatures. In practice, the signature of the whole library containing implementations and functors UDPLib (typically, its .mli file) is rewritten to abstract all interfaces except for the output of the Encryption functor.<br>module type UDPLib = sig<br>
module type UNSAFE<br>  module UDP1 : UNSAFE<br>
module UDP2 : UNSAFE<br>  module Reliable : UNSAFE -&gt; UNSAFE<br>
module CongestionControl : UNSAFE -&gt; UNSAFE<br>  module Encryption : UNSAFE -&gt;<br>
sig val send : string -&gt; unit ( ... ) end<br>
end<br>Just as type abstraction, signature abstraction can be used to enforce certain code patterns: users of UDPLib will only be able to use the content of modules after calling the Encryption functor, and yet they have the freedom to choose between different implementations and features:<br>module UDPKeyHandshake = Encryption(Reliable(UDP1))<br>
module UDPVideoStream  = Encryption(CongestionControl(UDP2))<br>
( etc )<br><br>Another use is to introduce polymorphism at the module level. Just as polymorphic functions can be used to factor code, module-level polymorphic functors can be used to factor module expressions. If a code happens to often feature functor applications of the form Hashtbl.Make(F(X)) or Set.Make(F(X)), one can define the MakeApply functor as follows:<br>( Factorizing common expressions )<br>
module type Type = sig module type T end<br>
module MakeApply<br>
(A:Type) (X: A.T)<br>
(B:Type) (F: A.T -&gt; B.T)<br>
(C:Type) (H: sig module Make : B.T -&gt; C.T end) = H.Make(F(X))<br>Downstream the code is rewritten into MakeApply(...)(X)(...)(F)(...)(Set) or MakeApply(...)(X)(...)(F)(...)(Hashtbl) Right now, the verbosity of such example would probably be a deal-breaker. We address this aspect at the end. Ignoring the verbosity, this can be useful for maintenance: by channeling all applications through MakeApply, only one place needs to be updated if the arity or order of arguments is changed. Similarly, if several functors expect a constant argument containing – for instance – global variables, a ApplyGv functor can be defined to always provide the right second argument, which can even latter be hidden away to the user of ApplyGv:<br>( Constant argument )<br>
module Gv : GlobalVars<br>
module ApplyGv (Y : sig module type A module type B end)<br>
(F : Y.A -&gt; GlobalVars -&gt; Y.B)(X : Y.A) = F(X)(Gv)<br>Downstream, code featuring F(X)(GlobalVars) is rewritten into ApplyGv(...)(F)(X) Then, the programmer can hide the GlobalVars module while letting users use ApplyGv, ensuring that global variables are not modified in uncontrolled ways by certain part of the program.<br>Finally, polymorphism can also be used by a developer to prevent unwanted dependencies on implementation details. If the body of a functor uses an argument with a submodule X, but actually does not depend on the content of S, abstracting it is a “good practice”.<br>module F (Arg : sig ... module X : S ... end) =<br>
... ( polymorphism is not enforced )<br>module F' (Y: sig module type S end)<br>
(Arg : sig ... module X : Y.S ... end ) =<br>
... ( polymorphism is enforced )<br><br>Fundamentally, these example are not surprising for developers that are used to rely on abstraction to protect invariants and factor code. Their specificity lies in the fact that there are at the module level, and therefore require projects with a certain size and a strong emphasis on modularity to be justified.<br><br>The challenge for understanding (and implementing) abstract signatures lies more in the meaning of the module-level polymorphism that they offer than the module level sealing, the latter being pretty straightforward. More specifically, the crux lies in the meaning of the instantiation of an abstract signature variable A by some other signature S, that happens when a polymorphic functor is applied. OCaml follows an unrestricted syntactical approach: A can be instantiated by any (well-formed) signature S. During instantiation, all occurrences of A are just replaced by S ; finally, the resulting signature is re-interpreted—as if it were written as is by the user.<br>However, this syntactical rewriting interferes with the variant interpretation of signatures, which can lead to surprising behaviors. We discuss this aspect first. The unrestricted aspect leads to the (infamous) Type : Type issue which has some theoretical consequences. We finish this section by mentioning other—more technical—issues.<br><br>The first key issue of this approach comes from the fact that signatures in OCaml have a variant interpretation: abstract fields (1) have a different meaning (sealing or polymorphism) depending on whether they occur in positive or negative positions, and (2) abstract fields open new scopes, i.e.&nbsp;duplicating an abstract type field introduces two different abstract types. Overall, OCaml signatures can be thought of as having implicit quantifiers: using a signature in positive or negative position changes its implicit quantifiers (from existential to universal) while duplicating a signature duplicates the quantifiers (and therefore introduces new incompatible abstract types).<br>Therefore, when instantiating an abstract signature with a signature that has abstract fields, the user must be aware of this, and mentally infer the meaning of the resulting signature. To illustrate how it can be confusing, let’s revisit the first motivating example and let’s assume that the developer actually want to expose part of the interface of the raw UDP libraries. One might be tempted to instantiate UNSAFE with something along the following lines:<br>module type UDPLib_expose = sig<br>
include UDPLib with module type UNSAFE =<br>
sig<br>
module type CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE ( this part remains abstract )<br>
module Safe : sig ... end ( this part is exposed )<br>
end<br>
end<br>This returns :<br>module type UDPLib_expose =  sig<br>
module type UNSAFE =<br>
sig<br>
module type CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE<br>
module Safe : sig ... end<br>
end<br>
module UDP1 : UNSAFE<br>
module UDP2 : UNSAFE<br>
module Reliable : UNSAFE -&gt; UNSAFE<br>
module CongestionControl : UNSAFE -&gt; UNSAFE<br>
module Encryption : UNSAFE -&gt; sig val send : string -&gt; unit ( ... ) end<br>
end<br>However, the syntactical rewriting and reinterpretation of this signature in the negative positions produces a counter-intuitive result. For instance, if we expand the signature of the argument for the functor Reliable (for instance) we see:<br>module Reliable :<br>
sig<br>
module type CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE<br>
module Safe : sig ... end<br>
end -&gt; UNSAFE<br>This means that the functor actually has to be polymorphic in the underlying implementation of CORE_UNSAFE, rather than using the internal details, which has the opposite meaning as before. If the user wants to hide a shared unsafe core, accessible to the functor when they were defined and then abstracted away, the following pattern may be used instead:<br>module type UDPLib_expose' = sig<br>
module type CORE_UNSAFE<br>
include UDPLib with module type UNSAFE = sig<br>
module type CORE_UNSAFE = CORE_UNSAFE<br>
module Unsafe : CORE_UNSAFE<br>
module Safe : sig ... end<br>
end<br>
end<br>Doing so, the instantiated signature does not contain abstract fields and therefore its variant reinterpretation will not introduce unwanted polymorphism. This observation is at the core of the proposal of this post.<br><br>Abstract module types are impredicative: a signature containing an abstract signature can be instantiated by itself. One can trick the subtyping algorithm into an infinite loop of instantiating an abstract signature by itself, as shown by <a data-tooltip-position="top" aria-label="https://sympa.inria.fr/sympa/arc/caml-list/1999-07/msg00027.html" rel="noopener" class="external-link" href="https://sympa.inria.fr/sympa/arc/caml-list/1999-07/msg00027.html" target="_blank">Andreas Rosseberg</a>, adapting an example from <a data-tooltip-position="top" aria-label="https://doi.org/10.1145/174675.176927" rel="noopener" class="external-link" href="https://doi.org/10.1145/174675.176927" target="_blank">Harper and Lillibridge (POPL ’94)</a>. This also allows type-checking of (non-terminating) programs with an absurd type, as shown by the encoding of the Girard’s paradox done by <a data-tooltip-position="top" aria-label="https://github.com/lpw25/girards-paradox/tree/master" rel="noopener" class="external-link" href="https://github.com/lpw25/girards-paradox/tree/master" target="_blank">Leo White</a>.<br><br>The current implementation of the typechecker does not handle abstract signatures correctly in some scenarios. It’s unclear if they are just bugs or pose theoretical challenges.<br><br>Inside a functor, module aliases are disallowed between the parameter and the body (for soundness reasons, due to coercive subtyping). However, this check can be bypassed by using an abstract signature that is then instantiated with an alias. If we try to use it to produce a functor that exports its argument as an alias, the typechecker crashes. This is discussed in <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/11441" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/issues/11441" target="_blank">#11441</a><br>( crashes the typechecker in current OCaml )<br>
module F (Type : sig module type T end)(Y : Type.T) = Y<br>module Crash (Y : sig end) =<br>
F(struct module type T = sig module X = Y end end)<br><br>The use of abstract signatures clashes with applicativity of functors, as discussed <a data-tooltip-position="top" aria-label="https://github.com/ocaml/ocaml/issues/12204" rel="noopener" class="external-link" href="https://github.com/ocaml/ocaml/issues/12204" target="_blank">in #12204</a>.<br><br>Another known issue is that the typechecker can abstract a signature when it contains unreachable type fields (types pointing to anonymous modules). This can lead to the production of invalid signatures : signatures that are refused by the typechecker when re-entered back in.<br>module F (Y: sig type t end) =<br>
struct<br>
module type A = sig<br>
type t = Y.t ( this will force the abstraction of all of A )<br>
type u<br>
end<br>
module X : A = struct type t = Y.t type u = int end<br>
type u = X.u<br>
end<br>module Test = F(struct type t end)<br>( returns )<br>
module Test : sig module type A module X : A type u = X.u end<br>Here, the type field type u = X.u is invalid as X has an abstract signature (and therefore, no fields).<br><br>In this section we explore solutions for fixing the issues of the current approach. The core criticism we make of the OCaml approach is that it is actually too expressive for its own good. Abstract signatures are impredicative: they can be instantiated by themselves. Having impredicative instantiation with variant reinterpretation is hard to track for the user and interacts in very subtle ways with other features of the module system, slowing down its development—and breaking its theoretical properties. To address this, we take the opposite stance and propose to make the system actually predicative: we restrict the set of signatures that can be used to instantiate an abstract signature. This also indirectly addresses the complexity of the variant reinterpretation.<br>We start with the simplest solution where instantiation of abstract signatures is restricted to signatures containing no abstract fields. Then, we propose to relax this restriction and allow for signatures that contain abstract type fields (but no abstract module types), which we call simple signatures. This will requires us to briefly discuss the need for module-level sharing.<br>In this section we focus on the theoretical aspects, but present them informally with examples. The practical aspects, notably syntax and inference, are discussed in the next section.<br><br>One might wonder why abstract types and abstract signatures syntactically resembles one another and yet, the latter is much more complex than the former. The key lies in the fact that abstract types can only be instantiated by concrete type expressions, without free variables. Informally, this:<br>sig<br>
type t<br>
val x : t<br>
val f : t -&gt; t<br>
end with type t = (int * 'a)<br>is not allowed, notably because (1) the scope of the abstract type variable 'a is unclear, (2) values of type t, like x, would be ill-typed.<br>Therefore, a first solution is to require abstract signatures to be instantiated only by concrete signatures, i.e.&nbsp;signatures with no abstract fields (neither types nor module types). This circumvents the clash between the rewriting and variant reinterpretation of abstract fields (by disallowing them).<br>This is simple and sound but prevents some valid uses of abstract types: in the first example, UNSAFE could not be instantiated with abstract type fields, forcing UDP1 and UDP2 to have the same type definitions.<br><br>If we want to relax the no-abstraction proposal, some abstract fields will be allowed when instantiating signatures. Then, the question of what sharing (i.e., type equalities) should be kept between different occurrences of the abstract fields arises.<br>In OCaml signatures, sharing between two modules is usually expressed at the core-level by rewriting the fields of the signature of the second module to refer to their counterpart in the first one. This cannot be done with abstract signatures, as they have no fields. Instead, the language needs module-level sharing, which in OCaml is very restricted. Indeed, it provides a form of module aliases (only for submodules, not at the top-level of a signature), but aliasing between a functor body and its parameter is not allowed—while it is typically the use-case for abstract signatures in polymorphic functors. Consider the following code:<br>( Code )<br>
module F1 (Y: sig module type A module X : A end) = Y.X<br>
module F2 (Y: sig module type A module X : A end) = (Y.X : Y.A)<br>Currently, the typechecker cannot distinguish between the two and returns the same signature, while we would expect the first one to keep the sharing between the parameter and the body.<br>( Currently, both are given the same type: )<br>
module F1 (Y: sig module type A module X : A end) : A<br>
module F2 (Y: sig module type A module X : A end) : A<br>As an example, we can consider the argument for the functors:<br>module Y = struct<br>
module type A = sig type t end<br>
module X = struct type t = int end<br>
end<br>module Test1 = F1(Y)<br>
module Test2 = F2(Y)<br>This returns :<br>module Test1 : sig type t end<br>
module Test2 : sig type t end<br>While we would expect :<br>module Test1 : sig type t = int end<br>
module Test2 : sig type t end<br>Two possible extensions would help tackle this issue.<br><br>A recently proposed experimentation, named lazy strengthening, extends the signature language with an operator S with P, where S is a signature and P a module path. It is interpreted as S strengthened by P, i.e.&nbsp;S in which all abstract fields are rewritten to point to their counterpart in P. Initially considered for performance reasons, it would allow for tracking of type equalities when using abstract signatures.<br>( Lazy strengthening would keep type equalities: )<br>
module F1 (Y: sig module type A module X : A end) = Y.A with Y.X<br><br>A more involved solution is the use of an extension of aliasing called transparent ascription, where both the alias and the signature are stored in the signature. The signature language would be extended with an operator (= P &lt; S). The technical implications of this choice are beyond the scope of this discussion.<br>( Transparent ascription would keep module equalities: )<br>
module F1 (Y: sig module type A module X : A end) : (= Y.X &lt; Y.A)<br><br>Maintaining a predicative approach, we propose to restrict instantiation only by simple signatures, i.e., signatures that may contain abstract type fields, but no abstract module types. This reintroduces the need to express module-level sharing and the mental gymnastic of variant re-interpretation of abstract type fields. However, it guarantees that all modules sharing the same abstract signature will also share the same structure (same fields) after instantiation, and can only differ in their type fields. We believe this makes for a good compromise.<br><br>One might wonder how restrictive is this proposal. Specifically, if we consider a simple polymorphic functor as:<br>module Apply (Y : sig module type A end) (F : Y.A -&gt; Y.A)(X : Y.A) = F(X)<br>The following partial application would be rejected:<br>( Rejected as A would be instantiated by sig module type B module X : B -&gt; B end )<br>
module Apply' = Apply(struct module type A = sig module type B module X : B -&gt; B end end)<br>However, this could be circumvented by eta-expanding, thus expliciting module type parameters, and instantiating only a simple signature:<br>( Accepted as A is instantiated by a signature with no abstract fields )<br>
module Apply'' = functor (Y:sig module type B end) -&gt;<br>
Apply(struct module type A = sig module type B = Y.B module X : B -&gt; B end end)<br><br>Concrete and simple signatures can be seen as the first two levels of the predicative approach for types declarations. There are no more levels for type declarations, as types cannot be partially abstract (see 3.1). Could it be useful to add even more expressivity and authorize instantiation by a signature containing again an abstract module type field (which would need to be restricted with a level system like universes)? We have found no example where this was useful. Besides, it would add a great layer of complexity.<br><br><br>A key aspect of abstract module types that reduces their usability is the verbosity of the syntax. Rather than having to pass signature as part of a module argument to a polymorphic functor, using a separate notation for module type parameters could be more concise. In practice, abstract signature arguments could be indicated by using brackets instead of parenthesis, and interleaved with normal module arguments, as in this example:<br>( At definition )<br>
module MakeApply<br>
[A] (X:A)<br>
[B] (F: A -&gt; B)<br>
[C] (H : sig module Make : B -&gt; C end)<br>
= H.Make(F(X))<br>module ApplyGv<br>
[A] [B] (F:A -&gt; GlobalVars -&gt; B) (X:A)<br>
= F(X)(Gv)<br>( At the call site )<br>
module M1 = MakeApply<br>
[T] (X)<br>
[Hashtbl.HashedType] (F)<br>
[Hashtbl.S] (Hashtbl)<br>module M2 = ApplyGv [A] [B] (F) (X)<br>Technically, this is not just syntactic sugar for anonymous parameters due to the fact that OCaml relies on names for applicativity of functors.<br><br>Following up on the previous point, usability of abstract signatures could even be improved with some form of inference at call sites. Further work is needed to understand to what extend this could be done.<br><br>We have presented the feature of abstract signatures in OCaml. After showing use cases via examples, we explained the issues associated with the unrestricted syntactical approach. Then, we propose a new specification: simple abstract signatures. In addition to making the behavior of abstract signatures much more predictable for the user, this approach can be fully formalized by translation into Fω (extended with predicative kinds).<br>As stated above, our goal here was both to sum up the current state and our proposal, but also to gather feedback from users or potential users. In particular, we want to see if it can indeed cover all use cases, and if we missed other usability problems.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/rethinking-ocaml-abstract-signatures.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Rethinking OCaml abstract signatures.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:32 GMT</pubDate></item><item><title><![CDATA[Tail recursion modulo cons]]></title><description><![CDATA[ 
 <br>If the last action of a function ff is to call another function gg, the language run-time doesn't need to keep ff's stack frame around when calling gg:<br>let f n =
  Printf.printf "Hello, World!";
  g (n + 1)
Copy<br>Instead, the run-time may `re-purpose' ff's stack frame for gg, saving space and time in stack (de)allocations. This optimisation, known as tail call elimination, is useful in many language paradigms. It is useful in functional programming languages for which recursion is the idiomatic way to repeat actions.<br>Unfortunately, many standard uses of recursion are not tail-call optimisable:<br>let rec map f = function
  | [] -&gt; []
  | x :: xs -&gt;
     let y = f x in
     y :: map f xs
Copy<br>The x::xsx::xs case proceeds as follows:<br>
<br>Compute y:=f(x)y:=f(x);
<br>Recursively compute t1:=map&nbsp;f&nbsp;xst1​:=map&nbsp;f&nbsp;xs;
<br>Allocate t2:=y::t1t2​:=y::t1​ on the heap;
<br>Return t2t2​.
<br>Step 3 prevents the tail-call optimisation: the runtime must build the list node after computing the tail with map f xs. Our map function is almost tail-recursive: if not for the data constructor (::), it would be. We call such functions 'tail recursive modulo cons'. There are two ways to make map fully tail-recursive:<br>
<br>We could build the result list in reverse order, then reverse it in one pass at the end. This is not ideal since our intermediate list requires time to build and creates work for the garbage collector.<br>

<br>We could change the list type to allow us to build the list node first, and later fill in the correct tail. In OCaml, this needs a ref indirection.<br>

<br>Let's try the latter approach. We introduce a ref and pass the 'tail to be filled in later' as an explicit argument:<br>type 'a mutable_list = (::) of 'a * 'a mutable_list ref | []

let rec map f xs =
    let rec inner res = function
     | [] -&gt; ()
     | x :: xs -&gt;
        let y = f x in
        (* create an 'incomplete' list node *)
        let tail = ref [] in
        res := y :: tail;
        inner tail !xs (* tail call! *)
    in
    let res = ref [] in
    inner res xs; !res
Copy<br>Putting 'incomplete' values into the heap as a user requires changing the list type to contain references, but the OCaml runtime doesn't have the same restriction! It can choose to modify 'immutable' heap contents if it wants, allowing our original map to be compiled to take O(1)O(1)-space and not generate any garbage!<br>The transformation given above can be applied whenever a function is 'tail recursive modulo cons': whenever the only actions after the last function call are heap allocations. The OCaml compiler doesn't yet make this optimisation, but it could! There are interesting details to be fixed, such as what happens when a garbage collection happens in the middle of the TRMC recursion.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fn-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fn-1" target="_blank">1</a><br>
Many thanks to <a data-tooltip-position="top" aria-label="https://discord.com/users/327286755562618891/" rel="noopener" class="external-link" href="https://discord.com/users/327286755562618891/" target="_blank">@Splingush</a> for corrections to this post.
<br><br>
<br><a rel="noopener" class="external-link" href="http://gallium.inria.fr/seminaires/transparents/20141027.Frederic.Bour.pdf" target="_blank">http://gallium.inria.fr/seminaires/transparents/20141027.Frederic.Bour.pdf</a><a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fnref-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/tail-recursion-modulo-cons#fnref-1" target="_blank">↩</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/tail-recursion-modulo-cons.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Tail recursion modulo cons.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:33 GMT</pubDate></item><item><title><![CDATA[Testing manpages]]></title><description><![CDATA[ 
 <br>Dune supports a <a data-tooltip-position="top" aria-label="https://dune.readthedocs.io/en/stable/concepts.html#diffing-and-promotion" rel="noopener" class="external-link" href="https://dune.readthedocs.io/en/stable/concepts.html#diffing-and-promotion" target="_blank"><code></code> action</a>diff that compares two files aa and bb and fails if aa ≠ bb. The magic of this action is that it allows the user to set a:=ba:=b if aa is a source file and bb is a generated file. This is used under the hood for code formatting with dune build @fmt:<br>
<br>for each file aa, generate a formatted file bb;
<br>assert that a=ba=b;
<br>if not, the user may run dune promote to set a:=ba:=b.
<br>The diff action can also be used to write 'self-correcting' tests: write a series of tests with some expected output; run each test and diff the output with an expected output; if any of the errors are expected, run dune promote to auto-correct the test.<br>One particularly useful type of 'self-correcting' test is an assertion of the output of the --help option to a binary. Snapshot the --help output in a help.txt file and diff it against the true --help output each time you run your tests. This has two advantages:<br>
<br>you are certain of how any PR will change your program's CLI;<br>

<br>the help.txt file serves as documentation that is guaranteed to be up-to-date.<br>

<br>For example, here's how it's being done in <a data-tooltip-position="top" aria-label="https://github.com/CraigFe/oskel" rel="noopener" class="external-link" href="https://github.com/CraigFe/oskel" target="_blank">CraigFe/oskel</a>:<br>(rule
 (with-stdout-to
  oskel-help.txt.gen
  (run oskel --help=plain)))

(rule
 (alias runtest)
 (action
  (diff oskel-help.txt oskel-help.txt.gen)))
Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/testing-manpages.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Testing manpages.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:36 GMT</pubDate></item><item><title><![CDATA[The _intf trick]]></title><description><![CDATA[ 
 <br>
In this post, I explain a trick for avoiding duplication of types between .ml and .mli files that will be familiar to anyone who's worked with Jane Street codebases.
<br><br>OCaml compilation units live a double life: one as source code (foo.ml) and one as header information (foo.mli).<br><img src="https://www.craigfe.io/posts/the-intf-trick/two_files.svg" referrerpolicy="no-referrer"><br>This works well in encouraging abstraction, so you'll often see less type information in the .mli than in the .ml, but any types that are not abstracted are duplicated. This is a big deal for functor-heavy projects, since large module types will end up being duplicated across the two files.<br>There's a couple of standard mitigations for this:<br>
<br>move all of your types into a single file with no corresponding .mli.
  Each foo.{ml,mli} file can now alias the types and module types from a central s.ml or types.ml file. Unfortunately, all those types are now defined separately from their point-of-use, making your codebase harder to understand and less scalable.<br>

<br>minimise the number of module types being defined.
  Since we're paying twice for each module type we define, it's natural to want to define as few of them as possible. For instance, we might avoid defining a MAKER type for our Make functor and just keep the constraints in the .mli file instead. Unfortunately, this hides the constraints from <a data-tooltip-position="top" aria-label="https://github.com/ocaml/merlin" rel="noopener" class="external-link" href="https://github.com/ocaml/merlin" target="_blank">Merlin</a>, so you won't discover any discrepancies until compile time:<br>

<br>(* --- foo.mli -------------------------------------------------------------- *)

module Make (A: Arg.S) : S with type arg = A.t

(* --- foo.ml --------------------------------------------------------------- *)

module Make (A : Arg.S) = struct

  (* We must define [type arg = A.t], but Merlin doesn't know this *)
  type arg = string

end
Copy<br>Both of these mitigations have their drawbacks. If only our foo.ml could refer to the module types defined in foo.mli. Hmm...<br><br>As with <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" target="_blank">most problems</a>, we can solve this with another layer of indirection. We add a third file, named foo_intf.ml. This file holds types and signatures, so is like our old foo.mli file, but has the distinct advantage that foo.ml can pull types from it:<br><img src="https://www.craigfe.io/posts/the-intf-trick/three_files.svg" referrerpolicy="no-referrer"><br>Now our types are defined in exactly one place, with no unnecessary duplication. The foo_intf.ml file contains all of the types required by foo.ml and also defines a special module type Intf to act as the public interface.<br>(* --- foo_intf.ml ---------------------------------------------------------- *)

(* Type definitions go here: *)

module type S = sig ... end
module type MAKER = functor (A: Arg.S) -&gt; S with type arg = A.t
type error = [ `Msg of string | `Code of int ]

(* The interface of [foo.ml]: *)

module type Intf = sig
  type error
  (** [error] is the type of errors returned by {!S}. *)

  module type S = S
  module type MAKER = MAKER

  module Make : MAKER
end

(* --- foo.ml --------------------------------------------------------------- *)

(* Fetch module types and type definitions from the [_intf] file *)
include Foo_intf

(* Implementation here as normal *)
module Make : MAKER = functor (A : Arg.S) -&gt; struct ... end

(* --- foo.mli -------------------------------------------------------------- *)

include Foo_intf.Intf (** @inline *)
Copy<br>There are some nice advantages to this approach:<br>
<br>
We've avoided duplicate definitions of foo's module types and kept them in the foo* namespace in our source tree. The code is now easier to change<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/the-intf-trick#fn-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/the-intf-trick#fn-1" target="_blank">1</a> and easier to understand.

<br>
Since we no longer have to minimise our use of module types, we can give the types of functors at the point of definition (module Make : MAKER = ...). This style works better with Merlin.

<br>The _intf style is commonly used in Jane Street packages (c.f. <a data-tooltip-position="top" aria-label="https://github.com/janestreet/higher_kinded/tree/master/src" rel="noopener" class="external-link" href="https://github.com/janestreet/higher_kinded/tree/master/src" target="_blank"><code></code></a>higher_kinded, <a data-tooltip-position="top" aria-label="https://github.com/janestreet/base/tree/master/src" rel="noopener" class="external-link" href="https://github.com/janestreet/base/tree/master/src" target="_blank"><code></code></a>base, <a data-tooltip-position="top" aria-label="https://github.com/janestreet/core/tree/master/src" rel="noopener" class="external-link" href="https://github.com/janestreet/core/tree/master/src" target="_blank"><code></code></a>core). Note that it's typically only used for files that export module types, for which this trick is most effective.<br>I hope you find this technique useful in making your OCaml code more concise and less frustrating to work with.<br><br><br>Use of the _intf trick is an implementation detail that (ideally) shouldn't be exposed in your documentation. At time of writing, Odoc renders all includes and type aliases with links to the source definition. In the case of included module types, you can use the @inline annotation to prevent Odoc from displaying the indirection:<br>include Foo_intf.Intf (** @inline *)
Copy<br>Unfortunately: (a) there's no equivalent trick for plain type definitions, and (b) any cross-references between module types will link to the true definition. This leaves you with rendered output like the following:<br>module Make : functor (Input : Foo__.Foo_intf.INPUT) -&gt; S
Copy<br>where INPUT is defined in the Foo_intf file but accessible to the user as Foo.INPUT (via an alias).<br>Fortunately, the <a data-tooltip-position="top" aria-label="https://github.com/ocaml/odoc/pull/439" rel="noopener" class="external-link" href="https://github.com/ocaml/odoc/pull/439" target="_blank">new Odoc model</a> solves this problem by generating links to "canonical" definitions of types, which are never taken from hidden modules (those with double underscores like Foo__). At time of writing, this new model hasn't yet been released.<br><br>An interesting side-effect of being able to reference interfaces from implementations is that you can use them to kick-start initial development on a file. If your development process begins by defining signatures, the .ml + .mli workflow requires a secondary step of "add stub implementations of everything" to sneak past the type-checker:<br>(* --- stack.mli ------------------------------------------------------------ *)

type 'a t
val empty : 'a t
val push : 'a t -&gt; 'a -&gt; 'a t
val pop : 'a t -&gt; ('a t * 'a) option

(* --- stack.ml ------------------------------------------------------------- *)

type 'a t
let empty = failwith "TODO"
let push = failwith "TODO"
let pop = failwith "TODO"
Copy<br>With an _intf file, we can provide all of these stubs in one go:<br>(* --- stack.ml ------------------------------------------------------------- *)

include (val (failwith "TODO") : Stack_intf.Intf)
Copy<br>(I learned about this trick from a <a data-tooltip-position="top" aria-label="https://blog.janestreet.com/simple-top-down-development-in-ocaml/" rel="noopener" class="external-link" href="https://blog.janestreet.com/simple-top-down-development-in-ocaml/" target="_blank">blog post</a> by Carl Eastlund.)<br><br>Emacs users with tuareg-mode can use tuareg-find-alternate-file to quickly jump between corresponding .ml and .mli files. If you use this feature (as I do), you'll want it to be aware of _intf files. This can be done by customising the tuareg-find-alternate-file variable to include the correspondence &lt;foo&gt;.ml ↔ &lt;foo&gt;_intf.ml:<br>;; Add support for `foo_intf.ml' ↔ `foo.ml' in tuareg-find-alternate-file
(custom-set-variables
 '(tuareg-other-file-alist
   (quote
    (("\\.mli\\'" (".ml" ".mll" ".mly"))
     ("_intf.ml\\'" (".ml"))
     ("\\.ml\\'" (".mli" "_intf.ml"))
     ("\\.mll\\'" (".mli"))
     ("\\.mly\\'" (".mli"))
     ("\\.eliomi\\'" (".eliom"))
     ("\\.eliom\\'" (".eliomi"))))))
Copy<br>If you're currently looking at some file foo.ml, tuareg-find-alternate-file will try to open foo_intf.ml and then foo.mli in that order. (If one of the two already has an open buffer, that will take priority.)<br><br>
<br>2020-06-10: changed the recommended name of the interface module type from Foo_intf.Foo to Foo_intf.Intf. In the time since I originally wrote this post, I've come to dislike the duplication of the module name using the Jane Street convention: in practice, Foo is often quite long and subjected to later renaming.
<br><br>
<br>via reducing the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Cognitive_dimensions_of_notations" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Cognitive_dimensions_of_notations" target="_blank">repetition viscosity</a> of our notation for types.<a data-tooltip-position="top" aria-label="https://www.craigfe.io/posts/the-intf-trick#fnref-1" rel="noopener" class="external-link" href="https://www.craigfe.io/posts/the-intf-trick#fnref-1" target="_blank">↩</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/the-_intf-trick.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/The _intf trick.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:36 GMT</pubDate><enclosure url="https://www.craigfe.io/posts/the-intf-trick/two_files.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://www.craigfe.io/posts/the-intf-trick/two_files.svg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Transitioning to Multicore with ThreadSanitizer]]></title><description><![CDATA[ 
 <br>The 5.0 release brought Multicore, Domain-based parallelism to the OCaml language. Parallel Domains performing uncoordinated operations on shared mutable memory locations may however cause data races. Such issues will unfortunately not <a data-tooltip-position="top" aria-label="https://blog.janestreet.com/oxidizing-ocaml-parallelism/" rel="noopener" class="external-link" href="https://blog.janestreet.com/oxidizing-ocaml-parallelism/" target="_blank">(yet)</a> be caught by OCaml's strong type system, meaning they may go unnoticed when introducing parallelism into an existing OCaml code base. In this guide, we will therefore study a step-wise workflow that utilises the <a data-tooltip-position="top" aria-label="https://github.com/ocaml-multicore/ocaml-tsan" rel="noopener" class="external-link" href="https://github.com/ocaml-multicore/ocaml-tsan" target="_blank">ThreadSanitizer (TSan)</a> tool to help make your OCaml code 5.x ready.<br>Note: TSan is currently only supported under Linux with AMD/Intel cpus. It furthermore requires at least GCC 11 or Clang 11 and the libunwind library.<br><br>Consider a little bank library with the following signature in bank.mli:<br>type t
(** a collective type representing a bank *)

val init : num_accounts:int -&gt; init_balance:int -&gt; t
(** [init ~num_accounts ~init_balance] creates a bank with [num_accounts] each
    containing [init_balance]. *)

val transfer : t -&gt; src_acc:int -&gt; dst_acc:int -&gt; amount:int -&gt; unit
(** [transfer t ~src_acc ~dst_acc ~amount] moves [amount] from account
    [src_acc] to account [dst_acc].
    @raise Invalid_argument if amount is not positive,
    if [src_acc] and [dst_acc] are the same, or if [src_acc] contains
    insufficient funds. *)

val iter_accounts : t -&gt; (account:int -&gt; balance:int -&gt; unit) -&gt; unit
(** [iter_accounts t f] applies [f] to each account from [t]
    one after another. *)
Copy<br>Underneath the hood, the library may have been implemented in various ways. Consider the following thread-unsafe implementation in bank.ml:<br>type t = int array

let init ~num_accounts ~init_balance =
  Array.make num_accounts init_balance

let transfer t ~src_acc ~dst_acc ~amount =
  begin
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    if t.(src_acc) &lt; amount then raise (Invalid_argument "Not enough money on account");
    t.(src_acc) &lt;- t.(src_acc) - amount;
    t.(dst_acc) &lt;- t.(dst_acc) + amount;
  end

let iter_accounts t f = (* inspect the bank accounts *)
  Array.iteri (fun account balance -&gt; f ~account ~balance) t;
Copy<br><br>Now if we want to see if this code is Multicore ready for OCaml 5.x, we can utilise the following workflow:<br>
<br>Install TSan
<br>Write a parallel test runner
<br>Run tests under TSan
<br>If TSan complains about data races, address the reported issue and go to step 2.
<br><br>We will now go through the proposed workflow for our example application.<br><br>For now, convenient 5.1.0+tsan and 5.0.0+tsan opam switches are available until TSan is officially included with the forthcoming 5.2.0 OCaml release. You can install such a TSan switch as follows:<br>opam switch create 5.1.0+tsan
Copy<br><br>For a start, we can test our library under parallel usage by running two Domains in parallel. Here's a quick little test runner in bank_test.ml utilising this idea:<br>let num_accounts = 7

let money_shuffle t = (* simulate an economy *)
  for i = 1 to 10 do
    Unix.sleepf 0.1 ; (* wait for a network request *)
    let src_acc = i mod num_accounts in
    let dst_acc = (i*3+1) mod num_accounts in
    try Bank.transfer t ~src_acc ~dst_acc ~amount:1 (* transfer $1 *)
    with Invalid_argument _ -&gt; ()
  done

let print_balances t = (* inspect the bank accounts *)
  for _ = 1 to 12 do
    let sum = ref 0 in
    Bank.iter_accounts t
      (fun ~account ~balance -&gt; Format.printf "%i %3i " account balance; sum := !sum + balance);
    Format.printf "  total = %i @." !sum;
    Unix.sleepf 0.1;
  done

let _ =
  let t = Bank.init ~num_accounts ~init_balance:100 in
  (* run the simulation and the debug view in parallel *)
  [| Domain.spawn (fun () -&gt; money_shuffle t);
     Domain.spawn (fun () -&gt; print_balances t);
  |]
  |&gt; Array.iter Domain.join
Copy<br>The runner creates a bank with 7 accounts containing $100 each and then runs two loops in parallel with:<br>
<br>One transfering money with money_shuffle
<br>Another one repeatedly printing the account balances with print_balances:
<br>$ opam switch 5.1.0
$ dune runtest
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 100 5 100 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5  99 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
Copy<br>From the above run under a regular 5.1.0 compiler, one may get the impression that everything is OK, as the balances sum to a total of $700 as expected, indicating that no money is lost.<br><br>Let us now perform the same test run under TSan. Doing so is as simple as follows and immediately complains about races:<br>$ opam switch 5.1.0+tsan
$ dune runtest
File "test/dune", line 2, characters 7-16:
2 |  (name bank_test)
           ^^^^^^^^^
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 100 5 100 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5  99 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
==================
WARNING: ThreadSanitizer: data race (pid=26148)
  Write of size 8 at 0x7f5b0c0fd6d8 by thread T4 (mutexes: write M85):
    #0 camlBank.transfer_322 lib/bank.ml:11 (bank_test.exe+0x6de4d)
    #1 camlDune__exe__Bank_test.money_shuffle_270 test/bank_test.ml:8 (bank_test.exe+0x6d7c5)
    #2 camlStdlib__Domain.body_703 /home/opam/.opam/5.1.0+tsan/.opam-switch/build/ocaml-variants.5.1.0+tsan/stdlib/domain.ml:202 (bank_test.exe+0xb06b0)
    #3 caml_start_program &lt;null&gt; (bank_test.exe+0x13fdfb)
    #4 caml_callback_exn runtime/callback.c:197 (bank_test.exe+0x106053)
    #5 caml_callback runtime/callback.c:293 (bank_test.exe+0x106b70)
    #6 domain_thread_func runtime/domain.c:1102 (bank_test.exe+0x10a2b1)

  Previous read of size 8 at 0x7f5b0c0fd6d8 by thread T1 (mutexes: write M81):
    #0 camlStdlib__Array.iteri_367 /home/opam/.opam/5.1.0+tsan/.opam-switch/build/ocaml-variants.5.1.0+tsan/stdlib/array.ml:136 (bank_test.exe+0xa0f36)
    #1 camlDune__exe__Bank_test.print_balances_496 test/bank_test.ml:15 (bank_test.exe+0x6d8f4)
    #2 camlStdlib__Domain.body_703 /home/opam/.opam/5.1.0+tsan/.opam-switch/build/ocaml-variants.5.1.0+tsan/stdlib/domain.ml:202 (bank_test.exe+0xb06b0)
    #3 caml_start_program &lt;null&gt; (bank_test.exe+0x13fdfb)
    #4 caml_callback_exn runtime/callback.c:197 (bank_test.exe+0x106053)
    #5 caml_callback runtime/callback.c:293 (bank_test.exe+0x106b70)
    #6 domain_thread_func runtime/domain.c:1102 (bank_test.exe+0x10a2b1)

  [...]
Copy<br>Notice we obtain a back trace of the two racing accesses, with<br>
<br>A write in one Domain coming from the array assignment in Bank.transfer
<br>A read in another Domain coming from a call to Stdlib.Array.iteri to read and print the array entries in print_balances.
<br><br>One way to address the reported races is to add a Mutex, ensuring exclusive access to the underlying array. A first attempt could be to wrap transfer and iter_accounts with lock-unlock calls as follows:<br>let lock = Mutex.create () (* addition *)

let transfer t ~src_acc ~dst_acc ~amount =
  begin
    Mutex.lock lock; (* addition *)
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    if t.(src_acc) &lt; amount then raise (Invalid_argument "Not enough money on account");
    t.(src_acc) &lt;- t.(src_acc) - amount;
    t.(dst_acc) &lt;- t.(dst_acc) + amount;
    Mutex.unlock lock; (* addition *)
  end

let iter_accounts t f = (* inspect the bank accounts *)
  Mutex.lock lock; (* addition *)
  Array.iteri (fun account balance -&gt; f ~account ~balance) t;
  Mutex.unlock lock (* addition *)
Copy<br>Rerunning our tests, we obtain:<br>$ dune runtest
File "test/dune", line 2, characters 7-16:
2 |  (name bank_test)
           ^^^^^^^^^
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
Fatal error: exception Sys_error("Mutex.lock: Resource deadlock avoided")
Copy<br>How come we may hit a resource deadlock error when adding just two pairs of Mutex.lock and Mutex.unlock calls?<br><br>Oh, wait! When raising an exception in transfer, we forgot to unlock the Mutex again. Let's adapt the function to do so:<br>let transfer t ~src_acc ~dst_acc ~amount =
  begin
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    Mutex.lock lock; (* addition *)
    if t.(src_acc) &lt; amount
    then (Mutex.unlock lock; (* addition *)
          raise (Invalid_argument "Not enough money on account"));
    t.(src_acc) &lt;- t.(src_acc) - amount;
    t.(dst_acc) &lt;- t.(dst_acc) + amount;
    Mutex.unlock lock; (* addition *)
  end
Copy<br>We can now rerun our tests under TSan to confirm the fix:<br>$ dune runtest
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2 100 3 100 4 100 5  99 6 101   total = 700
0 101 1  99 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1 100 2 100 3 100 4 100 5 100 6 100   total = 700
0 100 1  99 2 100 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
0 101 1  99 2  99 3 100 4 101 5 100 6 100   total = 700
Copy<br>This works well and TSan no longer complains, so our little library is ready for OCaml 5.x parallelism, hurrah!<br><br>The programming pattern of 'always-having-to-do-something-at-the-end' that we encountered with the missing Mutex.unlock is a recurring one for which OCaml offers a dedicate function:<br> Fun.protect : finally:(unit -&gt; unit) -&gt; (unit -&gt; 'a) -&gt; 'a
Copy<br>Using Fun.protect, we could have written our final fix as follows:<br>let transfer t ~src_acc ~dst_acc ~amount =
  begin
    if amount &lt;= 0 then raise (Invalid_argument "Amount has to be positive");
    if src_acc = dst_acc then raise (Invalid_argument "Cannot transfer to yourself");
    Mutex.lock lock; (* addition *)
    Fun.protect ~finally:(fun () -&gt; Mutex.unlock lock) (* addition *)
      (fun () -&gt;
         begin
           if t.(src_acc) &lt; amount
           then raise (Invalid_argument "Not enough money on account");
           t.(src_acc) &lt;- t.(src_acc) - amount;
           t.(dst_acc) &lt;- t.(dst_acc) + amount;
         end)
  end
Copy<br>Admittedly, using a Mutex to ensure exclusive access may be a bit heavy if performance is a concern. If this is the case, one option is to replace the underlying array with a lock-free data structure, such as the <a data-tooltip-position="top" aria-label="https://ocaml-multicore.github.io/kcas/doc/kcas_data/Kcas_data/Hashtbl/index.html" rel="noopener" class="external-link" href="https://ocaml-multicore.github.io/kcas/doc/kcas_data/Kcas_data/Hashtbl/index.html" target="_blank"><code></code> from<code></code></a>HashtblKcas_data.<br>As a final word of warning, Domains are so fast that in a too simple test runner, one Domain may complete before the second has even started up yet! This is problematic, as there will be no apparent parallelism for TSan to observe and check. In the above example, the calls to Unix.sleepf help ensure that the test runner is indeed parallel. A useful alternative trick is to coordinate on an Atomic to make sure both Domains are up and running before the parallel test code proceeds. To do so, we can adapt our parallel test runner as follows:<br>let _ =
  let wait = Atomic.make 2 in
  let t = Bank.init ~num_accounts ~init_balance:100 in
  (* run the simulation and the debug view in parallel *)
  [| Domain.spawn (fun () -&gt;
         Atomic.decr wait; while Atomic.get wait &gt; 0 do () done; money_shuffle t);
     Domain.spawn (fun () -&gt;
         Atomic.decr wait; while Atomic.get wait &gt; 0 do () done; print_balances t);
  |]
  |&gt; Array.iter Domain.join
Copy<br>With that warning in mind and TSan in hand, you should now be equipped to hunt for data races.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/ocaml/transitioning-to-multicore-with-threadsanitizer.html</link><guid isPermaLink="false">Computer Science/Programming Language/OCaml/Transitioning to Multicore with ThreadSanitizer.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:30 GMT</pubDate></item><item><title><![CDATA[Rescript @genType 生成的 TypeScript 代码中的 import 的问题]]></title><description><![CDATA[ 
 <br>Rescript 11 之后，@genType 被合并进编译器，无需任何依赖就能使用，当在 Rescript 中 @genType 了使用某些 Rescript built-in 的基本类型时，可能会生成有问题的 import 相关代码，例如：<br>@genType
module LoginResponse = {
  let status = response =&gt; {
    response
    -&gt;Js.Json.decodeObject
    -&gt;Option.flatMap(response =&gt; {
      response-&gt;Js_dict.get("status")
    })
  }
}
Copy<br>status 函数具有 Js.Json.t =&gt; option&lt;Js.Json.t&gt; 类型，那么在生成的 TypeScript 文件中，会出现这样的 import:<br>import type { Json_t as Js_Json_t } from "./Js.gen.tsx"
Copy<br>而 Js.gen.tsx 这个文件是不存在的，解决方案是使用 @genType 的 shim：<br>
<br>在 rescript.json 中的 gentypeconfig 中添加 shim 配置:
<br>...
"gentypeconfig": {
...
  "shims": {
    "Js": "Js"
  },
...
}
...
Copy<br>
<br>然后新建 Js.shim.ts:
<br>export type Json_t = unknown;

export type t = unknown;

export type Exn_t = Error;
Copy<br>
<br>删除原来由 @genType 生成的 TypeScript 文件并重新生成
<br>现在生成的 TypeScript 文件将会从 Js.shim.ts import 类型:<br>import type {Json_t as Js_Json_t} from '../../src/model/Js.shim.ts';
Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rescript/rescript-@gentype-生成的-typescript-代码中的-import-的问题.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rescript/Rescript @genType 生成的 TypeScript 代码中的 import 的问题.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 20 May 2024 11:04:46 GMT</pubDate></item><item><title><![CDATA[Rust NewType 模式]]></title><description><![CDATA[ 
 <br>New Type模式是一种软件设计模式，用于在已有类型的基础上创建一个新的类型。在Rust中，这通常是通过定义一个结构体，其中只包含一个单一成员。这个结构体（New Type）对外提供了一个新的、独立的类型，用于对原始类型增加额外的语义或限制。<br><br>struct Meters(f64);
struct Feet(f64);

let length_in_meters = Meters(100.0);
let length_in_feet = Feet(328.084);

// 编译器会防止以下代码执行，因为类型不匹配
// let wrong_length = Meters(length_in_feet); // 编译错误

// 正确的构造
fn add_lengths(length1: Meters, length2: Meters) -&gt; Meters {
    Meters(length1.0 + length2.0)
}
Copy<br>这个例子使用 newtype 模式避免将原始类型f64用于不同的量度，从而增强了类型的安全性。<br><br>struct Kilometers(f64);

impl Kilometers {
    fn to_miles(&amp;self) -&gt; f64 {
        self.0 * 0.621371
    }
}

let distance = Kilometers(10.0);
println!("The distance in miles is {}", distance.to_miles());
Copy<br>这里，Kilometers有一个方法to_miles，该方法是不会影响其他f64数据的。如果我们有另一个表示温度的f64类型，就不会意外调用到与距离相关的方法。<br>New Type模式同样适用于对Box&lt;dyn SomeTrait&gt;类型的包装，这可以在需要动态分派（动态调用实现了某个接口的不同类型的对象的方法）的时候提供便利。通过创建一个New Type来包装这样的Box&lt;dyn SomeTrait&gt;类型，可以提供自定义的方法或实现更多的trait，同时也可以让API更加清晰和易于使用。<br><br>在Rust中，New Type模式不仅是类型安全的，还是一种零成本抽象。这是因为Rust编译器在编译时期会进行足够的优化，以确保New Type的使用没有运行时开销。 Rust的零成本抽象原则确保了抽象不会引入额外的运行时成本。例如，当你使用Meters这样的New Type时，Rust确保：<br>
<br>无额外内存开销：Meters只包含一个f64，在内存中的表现和单独的f64是一样的。
<br>无额外运行时开销：使用Meters时，性能和直接使用f64完全相同。编译器会移除任何关于New Type的包装和解包的代码。
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-newtype-模式.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust NewType 模式.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 May 2024 03:01:07 GMT</pubDate></item><item><title><![CDATA[Rust Partial 语义]]></title><description><![CDATA[ 
 <br>
在Rust中，PartialEq和PartialOrd trait处理了不是所有值都可以相互比较的情况。
<br><br>PartialEq trait用于定义值相等性的比较。它的设计允许类型的值之间进行相等（==）和不等（!=）的比较。与其对应的 Eq trait 确保一个类型的所有值都是可以可靠比较的，即满足等价关系的特性，如自反性、对称性和传递性。<br>fn eq(&amp;self, other: &amp;Self) -&gt; bool;
fn ne(&amp;self, other: &amp;Self) -&gt; bool;
Copy<br>在大多数情况下，类型的值都能够完全比较相等性，这时可以实现Eq。然而，对于一些特殊类型的值，如浮点数，由于存在无穷大的正负值和NaN值，导致它们的比较更加复杂。例如，根据IEEE浮点数的标准，NaN与任何值（包括它自己）比较都不相等。<br><br>PartialOrd trait用于定义值之间的大小比较。类似于PartialEq，它允许部分比较大小，返回一个Option，表示比较结果可能存在，也可能不存在（即比较无法进行时返回None）:<br>fn partial_cmp(&amp;self, other: &amp;Self) -&gt; Option&lt;Ordering&gt;;
Copy<br>在全部比较可能的场景，我们会使用Ord trait，它要求实现cmp方法，总是返回一个Ordering，表示两个值之间的确切比较关系。Ord是在所有值都能够比较时使用的，例如整数和字符串。<br><br>Rust 设计 PartialEq 和 PartialOrd trait 主要出于以下几个理由：<br>
<br>非总序理念：并不是所有类型都有一个全局的排序方法。例如，复数之间就没有一个自然的大小顺序。为了避免为这些类型人为地赋予一个排序方法，Rust 提供了一个只需部分实现序列操作的选择。
<br>IEEE 浮点数标准：由于浮点数标准定义了特殊值（NaN, 正负无穷），以及NaN不等于自身的规则，浮点数在一些情况下不能进行相等性或大小比较。
<br>提升错误处理能力和安全性：通过返回 Option&lt;Ordering&gt;，partial_cmp 方法明确指出了失败的可能性，从而迫使程序员在使用时考虑并处理这种情况，增加了代码的正确性和稳健性。
<br>表达性和灵活性：这些 trait 允许开发者为自定义类型定义适当的相等性和排序行为，从而加强了 Rust 类型系统的表达性和灵活性。
<br>PartialEq 和 PartialOrd trait 的设计允许程序员选择精准的相等性和排序语义，同时明确了对于某些类型相等性比较和大小排序并不总是可能的事实。通过引入适度的复杂性，让 Rust 的类型系统更加安全。]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-partial-语义.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust Partial 语义.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 12 May 2024 13:32:23 GMT</pubDate></item><item><title><![CDATA[Rust 虚表布局规则介绍]]></title><description><![CDATA[ 
 <br>在 Rust 中，一个指向未知大小对象（!Sized）的引用或指针被实现为一个由两个 usize 大小的域构成的胖指针。这两个域中，其中一个域保存了被引用或被指向的对象的地址，另一个域保存了一个名为 metadata 的数据。对于 slice 的引用或指针来说，其 metadata 为 slice 的长度。对于 trait object 的引用或指针来说，其 metadata 为虚表（vtable）地址。与 C++ 虚表类似，Rust 虚表的存在使得诸多动态语言特性得以实现，例如动态派发（dynamic dispatch）、向上转换（upcasting）、向下转换（downcast）等。本文将对 Rust 中虚表的布局规则进行简要介绍，并在此过程中对 Rust 中若干动态特性的实现方法进行简要介绍。<br>
注意：Rust 虚表及其结构属于 Rust 语言的内部实现细节，不保证稳定性。本文所介绍的虚表布局仅反映本文创作时最新的 Rust 虚表结构[1]，在将来 Rust 虚表结构可能会发生变化。一个 Rust 程序的正确性不应该以任何方式依赖于 Rust 虚表的结构。
<br><br>Rust 程序中的所有虚表均以一个固定结构的 header 开头。Header 中按顺序包含三个usize 大小的字段：drop_in_place ，size 和 align 。在 header 之后是一系列的 usize 大小字段，其数量以及含义在每个虚表中可能都不同。<br>+---------------+
| drop_in_place |
+---------------+
| size          |
+---------------+
| align         |
+---------------+
| entry1        |
+---------------+
| entry2        |
+---------------+
| entry3        |
+---------------+
Copy<br>虚表 header 中的drop_in_place 是一个函数指针，其指向的函数能够原地 drop 当前胖指针所引用的对象。size 和 align 两个域分别给出对象的大小和内存对齐，这两个域共同构成一个 std::alloc::Layout 结构，可用于释放当前胖指针所引用的对象所占据的内存。虚表 header 的存在使得 trait object 总是能被销毁和释放。例如当销毁一个 Box&lt;dyn Trait&gt; 时，Box::&lt;dyn Trait&gt;::drop 会首先调用虚表中的 drop_in_place 函数原地销毁 Box 所引用的对象，然后再调用 dealloc 函数并传递虚表中的 size 和 align 释放堆空间。<br>在虚表 header 之后是一系列的字段。在最普遍的情况下，每个字段代表一个指向 trait 所定义的函数的指针。例如，对于下列 object safe 的 trait:<br>pub trait Trait {
    fn fun1(&amp;self);
    fn fun2(&amp;self);
    fn fun3(&amp;self);
}
Copy<br>如果类型T 实现了 Trait，那么为 T 生成的 Trait 虚表的结构为：<br>+--------------------------+
| fn drop_in_place(*mut T) |
+--------------------------+
| size of T                |
+--------------------------+
| align of T               |
+--------------------------+
| fn &lt;T as Trait&gt;::fun1    |
+--------------------------+
| fn &lt;T as Trait&gt;::fun2    |
+--------------------------+
| fn &lt;T as Trait&gt;::fun3    |
+--------------------------+
Copy<br>Trait 中的函数按照声明顺序依次排列在虚表 header 之后。当通过一个指向 T 对象的 &amp;dyn Trait 调用 fun2 函数时，程序会先从虚表的第 5 个域中得到为 T 实现的 Trait::fun2 函数的地址，然后再调用之。<br><br>Object safe 的 trait 可以有 super trait。例如：<br>pub trait Grand {
    fn grand_fun1(&amp;self);
    fn grand_fun2(&amp;self);
}

pub trait Parent : Grand {
    fn parent_fun1(&amp;self);
    fn parent_fun2(&amp;self);
}

pub trait Trait : Parent {
    fn fun(&amp;self);
}
Copy<br>如果类型T 实现了 Trait，那么此时为 T 生成的 Trait 虚表的结构为：<br>+-------------------------------+
| fn drop_in_place(*mut T)      |
+-------------------------------+
| size of T                     |
+-------------------------------+
| align of T                    |
+-------------------------------+
| fn &lt;T as Grand&gt;::grand_fun1   |
+-------------------------------+
| fn &lt;T as Grand&gt;::grand_fun2   |
+-------------------------------+
| fn &lt;T as Parent&gt;::parent_fun1 |
+-------------------------------+
| fn &lt;T as Parent&gt;::parent_fun2 |
+-------------------------------+
| fn &lt;T as Trait&gt;::fun          |
+-------------------------------+
Copy<br>可以看到，此时Trait 以及 Trait 的所有直接或间接父 trait 所定义的所有函数均包含在虚表 header 之后，且顺序为后序（即先排布 Trait 的父 trait 所定义的所有函数，最后再排布 Trait 所定义的所有函数）。这样的排布方式使得在得到 T 类型的 Trait 虚表的同时也同时得到了 T 类型的 Parent 虚表和 Grand 虚表。T 类型的 Grand 虚表恰好由 Trait 虚表的前五个域构成，T 类型的 Parent 虚表恰好由 Trait 虚表的前七项构成。这使得向上转换变得非常简单。<br>所谓向上转换，即 Rust 允许将&amp;dyn Trait 转换为 &amp;dyn Parent 或 &amp;dyn Grand 。在向上转换的过程中，胖指针的对象地址域保持不变，但 metadata 域可能需要进行调整，因为不同的 trait 可能具有不同的虚表地址。但在当前示例中，向上转换不需要调整 metadata 域，因为一个指向 Trait 虚表的指针同时也指向 Parent 虚表和 Grand 虚表。在后文中我们会进一步介绍需要调整 metadata 域的向上转换的情况。<br>
注意：目前 stable Rust 暂不支持向上转换。要使用向上转换特性，必须使用 nightly 工具链，并向源文件中添加 #![feature(trait_upcasting)] 特性开关。
<br><br>Trait 可以有多个 super trait。例如：<br>pub trait Base {
    fn base_fun1(&amp;self);
    fn base_fun2(&amp;self);
}

pub trait Left : Base {
    fn left_fun1(&amp;self);
    fn left_fun2(&amp;self);
}

pub trait Right : Base {
    fn right_fun1(&amp;self);
    fn right_fun2(&amp;self);
}

pub trait Trait : Left + Right {
    fn fun(&amp;self);
}
Copy<br>如果类型T 实现了 Trait，那么此时为 T 生成的 Trait 虚表的结构为：<br>+-----------------------------+
| fn drop_in_place(*mut T)    |
+-----------------------------+
| size of T                   |
+-----------------------------+
| align of T                  |
+-----------------------------+
| fn &lt;T as Base&gt;::base_fun1   |
+-----------------------------+
| fn &lt;T as Base&gt;::base_fun2   |
+-----------------------------+
| fn &lt;T as Left&gt;::left_fun1   |
+-----------------------------+
| fn &lt;T as Left&gt;::left_fun2   |
+-----------------------------+
| fn &lt;T as Right&gt;::right_fun1 |
+-----------------------------+
| fn &lt;T as Right&gt;::right_fun2 |
+-----------------------------+
| ptr to &lt;T as Right&gt;::vtable |
+-----------------------------+
| fn &lt;T as Trait&gt;::fun        |
+-----------------------------+
Copy<br>可以看到，此时Trait 及其所有直接或间接父 trait 所定义的所有函数仍然包含在虚表内，因此通过 &amp;dyn Trait 调用的函数仍然可以直接从虚表内得到其实际目标函数的地址。另外，Trait 虚表内仍然包含有效的 Base 虚表和 Left 虚表。因此，将 &amp;dyn Trait 向上转换为 &amp;dyn Left 或 &amp;dyn Base 仍然是极其简单的，不需要调整胖指针的 metadata 域。但是，将 &amp;dyn Trait 向上转换为 &amp;dyn Right 就需要调整 metadata 域了，因为 Trait 虚表内并不包含一个有效的 Right 虚表。这也是 Trait 虚表中 ptr to &lt;T as Right&gt;::vtable 域的作用：在执行向上转换时，程序会读取 Trait 虚表的这个域作为得到的 &amp;dyn Right 胖指针的 metadata 。这也是 Rust 向上转换与 C++ 向上转换的一个很大不同：在 C++ 中的向上转换通常并不需要访问虚表（除非需要执行跨虚继承边界的转换），但在 Rust 中向上转换可能需要访问虚表。<br>更加一般地，对于一个 object safe 的 traitTr，将其第一个父 trait、第一个父 trait 的第一个父 trait、…… 这一系列直接或间接父 trait 记为这个 trait 的 PrefixTrait 集合。在将 &amp;dyn Tr 向上转换时，如果转换到的目标 trait 包含在 PrefixTrait 集合内，那么这个向上转换是平凡的：不需要调整胖指针的 metadata 域。否则，这个向上转换需要在 Tr 的虚表内读取目标 trait 的虚表指针作为转换结果的 metadata 。在 Tr 的虚表结构中，位于 PrefixTrait 集合中的父 trait 只需要排布他们所定义的函数即可；对于其他父 trait 还需要额外在虚表内排布一个指向其虚表的指针用于向上转换。<br><br>Rust 提供了一个特殊的 trait：std::any::Any 。该 trait 支持向下转换，即可以将 &amp;dyn Any 转换为 T 。转换过程中会对胖指针所指向的对象的实际类型进行检查，确认其确实是一个 T 类型的对象。Any trait 的虚表结构有一些特殊；在虚表 header 之后，Any 虚表仅包含一个域，这个域直接给出胖指针指向的对象的类型标识（由一个 std::any::TypeId 类型的值表示）。例如，对于任意的 T: 'static，编译器为其生成的 Any 虚表为：<br>+--------------------------+
| fn drop_in_place(*mut T) |
+--------------------------+
| size of T                |
+--------------------------+
| align of T               |
+--------------------------+
| TypeId of T              |
+--------------------------+
Copy<br>在执行向下转换时，程序首先检查转换到的类型是否与虚表中给出的TypeId 所标识的类型一致。若类型检查通过，向下转换操作可以直接返回胖指针中的指针域作为转换结果。<br><br>
<br>Vtable format to support dyn upcasting coercion <a rel="noopener" class="external-link" href="https://rust-lang.github.io/dyn-upcasting-coercion-initiative/design-discussions/vtable-layout.*html" target="_blank">https://rust-lang.github.io/dyn-upcasting-coercion-initiative/design-discussions/vtable-layout.*html</a>*
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-虚表布局规则介绍.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust 虚表布局规则介绍.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 22 Mar 2024 09:03:05 GMT</pubDate></item><item><title><![CDATA[Rust 闭包 lifetime may not live long enough 问题]]></title><description><![CDATA[ 
 <br>代码：<br>...
    fn handlers(self) -&gt; crate::server::request::Handlers {
        vec![(
            "/tree",
            routing::get(move || async {
                (
                    StatusCode::OK,
                    Json(json!(self.clone().tree(self.clone().root))),
                )
            }),
        )]
    }
...
Copy<br>编译错误：<br>error: lifetime may not live long enough
  --&gt; src/storage/filesystem/mod.rs:45:34
   |
45 |               routing::get(move || async {
   |  __________________________-------_^
   | |                          |     |
   | |                          |     return type of closure `{async block@src/storage/filesystem/mod.rs:45:34: 50:14}` contains a lifetime `'2`
   | |                          lifetime `'1` represents this closure's body
46 | |                 (
47 | |                     StatusCode::OK,
48 | |                     Json(json!(self.clone().tree(self.clone().root))),
49 | |                 )
50 | |             }),
   | |_____________^ returning this value requires that `'1` must outlive `'2`
   |
   = note: closure implements `Fn`, so references to captured variables can't escape the closure
Copy<br>这是因为 handlers 里面的闭包捕获了一个引用，并且尝试返回一个包含该引用的值导致的。<br>细说就是：闭包内部使用了 self.clone() 来获取一个新的实例，然后在异步块中返回一个 JSON 对象，这个 JSON 对象依赖于 self.tree() 的结果。因为闭包捕获了 self 的引用，所以它必须保证 self 在闭包执行完毕后仍然有效。<br>解决这个问题的思路是：确保闭包中的所有引用都在闭包执行完毕之前就不再被使用。<br>
就是说，要将闭包的作用域限制在一个更短的生命周期内，或者使用其他方式来避免闭包捕获长期存在的引用:<br>
...
    fn handlers(self) -&gt; crate::server::request::Handlers {
        let tree = json!(self.clone().tree(self.clone().root));
        vec![(
            "/tree",
            routing::get(move || async { (StatusCode::OK, Json(tree)) }),
        )]
    }
...
Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/rust-闭包-lifetime-may-not-live-long-enough-问题.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/Rust 闭包 lifetime may not live long enough 问题.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Fri, 24 May 2024 10:03:50 GMT</pubDate></item><item><title><![CDATA[有关 Axum 中 WebSocket 的使用]]></title><description><![CDATA[ 
 <br><br>创建一个项目，并将 axum 添加到依赖中：<br>cargo new axum-ws-test
cd axum-ws-test
cargo add tokio -F full
cargo add serde_json
cargo add axum -F ws
cargo add rand
Copy<br>然后用自己喜欢的编辑器/IDE 打开整个项目，找到 Cargo.toml，可以看到 Cargo.toml 如下：<br>[dependencies]
axum = { version = "0.6.20", features = ["ws"] }
rand = "0.8.5"
serde_json = "1.0.107"
tokio = { version = "1.33.0", features = ["full"] }
Copy<br>
以上依赖版本为本文编写时的最新稳定版，需要注意和自己的版本区别，axum 的功能基本都有解释，可以查看 <a data-tooltip-position="top" aria-label="https://docs.rs/axum/latest/axum/#feature-flags" rel="noopener" class="external-link" href="https://docs.rs/axum/latest/axum/#feature-flags" target="_blank"><code></code> 文档</a>axum
<br><br>从官方文档可以看到，一个 axum 程序，包含了程序入口、路由、路由服务和 axum 服务端（即 axum::Server），我们先将其基本结构写入到 main.rs 的文件中（代码来自官方文档）：<br>use axum::{routing::get, Router};

// 主函数入口
#[tokio::main]
async fn main() {
    // 路由
    let app = Router::new().route("/", get(|| async { "Hello, World!" }));

    // axum 的 Server
    axum::Server::bind(&amp;"0.0.0.0:8081".parse().unwrap())
        .serve(app.into_make_service())
        .await
        .unwrap();
}
Copy<br>运行程序后，打开 <a rel="noopener" class="external-link" href="https://muqiuhan.github.io/wiki/localhost:8081" target="_blank">localhost:8081</a>，应该可以看见网页上有 Hello, World!。<br><br>前端使用 Vue，建议选择另一个文件夹来创建前端项目。输入下面的指令来创建前端项目，项目名称命名为 axum-test-front：<br>npm create vue@latest
cd axum-test-front
npm install
npm run dev
Copy<br><br>我们模拟的情况试试，前端每次点击按钮都会获取后端的一个随机数。一开始我们先不使用 WebSocket，来测试一下效果。<br><br>添加一个函数，用于前端获取随机数：<br>use axum::{response::Json, routing::get, Router};
use rand::Rng;
use serde_json::{json, Value};

#[tokio::main]
async fn main() {
    let app = Router::new()
        .route("/", get(|| async { "Hello, World!" }))
        // add here
        .route("/random", get(get_rand));

    // Server
}
// handler
async fn get_rand() -&gt; Json&lt;Value&gt; {
    let mut rng = rand::thread_rng();
    Json(json! ({"num": rng.gen_range(1..=100)}))
}
Copy<br>可以在浏览器中输入 <a rel="noopener" class="external-link" href="https://muqiuhan.github.io/wiki/localhost:8081/random" target="_blank">localhost:8081/random</a>来测试，每个刷新应该都可以得到一个新的 num 值。<br><br>前端在 App.vue 中添加一个按钮和一个用于显示获取到的随机数的节点，代码如下：<br>&lt;script&gt;
  export default {
    data() {
      return {
        num: null,
      };
    },
    methods: {
      get_random() {
        // 从后端的对应地址获取随机数
        fetch("http://localhost:8081/random", {
          mode: "cors",
          headers: {
            accpet: "application/json",
          },
        })
          .then((response) =&gt; response.json())
          .then((data) =&gt; {
            this.num = data.num;
          });
      },
    },
  };
&lt;/script&gt;

&lt;template&gt;
  &lt;main&gt;
    &lt;button @click="get_random()"&gt;click to get num&lt;/button&gt;
    &lt;div&gt;num is {{ num }}&lt;/div&gt;
  &lt;/main&gt;
&lt;/template&gt;
Copy<br>执行的时候会发现无法从后端拿到数据，这是因为后端没有配置跨域请求。<br><br>首先需要添加一个依赖，输入下面的指令添加：<br>cargo add tower-http -F cors
Copy<br>然后在创建路由之前，先新建一个跨域的许可：<br>use axum::{http::HeaderValue, response::Json, routing::get, Router};
use rand::Rng;
use serde_json::{json, Value};
use tower_http::cors::{Any, CorsLayer};

#[tokio::main]
async fn main() {
    // 跨域配置
    let cors = CorsLayer::new()
        .allow_methods(Any)
        .allow_headers(Any)
        .allow_origin("http://localhost:5173".parse::&lt;HeaderValue&gt;().unwrap());

    let app = Router::new()
        .route("/", get(|| async { "Hello, World!" }))
        // 为路由方法处理添加跨域许可
        .route("/random", get(get_rand).layer(cors));

    // Server
}
Copy<br>这时再运行后端和前端，打开前端的网页，点击按钮应该可以每次获取到不同的数字。打开开发者控制台，并选择网络（没有的话点加号或者 》 可以找到），再多次点击按钮，可以看到每次点击按钮都发送了一次 Http 请求。<br>
如果前端开发者控制台报 Uncaught (in promise) ReferenceError: num is not defined 这种错误，应该是在给 Vue 中 data 里的字段赋值的时候没有加 this 关键字，把 num 改为 this.num 即可
<br><br><br>添加一个新的函数，函数名为 handle_random，再添加一个函数名为 handle_random_socket：<br>async fn handle_random(ws_upgrade: WebSocketUpgrade) -&gt; Response {
    ws_upgrade.on_upgrade(handle_random_socket)
}

async fn handle_random_socket(mut socket: WebSocket) {
    while let Some(msg) = socket.recv().await {
        let msg = if let Ok(msg) = msg {
            msg
        } else {
            println!("Web Socket Closed");
            return;
        };

    }
}
Copy<br>在 handle_random 中，调用 ws_upgrade 的 on_upgrade 函数可以建立 Web Socket 连接。handle_random_socket 就是用于处理连接时的函数，此处先使用循环来接收来自连接另一端的消息，如果接收发生错误或者无法接收到，则视为连接关闭。WebSocket::recv() 函数在连接关闭后，才会返回 None。<br>设想建立连接后，前端发送一个 get 字符串，后端收到这个字符串，如果收到的确实是 get，则返回给前端一个随机数。那么要做的事情就很简单了，首先要匹配发送过来的消息是否是字符串且内容是否为 get。<br>async fn handle_random_socket(mut socket: WebSocket) {
    while let Some(msg) = socket.recv().await {
        let msg = if let Ok(msg) = msg {...};
        // 匹配字符串
        if let Message::Text(text) = msg {
            if text.eq("get") {
                todo!()
            }
        }
    }
}
Copy<br>在匹配成功后，将生成一个随机数，并返回给前端。这里会用到 WebSocket 的 Send 函数来返回响应，获取随机数可以用到之前写的函数。前端对数据的宽容性较大，所以可以考虑直接返回 JSON 格式的文本：<br>async fn handle_random_socket(mut socket: WebSocket) {
    while let Some(msg) = socket.recv().await {
        let msg = if let Ok(msg) = msg {...};

        if let Message::Text(text) = msg {
            if text.eq("get") {
                if socket
                    // 返回随机数
                    .send(Message::Text(get_rand().await.to_string()))
                    .await
                    .is_err()
                {
                    // 如果出错了就关闭连接
                    println!("Web Socket Closed");
                    return;
                }
            }
        }
    }
    println!("Web Socket Closed");
}
Copy<br>编写完成，最后将 handle_random 添加到路由中。<br>#[tokio::main]
async fn main() {
    // 跨域配置
    // ...
    // 路由
    let app = Router::new()
        .route("/", get(|| async { "Hello, World!" }))
        .route("/random", get(get_rand).layer(cors));
        .route("/ws/random", get(handle_random));

    // axum 的 Server
    // ...
}
Copy<br><br>在组件挂载时，会尝试创建一个 WebSocket 的连接，并且绑定一个函数，用于在收到消息后，设置 num 的值：<br>&lt;script&gt;
export default {
  data() {
    return {
      num: null,
      ws: null,
    };
  },
  methods: {
    build_connect() {
      // 防止子域的 this 与 vue 的 this 冲突
      var that = this;
      that.ws = new WebSocket("ws://localhost:8081/ws/random");
      // 收到消息后，设置 num 的值
      that.ws.addEventListener("message", function (event) {
        that.num = JSON.parse(event.data).num;
      });
    },
  },
  mounted() {
    this.build_connect();
    window.onclose = () =&gt; {
      this.ws.close();
    };
  },
};
Copy<br>之后的每次点击都会变为通过连接来向后端发送消息。按照逻辑修改后的代码如下：<br>get_random() {
  // 通过连接向后端发送信息
  this.ws.send("get");
},
Copy<br><br>首先启动后端：<br>cargo run
Copy<br>然后启动前端：<br>npm run dev
Copy<br>打开前端页面，点击按钮就可以看到每次都能够从后端获取到不同的随机数的效果了。<br><br>
<br><a data-tooltip-position="top" aria-label="https://docs.rs/axum" rel="noopener" class="external-link" href="https://docs.rs/axum" target="_blank">axum</a>

<br><a data-tooltip-position="top" aria-label="https://docs.rs/axum/latest/axum/extract/ws/index.html" rel="noopener" class="external-link" href="https://docs.rs/axum/latest/axum/extract/ws/index.html" target="_blank">axum::extract::ws - Rust</a>


<br><a data-tooltip-position="top" aria-label="https://cn.vuejs.org/" rel="noopener" class="external-link" href="https://cn.vuejs.org/" target="_blank">vue</a>
<br><a data-tooltip-position="top" aria-label="https://developer.mozilla.org/zh-CN/docs/Web/API/WebSocket" rel="noopener" class="external-link" href="https://developer.mozilla.org/zh-CN/docs/Web/API/WebSocket" target="_blank">WebSocket - Web API 接口参考 | MDN</a>
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/rust/有关-axum-中-websocket-的使用.html</link><guid isPermaLink="false">Computer Science/Programming Language/Rust/有关 Axum 中 WebSocket 的使用.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 19 May 2024 03:24:56 GMT</pubDate></item><item><title><![CDATA[F-bounded polymorphism in Scala]]></title><description><![CDATA[ 
 <br>When we talk about polymorphism in programming, we're referring to the ability of an entity to take on several forms. Among the various approaches to polymorphism, F-bounded polymorphism (or F-bounded quantification) is a particularly advanced technique in the context of object-oriented programming languages. It is characterized by its emphasis on relations between types, thereby combining the advantages of polymorphism and genericity. This concept plays a crucial role in maintaining coherent type hierarchies and promoting consistency in software development. As we explore the depths of type theory and programming, understanding F-bounded polymorphism opens doors to crafting more robust and dependable software systems.<br>In this article, I will start by explaining how I discovered F-bounded polymorphism (somewhat by coincidence) and how it helped me in a specific case. Then, we'll take a closer look at the theory behind polymorphism with quantification (basic and bounded), then F-bounded polymorphism, which we'll finally illustrate with a practical example. Although this concept (and polymorphism in general) exists in many languages, this article uses Scala for the examples.<br><br><br>Let's start with a true story. I recently worked for a client who wanted a platform where multiple versions of information could co-exist. After developing a git-like versioning library (honourable mention to my Scala mentor who will recognize himself) that wasn't perfectly suited to the business, my team and I restarted from scratch with a new history-like approach. For reasons of confidentiality and code propriety, the code examples shown here have nothing to do with the original code, either in terms of naming or implementation (details of which are omitted to focus on F-bounded polymorphism only). Let's start by defining:<br>trait Info[T] {
  def update(t: T): Info[T]

  ...
}
Copy<br>The idea is that Info can remember changes to an object T, and that for any T that needs to be versioned, Info is extended by a concrete class containing these changes field by field. An example would be:<br>case class Foo(foo: String)

case class Bar(foo: Foo, foos: List[Foo])

// definition of class Memory[T] does not matter here
case class FooInfo(fooMemory: Memory[String]) extends Info[Foo] {
  ... // implementation does not matter
}

case class BarInfo(
    fooInfo: FooInfo, 
    foosInfo: ListInfo[Foo]
) extends Info[Bar]
Copy<br>Let's now look at ListInfo, which, as its name suggests, represents the information of a list. To define such a class, we could imagine the following:<br>case class ListInfo[T](
    infos: List[Info[T]]
) extends Info[List[T]] {
  ...
}
Copy<br>However, as the line foosInfo: ListInfo[Foo] suggests, having a single parameter type T is not sufficient here, since the Info[T] type in infos: List[Info[T]] gives no information about the concrete class used. We can therefore modify the class as follows:<br>case class ListInfo[T, InfoType &lt;: Info[T]](
    infos: List[InfoType]
) extends Info[List[T]]
Copy<br>We now know which Info type is used for the item informations. In our example, BarInfo becomes:<br>case class BarInfo(
    fooInfo: FooInfo, 
    foosInfo: ListInfo[Foo, FooInfo]
) extends Info[Bar]
Copy<br>Now imagine that, in ListInfo, we have a method for updating a particular information:<br>case class ListInfo[T, InfoType &lt;: Info[T]](
    infos: List[InfoType]
) extends Info[List[T]] {
  def update(values: List[T]): Info[List[T]] = ???

  // imagine that update() uses this function to update the infos
  def updateInfoAtIndex(index: Int, t: T): ListInfo[T, InfoType] = {
    val updatedInfo = infos(index).update(t)
    copy(infos = infos.updated(index, updatedInfo))
  }
  ...
}
Copy<br>This code doesn't compile. Can you see the problem?<br>Here's the explanation. In the function updateInfoAtIndex(), the type of updatedInfo is the type of the Info trait's update() function, which, as a reminder, is:<br>def update(t: T): Info[T]
Copy<br>However, infos is of type List[InfoType], not List[Info[T]]. The compiler therefore returns the following error:<br>[error]  type mismatch;
[error]  found   : updatedInfo.type (with underlying type Info[T])
[error]  required: InfoType
[error]     copy(infos = infos.updated(index, updatedInfo))
[error]                                       ^
Copy<br>We're faced here with a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Type_erasure" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Type_erasure" target="_blank">type erasure</a> of the type InfoType in its wider type Info[T], which the compiler cannot resolve by itself. Now let's see what solutions are available to us.<br><br><br>The most obvious solution to our problem is type casting:<br>def updateInfoAtIndex(index: Int, t: T): ListInfo[T, InfoType] = {
  val updatedInfo: Info[T] = infos(index).update(t)
  copy(infos = infos.updated(index, updatedInfo.asInstanceOf[InfoType]))
}
Copy<br>However, this solution is sorely lacking in robustness and elegance.<br><br><br>Let's put the problem back at the centre of the table. Here, we lose information on the type of an object of type InfoType &lt;: Info[T], which goes back to its more general nature Info[T]. Intuitively, then, we might wonder whether there is a way of storing this lost information in Info[T] itself, at the type level. A first approach might be to use ClassTag's, but we won't cover this solution in this article. Another approach is to rewrite Info as follows:<br>trait Info[T, InfoType &lt;: Info[T, InfoType]] {
  def update(t: T): InfoType
}
Copy<br>The most remarkable thing here is the recursive definition of Info. Morally, InfoType remains a subtype of Info. However, with recursion, it is also a subtype of the type that "defines" it. This allows us to change the return of the update() function from Info[T] to InfoType. Let's look at the repercussions of this change on the ListInfo class:<br>case class ListInfo[T, InfoType &lt;: Info[T, InfoType]](
    infos: List[InfoType]
) extends Info[List[T], ListInfo[T, InfoType]] {
  def update(values: List[T]): ListInfo[T, InfoType] = ???

  def updateAtIndex(index: Int, t: T): ListInfo[T, InfoType] = {
    val updatedInfo: InfoType = infos(index).update(t) // expected type
    copy(infos = infos.updated(index, infos(index).update(t)))
  }
  ...
}
Copy<br>This code now compiles. Regarding the subtypes of Info[T], these need to be adapted slightly. In our example, we have<br>case class FooInfo(
    fooMemory: Memory[String]
) extends Info[Foo, FooInfo]

case class BarInfo(
    fooInfo: FooInfo, 
    foosInfo: ListInfo[Foo, FooInfo]
) extends Info[Bar, BarInfo]
Copy<br>Without really realizing it, we've just used F-bounded polymorphism. Let's talk about it in more detail.<br><br><br>F-bounded polymorphism is based on relationships between types. It is also nothing other than a special form of polymorphism. This concept is closely linked to that of type in programming languages. Now, you may ask: What is a type? Why do we need them in programming languages? A particularly appealing (and funny) answer comes from the article "On understanding types, data abstraction, and polymorphism" by L. Cardelli and P. Wegner, published in 1986:<br>
A type may be viewed as a set of clothes (or a suit of armor) that protects an underlying untyped representation from arbitrary or unintended use. It provides a protective covering that hides the underlying representation and constrains the way objects may interact with other objects. In an untyped system untyped objects are naked in that the underlying representation is exposed for all to see. Violating the type system involves removing the protective set of clothing and operating directly on the naked representation.
<br>The article concludes, among other things, that types are sets of values. So there are two types of language: those that are not typed, i.e. have only one type, known as monomorphic, and those that are typed, known as polymorphic. Within polymorphic languages, polymorphism can take several forms with which you're no doubt familiar: when a function works or appears to work on several different types (potentially every type), overloading, coercion, subtyping and so on. I urge you to delve into this magnificent article.<br>We'll now take a closer look at some common forms of polymorphism.<br><br><br>In type theory, quantification refers to universally or existentially quantifying type variables:<br>
<br>
Universal Quantification (∀) indicates that a property holds for all permissible types. Type specifications for variables of a universally quantified type have the following form (for any type expression σ(t)): p: ∀t.σ(t). This expresses the property that for every type t, p has the type σ(t).
  In Scala, universal quantification is typically used via generic types, allowing functions and data structures to operate over all types T. For instance, a generic function might be represented as:

<br>
    def identity[T](x: T): T = x
Copy
  Here, T is universally quantified: the function should work for any type T. Using the notation above, the identity function is written as: ∀t. t → t.<br>


<br>
Existential Quantification (∃) denotes that there exists at least one type for which a property holds. Formally, existential quantification is written as:
  p: ∃t.σ(t). In Scala, existential types, declared using a wildcard type (placeholder syntax), signify that a type exists without specifying it:<br>


<br>  def printFirst(list: List[_]): Unit = println(list.headOption)
Copy<br>In this function, the type of the list elements is existentially quantified. The function knows there exists some type, but it doesn’t specify or use it explicitly.<br>In reality, the wildcard type (placeholder syntax) is a syntactic sugar for the formal expression of existential types in Scala, which has the form<br>T forSome { Q } where Q is a sequence of type declarations. Type List[_] can therefore be rewritten as:<br>
<br>
    type L = List[t forSome { type t }]
Copy

<br>Note that replacing L with L[_] in the left-hand member is also valid. Quick question for you: how would you write the type List[List[_]] with this syntax? Or List[Int]? Or even the type representing any type? Hint: you'll find the answer in one of the sources of this article.<br>

<br>Let's end our explanation of basic quantification with these few wonderful lines, which I found while wandering through the code of the <a data-tooltip-position="top" aria-label="https://github.com/milessabin/shapeless" rel="noopener" class="external-link" href="https://github.com/milessabin/shapeless" target="_blank">shapeless</a> library:<br>type ¬[T] = T =&gt; Nothing

type ∃[P[_]] = P[T] forSome { type T }
type ∀[P[_]] = ¬[∃[({ type λ[X] = ¬[P[X]]})#λ]]
Copy<br>Reading and reflecting on these lines convinces me that Scala is and always will be my favourite programming language.<br><br><br>Bounded quantification is a conceptual extension of the idea of universal and existential quantification. In essence, it is the notion of constraining the range over which a quantification applies.<br>Bounded quantification essentially introduces a restrictive layer atop basic quantification, enabling explicit definition of permissible type range:<br>
<br>Upper-bounded (T≤B): ∀T: T ≤B Indicates "for all types T that are subtypes of B. In Scala, you can express upper-bounded quantification using the &lt;: symbol in type parameterization:
<br>def maxElement[T &lt;: Ordered[T]](a: T, b: T): T = if (a &lt; b) b else a
Copy<br>Here, T is constrained to be a subtype of Ordered[T], ensuring the elements can be ordered. More generally, if we have types A and B, and A is a subtype of B, it means that any value of type A can also be used in a context that expects type B.<br>
<br>Lower-bounded (B≤T): ∀T: B ≤T Denotes "for all types T that are supertypes of B". Scala represents lower-bounded quantification using the &gt;: symbol:
<br>def prependToSuperTypeList[B, T &gt;: B](element: B, list: List[T]): List[T] = element :: list
Copy<br>Here, T is a supertype of B, ensuring that an element of type B can be prepended to a list of type T.<br>Of course, an existentially quantified parameter type can also be constrained by bounded quantification, for example:<br>def compareElements[A &lt;: Seq[_ &lt;: Comparable[_]]](seq1: A, seq2: A): Boolean = {
  // Comparison logic here
  true
}
Copy<br><br><br>A key concept in F-Bounded Polymorphism is the F-Bound. In bounded quantification, when a type A is F-Bounded with respect to a type B, this means that instances of A are linked by a particular semantic relation to those of B. A formalization of F-bounded polymorphism appeared in 1989 in the article "F-Bounded Polymorphism for Object-Oriented Programming" by Peter Canning et al. This article presents F-bounded polymorphism (or quantification) as a natural extension of bounded quantification.<br>In this article, we can read this definition:<br>
We say that a universally quantified type is F-bounded if it has the form
∀t ⊆ F[t].σ
where F[t] is an expression, generally containing the type variable t.
<br>Let's now break this definition into its components:<br>
<br>∀t: This part represents universal quantification over a type variable t. As stated previously, in programming languages, it means that the statement applies to all possible values of the type variable t.
<br>⊆: This symbol represents the subtype relationship or upper-bounded quantification explained before.
<br>F[t]: This is the type bound associated with the type variable t. It defines a set of types that t must belong to. Importantly, F[t] is expressed in terms of the type variable t itself. This creates a recursive relationship, where the type bound refers to the type variable it is bounding.
<br>σ: This is the actual type expression that is being quantified over and constrained by the F-bounded type system. It represents the type structure that we are trying to define and apply constraints to.
<br>In summary, ∀t ⊆ F[t].σ means that for any type t, the type expression σ is constrained to be a subtype of the type bound F[t] which is defined in terms of the type t itself. In other words, if F[t] is a type of the form F[t] = {aᵢ: σᵢ[t]}, then the condition A ⊆ F[A] says that A must have the methods aᵢ and these methods must have arguments as specified by σᵢ [A], which are defined in terms of A.<br>In Scala, probably the simplest example of F-bounded polymorphism is this one:<br>trait T[U &lt;: T[U]]
Copy<br>Please take a few seconds to admire this.<br><br><br>Let's now explain F-bounded polymorphism with the analogy of musical instruments.<br>
<br>t: A specific musical instrument (e.g., a Guitar)<br>

<br>F[t]: When applied to t, results in an instrument constrained to harmonize with instruments of its own type<br>

<br>σ: { age: InstrumentAge, produceSound: () =&gt; Sound, playInTuneWith: (F[t]) =&gt; Harmony }<br>

<br>In simpler terms, any instrument t is valid only if it can play in tune with another instrument of the same type t and is capable of producing sound.<br>Let's now express the form∀t ⊆ F[t].σ in terms of Scala code:<br>enum Sound {
  case Strumming(instrumentAge: InstrumentAge)
  case Whistling(instrumentAge: InstrumentAge)
  case Harmony(sound1: Sound, sound2: Sound)
  // note that a recursive type is inherently polymorphic in nature,
  // as well as a sum type
}

import Sound._

enum InstrumentAge {
  case New,    // Crisp and clear sound.
         Old,    // Deep and resonating sound.
       Ancient // Faint and mellow sound.
}

trait Instrument[T &lt;: Instrument[T]] {
  val age: InstrumentAge
  def produceSound(): Sound
  def playInTuneWith(instrument: T): Harmony
}

case class Guitar(age: InstrumentAge) extends Instrument[Guitar] {
  def produceSound(): Sound = Strumming(age)

  def playInTuneWith(instrument: Guitar): Harmony = Harmony(produceSound(), instrument.produceSound())
}

case class Flute(age: InstrumentAge) extends Instrument[Flute] {
  def produceSound(): Sound = Whistling(age)

  def playInTuneWith(instrument: Flute): Harmony = Harmony(produceSound(), instrument.produceSound())
}
Copy<br>Here, Instrument is F-bounded. It ensures that a Guitar can only play in tune with another Guitar and a Flute can only play in tune with another Flute. Thus the following code does not compile because the playInTuneWith method expects a Piano.<br>// compilation error
case class Piano(val age: InstrumentAge) extends Instrument[Piano] {
  def produceSound(): Sound = Whistling(age)

  def playInTuneWith(instrument: Guitar): Harmony = Harmony(produceSound(), instrument.produceSound())
}
Copy<br>Tighter constraint with self-type annotation<br>Finally, in our example, the strength of the Scala type system allows us to go even further in our constraints with self-type annotation:<br>trait Instrument[T &lt;: Instrument[T]] { self: T =&gt;
    ...
}
Copy<br>This definition not only asserts that T is a subtype of Instrument[T] but also guarantees that any concrete class or trait extending Instrument[T] is itself of type T . This creates a tighter constraint than the first trait definition. With this annotation, the following code does not compile anymore<br>class BlueBird extends Instrument[Guitar] {
    ...
}
Copy<br>because BlueBird is not of type Guitar.<br><br><br>We have seen that bounded quantification is a powerful tool that helps in expressing more refined and precise relationships between types.<br>In particular, F-bounded polymorphism offers a sophisticated way of shaping type systems and ensuring logical constraints within programming languages and programs. The concept becomes simpler when seen in light of our musical analogy. It is a powerful concept in type theory that lets us constrain a type parameter based on the type itself. This recursive constraint ensures that subclasses adhere to specific type restrictions.<br>Using F-bounded polymorphism in Scala, we achieved a type-safe way to model real-world scenarios, like musical instruments playing in tune. This ensures that mistakes like trying to tune a guitar with a flute are caught during compilation, thus eliminating potential runtime errors.<br>In essence, F-bounded polymorphism offers an expressive and robust way to encapsulate and ensure type relations. It's like the maestro of a symphony, ensuring each instrument plays in perfect harmony, creating a melody that's both beautiful and error-free.<br><br><br>
<br>P. Canning, W. Cook, W. Hill and W. Olthof. F-Bounded Polymorphism for Object-Oriented Programming. Proceedings of the fourth international conference on Functional programming languages and computer architecture. 1989. <a rel="noopener" class="external-link" href="https://www.cs.utexas.edu/~wcook/papers/FBound89/CookFBound89.pdf" target="_blank">https://www.cs.utexas.edu/~wcook/papers/FBound89/CookFBound89.pdf</a><br>

<br>L. Cardelli and P. Wegner. On understanding types, data abstraction, and polymorphism. Computing Surveys, 17(4):471–522, 1986. <a rel="noopener" class="external-link" href="http://lucacardelli.name/papers/onunderstanding.a4.pdf" target="_blank">http://lucacardelli.name/papers/onunderstanding.a4.pdf</a><br>

<br>The Scala 2.11 specification of existential types. <a rel="noopener" class="external-link" href="https://www.scala-lang.org/files/archive/spec/2.11/03-types.html#existential-types" target="_blank">https://www.scala-lang.org/files/archive/spec/2.11/03-types.html#existential-types</a><br>

<br>The Curiously Recurring Template Pattern (CRTP) in C++. See for example <a rel="noopener" class="external-link" href="https://en.cppreference.com/w/cpp/language/crtp" target="_blank">https://en.cppreference.com/w/cpp/language/crtp</a>.
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/scala/f-bounded-polymorphism-in-scala.html</link><guid isPermaLink="false">Computer Science/Programming Language/Scala/F-bounded polymorphism in Scala.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:33 GMT</pubDate></item><item><title><![CDATA[Overview]]></title><description><![CDATA[ 
 <br><br>This is the second post in a series about inference of machine learning models using scala. The first post can be found <a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/" target="_blank">here</a>. This post will detail how to use a functional streaming library (<a data-tooltip-position="top" aria-label="https://fs2.io/#/" rel="noopener" class="external-link" href="https://fs2.io/#/" target="_blank">fs2</a>) to perform machine learning model inference using the <a data-tooltip-position="top" aria-label="https://github.com/triton-inference-server/server" rel="noopener" class="external-link" href="https://github.com/triton-inference-server/server" target="_blank">Triton Inference Server</a> and <a data-tooltip-position="top" aria-label="https://grpc.io/" rel="noopener" class="external-link" href="https://grpc.io/" target="_blank">gRPC</a>. The post will be broken up into a few different parts. First we will set up our scala, python and docker dependencies. Then we will get Triton up and running using Docker. Finally we will set up fs2 to read from a text file containing image paths. We will use <a data-tooltip-position="top" aria-label="https://opencv.org/" rel="noopener" class="external-link" href="https://opencv.org/" target="_blank">opencv</a> to format our images into the representation Triton expects. Finally we will load images and send them to Triton in batches, displaying the result to the console.<br>The github repo for these tutorials can be found <a data-tooltip-position="top" aria-label="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" rel="noopener" class="external-link" href="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" target="_blank">here</a><br><br>It is expected that you have the following tools installed:<br>
<br>scala build tool <a data-tooltip-position="top" aria-label="https://www.scala-sbt.org/" rel="noopener" class="external-link" href="https://www.scala-sbt.org/" target="_blank">sbt</a>
<br>python build tool <a data-tooltip-position="top" aria-label="https://python-poetry.org/" rel="noopener" class="external-link" href="https://python-poetry.org/" target="_blank">poetry</a>
<br>Cuda toolkit and Nvidia Docker. More detailed installation tips can be found in the <a data-tooltip-position="top" aria-label="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" rel="noopener" class="external-link" href="https://github.com/MattLangsenkamp/scala-machine-learning-deployment" target="_blank">github readme</a>
<br><br>First create a new project using the scala 3 giter template/sbt and move to the newly created directory.<br>sbt new scala/scala3.g8 
#   name [Scala 3 Project Template]: scalamachinelearningdeployment 
#   Template applied in ./scalamachinelearningdeployment cd scalamachinelearningdeployment`
Copy<br>Next we will add the fs2 gRPC plugin. Add the following to project/plugins.sbt. This is what will turn our .proto files into code we can use to talk with Triton. We will talk more about .proto files and gRPC later.<br>addSbtPlugin("org.typelevel" % "sbt-fs2-grpc" % "2.7.4")`
Copy<br>We then need to create a module to store our .proto files in, and to run code generation from.<br>mkdir -p protobuf/src/main/protobuf/`
Copy<br>Create a file called downloadprotos.sh and add the following content. These are the proto files provided by the Triton Inference Server. They allow for us to communicate with Triton in any language that can generate code from .proto files.<br>for PROTO in 'grpc_service' 'health' 'model_config' 
do     
  wget -O ./protobuf/src/main/protobuf/$PROTO.proto https://raw.githubusercontent.com/triton-inference-server/common/main/protobuf/$PROTO.proto 
done
Copy<br>Then run the script to download the files.<br>chmod +x downloadprotos.sh
./downloadprotos.sh`
Copy<br>Finally we need to configure our build.sbt. There are a couple key steps to make note of:<br>
<br>Create variables to manage our dependencies
<br>Create a module for the protobuf subdirectory, explicitly stating we depend on the gRPC plugin
<br>Add our dependencies to our root module and make the root module depend to the protobuf module
<br>Copy]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/scala/inference-of-machine-learning-models-using-scala.html</link><guid isPermaLink="false">Computer Science/Programming Language/Scala/Inference of machine learning models using scala.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:30 GMT</pubDate></item><item><title><![CDATA[Refinement types in Scala 3]]></title><description><![CDATA[ 
 <br>When familiarizing myself with additions in Scala 3, it was improvements in meta-programming capabilities that caught my eye. I wondered what it would take to implement a simple refinement types library. It is definitely not my plan to end up with a full-blown refinement library as <a data-tooltip-position="top" aria-label="https://github.com/fthomas/refined" rel="noopener" class="external-link" href="https://github.com/fthomas/refined" target="_blank">refined</a>, but only to understand if the language's new version can offer some improvements in the process.<br>We will use literal types as a basic construct. The introduction of literal types was a subject of <a data-tooltip-position="top" aria-label="https://docs.scala-lang.org/sips/42.type.html" rel="noopener" class="external-link" href="https://docs.scala-lang.org/sips/42.type.html" target="_blank">SIP-23</a>, and they've been already included in Scala 2.13. They enable you to use literals of primitive types at places where types are expected:<br>val a: 4 = 4
// val b: 5 = a // fails compilation with: 
// Found:    (a : (4 : Int))
// Required: (5 : Int)
Copy<br>What Scala 3 adds to the mix are compile-time operators on literal types. You can found them in the scala.compiletime package:<br>type PlusTwo[T &lt;: Int] = scala.compiletime.ops.int.+[2, T]

val a: PlusTwo[4] = 6
// val bb: PlusTwo[4] = 7 // fails compilation with:
// Found:    (7 : Int)
// Required: (6 : Int)
Copy<br>In the definition of PlusTwo I wanted to stress that + is a type operator, hence the prefix notation. In practice infix operator might be more convenient:<br>import scala.compiletime.ops.int.*

type PlusTwo[T &lt;: Int] = 2 + T
Copy<br>One of operators available in compiletime is comparison operator &lt; which looks particularly useful in scope of refinement types:<br>type &lt;[X &lt;: Int, Y &lt;: Int] &lt;: Boolean
Copy<br>Here are some examples of its usage:<br>val a: 5 &lt; 10 = true
// val b: 15 &lt; 10 = true // fails compilation with:
// Found:    (true : Boolean)
// Required: (false : Boolean)
Copy<br>However, in context of refinement types we would like to use &lt; as a kind of type bound as opposed to just an operator returning Boolean. Therefore, while &lt; is definitely helpful, it's not sufficient by itself to express refinement types. We're striving for something akin to:<br>// val a: Int &lt; 10 = 5 // fails compilation
Copy<br><br>To be able to use comparison operators as a type bound we have to build some minimal structure:<br>trait Validated[PredRes &lt;: Boolean]
given Validated[true] = new Validated[true] {}

trait RefinedInt[Predicate[_ &lt;: Int] &lt;: Boolean]
def validate[V &lt;: Int, Predicate[_ &lt;: Int] &lt;: Boolean]
    (using Validated[Predicate[V]]): RefinedInt[Predicate] = new RefinedInt {}
Copy<br>The idea behind this is that code invoking validate will compile only if Predicate[V] evaluates to type true. Therefore, the whole business of validation will be offloaded to the second type parameter of validate.<br>Such minimal structure is enough to express something like this:<br>type LowerThan10[V &lt;: Int] = V &lt; 10
val lowerThan10: RefinedInt[LowerThan10] = validate[4, LowerThan10]
Copy<br>An equivalent written with type lambda:<br>val lowerThan10: RefinedInt[[V &lt;: Int] =&gt;&gt; V &lt; 10] = validate[4, [V &lt;: Int] =&gt;&gt; V &lt; 10]
Copy<br>I must admit the latter looks uglier, but in general, it is preferred as it allows us to avoid coming up with an unnecessary type name.<br>If you want to see how type lambdas are being used in the wild, I recommend taking a look at <a data-tooltip-position="top" aria-label="https://github.com/lampepfl/dotty/blob/master/library/src/scala/Tuple.scala#L139" rel="noopener" class="external-link" href="https://github.com/lampepfl/dotty/blob/master/library/src/scala/Tuple.scala#L139" target="_blank">type-level implementations</a> of scala.Tuple higher kinded types of Filter or Fold.<br>This encoding, while very simplistic and not the most convenient to use, is quite flexible. Thanks to operators in compiletime.ops.bool we can build more complicated predicates as the following:<br>import scala.compiletime.ops.boolean.*

validate[7, [V &lt;: Int] =&gt;&gt; V &gt; 5 &amp;&amp; V &lt; 10]
Copy<br>If we try to pass incorrect input we will get a compilation error:<br>validate[4, [V &lt;: Int] =&gt;&gt; V &gt; 5 &amp;&amp; V &lt; 10]
// no implicit argument of type iteration1.Validated[(false : Boolean)] was found for parameter x$2 of method validate in package iteration1
// L25:   validate[4, [V &lt;: Int] =&gt;&gt; V &gt; 5 &amp;&amp; V &lt; 10]
Copy<br>The compilation error is not very helpful, especially compared to the message produced by <a data-tooltip-position="top" aria-label="https://github.com/fthomas/refined" rel="noopener" class="external-link" href="https://github.com/fthomas/refined" target="_blank">refined</a>:<br>Left predicate of ((4 &gt; 5) &amp;&amp; (4 &lt; 10)) failed: Predicate failed: (4 &gt; 5).
Copy<br>It's something we will work on in iteration 2. That being said, with just a few lines of code we were able to get that basic version working.<br><br>Mechanisms we've used so far, compiletime operators and implicit resolution, are not enough to implement friendly validation errors. That's because the result of implicit resolution is binary - either the implicit had been found or not. We need richer information in case of failure.<br>To do that, we will explore another new feature of Scala 3, which is inline.<br>First, we need to define an ADT for predicates:<br>sealed trait Pred
class And[A &lt;: Pred, B &lt;: Pred]         extends Pred
class Leaf                              extends Pred
class LowerThan[T &lt;: Int &amp; Singleton]   extends Leaf
class GreaterThan[T &lt;: Int &amp; Singleton] extends Leaf
Copy<br>The only notable thing in the above is mixing in Singleton. It restricts type T into being a singleton type, so that LowerThan[Int] will not compile.<br>Then, we have to interpret this ADT at compile-time:<br>import scala.compiletime.*
import scala.compiletime.ops.int.*

trait Validated[E &lt;: Pred]

implicit inline def mkVal[V &lt;: Int &amp; Singleton, E &lt;: Pred](v: V): Validated[E] =
  inline erasedValue[E] match
    case _: LowerThan[t] =&gt;
      inline if constValue[V] &lt; constValue[t]
        then new Validated[E] {}
        else
          inline val vs    = constValue[ToString[V]]
          inline val limit = constValue[ToString[t]]
          error("Validation failed: " + vs + " &lt; " + limit)
    case _: GreaterThan[t] =&gt; // ommited here since it's symmetrical to LowerThan
    case _: And[a, b] =&gt;
      inline mkVal[V, a](v) match
        case _: Validated[_] =&gt;
          inline mkVal[V, b](v) match
            case _: Validated[_] =&gt; new Validated[E] {}
Copy<br>There are a few things worth noting here:<br>
<br>
mkVal has an inline modifier which tells the compiler that it should inline any invocation of this method at compile-time. If it's not possible, compiler will fail the compilation

<br>
erasedValue comes from compiletime package. It's usually used in tandem with inline match. It allows us to match on the expression type, but we cannot access extracted value as that code is executed at compile-time

<br>
You could have noticed a lower-case letter used for the type parameter in case _: LowerThan[t], something against the usual convention. That was not a choice though. In Scala 3 you must use a lower-case identifier for a type being extracted from a pattern match. Using case _: LowerThan[T] would mean that the match would succeed only if LowerThan is parametrized with an already known type T. I like to compare that to term-level pattern match in which there's also a distinction between case a =&gt; and case `a` =&gt;, which in regards to types becomes case _: V[a] and case _: V[A] respectively

<br>
constValue comes from compiletime too. It returns the value of a singleton type

<br>
ToString is a type-level counterpart of toString available only for singleton types of Int, so that val a: ToString[5] = "5" holds

<br>
Calling error fails the compilation with provided message. In Scala 3.0.0 it cannot be invoked with interpolated string, yet it might be <a data-tooltip-position="top" aria-label="https://github.com/lampepfl/dotty/issues/10315" rel="noopener" class="external-link" href="https://github.com/lampepfl/dotty/issues/10315" target="_blank">possible in future</a>.

<br>
mkVal is defined as an implicit conversion so it will never be called explicitly

<br>Once you got acquainted with these new Scala constructs, the code should not be hard to follow. The great news is that it's all it takes to have reasonable refinements types for Int <a data-tooltip-position="top" aria-label="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote1" rel="noopener" class="external-link" href="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote1" target="_blank">1</a>. Let's try it out:<br>val a: Validated[LowerThan[10]] = 6
val b: Validated[GreaterThan[5] And LowerThan[10]] = 6

// val y: Validated[GreaterThan[5] And LowerThan[10]] = 1
// fails with:
// Validation failed: 1 &gt; 5
Copy<br><br>Since the core functionality of refined boils down to preventing some code from being compiled, we have to specify negative test cases as code snippets that do not compile. In Scala 3 there's a built-in operation for that: scala.compiletime.testing.typeCheckErrors. We can employ it to write assertions:<br>import scala.compiletime.testing.typeCheckErrors

class IntSpec extends munit.FunSuite:
  test("Those should not compile") {
    val errs = typeCheckErrors("val x: Validated[LowerThan[10]] = 16")
    assertEquals(errs.map(_.message), List("Validation failed: 16 &lt; 10"))
  }
Copy<br>If you're interested in cross-compiling your code you would be better off using munit's compilerErrors which for Scala 3 <a data-tooltip-position="top" aria-label="https://github.com/scalameta/munit/blob/7761b08fcf34396d90b22b1d086bdfd05bb733b0/munit/shared/src/main/scala-3/munit/internal/MacroCompat.scala#L38" rel="noopener" class="external-link" href="https://github.com/scalameta/munit/blob/7761b08fcf34396d90b22b1d086bdfd05bb733b0/munit/shared/src/main/scala-3/munit/internal/MacroCompat.scala#L38" target="_blank">uses</a> said built-in.<br>Once we have an implementation for Int, it would be interesting to do the same for String.<br><br>We will use the following as a motivating example:<br>val a: String Refined StartsWith["abc"] = "abcd"
Copy<br>We had to add another type parameter in addition to the predicate. To express that that Refined[T, Predicate] type was introduced. You can find the whole code of that <a data-tooltip-position="top" aria-label="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3/src/main/scala/iteration3" rel="noopener" class="external-link" href="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3/src/main/scala/iteration3" target="_blank">iteration</a> in the accompanying repository. However, most of it is a straightforward structure not related to metaprogramming so we will jump right to the relevant bits instead.<br>What's interesting is how to actually implement StartsWith predicate at compile-time.<br>The first attempt might be to do the same thing that was done with Int:<br>transparent inline def checkPredString[V &lt;: String &amp; Singleton, E &lt;: Pred]: Boolean =
    inline erasedValue[E] match
      case _: StartsWith[t] =&gt;
        inline if constValue[V].startsWith(constValue[t])
        ...
Copy<br>If we try to invoke it, it will end up with such compilation error:<br>Cannot reduce `inline if` because its condition is not a constant value: "abcd".startsWith("abc")
Copy<br>The problem is that we're trying to call non-inline method startsWith from an inline method. Since compiler cannot reduce startsWith it just cannot be invoked there.<br>The solution to that problem is writing a simple macro:<br>transparent inline def startsWith(inline v: String, inline pred: String): Boolean =
    ${ startsWithC('v, 'pred)  }

def startsWithC(v: Expr[String], pred: Expr[String])(using Quotes): Expr[Boolean] =
    val res = v.valueOrError.startsWith(pred.valueOrError)
    Expr(res)
Copy<br>In the macro implementation (i.e. startsWithC) we are not limited to calling only inline methods; therefore, we can call String.startsWith. From my limited experience with Scala 3 macros, the tricky part is to get a value of type T from Expr[T] for non-primitive types.<br>We can freely call macro startsWith from the inline method checkPredString which completes our exercise.<br><br>Another new Scala 3 feature used in the macro definition is modifier transparent. When it's used in inline method signature <a data-tooltip-position="top" aria-label="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote2" rel="noopener" class="external-link" href="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#footnote2" target="_blank">2</a>, it allows compiler to specialize return type to a more precise type.<br>Getting back to our example, let's take a look at how checkPredString is used:<br>implicit inline def mkValString[V &lt;: String &amp; Singleton, E &lt;: Pred](v: V): Refined[V, E] =
    inline if checkPredString[V, E]
    then Refined.unsafeApply(v)
    else error("Validation failed")
Copy<br>If we remove transparent from checkPredString signature the above code would fail compilation with that message:<br>Cannot reduce `inline if` because its condition is not a constant value: (true:Boolean).&amp;&amp;(true:Boolean):Boolean
Copy<br>Without transparent compiler sees return type of checkPredString as a Boolean which is not enough to inline the code. In contrast to that, with transparent, it would be a concrete type true or false depending on the validation result.<br><br>We've ended up with a code supporting simple compile-time predicates for Int and String with very few lines of code. If you're familiar with Scala 3 metaprogramming building blocks such as inline, the code is straigforward to read. Of course, compared to real refinement types libraries there are many capabilities missing, like predicates inference or runtime lifting to refined types. This is something I explore in <a data-tooltip-position="top" aria-label="https://github.com/note/mini-refined" rel="noopener" class="external-link" href="https://github.com/note/mini-refined" target="_blank">mini-refined</a>.<br>If you're interested more in Scala 3 metaprogramming capabilities, explore links in the next section.<br><br>
<br><a data-tooltip-position="top" aria-label="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3" rel="noopener" class="external-link" href="https://github.com/note/blog-examples/tree/master/build-your-own-refinement-types-in-scala3" target="_blank">full source code</a> of examples presented in this article
<br><a data-tooltip-position="top" aria-label="http://dotty.epfl.ch/docs/reference/metaprogramming/toc.html" rel="noopener" class="external-link" href="http://dotty.epfl.ch/docs/reference/metaprogramming/toc.html" target="_blank">Scala 3 documentation</a> on metaprogramming
<br><a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=OPBuCQRgyV4" rel="noopener" class="external-link" href="https://www.youtube.com/watch?v=OPBuCQRgyV4" target="_blank">talk</a> by Josh Suereth on inline
<br><a data-tooltip-position="top" aria-label="https://github.com/note/mini-refined" rel="noopener" class="external-link" href="https://github.com/note/mini-refined" target="_blank">mini-refined</a> - project exploring further ideas proposed in this article
<br>1: One thing might bother you in the method signature itself:<br>implicit inline def mkVal[V &lt;: Int &amp; Singleton, E &lt;: Pred](v: V): Validated[E]
Copy<br>Why do we provide value being validated both on type-level and on term-level? Wouldn't such signature suffice:<br>implicit inline def mkVal[E &lt;: Pred](v: Int &amp; Singleton): Validated[E]
Copy<br>Given the new signature we just need to replace all constValue[V] in the previous implementation with v and that's it, right?<br>The answer is mostly yes. It would compile indeed but whenever you call mkVal with a value failing validation, instead of a nice error message you would get:<br>"A literal string is expected as an argument to `compiletime.error`. Got \"Validation failed: \".+(4).+(\" &lt; \").+(limit)
Copy<br>The issue, again, is lack of <a data-tooltip-position="top" aria-label="https://github.com/lampepfl/dotty/issues/10315" rel="noopener" class="external-link" href="https://github.com/lampepfl/dotty/issues/10315" target="_blank">constant folding</a> which occurs in this line:<br>error("Validation failed: " + v + " &lt; " + limit)
Copy<br>Reminder - in the previous version we used constValue[V] instead of just v. Therefore, at least for now, we have to duplicate validated value on both type- and term-level. <a data-tooltip-position="top" aria-label="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#afootnote1" rel="noopener" class="external-link" href="https://msitko.pl/blog/build-your-own-refinement-types-in-scala3.html#afootnote1" target="_blank">↩</a><br>2: transparent can be also used in <a data-tooltip-position="top" aria-label="https://dotty.epfl.ch/docs/reference/other-new-features/transparent-traits.html#transparent-traits" rel="noopener" class="external-link" href="https://dotty.epfl.ch/docs/reference/other-new-features/transparent-traits.html#transparent-traits" target="_blank">conjunction with traits</a>, in which case it has entirely]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/scala/refinement-types-in-scala-3.html</link><guid isPermaLink="false">Computer Science/Programming Language/Scala/Refinement types in Scala 3.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:33 GMT</pubDate></item><item><title><![CDATA[Overview]]></title><description><![CDATA[ 
 <br><br>This is the first in series of posts which will cover how one might setup a server to perform efficient inference of neural network models on both CPUs and GPUs using the <a data-tooltip-position="top" aria-label="https://www.scala-lang.org/" rel="noopener" class="external-link" href="https://www.scala-lang.org/" target="_blank">Scala</a> programming language. This entry will introduce key concepts at a high level as well as introduce the baseline neural network model we will use throughout the series.<br>Note that throughout this series certain concept or technologies will be mentioned but not explained in detail. This is because most of these technologies are deep and complex in their own right and there is simply not enough time to discuss them here. Instead we will provide a brief description, a link to find more information and a justification as to why that technology is important.<br>One such topic is machine learning and neural networks as a whole. The motivation of this series is not to become an expert on machine learning or neural networks. In fact the training of neural networks will not be covered in any capacity. We are simply interested with the integration of a pre-trained network into a live application. We model a given network as a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Pure_function" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Pure_function" target="_blank">pure function</a> which accepts an input tensor A as well as weights W, and returns an output B.<br><br>These posts will be geared towards those with some experience with Scala, specifically with experience with the <a data-tooltip-position="top" aria-label="https://typelevel.org/" rel="noopener" class="external-link" href="https://typelevel.org/" target="_blank">Typelevel</a> ecosystem. The idea is that this series will help provide a happy path to getting started with high performance machine learning inference, for developers using this stack. However if you do not have a ton of experience with Scala or Typelevel, you should still be able to follow along as we will link to relevant documentation, and tools like Triton, gRPC and ONNX are language agnostic, so the knowledge you gain on these topics will be transferable to other languages.<br><br>The motivation for these posts is two-fold. First is that neural networks are increasingly common part of solutions in just about every technical domain, and thus it is important to leverage them in a efficient manner. Solutions such as <a data-tooltip-position="top" aria-label="https://aws.amazon.com/pm/sagemaker/" rel="noopener" class="external-link" href="https://aws.amazon.com/pm/sagemaker/" target="_blank">AWS Sagemaker</a>, <a data-tooltip-position="top" aria-label="https://www.paperspace.com/" rel="noopener" class="external-link" href="https://www.paperspace.com/" target="_blank">Digital Ocean Paperspace</a> and <a data-tooltip-position="top" aria-label="https://azure.microsoft.com/en-us/products/machine-learning/" rel="noopener" class="external-link" href="https://azure.microsoft.com/en-us/products/machine-learning/" target="_blank">Azure Machine Learning</a> exist to fill this gap, but there are reasons you would not want to use those services, whether it be organization rules on <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Data_governance" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Data_governance" target="_blank">data governance</a>, the avoidance of vendor lock or simply that those services aren’t an appropriate solution to your specific problem. The second motivation is simply to learn. Taking a problem, approaching it from multiple angles and then continuously refining and analyzing our solutions is a great way to gain a deep understanding of a certain domain.<br><br>We will use the well studied problem of image classification as our baseline problem. Image classification has uses in everywhere from self-driving vehicles to <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2202.08546" rel="noopener" class="external-link" href="https://arxiv.org/abs/2202.08546" target="_blank">medical image analysis</a>. <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1506.02640" rel="noopener" class="external-link" href="https://arxiv.org/abs/1506.02640" target="_blank">YOLO</a> (You Only Look Once) is a family of vision models can be used to address tasks such as <a data-tooltip-position="top" aria-label="https://docs.ultralytics.com/tasks/" rel="noopener" class="external-link" href="https://docs.ultralytics.com/tasks/" target="_blank">classification, detection and segmentation</a>. At the time of writing the most recent version of YOLO is YOLOv8 and that is the model we will use throughout this series. <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.09972" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.09972" target="_blank">YOLOv8</a> was trained on the <a data-tooltip-position="top" aria-label="https://www.image-net.org/" rel="noopener" class="external-link" href="https://www.image-net.org/" target="_blank">ImageNet</a>, which is a dataset of hierarchically organized concepts into nodes in a tree. Each node has around 1000 images that relate to it. Our systems will take an image as input and return the top K predictions for the label that best describes the image, along with the probability assigned to that label. The diagram below shows an example of the inference process at a high level. A user sends a picture of a golden retriever as an HTTP request via curl, the service processes the request and returns a map from the image name to an ordered list of tuples where the first element is the assigned probability that the image belongs to the label, which is the second element.<br>Simplified Inference Process. A request is made to our live service and a mapping of the image to a sorted list of pairs is returned.<br><br><br>The total amount of requests that a system can process over a period of time. Systems are often measured using throughput as high throughput systems scale better in general. Consider a case in which you expect to be be receiving ~1000 requests/second for a sustained period of time and you want each request to take no more than one second. If your system has a measured throughput of 100 requests/second you now know that you will need to run and load balance along at least 10 instances of your hypothetical service. Increasing the throughput of your system will reduce the number of instances you need to run.<br><br>The total time in-between when a network request is sent to a system, and when a response is received. As programmers we generally want to develop low latency systems. What is determined as low enough depends on a use case. For a search engine like google or an internet database like <a data-tooltip-position="top" aria-label="https://www.imdb.com/" rel="noopener" class="external-link" href="https://www.imdb.com/" target="_blank">IMDB</a>, a few fractions of a second of latency is often low enough, as humans tend to perceive that as instantaneous. However for something like a self driving car or an application that deals with high frequency financial data, a few milliseconds of latency may be the target. We care about latency as that will be one of the metrics we use to benchmark our systems.<br><br>Central Processing Unit. The main processor on a given machine. Unless explicitly specified otherwise the instructions generated by a programming language will run on this processor.<br><br>Graphical Processing Unit. A multi-core processor capable of efficiently performing <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Embarrassingly_parallel" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Embarrassingly_parallel" target="_blank">embarrassingly parallel</a> tasks.&nbsp; We care about them as tensor operations commonly found in neural networks are often embarrassingly parallel and thus the training and evaluation of neural networks is greatly accelerated by a GPU.<br><br><a data-tooltip-position="top" aria-label="https://protobuf.dev/" rel="noopener" class="external-link" href="https://protobuf.dev/" target="_blank">Protocol Buffers are language-neutral, platform-neutral extensible mechanisms for serializing structured data</a>. We care about them as they are a building block for both gRPC and the ONNX file format, and they allow for type safe network RPC calls.<br><br><a data-tooltip-position="top" aria-label="https://grpc.io/" rel="noopener" class="external-link" href="https://grpc.io/" target="_blank">Googles Remote Procedure Call framework</a> is a generalized method for different systems to communicate with each other. Data is serialized using Protocol Buffers and is then sent across network boundaries in an extremely efficient way. Aside from being very performant gRPC is also driven by a specification language designed to have code generation tools built around it. This means that you can expose a gRPC server written in Scala and then generate a client (stub) in python, Ruby, Go or any other language to interact with the Scala service. It is important to note that gRPC is not supported by browsers and is generally used by back-end services to communicate with each other.<br><br><a data-tooltip-position="top" aria-label="https://onnx.ai/" rel="noopener" class="external-link" href="https://onnx.ai/" target="_blank">Open Neural Network Exchange</a> is an open format used to describe a neural network. This format is de-coupled from the actual runtime that executes the network. The ability to swap out back-ends is very powerful as different problems will have different hardware constraints. If you do not have GPUs available you can use the default CPU runtime. If you do have access to GPUs then you can use TensorRT or a <a data-tooltip-position="top" aria-label="https://developer.nvidia.com/cuda-toolkit" rel="noopener" class="external-link" href="https://developer.nvidia.com/cuda-toolkit" target="_blank">CUDA</a> runtime. As advances in the machine learning field advance more performant runtimes may be developed and if they choose to support ONNX then you will be able to use them with little to no refactoring.&nbsp; Most frameworks for developing and training neural networks such as PyTorch, TensorFlow or MXNet support exporting to ONNX format. This means you can have multiple different projects using different frameworks to develop your deliverables and by exporting them to ONNX you wont have to change your deployment strategy.<br><br>An SDK developed by Nvidia to optimize and accelerate inference on GPUs. Built on top of CUDA libraries. We want to use it as it can provide state of the art latency and throughput.<br><br>When performing model inference a neural network can either process one tensor at a time, or it can process a batch of tensors. Due to the parallel nature of neural computation processing a batch of tensors is often much more efficient. When deploying a live service that can take requests from multiple sources, we may not have enough data to “fill” a batch with any given request. In-flight batching is the process of taking requests that arrive at roughly the same time and dynamically batch them together.<br><br>A model deployment server developed by Nvidia. It takes a model and creates either HTTP or gRPC endpoints to serve requests. It can target different back-ends such as ONNX or TensorRT. It also has the ability to perform inflight batching.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html</link><guid isPermaLink="false">Computer Science/Programming Language/Scala/Setup a server to perform efficient inference of neural network models.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:14:29 GMT</pubDate></item><item><title><![CDATA[Type checking algorithm]]></title><description><![CDATA[ 
 <br><br><br><br><br><br><br><br><br>]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/type-theory/type-checking-algorithm.html</link><guid isPermaLink="false">Computer Science/Programming Language/Type Theory/Type checking algorithm.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Mon, 15 Jan 2024 12:10:29 GMT</pubDate></item><item><title><![CDATA[TypeScript With Rust Errors, No Try Catch, Heresy.]]></title><description><![CDATA[ 
 <br>
It’s hard to miss things when you don’t know different things exist
<br>The first problem is, and personally, I believe it’s the biggest JavaScript problem ever: we don’t know what can throw an error. From a JavaScript error perspective, it’s the same as the following:<br>try {
  let data = “Hello”;
} catch (err) {
  console.error(err);
}
Copy<br>JavaScript doesn’t know; JavaScript doesn’t care. You should know.<br>Second thing, this is perfectly viable code:<br>const request = { name: “test”, value: 2n };
const body = JSON.stringify(request);
const response = await fetch("https://example.com", {
  method: “POST”,
  body,
});
if (!response.ok) {
  return;
}
Copy<br>No errors, no linters, even though this can break your app.<br>Right now, in my head, I can hear, “What’s the problem, just use try/catch everywhere.” Here comes the third problem: we don’t know which one is thrown. Of course, we can somehow guess by the error message, but what about bigger services/functions with many places where errors can happen? Are you sure you are handling all of them properly with one try/catch?<br><br>let greeting_file_result = File::open(“hello.txt”);  
let greeting_file = match greeting_file_result {  
  Ok(file) =&gt; file,  
  Err(error) =&gt; panic!("Problem opening the file: {:?}", error),  
};
Copy<br>The most verbose of the three shown here and, ironically, the best one. So, first of all, Rust handles the errors using its amazing enums (they are not the same as TypeScript enums!). Without going into detail, what is important here is that it uses an enum called Result with two variants: Ok and Err. As you might guess, Ok holds a value and Err holds…surprise, an error :D.<br>The summary here is that Rust always know where there might be an error. And it force you to deal with it right where it appears (mostly). No hidden ones, no guessing, no breaking app with a surprise face.<br>And this approach is just better. By A MILE.<br>We cannot make TypeScript errors work like the Rust. The limiting factor here is the language itself; it doesn’t have the proper tools to do that.<br>But what we can do is try to make it similar. And make it simple:<br>export type Safe&lt;T&gt; =  
  | {  
    success: true;  
    data: T;  
  }  
  | {  
    success: false;  
    error: string;  
  };
Copy<br>we do need a few try/catches. The good thing is we only need about two, not 100,000:<br>export function safe&lt;T&gt;(promise: Promise&lt;T&gt;, err?: string): Promise&lt;Safe&lt;T&gt;&gt;;
export function safe&lt;T&gt;(func: () =&gt; T, err?: string): Safe&lt;T&gt;;
export function safe&lt;T&gt;(
  promiseOrFunc: Promise&lt;T&gt; | (() =&gt; T),
  err?: string,
): Promise&lt;Safe&lt;T&gt;&gt; | Safe&lt;T&gt; {
  if (promiseOrFunc instanceof Promise) {
    return safeAsync(promiseOrFunc, err);
  }
  return safeSync(promiseOrFunc, err);
}

async function safeAsync&lt;T&gt;(
  promise: Promise&lt;T&gt;, 
  err?: string
): Promise&lt;Safe&lt;T&gt;&gt; {
  try {
    const data = await promise;
    return { data, success: true };
  } catch (e) {
    console.error(e);
    if (err !== undefined) {
      return { success: false, error: err };
    }
    if (e instanceof Error) {
      return { success: false, error: e.message };
    }
    return { success: false, error: "Something went wrong" };
  }
}

function safeSync&lt;T&gt;(
  func: () =&gt; T, 
  err?: string
): Safe&lt;T&gt; {
  try {
    const data = func();
    return { data, success: true };
  } catch (e) {
    console.error(e);
    if (err !== undefined) {
      return { success: false, error: err };
    }
    if (e instanceof Error) {
      return { success: false, error: e.message };
    }
    return { success: false, error: "Something went wrong" };
  }
}
Copy<br>This is just a wrapper with our Safe type as the return one. But sometimes simple things are all you need. Let’s combine them with the example from above.<br>const request = { name: “test”, value: 2n };  
const body = safe(  
  () =&gt; JSON.stringify(request),  
  “Failed to serialize request”,  
);  
if (!body.success) {  
  // handle error (body.error)  
  return;  
}  
const response = await safe(  
  fetch("https://example.com", {  
    method: “POST”,  
    body: body.data,  
  }),  
);  
if (!response.success) {  
  // handle error (response.error)  
  return;  
}  
if (!response.data.ok) {  
  // handle network error  
  return;  
}  
// handle response (body.data)
Copy<br>New solution is longer, but it performs better because of the following reasons:<br>
<br>no try/catch
<br>we handle each error where it occurs
<br>we can specify an error message for a specific function
<br>we have a nice top-to-bottom logic, all errors on top, then only the response at the bottom
]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/typescript/typescript-with-rust-errors,-no-try-catch,-heresy..html</link><guid isPermaLink="false">Computer Science/Programming Language/Typescript/TypeScript With Rust Errors, No Try Catch, Heresy..md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sat, 11 May 2024 10:07:24 GMT</pubDate></item><item><title><![CDATA[WebAssembly原理与核心技术]]></title><description><![CDATA[ 
 <br>“WebAssembly原理与核心技术.epub” could not be found.]]></description><link>https://muqiuhan.github.io/wiki/computer-science/programming-language/webassmbly/webassembly原理与核心技术.html</link><guid isPermaLink="false">Computer Science/Programming Language/WebAssmbly/WebAssembly原理与核心技术.md</guid><dc:creator><![CDATA[韩暮秋]]></dc:creator><pubDate>Sun, 07 Jan 2024 11:20:53 GMT</pubDate></item></channel></rss>