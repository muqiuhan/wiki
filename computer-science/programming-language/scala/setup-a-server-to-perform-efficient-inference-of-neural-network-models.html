<!DOCTYPE html> <html><head>
		<title>Setup a server to perform efficient inference of neural network models</title>
		<base href="../../../">
		<meta id="root-path" root-path="../../../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="韩暮秋的个人维基 - Setup a server to perform efficient inference of neural network models">
		<meta property="og:title" content="Setup a server to perform efficient inference of neural network models">
		<meta property="og:description" content="韩暮秋的个人维基 - Setup a server to perform efficient inference of neural network models">
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://muqiuhan.github.io/wiki/computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html">
		<meta property="og:image" content="undefined">
		<meta property="og:site_name" content="韩暮秋的个人维基">
		<meta name="author" content="韩暮秋"><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://muqiuhan.github.io/wiki/lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="lib/scripts/pixi.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="lib/scripts/minisearch.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.ico"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager theme-light show-inline-title show-ribbon pt-color-scheme-style-minimal-lt pt-accent-style-borderandfilled-lt pt-accent-color-grey-lt pt-highlight-text-accent-lt pt-accent-style-borderandfilled-dt pt-accent-color-purple-dt pt-highlight-text-accent-dt pt-focused-style-main-only pt-font-disable-ligatures pt-highlight-style-borderandfilled-lt pt-highlight-text-color-lt pt-highlight-style-borderandfilled-dt pt-highlight-text-color-dt pt-file-explorer-folder-icon-default pt-colored-folders-style-rainbow pt-center-kanban-title-text pt-kanban-background-grid-lt pt-kanban-background-grid-dt pt-icon-folder-accent"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles"></style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="Overview"><p dir="auto">Overview</p></h1><div class="heading-wrapper"><div class="heading-children"><div><p dir="auto">This is the first in series of posts which will cover how one might setup a server to perform efficient inference of neural network models on both CPUs and GPUs using the <a data-tooltip-position="top" aria-label="https://www.scala-lang.org/" rel="noopener" class="external-link" href="https://www.scala-lang.org/" target="_blank">Scala</a> programming language. This entry will introduce key concepts at a high level as well as introduce the baseline neural network model we will use throughout the series.</p></div><div><p dir="auto">Note that throughout this series certain concept or technologies will be mentioned but not explained in detail. This is because most of these technologies are deep and complex in their own right and there is simply not enough time to discuss them here. Instead we will provide a brief description, a link to find more information and a justification as to why that technology is important.</p></div><div><p dir="auto">One such topic is machine learning and neural networks as a whole. The motivation of this series is not to become an expert on machine learning or neural networks. In fact the training of neural networks will not be covered in any capacity. We are simply interested with the integration of a pre-trained network into a live application. We model a given network as a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Pure_function" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Pure_function" target="_blank">pure function</a> which accepts an input tensor A as well as weights W, and returns an output B.</p></div></div></div><div class="heading-wrapper"><h1 data-heading="Intended Audience" dir="auto" class="heading" id="Intended_Audience">Intended Audience</h1><div class="heading-children"><div><p dir="auto">These posts will be geared towards those with some experience with Scala, specifically with experience with the <a data-tooltip-position="top" aria-label="https://typelevel.org/" rel="noopener" class="external-link" href="https://typelevel.org/" target="_blank">Typelevel</a> ecosystem. The idea is that this series will help provide a happy path to getting started with high performance machine learning inference, for developers using this stack. However if you do not have a ton of experience with Scala or Typelevel, you should still be able to follow along as we will link to relevant documentation, and tools like Triton, gRPC and ONNX are language agnostic, so the knowledge you gain on these topics will be transferable to other languages.</p></div></div></div><div class="heading-wrapper"><h1 data-heading="Motivation" dir="auto" class="heading" id="Motivation">Motivation</h1><div class="heading-children"><div><p dir="auto">The motivation for these posts is two-fold. First is that neural networks are increasingly common part of solutions in just about every technical domain, and thus it is important to leverage them in a efficient manner. Solutions such as <a data-tooltip-position="top" aria-label="https://aws.amazon.com/pm/sagemaker/" rel="noopener" class="external-link" href="https://aws.amazon.com/pm/sagemaker/" target="_blank">AWS Sagemaker</a>, <a data-tooltip-position="top" aria-label="https://www.paperspace.com/" rel="noopener" class="external-link" href="https://www.paperspace.com/" target="_blank">Digital Ocean Paperspace</a> and <a data-tooltip-position="top" aria-label="https://azure.microsoft.com/en-us/products/machine-learning/" rel="noopener" class="external-link" href="https://azure.microsoft.com/en-us/products/machine-learning/" target="_blank">Azure Machine Learning</a> exist to fill this gap, but there are reasons you would not want to use those services, whether it be organization rules on <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Data_governance" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Data_governance" target="_blank">data governance</a>, the avoidance of vendor lock or simply that those services aren’t an appropriate solution to your specific problem. The second motivation is simply to learn. Taking a problem, approaching it from multiple angles and then continuously refining and analyzing our solutions is a great way to gain a deep understanding of a certain domain.</p></div></div></div><div class="heading-wrapper"><h1 data-heading="Baseline Model" dir="auto" class="heading" id="Baseline_Model">Baseline Model</h1><div class="heading-children"><div><p dir="auto">We will use the well studied problem of image classification as our baseline problem. Image classification has uses in everywhere from self-driving vehicles to <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2202.08546" rel="noopener" class="external-link" href="https://arxiv.org/abs/2202.08546" target="_blank">medical image analysis</a>. <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1506.02640" rel="noopener" class="external-link" href="https://arxiv.org/abs/1506.02640" target="_blank">YOLO</a> (You Only Look Once) is a family of vision models can be used to address tasks such as <a data-tooltip-position="top" aria-label="https://docs.ultralytics.com/tasks/" rel="noopener" class="external-link" href="https://docs.ultralytics.com/tasks/" target="_blank">classification, detection and segmentation</a>. At the time of writing the most recent version of YOLO is YOLOv8 and that is the model we will use throughout this series. <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.09972" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.09972" target="_blank">YOLOv8</a> was trained on the <a data-tooltip-position="top" aria-label="https://www.image-net.org/" rel="noopener" class="external-link" href="https://www.image-net.org/" target="_blank">ImageNet</a>, which is a dataset of hierarchically organized concepts into nodes in a tree. Each node has around 1000 images that relate to it. Our systems will take an image as input and return the top K predictions for the label that best describes the image, along with the probability assigned to that label. The diagram below shows an example of the inference process at a high level. A user sends a picture of a golden retriever as an HTTP request via curl, the service processes the request and returns a map from the image name to an ordered list of tuples where the first element is the assigned probability that the image belongs to the label, which is the second element.</p></div><div><p dir="auto"><em>Simplified Inference Process. A request is made to our live service and a mapping of the image to a sorted list of pairs is returned.</em></p></div></div></div><div class="heading-wrapper"><h1 data-heading="Concepts" dir="auto" class="heading" id="Concepts">Concepts</h1><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="Throughput[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput)" dir="auto" class="heading" id="Throughput[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Throughput<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto">The total amount of requests that a system can process over a period of time. Systems are often measured using throughput as high throughput systems scale better in general. Consider a case in which you expect to be be receiving ~1000 requests/second for a sustained period of time and you want each request to take no more than one second. If your system has a measured throughput of 100 requests/second you now know that you will need to run and load balance along at least 10 instances of your hypothetical service. Increasing the throughput of your system will reduce the number of instances you need to run.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Latency[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency)" dir="auto" class="heading" id="Latency[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Latency<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto">The total time in-between when a network request is sent to a system, and when a response is received. As programmers we generally want to develop low latency systems. What is determined as low enough depends on a use case. For a search engine like google or an internet database like <a data-tooltip-position="top" aria-label="https://www.imdb.com/" rel="noopener" class="external-link" href="https://www.imdb.com/" target="_blank">IMDB</a>, a few fractions of a second of latency is often low enough, as humans tend to perceive that as instantaneous. However for something like a self driving car or an application that deals with high frequency financial data, a few milliseconds of latency may be the target. We care about latency as that will be one of the metrics we use to benchmark our systems.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="CPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu)" dir="auto" class="heading" id="CPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>CPU<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto">Central Processing Unit. The main processor on a given machine. Unless explicitly specified otherwise the instructions generated by a programming language will run on this processor.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="GPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu)" dir="auto" class="heading" id="GPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>GPU<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto">Graphical Processing Unit. A multi-core processor capable of efficiently performing <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Embarrassingly_parallel" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Embarrassingly_parallel" target="_blank">embarrassingly parallel</a> tasks.&nbsp; We care about them as tensor operations commonly found in neural networks are often embarrassingly parallel and thus the training and evaluation of neural networks is greatly accelerated by a GPU.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="Protocol Buffers[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers)" dir="auto" class="heading" id="Protocol_Buffers[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>Protocol Buffers<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://protobuf.dev/" rel="noopener" class="external-link" href="https://protobuf.dev/" target="_blank">Protocol Buffers are language-neutral, platform-neutral extensible mechanisms for serializing structured data</a>. We care about them as they are a building block for both gRPC and the ONNX file format, and they allow for type safe network RPC calls.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="gRPC[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc)" dir="auto" class="heading" id="gRPC[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>gRPC<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://grpc.io/" rel="noopener" class="external-link" href="https://grpc.io/" target="_blank">Googles Remote Procedure Call framework</a> is a generalized method for different systems to communicate with each other. Data is serialized using Protocol Buffers and is then sent across network boundaries in an extremely efficient way. Aside from being very performant gRPC is also driven by a specification language designed to have code generation tools built around it. This means that you can expose a gRPC server written in Scala and then generate a client (stub) in python, Ruby, Go or any other language to interact with the Scala service. It is important to note that gRPC is not supported by browsers and is generally used by back-end services to communicate with each other.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="ONNX[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx)" dir="auto" class="heading" id="ONNX[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>ONNX<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto"><a data-tooltip-position="top" aria-label="https://onnx.ai/" rel="noopener" class="external-link" href="https://onnx.ai/" target="_blank">Open Neural Network Exchange</a> is an open format used to describe a neural network. This format is de-coupled from the actual runtime that executes the network. The ability to swap out back-ends is very powerful as different problems will have different hardware constraints. If you do not have GPUs available you can use the default CPU runtime. If you do have access to GPUs then you can use TensorRT or a <a data-tooltip-position="top" aria-label="https://developer.nvidia.com/cuda-toolkit" rel="noopener" class="external-link" href="https://developer.nvidia.com/cuda-toolkit" target="_blank">CUDA</a> runtime. As advances in the machine learning field advance more performant runtimes may be developed and if they choose to support ONNX then you will be able to use them with little to no refactoring.&nbsp; Most frameworks for developing and training neural networks such as PyTorch, TensorFlow or MXNet support exporting to ONNX format. This means you can have multiple different projects using different frameworks to develop your deliverables and by exporting them to ONNX you wont have to change your deployment strategy.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="[TensorRT](https://github.com/NVIDIA/TensorRT)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt)" dir="auto" class="heading" id="[TensorRT](https://github.com/NVIDIA/TensorRT)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><a data-tooltip-position="top" aria-label="https://github.com/NVIDIA/TensorRT" rel="noopener" class="external-link" href="https://github.com/NVIDIA/TensorRT" target="_blank">TensorRT</a><a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto">An SDK developed by Nvidia to optimize and accelerate inference on GPUs. Built on top of CUDA libraries. We want to use it as it can provide state of the art latency and throughput.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="In-flight Batching[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching)" dir="auto" class="heading" id="In-flight_Batching[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>In-flight Batching<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto">When performing model inference a neural network can either process one tensor at a time, or it can process a batch of tensors. Due to the parallel nature of neural computation processing a batch of tensors is often much more efficient. When deploying a live service that can take requests from multiple sources, we may not have enough data to “fill” a batch with any given request. In-flight batching is the process of taking requests that arrive at roughly the same time and dynamically batch them together.</p></div></div></div><div class="heading-wrapper"><h3 data-heading="[Triton Inference Server](https://github.com/triton-inference-server/server)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server)" dir="auto" class="heading" id="[Triton_Inference_Server](https://github.com/triton-inference-server/server)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server)"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><a data-tooltip-position="top" aria-label="https://github.com/triton-inference-server/server" rel="noopener" class="external-link" href="https://github.com/triton-inference-server/server" target="_blank">Triton Inference Server</a><a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server" target="_blank"></a></h3><div class="heading-children"><div><p dir="auto">A model deployment server developed by Nvidia. It takes a model and creates either HTTP or gRPC endpoints to serve requests. It can target different back-ends such as ONNX or TensorRT. It also has the ability to perform inflight batching.</p></div><div class="mod-footer"></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Overview"><div class="tree-item-contents heading-link" heading-name="Overview"><span class="tree-item-title">Overview</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Intended_Audience"><div class="tree-item-contents heading-link" heading-name="Intended Audience"><span class="tree-item-title">Intended Audience</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Motivation"><div class="tree-item-contents heading-link" heading-name="Motivation"><span class="tree-item-title">Motivation</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Baseline_Model"><div class="tree-item-contents heading-link" heading-name="Baseline Model"><span class="tree-item-title">Baseline Model</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="1"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Concepts"><div class="tree-item-contents heading-link" heading-name="Concepts"><span class="tree-item-title">Concepts</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Throughput[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput)"><div class="tree-item-contents heading-link" heading-name="Throughput[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput)"><span class="tree-item-title">Throughput<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#throughput" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Latency[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency)"><div class="tree-item-contents heading-link" heading-name="Latency[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency)"><span class="tree-item-title">Latency<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#latency" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#CPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu)"><div class="tree-item-contents heading-link" heading-name="CPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu)"><span class="tree-item-title">CPU<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#cpu" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#GPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu)"><div class="tree-item-contents heading-link" heading-name="GPU[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu)"><span class="tree-item-title">GPU<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#gpu" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#Protocol_Buffers[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers)"><div class="tree-item-contents heading-link" heading-name="Protocol Buffers[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers)"><span class="tree-item-title">Protocol Buffers<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#protocol-buffers" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#gRPC[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc)"><div class="tree-item-contents heading-link" heading-name="gRPC[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc)"><span class="tree-item-title">gRPC<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#grpc" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#ONNX[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx)"><div class="tree-item-contents heading-link" heading-name="ONNX[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx)"><span class="tree-item-title">ONNX<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#onnx" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#[TensorRT](https://github.com/NVIDIA/TensorRT)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt)"><div class="tree-item-contents heading-link" heading-name="[TensorRT](https://github.com/NVIDIA/TensorRT)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt)"><span class="tree-item-title"><a data-tooltip-position="top" aria-label="https://github.com/NVIDIA/TensorRT" rel="noopener" class="external-link" href="https://github.com/NVIDIA/TensorRT" target="_blank">TensorRT</a><a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#tensorrt" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#In-flight_Batching[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching)"><div class="tree-item-contents heading-link" heading-name="In-flight Batching[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching)"><span class="tree-item-title">In-flight Batching<a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#in-flight-batching" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="computer-science/programming-language/scala/setup-a-server-to-perform-efficient-inference-of-neural-network-models.html#[Triton_Inference_Server](https://github.com/triton-inference-server/server)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server)"><div class="tree-item-contents heading-link" heading-name="[Triton Inference Server](https://github.com/triton-inference-server/server)[](https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server)"><span class="tree-item-title"><a data-tooltip-position="top" aria-label="https://github.com/triton-inference-server/server" rel="noopener" class="external-link" href="https://github.com/triton-inference-server/server" target="_blank">Triton Inference Server</a><a data-tooltip-position="top" aria-label="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server" rel="noopener" class="external-link" href="https://mattlangsenkamp.github.io/posts/scala-machine-learning-deployment-entry-0/#triton-inference-server" target="_blank"></a></span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>